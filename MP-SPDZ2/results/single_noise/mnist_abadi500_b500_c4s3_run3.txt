Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.39644
Epoch 1.2: Loss = 2.33241
Epoch 1.3: Loss = 2.33223
Epoch 1.4: Loss = 2.25697
Epoch 1.5: Loss = 2.20917
Epoch 1.6: Loss = 2.16754
Epoch 1.7: Loss = 2.12907
Epoch 1.8: Loss = 2.09888
Epoch 1.9: Loss = 2.05591
Epoch 1.10: Loss = 2.00536
Epoch 1.11: Loss = 1.97714
Epoch 1.12: Loss = 1.94099
Epoch 1.13: Loss = 1.90826
Epoch 1.14: Loss = 1.83943
Epoch 1.15: Loss = 1.79117
Epoch 1.16: Loss = 1.82382
Epoch 1.17: Loss = 1.74127
Epoch 1.18: Loss = 1.71472
Epoch 1.19: Loss = 1.66776
Epoch 1.20: Loss = 1.70015
Epoch 1.21: Loss = 1.62381
Epoch 1.22: Loss = 1.62862
Epoch 1.23: Loss = 1.60291
Epoch 1.24: Loss = 1.54611
Epoch 1.25: Loss = 1.5424
Epoch 1.26: Loss = 1.4942
Epoch 1.27: Loss = 1.41881
Epoch 1.28: Loss = 1.43271
Epoch 1.29: Loss = 1.43581
Epoch 1.30: Loss = 1.34554
Epoch 1.31: Loss = 1.3215
Epoch 1.32: Loss = 1.3743
Epoch 1.33: Loss = 1.2775
Epoch 1.34: Loss = 1.30383
Epoch 1.35: Loss = 1.2912
Epoch 1.36: Loss = 1.19383
Epoch 1.37: Loss = 1.17349
Epoch 1.38: Loss = 1.21507
Epoch 1.39: Loss = 1.18733
Epoch 1.40: Loss = 1.19142
Epoch 1.41: Loss = 1.17422
Epoch 1.42: Loss = 1.04935
Epoch 1.43: Loss = 1.06578
Epoch 1.44: Loss = 1.11716
Epoch 1.45: Loss = 1.06369
Epoch 1.46: Loss = 1.03726
Epoch 1.47: Loss = 0.993866
Epoch 1.48: Loss = 1.07404
Epoch 1.49: Loss = 0.953735
Epoch 1.50: Loss = 1.03691
Epoch 1.51: Loss = 0.972702
Epoch 1.52: Loss = 0.960175
Epoch 1.53: Loss = 0.948853
Epoch 1.54: Loss = 0.942917
Epoch 1.55: Loss = 0.905991
Epoch 1.56: Loss = 0.970474
Epoch 1.57: Loss = 0.877808
Epoch 1.58: Loss = 0.879349
Epoch 1.59: Loss = 0.914856
Epoch 1.60: Loss = 0.848328
Epoch 1.61: Loss = 0.909256
Epoch 1.62: Loss = 0.895279
Epoch 1.63: Loss = 0.865143
Epoch 1.64: Loss = 0.835175
Epoch 1.65: Loss = 0.900482
Epoch 1.66: Loss = 0.892365
Epoch 1.67: Loss = 0.791351
Epoch 1.68: Loss = 0.854385
Epoch 1.69: Loss = 0.789215
Epoch 1.70: Loss = 0.779846
Epoch 1.71: Loss = 0.790863
Epoch 1.72: Loss = 0.803696
Epoch 1.73: Loss = 0.796082
Epoch 1.74: Loss = 0.78009
Epoch 1.75: Loss = 0.815903
Epoch 1.76: Loss = 0.772812
Epoch 1.77: Loss = 0.809662
Epoch 1.78: Loss = 0.733597
Epoch 1.79: Loss = 0.737457
Epoch 1.80: Loss = 0.709595
Epoch 1.81: Loss = 0.647293
Epoch 1.82: Loss = 0.66893
Epoch 1.83: Loss = 0.663589
Epoch 1.84: Loss = 0.729935
Epoch 1.85: Loss = 0.752884
Epoch 1.86: Loss = 0.630341
Epoch 1.87: Loss = 0.745285
Epoch 1.88: Loss = 0.658661
Epoch 1.89: Loss = 0.640945
Epoch 1.90: Loss = 0.686844
Epoch 1.91: Loss = 0.640732
Epoch 1.92: Loss = 0.658218
Epoch 1.93: Loss = 0.639236
Epoch 1.94: Loss = 0.669876
Epoch 1.95: Loss = 0.600494
Epoch 1.96: Loss = 0.670135
Epoch 1.97: Loss = 0.659424
Epoch 1.98: Loss = 0.617676
Epoch 1.99: Loss = 0.615768
Epoch 1.100: Loss = 0.600449
Epoch 1.101: Loss = 0.569839
Epoch 1.102: Loss = 0.611359
Epoch 1.103: Loss = 0.61908
Epoch 1.104: Loss = 0.599136
Epoch 1.105: Loss = 0.682404
Epoch 1.106: Loss = 0.589584
Epoch 1.107: Loss = 0.612427
Epoch 1.108: Loss = 0.592178
Epoch 1.109: Loss = 0.587585
Epoch 1.110: Loss = 0.560379
Epoch 1.111: Loss = 0.61525
Epoch 1.112: Loss = 0.560837
Epoch 1.113: Loss = 0.693832
Epoch 1.114: Loss = 0.585495
Epoch 1.115: Loss = 0.639603
Epoch 1.116: Loss = 0.578506
Epoch 1.117: Loss = 0.510803
Epoch 1.118: Loss = 0.607407
Epoch 1.119: Loss = 0.564041
Epoch 1.120: Loss = 0.568344
TRAIN LOSS = 1.07404
TRAIN ACC = 71.8597 % (43118/60000)
Loss = 0.592209
Loss = 0.65361
Loss = 0.754105
Loss = 0.697403
Loss = 0.729065
Loss = 0.616882
Loss = 0.581711
Loss = 0.756271
Loss = 0.707794
Loss = 0.669113
Loss = 0.343277
Loss = 0.518204
Loss = 0.370071
Loss = 0.560898
Loss = 0.431152
Loss = 0.432281
Loss = 0.425735
Loss = 0.233719
Loss = 0.430038
Loss = 0.68338
TEST LOSS = 0.559346
TEST ACC = 431.18 % (8371/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.513565
Epoch 2.2: Loss = 0.547729
Epoch 2.3: Loss = 0.567352
Epoch 2.4: Loss = 0.591187
Epoch 2.5: Loss = 0.568359
Epoch 2.6: Loss = 0.529068
Epoch 2.7: Loss = 0.552933
Epoch 2.8: Loss = 0.521194
Epoch 2.9: Loss = 0.544052
Epoch 2.10: Loss = 0.518723
Epoch 2.11: Loss = 0.511337
Epoch 2.12: Loss = 0.502228
Epoch 2.13: Loss = 0.626633
Epoch 2.14: Loss = 0.563599
Epoch 2.15: Loss = 0.541794
Epoch 2.16: Loss = 0.584076
Epoch 2.17: Loss = 0.555099
Epoch 2.18: Loss = 0.519714
Epoch 2.19: Loss = 0.509766
Epoch 2.20: Loss = 0.531265
Epoch 2.21: Loss = 0.487762
Epoch 2.22: Loss = 0.563141
Epoch 2.23: Loss = 0.556976
Epoch 2.24: Loss = 0.498367
Epoch 2.25: Loss = 0.548141
Epoch 2.26: Loss = 0.583008
Epoch 2.27: Loss = 0.465302
Epoch 2.28: Loss = 0.60437
Epoch 2.29: Loss = 0.539459
Epoch 2.30: Loss = 0.51001
Epoch 2.31: Loss = 0.546036
Epoch 2.32: Loss = 0.497009
Epoch 2.33: Loss = 0.577606
Epoch 2.34: Loss = 0.546173
Epoch 2.35: Loss = 0.531891
Epoch 2.36: Loss = 0.552811
Epoch 2.37: Loss = 0.549255
Epoch 2.38: Loss = 0.504486
Epoch 2.39: Loss = 0.495422
Epoch 2.40: Loss = 0.562057
Epoch 2.41: Loss = 0.481171
Epoch 2.42: Loss = 0.487473
Epoch 2.43: Loss = 0.51445
Epoch 2.44: Loss = 0.453018
Epoch 2.45: Loss = 0.526123
Epoch 2.46: Loss = 0.52179
Epoch 2.47: Loss = 0.467255
Epoch 2.48: Loss = 0.490891
Epoch 2.49: Loss = 0.524246
Epoch 2.50: Loss = 0.458023
Epoch 2.51: Loss = 0.549774
Epoch 2.52: Loss = 0.535294
Epoch 2.53: Loss = 0.501709
Epoch 2.54: Loss = 0.449677
Epoch 2.55: Loss = 0.521835
Epoch 2.56: Loss = 0.41748
Epoch 2.57: Loss = 0.463531
Epoch 2.58: Loss = 0.530823
Epoch 2.59: Loss = 0.50029
Epoch 2.60: Loss = 0.52684
Epoch 2.61: Loss = 0.438446
Epoch 2.62: Loss = 0.469055
Epoch 2.63: Loss = 0.477539
Epoch 2.64: Loss = 0.529083
Epoch 2.65: Loss = 0.464218
Epoch 2.66: Loss = 0.47258
Epoch 2.67: Loss = 0.49292
Epoch 2.68: Loss = 0.470718
Epoch 2.69: Loss = 0.447525
Epoch 2.70: Loss = 0.488434
Epoch 2.71: Loss = 0.489746
Epoch 2.72: Loss = 0.426453
Epoch 2.73: Loss = 0.460983
Epoch 2.74: Loss = 0.455643
Epoch 2.75: Loss = 0.428787
Epoch 2.76: Loss = 0.476593
Epoch 2.77: Loss = 0.461212
Epoch 2.78: Loss = 0.436783
Epoch 2.79: Loss = 0.422531
Epoch 2.80: Loss = 0.426422
Epoch 2.81: Loss = 0.431717
Epoch 2.82: Loss = 0.469299
Epoch 2.83: Loss = 0.469025
Epoch 2.84: Loss = 0.463348
Epoch 2.85: Loss = 0.441559
Epoch 2.86: Loss = 0.439346
Epoch 2.87: Loss = 0.489685
Epoch 2.88: Loss = 0.497787
Epoch 2.89: Loss = 0.45105
Epoch 2.90: Loss = 0.436447
Epoch 2.91: Loss = 0.433762
Epoch 2.92: Loss = 0.529678
Epoch 2.93: Loss = 0.532791
Epoch 2.94: Loss = 0.4608
Epoch 2.95: Loss = 0.519516
Epoch 2.96: Loss = 0.429565
Epoch 2.97: Loss = 0.50296
Epoch 2.98: Loss = 0.447113
Epoch 2.99: Loss = 0.578857
Epoch 2.100: Loss = 0.435837
Epoch 2.101: Loss = 0.474075
Epoch 2.102: Loss = 0.480698
Epoch 2.103: Loss = 0.396866
Epoch 2.104: Loss = 0.463013
Epoch 2.105: Loss = 0.422623
Epoch 2.106: Loss = 0.495712
Epoch 2.107: Loss = 0.476212
Epoch 2.108: Loss = 0.436447
Epoch 2.109: Loss = 0.504257
Epoch 2.110: Loss = 0.434921
Epoch 2.111: Loss = 0.400818
Epoch 2.112: Loss = 0.492462
Epoch 2.113: Loss = 0.357025
Epoch 2.114: Loss = 0.43428
Epoch 2.115: Loss = 0.472336
Epoch 2.116: Loss = 0.46257
Epoch 2.117: Loss = 0.473236
Epoch 2.118: Loss = 0.395157
Epoch 2.119: Loss = 0.428024
Epoch 2.120: Loss = 0.372238
TRAIN LOSS = 0.492584
TRAIN ACC = 85.3104 % (51189/60000)
Loss = 0.419525
Loss = 0.510254
Loss = 0.588669
Loss = 0.555328
Loss = 0.592072
Loss = 0.454483
Loss = 0.414383
Loss = 0.614822
Loss = 0.543381
Loss = 0.529556
Loss = 0.23085
Loss = 0.373978
Loss = 0.306931
Loss = 0.419327
Loss = 0.271667
Loss = 0.3246
Loss = 0.288437
Loss = 0.114929
Loss = 0.277557
Loss = 0.569946
TEST LOSS = 0.420035
TEST ACC = 511.89 % (8748/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.476273
Epoch 3.2: Loss = 0.492599
Epoch 3.3: Loss = 0.411774
Epoch 3.4: Loss = 0.43158
Epoch 3.5: Loss = 0.493103
Epoch 3.6: Loss = 0.458939
Epoch 3.7: Loss = 0.456589
Epoch 3.8: Loss = 0.353622
Epoch 3.9: Loss = 0.511078
Epoch 3.10: Loss = 0.358765
Epoch 3.11: Loss = 0.431656
Epoch 3.12: Loss = 0.336868
Epoch 3.13: Loss = 0.441071
Epoch 3.14: Loss = 0.376877
Epoch 3.15: Loss = 0.426208
Epoch 3.16: Loss = 0.438766
Epoch 3.17: Loss = 0.461426
Epoch 3.18: Loss = 0.476639
Epoch 3.19: Loss = 0.46402
Epoch 3.20: Loss = 0.432114
Epoch 3.21: Loss = 0.440338
Epoch 3.22: Loss = 0.446854
Epoch 3.23: Loss = 0.414185
Epoch 3.24: Loss = 0.451141
Epoch 3.25: Loss = 0.357925
Epoch 3.26: Loss = 0.407303
Epoch 3.27: Loss = 0.425995
Epoch 3.28: Loss = 0.384171
Epoch 3.29: Loss = 0.471191
Epoch 3.30: Loss = 0.532196
Epoch 3.31: Loss = 0.455963
Epoch 3.32: Loss = 0.457947
Epoch 3.33: Loss = 0.359024
Epoch 3.34: Loss = 0.472855
Epoch 3.35: Loss = 0.405121
Epoch 3.36: Loss = 0.475571
Epoch 3.37: Loss = 0.516144
Epoch 3.38: Loss = 0.39859
Epoch 3.39: Loss = 0.4599
Epoch 3.40: Loss = 0.453934
Epoch 3.41: Loss = 0.388992
Epoch 3.42: Loss = 0.411102
Epoch 3.43: Loss = 0.392105
Epoch 3.44: Loss = 0.398254
Epoch 3.45: Loss = 0.393707
Epoch 3.46: Loss = 0.443695
Epoch 3.47: Loss = 0.427567
Epoch 3.48: Loss = 0.360748
Epoch 3.49: Loss = 0.392151
Epoch 3.50: Loss = 0.40036
Epoch 3.51: Loss = 0.409286
Epoch 3.52: Loss = 0.400299
Epoch 3.53: Loss = 0.431503
Epoch 3.54: Loss = 0.431839
Epoch 3.55: Loss = 0.404175
Epoch 3.56: Loss = 0.447418
Epoch 3.57: Loss = 0.422485
Epoch 3.58: Loss = 0.524353
Epoch 3.59: Loss = 0.355988
Epoch 3.60: Loss = 0.442642
Epoch 3.61: Loss = 0.434601
Epoch 3.62: Loss = 0.398193
Epoch 3.63: Loss = 0.428818
Epoch 3.64: Loss = 0.419296
Epoch 3.65: Loss = 0.364197
Epoch 3.66: Loss = 0.386002
Epoch 3.67: Loss = 0.409561
Epoch 3.68: Loss = 0.357574
Epoch 3.69: Loss = 0.36232
Epoch 3.70: Loss = 0.321976
Epoch 3.71: Loss = 0.337189
Epoch 3.72: Loss = 0.391174
Epoch 3.73: Loss = 0.378403
Epoch 3.74: Loss = 0.341599
Epoch 3.75: Loss = 0.448395
Epoch 3.76: Loss = 0.39386
Epoch 3.77: Loss = 0.399307
Epoch 3.78: Loss = 0.421829
Epoch 3.79: Loss = 0.444611
Epoch 3.80: Loss = 0.430038
Epoch 3.81: Loss = 0.401978
Epoch 3.82: Loss = 0.452377
Epoch 3.83: Loss = 0.41893
Epoch 3.84: Loss = 0.44545
Epoch 3.85: Loss = 0.380402
Epoch 3.86: Loss = 0.35611
Epoch 3.87: Loss = 0.415497
Epoch 3.88: Loss = 0.43425
Epoch 3.89: Loss = 0.372833
Epoch 3.90: Loss = 0.417572
Epoch 3.91: Loss = 0.429764
Epoch 3.92: Loss = 0.382965
Epoch 3.93: Loss = 0.366943
Epoch 3.94: Loss = 0.408905
Epoch 3.95: Loss = 0.440109
Epoch 3.96: Loss = 0.417191
Epoch 3.97: Loss = 0.39003
Epoch 3.98: Loss = 0.455276
Epoch 3.99: Loss = 0.317413
Epoch 3.100: Loss = 0.43692
Epoch 3.101: Loss = 0.445175
Epoch 3.102: Loss = 0.433487
Epoch 3.103: Loss = 0.421646
Epoch 3.104: Loss = 0.410522
Epoch 3.105: Loss = 0.385834
Epoch 3.106: Loss = 0.483322
Epoch 3.107: Loss = 0.399261
Epoch 3.108: Loss = 0.476685
Epoch 3.109: Loss = 0.361588
Epoch 3.110: Loss = 0.38295
Epoch 3.111: Loss = 0.449356
Epoch 3.112: Loss = 0.424088
Epoch 3.113: Loss = 0.426117
Epoch 3.114: Loss = 0.432861
Epoch 3.115: Loss = 0.380463
Epoch 3.116: Loss = 0.406998
Epoch 3.117: Loss = 0.388748
Epoch 3.118: Loss = 0.376358
Epoch 3.119: Loss = 0.428741
Epoch 3.120: Loss = 0.468109
TRAIN LOSS = 0.418228
TRAIN ACC = 87.3688 % (52424/60000)
Loss = 0.368713
Loss = 0.450455
Loss = 0.525833
Loss = 0.524826
Loss = 0.538956
Loss = 0.401962
Loss = 0.354279
Loss = 0.571793
Loss = 0.499039
Loss = 0.482483
Loss = 0.206375
Loss = 0.330948
Loss = 0.288574
Loss = 0.373932
Loss = 0.22348
Loss = 0.307175
Loss = 0.243835
Loss = 0.0808563
Loss = 0.243118
Loss = 0.541397
TEST LOSS = 0.377901
TEST ACC = 524.239 % (8869/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.413879
Epoch 4.2: Loss = 0.479797
Epoch 4.3: Loss = 0.383423
Epoch 4.4: Loss = 0.444656
Epoch 4.5: Loss = 0.390961
Epoch 4.6: Loss = 0.371933
Epoch 4.7: Loss = 0.353668
Epoch 4.8: Loss = 0.52005
Epoch 4.9: Loss = 0.340637
Epoch 4.10: Loss = 0.413788
Epoch 4.11: Loss = 0.402985
Epoch 4.12: Loss = 0.338882
Epoch 4.13: Loss = 0.338211
Epoch 4.14: Loss = 0.335312
Epoch 4.15: Loss = 0.381714
Epoch 4.16: Loss = 0.395309
Epoch 4.17: Loss = 0.482101
Epoch 4.18: Loss = 0.378403
Epoch 4.19: Loss = 0.415192
Epoch 4.20: Loss = 0.377396
Epoch 4.21: Loss = 0.423874
Epoch 4.22: Loss = 0.415619
Epoch 4.23: Loss = 0.345413
Epoch 4.24: Loss = 0.340561
Epoch 4.25: Loss = 0.382446
Epoch 4.26: Loss = 0.413788
Epoch 4.27: Loss = 0.396286
Epoch 4.28: Loss = 0.426773
Epoch 4.29: Loss = 0.424728
Epoch 4.30: Loss = 0.346664
Epoch 4.31: Loss = 0.359909
Epoch 4.32: Loss = 0.433762
Epoch 4.33: Loss = 0.384262
Epoch 4.34: Loss = 0.419083
Epoch 4.35: Loss = 0.382431
Epoch 4.36: Loss = 0.312332
Epoch 4.37: Loss = 0.443665
Epoch 4.38: Loss = 0.341217
Epoch 4.39: Loss = 0.412598
Epoch 4.40: Loss = 0.352264
Epoch 4.41: Loss = 0.343704
Epoch 4.42: Loss = 0.354309
Epoch 4.43: Loss = 0.444336
Epoch 4.44: Loss = 0.316849
Epoch 4.45: Loss = 0.384109
Epoch 4.46: Loss = 0.393555
Epoch 4.47: Loss = 0.37413
Epoch 4.48: Loss = 0.375427
Epoch 4.49: Loss = 0.436081
Epoch 4.50: Loss = 0.390717
Epoch 4.51: Loss = 0.31601
Epoch 4.52: Loss = 0.410065
Epoch 4.53: Loss = 0.423294
Epoch 4.54: Loss = 0.396942
Epoch 4.55: Loss = 0.410339
Epoch 4.56: Loss = 0.428268
Epoch 4.57: Loss = 0.423386
Epoch 4.58: Loss = 0.440353
Epoch 4.59: Loss = 0.380661
Epoch 4.60: Loss = 0.459442
Epoch 4.61: Loss = 0.294922
Epoch 4.62: Loss = 0.395813
Epoch 4.63: Loss = 0.40538
Epoch 4.64: Loss = 0.431625
Epoch 4.65: Loss = 0.454041
Epoch 4.66: Loss = 0.360352
Epoch 4.67: Loss = 0.415863
Epoch 4.68: Loss = 0.381271
Epoch 4.69: Loss = 0.322784
Epoch 4.70: Loss = 0.429291
Epoch 4.71: Loss = 0.400314
Epoch 4.72: Loss = 0.376602
Epoch 4.73: Loss = 0.428894
Epoch 4.74: Loss = 0.342819
Epoch 4.75: Loss = 0.390152
Epoch 4.76: Loss = 0.337418
Epoch 4.77: Loss = 0.362534
Epoch 4.78: Loss = 0.401337
Epoch 4.79: Loss = 0.411301
Epoch 4.80: Loss = 0.340729
Epoch 4.81: Loss = 0.43692
Epoch 4.82: Loss = 0.341324
Epoch 4.83: Loss = 0.449936
Epoch 4.84: Loss = 0.373901
Epoch 4.85: Loss = 0.325897
Epoch 4.86: Loss = 0.373764
Epoch 4.87: Loss = 0.463074
Epoch 4.88: Loss = 0.391357
Epoch 4.89: Loss = 0.404648
Epoch 4.90: Loss = 0.379959
Epoch 4.91: Loss = 0.47612
Epoch 4.92: Loss = 0.384323
Epoch 4.93: Loss = 0.343033
Epoch 4.94: Loss = 0.449158
Epoch 4.95: Loss = 0.431183
Epoch 4.96: Loss = 0.367935
Epoch 4.97: Loss = 0.341476
Epoch 4.98: Loss = 0.296082
Epoch 4.99: Loss = 0.419785
Epoch 4.100: Loss = 0.389404
Epoch 4.101: Loss = 0.356064
Epoch 4.102: Loss = 0.324844
Epoch 4.103: Loss = 0.379654
Epoch 4.104: Loss = 0.327881
Epoch 4.105: Loss = 0.311127
Epoch 4.106: Loss = 0.354614
Epoch 4.107: Loss = 0.418625
Epoch 4.108: Loss = 0.407227
Epoch 4.109: Loss = 0.35498
Epoch 4.110: Loss = 0.410492
Epoch 4.111: Loss = 0.419968
Epoch 4.112: Loss = 0.391876
Epoch 4.113: Loss = 0.408585
Epoch 4.114: Loss = 0.459579
Epoch 4.115: Loss = 0.42067
Epoch 4.116: Loss = 0.337845
Epoch 4.117: Loss = 0.385162
Epoch 4.118: Loss = 0.398605
Epoch 4.119: Loss = 0.331085
Epoch 4.120: Loss = 0.321442
TRAIN LOSS = 0.38884
TRAIN ACC = 88.295 % (52980/60000)
Loss = 0.342316
Loss = 0.439682
Loss = 0.498306
Loss = 0.517212
Loss = 0.512451
Loss = 0.37207
Loss = 0.339005
Loss = 0.549835
Loss = 0.491959
Loss = 0.475662
Loss = 0.195099
Loss = 0.298004
Loss = 0.26178
Loss = 0.347031
Loss = 0.202942
Loss = 0.279434
Loss = 0.222565
Loss = 0.0612488
Loss = 0.226929
Loss = 0.516357
TEST LOSS = 0.357494
TEST ACC = 529.799 % (8955/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.385483
Epoch 5.2: Loss = 0.407913
Epoch 5.3: Loss = 0.403137
Epoch 5.4: Loss = 0.410858
Epoch 5.5: Loss = 0.387939
Epoch 5.6: Loss = 0.420792
Epoch 5.7: Loss = 0.361298
Epoch 5.8: Loss = 0.305466
Epoch 5.9: Loss = 0.374725
Epoch 5.10: Loss = 0.379974
Epoch 5.11: Loss = 0.316238
Epoch 5.12: Loss = 0.380844
Epoch 5.13: Loss = 0.351212
Epoch 5.14: Loss = 0.380875
Epoch 5.15: Loss = 0.379944
Epoch 5.16: Loss = 0.365051
Epoch 5.17: Loss = 0.390457
Epoch 5.18: Loss = 0.453033
Epoch 5.19: Loss = 0.39415
Epoch 5.20: Loss = 0.371841
Epoch 5.21: Loss = 0.412033
Epoch 5.22: Loss = 0.348846
Epoch 5.23: Loss = 0.296799
Epoch 5.24: Loss = 0.332169
Epoch 5.25: Loss = 0.341934
Epoch 5.26: Loss = 0.432404
Epoch 5.27: Loss = 0.456665
Epoch 5.28: Loss = 0.428955
Epoch 5.29: Loss = 0.440659
Epoch 5.30: Loss = 0.391327
Epoch 5.31: Loss = 0.352509
Epoch 5.32: Loss = 0.408157
Epoch 5.33: Loss = 0.413925
Epoch 5.34: Loss = 0.321915
Epoch 5.35: Loss = 0.376755
Epoch 5.36: Loss = 0.393417
Epoch 5.37: Loss = 0.331696
Epoch 5.38: Loss = 0.328674
Epoch 5.39: Loss = 0.358459
Epoch 5.40: Loss = 0.377426
Epoch 5.41: Loss = 0.328766
Epoch 5.42: Loss = 0.330795
Epoch 5.43: Loss = 0.421356
Epoch 5.44: Loss = 0.355591
Epoch 5.45: Loss = 0.482559
Epoch 5.46: Loss = 0.364395
Epoch 5.47: Loss = 0.342834
Epoch 5.48: Loss = 0.401123
Epoch 5.49: Loss = 0.342255
Epoch 5.50: Loss = 0.465836
Epoch 5.51: Loss = 0.361786
Epoch 5.52: Loss = 0.422943
Epoch 5.53: Loss = 0.421906
Epoch 5.54: Loss = 0.324326
Epoch 5.55: Loss = 0.368591
Epoch 5.56: Loss = 0.333939
Epoch 5.57: Loss = 0.316956
Epoch 5.58: Loss = 0.338531
Epoch 5.59: Loss = 0.39357
Epoch 5.60: Loss = 0.301254
Epoch 5.61: Loss = 0.373093
Epoch 5.62: Loss = 0.382828
Epoch 5.63: Loss = 0.322571
Epoch 5.64: Loss = 0.377518
Epoch 5.65: Loss = 0.360107
Epoch 5.66: Loss = 0.364807
Epoch 5.67: Loss = 0.377731
Epoch 5.68: Loss = 0.324112
Epoch 5.69: Loss = 0.353043
Epoch 5.70: Loss = 0.387436
Epoch 5.71: Loss = 0.384613
Epoch 5.72: Loss = 0.424225
Epoch 5.73: Loss = 0.435806
Epoch 5.74: Loss = 0.398056
Epoch 5.75: Loss = 0.328735
Epoch 5.76: Loss = 0.361893
Epoch 5.77: Loss = 0.31958
Epoch 5.78: Loss = 0.431137
Epoch 5.79: Loss = 0.460037
Epoch 5.80: Loss = 0.339615
Epoch 5.81: Loss = 0.431656
Epoch 5.82: Loss = 0.353455
Epoch 5.83: Loss = 0.437897
Epoch 5.84: Loss = 0.377441
Epoch 5.85: Loss = 0.283249
Epoch 5.86: Loss = 0.405243
Epoch 5.87: Loss = 0.424622
Epoch 5.88: Loss = 0.352524
Epoch 5.89: Loss = 0.397903
Epoch 5.90: Loss = 0.386124
Epoch 5.91: Loss = 0.336685
Epoch 5.92: Loss = 0.316742
Epoch 5.93: Loss = 0.319855
Epoch 5.94: Loss = 0.327637
Epoch 5.95: Loss = 0.300034
Epoch 5.96: Loss = 0.447845
Epoch 5.97: Loss = 0.406891
Epoch 5.98: Loss = 0.390915
Epoch 5.99: Loss = 0.410492
Epoch 5.100: Loss = 0.395615
Epoch 5.101: Loss = 0.323227
Epoch 5.102: Loss = 0.327896
Epoch 5.103: Loss = 0.347672
Epoch 5.104: Loss = 0.348557
Epoch 5.105: Loss = 0.403534
Epoch 5.106: Loss = 0.372116
Epoch 5.107: Loss = 0.289322
Epoch 5.108: Loss = 0.382553
Epoch 5.109: Loss = 0.355515
Epoch 5.110: Loss = 0.41185
Epoch 5.111: Loss = 0.400757
Epoch 5.112: Loss = 0.359375
Epoch 5.113: Loss = 0.348801
Epoch 5.114: Loss = 0.388733
Epoch 5.115: Loss = 0.321548
Epoch 5.116: Loss = 0.38266
Epoch 5.117: Loss = 0.369965
Epoch 5.118: Loss = 0.341232
Epoch 5.119: Loss = 0.428085
Epoch 5.120: Loss = 0.351379
TRAIN LOSS = 0.373749
TRAIN ACC = 88.9313 % (53361/60000)
Loss = 0.325836
Loss = 0.434021
Loss = 0.499435
Loss = 0.529861
Loss = 0.512711
Loss = 0.359787
Loss = 0.331635
Loss = 0.559372
Loss = 0.483505
Loss = 0.480911
Loss = 0.169678
Loss = 0.280777
Loss = 0.255234
Loss = 0.319519
Loss = 0.189255
Loss = 0.247726
Loss = 0.212967
Loss = 0.0538788
Loss = 0.220963
Loss = 0.504135
TEST LOSS = 0.34856
TEST ACC = 533.609 % (8985/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.35553
Epoch 6.2: Loss = 0.326462
Epoch 6.3: Loss = 0.403793
Epoch 6.4: Loss = 0.351837
Epoch 6.5: Loss = 0.375885
Epoch 6.6: Loss = 0.429504
Epoch 6.7: Loss = 0.363953
Epoch 6.8: Loss = 0.361694
Epoch 6.9: Loss = 0.402466
Epoch 6.10: Loss = 0.341537
Epoch 6.11: Loss = 0.34671
Epoch 6.12: Loss = 0.338104
Epoch 6.13: Loss = 0.350052
Epoch 6.14: Loss = 0.354767
Epoch 6.15: Loss = 0.300354
Epoch 6.16: Loss = 0.467453
Epoch 6.17: Loss = 0.308792
Epoch 6.18: Loss = 0.496094
Epoch 6.19: Loss = 0.410019
Epoch 6.20: Loss = 0.352631
Epoch 6.21: Loss = 0.319122
Epoch 6.22: Loss = 0.496429
Epoch 6.23: Loss = 0.37822
Epoch 6.24: Loss = 0.450638
Epoch 6.25: Loss = 0.362076
Epoch 6.26: Loss = 0.403397
Epoch 6.27: Loss = 0.398407
Epoch 6.28: Loss = 0.323013
Epoch 6.29: Loss = 0.330688
Epoch 6.30: Loss = 0.298462
Epoch 6.31: Loss = 0.353287
Epoch 6.32: Loss = 0.337845
Epoch 6.33: Loss = 0.351685
Epoch 6.34: Loss = 0.347382
Epoch 6.35: Loss = 0.363434
Epoch 6.36: Loss = 0.337936
Epoch 6.37: Loss = 0.393494
Epoch 6.38: Loss = 0.426666
Epoch 6.39: Loss = 0.370163
Epoch 6.40: Loss = 0.372543
Epoch 6.41: Loss = 0.33876
Epoch 6.42: Loss = 0.447998
Epoch 6.43: Loss = 0.421616
Epoch 6.44: Loss = 0.481415
Epoch 6.45: Loss = 0.315552
Epoch 6.46: Loss = 0.322113
Epoch 6.47: Loss = 0.427521
Epoch 6.48: Loss = 0.290771
Epoch 6.49: Loss = 0.386917
Epoch 6.50: Loss = 0.432297
Epoch 6.51: Loss = 0.390076
Epoch 6.52: Loss = 0.392899
Epoch 6.53: Loss = 0.332138
Epoch 6.54: Loss = 0.401428
Epoch 6.55: Loss = 0.408707
Epoch 6.56: Loss = 0.382919
Epoch 6.57: Loss = 0.366104
Epoch 6.58: Loss = 0.418671
Epoch 6.59: Loss = 0.504562
Epoch 6.60: Loss = 0.334534
Epoch 6.61: Loss = 0.375122
Epoch 6.62: Loss = 0.348404
Epoch 6.63: Loss = 0.38443
Epoch 6.64: Loss = 0.361359
Epoch 6.65: Loss = 0.403137
Epoch 6.66: Loss = 0.470642
Epoch 6.67: Loss = 0.387802
Epoch 6.68: Loss = 0.296036
Epoch 6.69: Loss = 0.321365
Epoch 6.70: Loss = 0.294968
Epoch 6.71: Loss = 0.30307
Epoch 6.72: Loss = 0.384781
Epoch 6.73: Loss = 0.344116
Epoch 6.74: Loss = 0.35733
Epoch 6.75: Loss = 0.308426
Epoch 6.76: Loss = 0.355057
Epoch 6.77: Loss = 0.425201
Epoch 6.78: Loss = 0.396317
Epoch 6.79: Loss = 0.40686
Epoch 6.80: Loss = 0.315735
Epoch 6.81: Loss = 0.378311
Epoch 6.82: Loss = 0.370789
Epoch 6.83: Loss = 0.31218
Epoch 6.84: Loss = 0.353806
Epoch 6.85: Loss = 0.301239
Epoch 6.86: Loss = 0.37442
Epoch 6.87: Loss = 0.340546
Epoch 6.88: Loss = 0.280899
Epoch 6.89: Loss = 0.355301
Epoch 6.90: Loss = 0.327408
Epoch 6.91: Loss = 0.417374
Epoch 6.92: Loss = 0.364334
Epoch 6.93: Loss = 0.350403
Epoch 6.94: Loss = 0.335342
Epoch 6.95: Loss = 0.336151
Epoch 6.96: Loss = 0.368439
Epoch 6.97: Loss = 0.365952
Epoch 6.98: Loss = 0.491928
Epoch 6.99: Loss = 0.284363
Epoch 6.100: Loss = 0.424408
Epoch 6.101: Loss = 0.375092
Epoch 6.102: Loss = 0.301071
Epoch 6.103: Loss = 0.420502
Epoch 6.104: Loss = 0.38475
Epoch 6.105: Loss = 0.353195
Epoch 6.106: Loss = 0.364746
Epoch 6.107: Loss = 0.339233
Epoch 6.108: Loss = 0.39064
Epoch 6.109: Loss = 0.359909
Epoch 6.110: Loss = 0.333466
Epoch 6.111: Loss = 0.32576
Epoch 6.112: Loss = 0.329712
Epoch 6.113: Loss = 0.406631
Epoch 6.114: Loss = 0.332489
Epoch 6.115: Loss = 0.310287
Epoch 6.116: Loss = 0.345291
Epoch 6.117: Loss = 0.3815
Epoch 6.118: Loss = 0.428238
Epoch 6.119: Loss = 0.312469
Epoch 6.120: Loss = 0.432755
TRAIN LOSS = 0.368759
TRAIN ACC = 89.296 % (53580/60000)
Loss = 0.306946
Loss = 0.422775
Loss = 0.480972
Loss = 0.508759
Loss = 0.516678
Loss = 0.369583
Loss = 0.314514
Loss = 0.557892
Loss = 0.48349
Loss = 0.467499
Loss = 0.165176
Loss = 0.274292
Loss = 0.268463
Loss = 0.331024
Loss = 0.182022
Loss = 0.241364
Loss = 0.212997
Loss = 0.0539703
Loss = 0.219223
Loss = 0.519821
TEST LOSS = 0.344873
TEST ACC = 535.799 % (9018/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.397583
Epoch 7.2: Loss = 0.330551
Epoch 7.3: Loss = 0.448563
Epoch 7.4: Loss = 0.392029
Epoch 7.5: Loss = 0.324631
Epoch 7.6: Loss = 0.351135
Epoch 7.7: Loss = 0.323959
Epoch 7.8: Loss = 0.478607
Epoch 7.9: Loss = 0.410828
Epoch 7.10: Loss = 0.386292
Epoch 7.11: Loss = 0.35527
Epoch 7.12: Loss = 0.435837
Epoch 7.13: Loss = 0.329987
Epoch 7.14: Loss = 0.369705
Epoch 7.15: Loss = 0.37883
Epoch 7.16: Loss = 0.332413
Epoch 7.17: Loss = 0.371277
Epoch 7.18: Loss = 0.362
Epoch 7.19: Loss = 0.393112
Epoch 7.20: Loss = 0.307144
Epoch 7.21: Loss = 0.300949
Epoch 7.22: Loss = 0.232681
Epoch 7.23: Loss = 0.458176
Epoch 7.24: Loss = 0.338364
Epoch 7.25: Loss = 0.322876
Epoch 7.26: Loss = 0.352844
Epoch 7.27: Loss = 0.335983
Epoch 7.28: Loss = 0.392654
Epoch 7.29: Loss = 0.375854
Epoch 7.30: Loss = 0.324142
Epoch 7.31: Loss = 0.411133
Epoch 7.32: Loss = 0.312119
Epoch 7.33: Loss = 0.360962
Epoch 7.34: Loss = 0.294922
Epoch 7.35: Loss = 0.374084
Epoch 7.36: Loss = 0.36113
Epoch 7.37: Loss = 0.304245
Epoch 7.38: Loss = 0.405319
Epoch 7.39: Loss = 0.336029
Epoch 7.40: Loss = 0.334
Epoch 7.41: Loss = 0.388412
Epoch 7.42: Loss = 0.410797
Epoch 7.43: Loss = 0.380035
Epoch 7.44: Loss = 0.287582
Epoch 7.45: Loss = 0.389832
Epoch 7.46: Loss = 0.379578
Epoch 7.47: Loss = 0.295746
Epoch 7.48: Loss = 0.328049
Epoch 7.49: Loss = 0.310074
Epoch 7.50: Loss = 0.377533
Epoch 7.51: Loss = 0.409866
Epoch 7.52: Loss = 0.331772
Epoch 7.53: Loss = 0.443512
Epoch 7.54: Loss = 0.380981
Epoch 7.55: Loss = 0.386993
Epoch 7.56: Loss = 0.300659
Epoch 7.57: Loss = 0.388397
Epoch 7.58: Loss = 0.347015
Epoch 7.59: Loss = 0.381104
Epoch 7.60: Loss = 0.401093
Epoch 7.61: Loss = 0.419662
Epoch 7.62: Loss = 0.295197
Epoch 7.63: Loss = 0.323654
Epoch 7.64: Loss = 0.419266
Epoch 7.65: Loss = 0.442749
Epoch 7.66: Loss = 0.380966
Epoch 7.67: Loss = 0.323593
Epoch 7.68: Loss = 0.337189
Epoch 7.69: Loss = 0.35228
Epoch 7.70: Loss = 0.323273
Epoch 7.71: Loss = 0.430389
Epoch 7.72: Loss = 0.385864
Epoch 7.73: Loss = 0.378754
Epoch 7.74: Loss = 0.413315
Epoch 7.75: Loss = 0.365952
Epoch 7.76: Loss = 0.329239
Epoch 7.77: Loss = 0.405228
Epoch 7.78: Loss = 0.316635
Epoch 7.79: Loss = 0.331894
Epoch 7.80: Loss = 0.305435
Epoch 7.81: Loss = 0.324509
Epoch 7.82: Loss = 0.382889
Epoch 7.83: Loss = 0.355637
Epoch 7.84: Loss = 0.468109
Epoch 7.85: Loss = 0.436966
Epoch 7.86: Loss = 0.33226
Epoch 7.87: Loss = 0.320053
Epoch 7.88: Loss = 0.3293
Epoch 7.89: Loss = 0.37146
Epoch 7.90: Loss = 0.366135
Epoch 7.91: Loss = 0.324188
Epoch 7.92: Loss = 0.334564
Epoch 7.93: Loss = 0.437241
Epoch 7.94: Loss = 0.291519
Epoch 7.95: Loss = 0.339554
Epoch 7.96: Loss = 0.46933
Epoch 7.97: Loss = 0.313278
Epoch 7.98: Loss = 0.407272
Epoch 7.99: Loss = 0.315079
Epoch 7.100: Loss = 0.392151
Epoch 7.101: Loss = 0.383392
Epoch 7.102: Loss = 0.309723
Epoch 7.103: Loss = 0.371246
Epoch 7.104: Loss = 0.345032
Epoch 7.105: Loss = 0.381714
Epoch 7.106: Loss = 0.397751
Epoch 7.107: Loss = 0.38562
Epoch 7.108: Loss = 0.385757
Epoch 7.109: Loss = 0.360367
Epoch 7.110: Loss = 0.400497
Epoch 7.111: Loss = 0.418427
Epoch 7.112: Loss = 0.436569
Epoch 7.113: Loss = 0.382446
Epoch 7.114: Loss = 0.337143
Epoch 7.115: Loss = 0.442062
Epoch 7.116: Loss = 0.403076
Epoch 7.117: Loss = 0.369904
Epoch 7.118: Loss = 0.322845
Epoch 7.119: Loss = 0.421295
Epoch 7.120: Loss = 0.377899
TRAIN LOSS = 0.366272
TRAIN ACC = 89.4791 % (53690/60000)
Loss = 0.301178
Loss = 0.408936
Loss = 0.489014
Loss = 0.508621
Loss = 0.510284
Loss = 0.378189
Loss = 0.309158
Loss = 0.556091
Loss = 0.482773
Loss = 0.462677
Loss = 0.149796
Loss = 0.290741
Loss = 0.283478
Loss = 0.325836
Loss = 0.189133
Loss = 0.240967
Loss = 0.2155
Loss = 0.0507507
Loss = 0.213409
Loss = 0.521362
TEST LOSS = 0.344395
TEST ACC = 536.899 % (9016/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.443298
Epoch 8.2: Loss = 0.300522
Epoch 8.3: Loss = 0.320877
Epoch 8.4: Loss = 0.271881
Epoch 8.5: Loss = 0.328293
Epoch 8.6: Loss = 0.439285
Epoch 8.7: Loss = 0.359268
Epoch 8.8: Loss = 0.353058
Epoch 8.9: Loss = 0.358856
Epoch 8.10: Loss = 0.380142
Epoch 8.11: Loss = 0.358398
Epoch 8.12: Loss = 0.331894
Epoch 8.13: Loss = 0.348175
Epoch 8.14: Loss = 0.390625
Epoch 8.15: Loss = 0.340729
Epoch 8.16: Loss = 0.315491
Epoch 8.17: Loss = 0.370575
Epoch 8.18: Loss = 0.343018
Epoch 8.19: Loss = 0.441833
Epoch 8.20: Loss = 0.391388
Epoch 8.21: Loss = 0.39772
Epoch 8.22: Loss = 0.39003
Epoch 8.23: Loss = 0.300613
Epoch 8.24: Loss = 0.408112
Epoch 8.25: Loss = 0.394028
Epoch 8.26: Loss = 0.325134
Epoch 8.27: Loss = 0.49115
Epoch 8.28: Loss = 0.340881
Epoch 8.29: Loss = 0.320618
Epoch 8.30: Loss = 0.308044
Epoch 8.31: Loss = 0.381088
Epoch 8.32: Loss = 0.408264
Epoch 8.33: Loss = 0.376022
Epoch 8.34: Loss = 0.405472
Epoch 8.35: Loss = 0.4431
Epoch 8.36: Loss = 0.387894
Epoch 8.37: Loss = 0.382202
Epoch 8.38: Loss = 0.336182
Epoch 8.39: Loss = 0.436691
Epoch 8.40: Loss = 0.422531
Epoch 8.41: Loss = 0.351654
Epoch 8.42: Loss = 0.305054
Epoch 8.43: Loss = 0.335938
Epoch 8.44: Loss = 0.295074
Epoch 8.45: Loss = 0.394928
Epoch 8.46: Loss = 0.441422
Epoch 8.47: Loss = 0.4505
Epoch 8.48: Loss = 0.345993
Epoch 8.49: Loss = 0.346283
Epoch 8.50: Loss = 0.369278
Epoch 8.51: Loss = 0.425323
Epoch 8.52: Loss = 0.349487
Epoch 8.53: Loss = 0.295074
Epoch 8.54: Loss = 0.348297
Epoch 8.55: Loss = 0.356949
Epoch 8.56: Loss = 0.420258
Epoch 8.57: Loss = 0.349503
Epoch 8.58: Loss = 0.395462
Epoch 8.59: Loss = 0.282806
Epoch 8.60: Loss = 0.273636
Epoch 8.61: Loss = 0.389603
Epoch 8.62: Loss = 0.2892
Epoch 8.63: Loss = 0.311859
Epoch 8.64: Loss = 0.416809
Epoch 8.65: Loss = 0.320984
Epoch 8.66: Loss = 0.460602
Epoch 8.67: Loss = 0.333038
Epoch 8.68: Loss = 0.392334
Epoch 8.69: Loss = 0.41684
Epoch 8.70: Loss = 0.308731
Epoch 8.71: Loss = 0.337173
Epoch 8.72: Loss = 0.311646
Epoch 8.73: Loss = 0.474976
Epoch 8.74: Loss = 0.386169
Epoch 8.75: Loss = 0.290222
Epoch 8.76: Loss = 0.341064
Epoch 8.77: Loss = 0.299744
Epoch 8.78: Loss = 0.413406
Epoch 8.79: Loss = 0.376633
Epoch 8.80: Loss = 0.404541
Epoch 8.81: Loss = 0.270004
Epoch 8.82: Loss = 0.391144
Epoch 8.83: Loss = 0.392242
Epoch 8.84: Loss = 0.306946
Epoch 8.85: Loss = 0.326996
Epoch 8.86: Loss = 0.361649
Epoch 8.87: Loss = 0.399719
Epoch 8.88: Loss = 0.318146
Epoch 8.89: Loss = 0.32515
Epoch 8.90: Loss = 0.315903
Epoch 8.91: Loss = 0.369644
Epoch 8.92: Loss = 0.286392
Epoch 8.93: Loss = 0.403259
Epoch 8.94: Loss = 0.396652
Epoch 8.95: Loss = 0.368042
Epoch 8.96: Loss = 0.299301
Epoch 8.97: Loss = 0.410187
Epoch 8.98: Loss = 0.430573
Epoch 8.99: Loss = 0.33992
Epoch 8.100: Loss = 0.407227
Epoch 8.101: Loss = 0.543243
Epoch 8.102: Loss = 0.290314
Epoch 8.103: Loss = 0.306061
Epoch 8.104: Loss = 0.392548
Epoch 8.105: Loss = 0.423325
Epoch 8.106: Loss = 0.300385
Epoch 8.107: Loss = 0.393967
Epoch 8.108: Loss = 0.354218
Epoch 8.109: Loss = 0.339523
Epoch 8.110: Loss = 0.325104
Epoch 8.111: Loss = 0.36528
Epoch 8.112: Loss = 0.337814
Epoch 8.113: Loss = 0.333786
Epoch 8.114: Loss = 0.307373
Epoch 8.115: Loss = 0.452347
Epoch 8.116: Loss = 0.37204
Epoch 8.117: Loss = 0.381119
Epoch 8.118: Loss = 0.294739
Epoch 8.119: Loss = 0.326752
Epoch 8.120: Loss = 0.358856
TRAIN LOSS = 0.363083
TRAIN ACC = 89.8315 % (53902/60000)
Loss = 0.295258
Loss = 0.387878
Loss = 0.491684
Loss = 0.483902
Loss = 0.506012
Loss = 0.357574
Loss = 0.288132
Loss = 0.548065
Loss = 0.490646
Loss = 0.466751
Loss = 0.155136
Loss = 0.276215
Loss = 0.270035
Loss = 0.315552
Loss = 0.181656
Loss = 0.243546
Loss = 0.210953
Loss = 0.046402
Loss = 0.209671
Loss = 0.523102
TEST LOSS = 0.337408
TEST ACC = 539.02 % (9044/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.386932
Epoch 9.2: Loss = 0.373734
Epoch 9.3: Loss = 0.387558
Epoch 9.4: Loss = 0.322266
Epoch 9.5: Loss = 0.300049
Epoch 9.6: Loss = 0.445618
Epoch 9.7: Loss = 0.309113
Epoch 9.8: Loss = 0.427246
Epoch 9.9: Loss = 0.410538
Epoch 9.10: Loss = 0.355545
Epoch 9.11: Loss = 0.336655
Epoch 9.12: Loss = 0.331284
Epoch 9.13: Loss = 0.346588
Epoch 9.14: Loss = 0.386459
Epoch 9.15: Loss = 0.315384
Epoch 9.16: Loss = 0.365219
Epoch 9.17: Loss = 0.38063
Epoch 9.18: Loss = 0.471466
Epoch 9.19: Loss = 0.304794
Epoch 9.20: Loss = 0.318817
Epoch 9.21: Loss = 0.39447
Epoch 9.22: Loss = 0.272369
Epoch 9.23: Loss = 0.395248
Epoch 9.24: Loss = 0.347107
Epoch 9.25: Loss = 0.407272
Epoch 9.26: Loss = 0.318314
Epoch 9.27: Loss = 0.355865
Epoch 9.28: Loss = 0.276001
Epoch 9.29: Loss = 0.475403
Epoch 9.30: Loss = 0.360596
Epoch 9.31: Loss = 0.402161
Epoch 9.32: Loss = 0.405487
Epoch 9.33: Loss = 0.316177
Epoch 9.34: Loss = 0.354492
Epoch 9.35: Loss = 0.412994
Epoch 9.36: Loss = 0.374649
Epoch 9.37: Loss = 0.323624
Epoch 9.38: Loss = 0.39241
Epoch 9.39: Loss = 0.431
Epoch 9.40: Loss = 0.395065
Epoch 9.41: Loss = 0.37384
Epoch 9.42: Loss = 0.489014
Epoch 9.43: Loss = 0.333969
Epoch 9.44: Loss = 0.280884
Epoch 9.45: Loss = 0.381271
Epoch 9.46: Loss = 0.335114
Epoch 9.47: Loss = 0.419052
Epoch 9.48: Loss = 0.305725
Epoch 9.49: Loss = 0.317535
Epoch 9.50: Loss = 0.385086
Epoch 9.51: Loss = 0.328964
Epoch 9.52: Loss = 0.363495
Epoch 9.53: Loss = 0.380219
Epoch 9.54: Loss = 0.459991
Epoch 9.55: Loss = 0.325699
Epoch 9.56: Loss = 0.295883
Epoch 9.57: Loss = 0.342422
Epoch 9.58: Loss = 0.293442
Epoch 9.59: Loss = 0.333008
Epoch 9.60: Loss = 0.465179
Epoch 9.61: Loss = 0.317566
Epoch 9.62: Loss = 0.371155
Epoch 9.63: Loss = 0.290405
Epoch 9.64: Loss = 0.350632
Epoch 9.65: Loss = 0.342163
Epoch 9.66: Loss = 0.384216
Epoch 9.67: Loss = 0.357819
Epoch 9.68: Loss = 0.42775
Epoch 9.69: Loss = 0.348679
Epoch 9.70: Loss = 0.384735
Epoch 9.71: Loss = 0.320999
Epoch 9.72: Loss = 0.371262
Epoch 9.73: Loss = 0.293335
Epoch 9.74: Loss = 0.353912
Epoch 9.75: Loss = 0.364502
Epoch 9.76: Loss = 0.364227
Epoch 9.77: Loss = 0.375229
Epoch 9.78: Loss = 0.41629
Epoch 9.79: Loss = 0.328979
Epoch 9.80: Loss = 0.415955
Epoch 9.81: Loss = 0.275879
Epoch 9.82: Loss = 0.331238
Epoch 9.83: Loss = 0.394409
Epoch 9.84: Loss = 0.280899
Epoch 9.85: Loss = 0.482864
Epoch 9.86: Loss = 0.382919
Epoch 9.87: Loss = 0.439774
Epoch 9.88: Loss = 0.363998
Epoch 9.89: Loss = 0.346924
Epoch 9.90: Loss = 0.31105
Epoch 9.91: Loss = 0.333298
Epoch 9.92: Loss = 0.391464
Epoch 9.93: Loss = 0.357178
Epoch 9.94: Loss = 0.310577
Epoch 9.95: Loss = 0.332077
Epoch 9.96: Loss = 0.25769
Epoch 9.97: Loss = 0.300552
Epoch 9.98: Loss = 0.365189
Epoch 9.99: Loss = 0.423187
Epoch 9.100: Loss = 0.310516
Epoch 9.101: Loss = 0.397141
Epoch 9.102: Loss = 0.282028
Epoch 9.103: Loss = 0.414597
Epoch 9.104: Loss = 0.350555
Epoch 9.105: Loss = 0.266449
Epoch 9.106: Loss = 0.331421
Epoch 9.107: Loss = 0.470917
Epoch 9.108: Loss = 0.278137
Epoch 9.109: Loss = 0.320496
Epoch 9.110: Loss = 0.400742
Epoch 9.111: Loss = 0.341446
Epoch 9.112: Loss = 0.33786
Epoch 9.113: Loss = 0.262589
Epoch 9.114: Loss = 0.357162
Epoch 9.115: Loss = 0.265274
Epoch 9.116: Loss = 0.396774
Epoch 9.117: Loss = 0.493881
Epoch 9.118: Loss = 0.228439
Epoch 9.119: Loss = 0.367905
Epoch 9.120: Loss = 0.335144
TRAIN LOSS = 0.358063
TRAIN ACC = 89.9963 % (54000/60000)
Loss = 0.289261
Loss = 0.396851
Loss = 0.479721
Loss = 0.50589
Loss = 0.49884
Loss = 0.353561
Loss = 0.283844
Loss = 0.546616
Loss = 0.487946
Loss = 0.475571
Loss = 0.157455
Loss = 0.263855
Loss = 0.276932
Loss = 0.316254
Loss = 0.16803
Loss = 0.265259
Loss = 0.205994
Loss = 0.0408783
Loss = 0.210663
Loss = 0.497803
TEST LOSS = 0.336061
TEST ACC = 539.999 % (9059/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.343353
Epoch 10.2: Loss = 0.333145
Epoch 10.3: Loss = 0.352875
Epoch 10.4: Loss = 0.337051
Epoch 10.5: Loss = 0.387054
Epoch 10.6: Loss = 0.409103
Epoch 10.7: Loss = 0.299164
Epoch 10.8: Loss = 0.25322
Epoch 10.9: Loss = 0.375854
Epoch 10.10: Loss = 0.459061
Epoch 10.11: Loss = 0.424911
Epoch 10.12: Loss = 0.372131
Epoch 10.13: Loss = 0.373245
Epoch 10.14: Loss = 0.301514
Epoch 10.15: Loss = 0.480774
Epoch 10.16: Loss = 0.330063
Epoch 10.17: Loss = 0.363922
Epoch 10.18: Loss = 0.504852
Epoch 10.19: Loss = 0.253708
Epoch 10.20: Loss = 0.292114
Epoch 10.21: Loss = 0.403915
Epoch 10.22: Loss = 0.295059
Epoch 10.23: Loss = 0.342758
Epoch 10.24: Loss = 0.376831
Epoch 10.25: Loss = 0.394272
Epoch 10.26: Loss = 0.335724
Epoch 10.27: Loss = 0.384781
Epoch 10.28: Loss = 0.404434
Epoch 10.29: Loss = 0.312027
Epoch 10.30: Loss = 0.300598
Epoch 10.31: Loss = 0.362061
Epoch 10.32: Loss = 0.391251
Epoch 10.33: Loss = 0.328934
Epoch 10.34: Loss = 0.291397
Epoch 10.35: Loss = 0.386353
Epoch 10.36: Loss = 0.29158
Epoch 10.37: Loss = 0.357544
Epoch 10.38: Loss = 0.302139
Epoch 10.39: Loss = 0.365616
Epoch 10.40: Loss = 0.446198
Epoch 10.41: Loss = 0.277084
Epoch 10.42: Loss = 0.42598
Epoch 10.43: Loss = 0.273422
Epoch 10.44: Loss = 0.385193
Epoch 10.45: Loss = 0.282455
Epoch 10.46: Loss = 0.415451
Epoch 10.47: Loss = 0.358429
Epoch 10.48: Loss = 0.343842
Epoch 10.49: Loss = 0.328644
Epoch 10.50: Loss = 0.349899
Epoch 10.51: Loss = 0.326141
Epoch 10.52: Loss = 0.333023
Epoch 10.53: Loss = 0.521271
Epoch 10.54: Loss = 0.451523
Epoch 10.55: Loss = 0.303955
Epoch 10.56: Loss = 0.395676
Epoch 10.57: Loss = 0.260925
Epoch 10.58: Loss = 0.378937
Epoch 10.59: Loss = 0.358109
Epoch 10.60: Loss = 0.371307
Epoch 10.61: Loss = 0.403046
Epoch 10.62: Loss = 0.311722
Epoch 10.63: Loss = 0.308884
Epoch 10.64: Loss = 0.375748
Epoch 10.65: Loss = 0.418457
Epoch 10.66: Loss = 0.385178
Epoch 10.67: Loss = 0.355576
Epoch 10.68: Loss = 0.475891
Epoch 10.69: Loss = 0.337418
Epoch 10.70: Loss = 0.306717
Epoch 10.71: Loss = 0.297424
Epoch 10.72: Loss = 0.342682
Epoch 10.73: Loss = 0.325775
Epoch 10.74: Loss = 0.311127
Epoch 10.75: Loss = 0.404602
Epoch 10.76: Loss = 0.312454
Epoch 10.77: Loss = 0.433197
Epoch 10.78: Loss = 0.346451
Epoch 10.79: Loss = 0.411377
Epoch 10.80: Loss = 0.364716
Epoch 10.81: Loss = 0.34668
Epoch 10.82: Loss = 0.372589
Epoch 10.83: Loss = 0.357147
Epoch 10.84: Loss = 0.386002
Epoch 10.85: Loss = 0.316162
Epoch 10.86: Loss = 0.283188
Epoch 10.87: Loss = 0.353592
Epoch 10.88: Loss = 0.330093
Epoch 10.89: Loss = 0.292526
Epoch 10.90: Loss = 0.430191
Epoch 10.91: Loss = 0.304794
Epoch 10.92: Loss = 0.345047
Epoch 10.93: Loss = 0.276047
Epoch 10.94: Loss = 0.316986
Epoch 10.95: Loss = 0.31662
Epoch 10.96: Loss = 0.307404
Epoch 10.97: Loss = 0.339706
Epoch 10.98: Loss = 0.360901
Epoch 10.99: Loss = 0.383698
Epoch 10.100: Loss = 0.392899
Epoch 10.101: Loss = 0.366821
Epoch 10.102: Loss = 0.422821
Epoch 10.103: Loss = 0.312317
Epoch 10.104: Loss = 0.319519
Epoch 10.105: Loss = 0.327072
Epoch 10.106: Loss = 0.351181
Epoch 10.107: Loss = 0.291962
Epoch 10.108: Loss = 0.335602
Epoch 10.109: Loss = 0.366302
Epoch 10.110: Loss = 0.387802
Epoch 10.111: Loss = 0.275467
Epoch 10.112: Loss = 0.317963
Epoch 10.113: Loss = 0.42189
Epoch 10.114: Loss = 0.285782
Epoch 10.115: Loss = 0.320847
Epoch 10.116: Loss = 0.377121
Epoch 10.117: Loss = 0.310593
Epoch 10.118: Loss = 0.339081
Epoch 10.119: Loss = 0.362427
Epoch 10.120: Loss = 0.319031
TRAIN LOSS = 0.352631
TRAIN ACC = 90.3595 % (54218/60000)
Loss = 0.289566
Loss = 0.395126
Loss = 0.485031
Loss = 0.498642
Loss = 0.489288
Loss = 0.353928
Loss = 0.299225
Loss = 0.524979
Loss = 0.482849
Loss = 0.46373
Loss = 0.148651
Loss = 0.277374
Loss = 0.275238
Loss = 0.290359
Loss = 0.169037
Loss = 0.275986
Loss = 0.206314
Loss = 0.0358887
Loss = 0.20607
Loss = 0.496368
TEST LOSS = 0.333182
TEST ACC = 542.178 % (9080/10000)

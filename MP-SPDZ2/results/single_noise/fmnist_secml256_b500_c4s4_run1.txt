Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.39005
Epoch 1.2: Loss = 2.32146
Epoch 1.3: Loss = 2.22704
Epoch 1.4: Loss = 2.19713
Epoch 1.5: Loss = 2.14525
Epoch 1.6: Loss = 2.07074
Epoch 1.7: Loss = 2.00739
Epoch 1.8: Loss = 1.99261
Epoch 1.9: Loss = 1.9193
Epoch 1.10: Loss = 1.92192
Epoch 1.11: Loss = 1.8344
Epoch 1.12: Loss = 1.76207
Epoch 1.13: Loss = 1.79735
Epoch 1.14: Loss = 1.71152
Epoch 1.15: Loss = 1.7305
Epoch 1.16: Loss = 1.61205
Epoch 1.17: Loss = 1.63072
Epoch 1.18: Loss = 1.58827
Epoch 1.19: Loss = 1.54364
Epoch 1.20: Loss = 1.50912
Epoch 1.21: Loss = 1.50626
Epoch 1.22: Loss = 1.4854
Epoch 1.23: Loss = 1.41473
Epoch 1.24: Loss = 1.38113
Epoch 1.25: Loss = 1.37372
Epoch 1.26: Loss = 1.35156
Epoch 1.27: Loss = 1.32214
Epoch 1.28: Loss = 1.37071
Epoch 1.29: Loss = 1.25467
Epoch 1.30: Loss = 1.21918
Epoch 1.31: Loss = 1.21385
Epoch 1.32: Loss = 1.21442
Epoch 1.33: Loss = 1.21126
Epoch 1.34: Loss = 1.19487
Epoch 1.35: Loss = 1.17899
Epoch 1.36: Loss = 1.12903
Epoch 1.37: Loss = 1.11111
Epoch 1.38: Loss = 1.1945
Epoch 1.39: Loss = 1.12119
Epoch 1.40: Loss = 1.09343
Epoch 1.41: Loss = 1.14244
Epoch 1.42: Loss = 1.05809
Epoch 1.43: Loss = 1.09619
Epoch 1.44: Loss = 1.05722
Epoch 1.45: Loss = 1.03029
Epoch 1.46: Loss = 1.03732
Epoch 1.47: Loss = 1.01277
Epoch 1.48: Loss = 1.01122
Epoch 1.49: Loss = 1.03229
Epoch 1.50: Loss = 1.00752
Epoch 1.51: Loss = 0.979294
Epoch 1.52: Loss = 0.992783
Epoch 1.53: Loss = 0.953186
Epoch 1.54: Loss = 0.923691
Epoch 1.55: Loss = 0.871338
Epoch 1.56: Loss = 0.926727
Epoch 1.57: Loss = 0.864243
Epoch 1.58: Loss = 0.952301
Epoch 1.59: Loss = 0.899719
Epoch 1.60: Loss = 0.96196
Epoch 1.61: Loss = 0.981979
Epoch 1.62: Loss = 0.927643
Epoch 1.63: Loss = 0.911621
Epoch 1.64: Loss = 0.886993
Epoch 1.65: Loss = 0.942276
Epoch 1.66: Loss = 0.868576
Epoch 1.67: Loss = 0.915131
Epoch 1.68: Loss = 0.868179
Epoch 1.69: Loss = 0.821472
Epoch 1.70: Loss = 0.865189
Epoch 1.71: Loss = 0.84761
Epoch 1.72: Loss = 0.973648
Epoch 1.73: Loss = 0.854858
Epoch 1.74: Loss = 0.888992
Epoch 1.75: Loss = 0.81842
Epoch 1.76: Loss = 0.795914
Epoch 1.77: Loss = 0.874954
Epoch 1.78: Loss = 0.847351
Epoch 1.79: Loss = 0.909149
Epoch 1.80: Loss = 0.82518
Epoch 1.81: Loss = 0.814865
Epoch 1.82: Loss = 0.804962
Epoch 1.83: Loss = 0.919174
Epoch 1.84: Loss = 0.843018
Epoch 1.85: Loss = 0.888458
Epoch 1.86: Loss = 0.870651
Epoch 1.87: Loss = 0.871063
Epoch 1.88: Loss = 0.79335
Epoch 1.89: Loss = 0.816162
Epoch 1.90: Loss = 0.867615
Epoch 1.91: Loss = 0.750977
Epoch 1.92: Loss = 0.761154
Epoch 1.93: Loss = 0.923279
Epoch 1.94: Loss = 0.823273
Epoch 1.95: Loss = 0.808472
Epoch 1.96: Loss = 0.847778
Epoch 1.97: Loss = 0.792587
Epoch 1.98: Loss = 0.805237
Epoch 1.99: Loss = 0.761246
Epoch 1.100: Loss = 0.876129
Epoch 1.101: Loss = 0.783813
Epoch 1.102: Loss = 0.804123
Epoch 1.103: Loss = 0.799927
Epoch 1.104: Loss = 0.79274
Epoch 1.105: Loss = 0.728439
Epoch 1.106: Loss = 0.78215
Epoch 1.107: Loss = 0.800171
Epoch 1.108: Loss = 0.794037
Epoch 1.109: Loss = 0.800186
Epoch 1.110: Loss = 0.849808
Epoch 1.111: Loss = 0.821945
Epoch 1.112: Loss = 0.78302
Epoch 1.113: Loss = 0.796585
Epoch 1.114: Loss = 0.771667
Epoch 1.115: Loss = 0.775558
Epoch 1.116: Loss = 0.711136
Epoch 1.117: Loss = 0.744202
Epoch 1.118: Loss = 0.821259
Epoch 1.119: Loss = 0.790344
Epoch 1.120: Loss = 0.76236
TRAIN LOSS = 1.10872
TRAIN ACC = 62.3123 % (37389/60000)
Loss = 0.678543
Loss = 0.79776
Loss = 0.76796
Loss = 0.69696
Loss = 0.699661
Loss = 0.830551
Loss = 0.85025
Loss = 0.835175
Loss = 0.745773
Loss = 0.711838
Loss = 0.810028
Loss = 0.770676
Loss = 0.749496
Loss = 0.776703
Loss = 0.725311
Loss = 0.822678
Loss = 0.68721
Loss = 0.760483
Loss = 0.782669
Loss = 0.755142
TEST LOSS = 0.762743
TEST ACC = 373.889 % (7285/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.758789
Epoch 2.2: Loss = 0.708069
Epoch 2.3: Loss = 0.733917
Epoch 2.4: Loss = 0.81839
Epoch 2.5: Loss = 0.772095
Epoch 2.6: Loss = 0.735153
Epoch 2.7: Loss = 0.793076
Epoch 2.8: Loss = 0.763611
Epoch 2.9: Loss = 0.727325
Epoch 2.10: Loss = 0.70192
Epoch 2.11: Loss = 0.74321
Epoch 2.12: Loss = 0.737808
Epoch 2.13: Loss = 0.787491
Epoch 2.14: Loss = 0.800583
Epoch 2.15: Loss = 0.703445
Epoch 2.16: Loss = 0.800949
Epoch 2.17: Loss = 0.676559
Epoch 2.18: Loss = 0.749252
Epoch 2.19: Loss = 0.703827
Epoch 2.20: Loss = 0.771027
Epoch 2.21: Loss = 0.70607
Epoch 2.22: Loss = 0.765976
Epoch 2.23: Loss = 0.693451
Epoch 2.24: Loss = 0.799423
Epoch 2.25: Loss = 0.741882
Epoch 2.26: Loss = 0.638031
Epoch 2.27: Loss = 0.765244
Epoch 2.28: Loss = 0.720306
Epoch 2.29: Loss = 0.725861
Epoch 2.30: Loss = 0.7323
Epoch 2.31: Loss = 0.719315
Epoch 2.32: Loss = 0.806
Epoch 2.33: Loss = 0.754044
Epoch 2.34: Loss = 0.751816
Epoch 2.35: Loss = 0.807968
Epoch 2.36: Loss = 0.683731
Epoch 2.37: Loss = 0.642303
Epoch 2.38: Loss = 0.740753
Epoch 2.39: Loss = 0.816055
Epoch 2.40: Loss = 0.695953
Epoch 2.41: Loss = 0.800812
Epoch 2.42: Loss = 0.737213
Epoch 2.43: Loss = 0.705002
Epoch 2.44: Loss = 0.71225
Epoch 2.45: Loss = 0.708572
Epoch 2.46: Loss = 0.704376
Epoch 2.47: Loss = 0.659363
Epoch 2.48: Loss = 0.637833
Epoch 2.49: Loss = 0.700943
Epoch 2.50: Loss = 0.771957
Epoch 2.51: Loss = 0.731628
Epoch 2.52: Loss = 0.655838
Epoch 2.53: Loss = 0.756836
Epoch 2.54: Loss = 0.647919
Epoch 2.55: Loss = 0.69902
Epoch 2.56: Loss = 0.706772
Epoch 2.57: Loss = 0.741669
Epoch 2.58: Loss = 0.572678
Epoch 2.59: Loss = 0.74057
Epoch 2.60: Loss = 0.794495
Epoch 2.61: Loss = 0.693481
Epoch 2.62: Loss = 0.706573
Epoch 2.63: Loss = 0.74884
Epoch 2.64: Loss = 0.732483
Epoch 2.65: Loss = 0.662888
Epoch 2.66: Loss = 0.689804
Epoch 2.67: Loss = 0.64241
Epoch 2.68: Loss = 0.743652
Epoch 2.69: Loss = 0.651535
Epoch 2.70: Loss = 0.693756
Epoch 2.71: Loss = 0.681885
Epoch 2.72: Loss = 0.704727
Epoch 2.73: Loss = 0.653534
Epoch 2.74: Loss = 0.653519
Epoch 2.75: Loss = 0.678543
Epoch 2.76: Loss = 0.78566
Epoch 2.77: Loss = 0.688034
Epoch 2.78: Loss = 0.700119
Epoch 2.79: Loss = 0.706558
Epoch 2.80: Loss = 0.634995
Epoch 2.81: Loss = 0.696152
Epoch 2.82: Loss = 0.745621
Epoch 2.83: Loss = 0.633194
Epoch 2.84: Loss = 0.627548
Epoch 2.85: Loss = 0.702576
Epoch 2.86: Loss = 0.707764
Epoch 2.87: Loss = 0.669113
Epoch 2.88: Loss = 0.800217
Epoch 2.89: Loss = 0.69635
Epoch 2.90: Loss = 0.712692
Epoch 2.91: Loss = 0.652176
Epoch 2.92: Loss = 0.639603
Epoch 2.93: Loss = 0.686951
Epoch 2.94: Loss = 0.702682
Epoch 2.95: Loss = 0.649887
Epoch 2.96: Loss = 0.591507
Epoch 2.97: Loss = 0.74176
Epoch 2.98: Loss = 0.653839
Epoch 2.99: Loss = 0.710587
Epoch 2.100: Loss = 0.727661
Epoch 2.101: Loss = 0.721115
Epoch 2.102: Loss = 0.600113
Epoch 2.103: Loss = 0.702728
Epoch 2.104: Loss = 0.710892
Epoch 2.105: Loss = 0.725082
Epoch 2.106: Loss = 0.671539
Epoch 2.107: Loss = 0.643326
Epoch 2.108: Loss = 0.607788
Epoch 2.109: Loss = 0.524673
Epoch 2.110: Loss = 0.602325
Epoch 2.111: Loss = 0.789978
Epoch 2.112: Loss = 0.620377
Epoch 2.113: Loss = 0.61908
Epoch 2.114: Loss = 0.686386
Epoch 2.115: Loss = 0.744125
Epoch 2.116: Loss = 0.689667
Epoch 2.117: Loss = 0.730209
Epoch 2.118: Loss = 0.665817
Epoch 2.119: Loss = 0.678696
Epoch 2.120: Loss = 0.817291
TRAIN LOSS = 0.708603
TRAIN ACC = 75.56 % (45338/60000)
Loss = 0.604355
Loss = 0.725815
Loss = 0.671051
Loss = 0.607346
Loss = 0.636475
Loss = 0.776367
Loss = 0.81778
Loss = 0.794739
Loss = 0.706299
Loss = 0.659195
Loss = 0.774063
Loss = 0.77359
Loss = 0.697495
Loss = 0.693344
Loss = 0.690247
Loss = 0.745407
Loss = 0.622223
Loss = 0.687408
Loss = 0.746689
Loss = 0.698105
TEST LOSS = 0.706399
TEST ACC = 453.38 % (7641/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.647003
Epoch 3.2: Loss = 0.674133
Epoch 3.3: Loss = 0.689758
Epoch 3.4: Loss = 0.689667
Epoch 3.5: Loss = 0.65062
Epoch 3.6: Loss = 0.689789
Epoch 3.7: Loss = 0.673904
Epoch 3.8: Loss = 0.733841
Epoch 3.9: Loss = 0.730423
Epoch 3.10: Loss = 0.680313
Epoch 3.11: Loss = 0.624313
Epoch 3.12: Loss = 0.654007
Epoch 3.13: Loss = 0.758896
Epoch 3.14: Loss = 0.662109
Epoch 3.15: Loss = 0.562012
Epoch 3.16: Loss = 0.628967
Epoch 3.17: Loss = 0.628052
Epoch 3.18: Loss = 0.743073
Epoch 3.19: Loss = 0.78299
Epoch 3.20: Loss = 0.732025
Epoch 3.21: Loss = 0.632568
Epoch 3.22: Loss = 0.593628
Epoch 3.23: Loss = 0.703171
Epoch 3.24: Loss = 0.727066
Epoch 3.25: Loss = 0.77356
Epoch 3.26: Loss = 0.679871
Epoch 3.27: Loss = 0.577866
Epoch 3.28: Loss = 0.79631
Epoch 3.29: Loss = 0.71347
Epoch 3.30: Loss = 0.774673
Epoch 3.31: Loss = 0.738861
Epoch 3.32: Loss = 0.594955
Epoch 3.33: Loss = 0.776154
Epoch 3.34: Loss = 0.584122
Epoch 3.35: Loss = 0.733078
Epoch 3.36: Loss = 0.701813
Epoch 3.37: Loss = 0.603607
Epoch 3.38: Loss = 0.67244
Epoch 3.39: Loss = 0.675613
Epoch 3.40: Loss = 0.725464
Epoch 3.41: Loss = 0.551682
Epoch 3.42: Loss = 0.725632
Epoch 3.43: Loss = 0.727661
Epoch 3.44: Loss = 0.751694
Epoch 3.45: Loss = 0.69043
Epoch 3.46: Loss = 0.657578
Epoch 3.47: Loss = 0.616516
Epoch 3.48: Loss = 0.673523
Epoch 3.49: Loss = 0.511459
Epoch 3.50: Loss = 0.624359
Epoch 3.51: Loss = 0.696289
Epoch 3.52: Loss = 0.66626
Epoch 3.53: Loss = 0.724884
Epoch 3.54: Loss = 0.713058
Epoch 3.55: Loss = 0.743454
Epoch 3.56: Loss = 0.66243
Epoch 3.57: Loss = 0.616364
Epoch 3.58: Loss = 0.59256
Epoch 3.59: Loss = 0.748047
Epoch 3.60: Loss = 0.604477
Epoch 3.61: Loss = 0.681854
Epoch 3.62: Loss = 0.656311
Epoch 3.63: Loss = 0.732391
Epoch 3.64: Loss = 0.636703
Epoch 3.65: Loss = 0.661057
Epoch 3.66: Loss = 0.729355
Epoch 3.67: Loss = 0.695526
Epoch 3.68: Loss = 0.65918
Epoch 3.69: Loss = 0.642868
Epoch 3.70: Loss = 0.644241
Epoch 3.71: Loss = 0.699051
Epoch 3.72: Loss = 0.634186
Epoch 3.73: Loss = 0.635162
Epoch 3.74: Loss = 0.61142
Epoch 3.75: Loss = 0.678925
Epoch 3.76: Loss = 0.674164
Epoch 3.77: Loss = 0.654465
Epoch 3.78: Loss = 0.6754
Epoch 3.79: Loss = 0.770676
Epoch 3.80: Loss = 0.590683
Epoch 3.81: Loss = 0.761765
Epoch 3.82: Loss = 0.580521
Epoch 3.83: Loss = 0.758087
Epoch 3.84: Loss = 0.646851
Epoch 3.85: Loss = 0.701736
Epoch 3.86: Loss = 0.769287
Epoch 3.87: Loss = 0.595383
Epoch 3.88: Loss = 0.65451
Epoch 3.89: Loss = 0.621399
Epoch 3.90: Loss = 0.622925
Epoch 3.91: Loss = 0.690964
Epoch 3.92: Loss = 0.638428
Epoch 3.93: Loss = 0.644974
Epoch 3.94: Loss = 0.686584
Epoch 3.95: Loss = 0.663483
Epoch 3.96: Loss = 0.745056
Epoch 3.97: Loss = 0.678558
Epoch 3.98: Loss = 0.905716
Epoch 3.99: Loss = 0.646973
Epoch 3.100: Loss = 0.644989
Epoch 3.101: Loss = 0.690033
Epoch 3.102: Loss = 0.706177
Epoch 3.103: Loss = 0.733521
Epoch 3.104: Loss = 0.728287
Epoch 3.105: Loss = 0.68251
Epoch 3.106: Loss = 0.672058
Epoch 3.107: Loss = 0.698318
Epoch 3.108: Loss = 0.740051
Epoch 3.109: Loss = 0.68869
Epoch 3.110: Loss = 0.776291
Epoch 3.111: Loss = 0.716827
Epoch 3.112: Loss = 0.560333
Epoch 3.113: Loss = 0.681
Epoch 3.114: Loss = 0.630203
Epoch 3.115: Loss = 0.730316
Epoch 3.116: Loss = 0.677185
Epoch 3.117: Loss = 0.606018
Epoch 3.118: Loss = 0.717285
Epoch 3.119: Loss = 0.623215
Epoch 3.120: Loss = 0.802261
TRAIN LOSS = 0.679977
TRAIN ACC = 78.2394 % (46945/60000)
Loss = 0.584015
Loss = 0.748627
Loss = 0.639679
Loss = 0.578705
Loss = 0.630554
Loss = 0.769257
Loss = 0.797455
Loss = 0.785904
Loss = 0.673706
Loss = 0.629074
Loss = 0.789719
Loss = 0.778748
Loss = 0.711395
Loss = 0.687271
Loss = 0.648926
Loss = 0.733337
Loss = 0.621124
Loss = 0.701004
Loss = 0.717499
Loss = 0.694244
TEST LOSS = 0.696012
TEST ACC = 469.449 % (7798/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.722443
Epoch 4.2: Loss = 0.588821
Epoch 4.3: Loss = 0.736755
Epoch 4.4: Loss = 0.618454
Epoch 4.5: Loss = 0.771454
Epoch 4.6: Loss = 0.67395
Epoch 4.7: Loss = 0.703552
Epoch 4.8: Loss = 0.58194
Epoch 4.9: Loss = 0.696243
Epoch 4.10: Loss = 0.635162
Epoch 4.11: Loss = 0.571854
Epoch 4.12: Loss = 0.506882
Epoch 4.13: Loss = 0.68631
Epoch 4.14: Loss = 0.697388
Epoch 4.15: Loss = 0.696594
Epoch 4.16: Loss = 0.659149
Epoch 4.17: Loss = 0.632889
Epoch 4.18: Loss = 0.596741
Epoch 4.19: Loss = 0.679535
Epoch 4.20: Loss = 0.598801
Epoch 4.21: Loss = 0.613235
Epoch 4.22: Loss = 0.622513
Epoch 4.23: Loss = 0.767136
Epoch 4.24: Loss = 0.561691
Epoch 4.25: Loss = 0.63974
Epoch 4.26: Loss = 0.716492
Epoch 4.27: Loss = 0.52861
Epoch 4.28: Loss = 0.667221
Epoch 4.29: Loss = 0.604156
Epoch 4.30: Loss = 0.582504
Epoch 4.31: Loss = 0.624252
Epoch 4.32: Loss = 0.698715
Epoch 4.33: Loss = 0.685211
Epoch 4.34: Loss = 0.730911
Epoch 4.35: Loss = 0.765427
Epoch 4.36: Loss = 0.684875
Epoch 4.37: Loss = 0.670227
Epoch 4.38: Loss = 0.721436
Epoch 4.39: Loss = 0.780167
Epoch 4.40: Loss = 0.631958
Epoch 4.41: Loss = 0.692535
Epoch 4.42: Loss = 0.767532
Epoch 4.43: Loss = 0.746552
Epoch 4.44: Loss = 0.683426
Epoch 4.45: Loss = 0.763718
Epoch 4.46: Loss = 0.589828
Epoch 4.47: Loss = 0.655731
Epoch 4.48: Loss = 0.681137
Epoch 4.49: Loss = 0.620682
Epoch 4.50: Loss = 0.645752
Epoch 4.51: Loss = 0.628998
Epoch 4.52: Loss = 0.697525
Epoch 4.53: Loss = 0.664749
Epoch 4.54: Loss = 0.689789
Epoch 4.55: Loss = 0.718018
Epoch 4.56: Loss = 0.692566
Epoch 4.57: Loss = 0.580338
Epoch 4.58: Loss = 0.687698
Epoch 4.59: Loss = 0.610107
Epoch 4.60: Loss = 0.728363
Epoch 4.61: Loss = 0.626602
Epoch 4.62: Loss = 0.592545
Epoch 4.63: Loss = 0.708969
Epoch 4.64: Loss = 0.575546
Epoch 4.65: Loss = 0.790237
Epoch 4.66: Loss = 0.689514
Epoch 4.67: Loss = 0.670868
Epoch 4.68: Loss = 0.71962
Epoch 4.69: Loss = 0.761948
Epoch 4.70: Loss = 0.673309
Epoch 4.71: Loss = 0.637375
Epoch 4.72: Loss = 0.714249
Epoch 4.73: Loss = 0.721176
Epoch 4.74: Loss = 0.617126
Epoch 4.75: Loss = 0.659805
Epoch 4.76: Loss = 0.673401
Epoch 4.77: Loss = 0.844803
Epoch 4.78: Loss = 0.762741
Epoch 4.79: Loss = 0.729156
Epoch 4.80: Loss = 0.738327
Epoch 4.81: Loss = 0.84137
Epoch 4.82: Loss = 0.644424
Epoch 4.83: Loss = 0.782623
Epoch 4.84: Loss = 0.574448
Epoch 4.85: Loss = 0.614014
Epoch 4.86: Loss = 0.718384
Epoch 4.87: Loss = 0.637497
Epoch 4.88: Loss = 0.782135
Epoch 4.89: Loss = 0.784195
Epoch 4.90: Loss = 0.550995
Epoch 4.91: Loss = 0.730911
Epoch 4.92: Loss = 0.616943
Epoch 4.93: Loss = 0.740646
Epoch 4.94: Loss = 0.724869
Epoch 4.95: Loss = 0.690674
Epoch 4.96: Loss = 0.740768
Epoch 4.97: Loss = 0.774643
Epoch 4.98: Loss = 0.680603
Epoch 4.99: Loss = 0.713699
Epoch 4.100: Loss = 0.694946
Epoch 4.101: Loss = 0.661026
Epoch 4.102: Loss = 0.724594
Epoch 4.103: Loss = 0.649261
Epoch 4.104: Loss = 0.749817
Epoch 4.105: Loss = 0.724579
Epoch 4.106: Loss = 0.676178
Epoch 4.107: Loss = 0.670074
Epoch 4.108: Loss = 0.652695
Epoch 4.109: Loss = 0.56572
Epoch 4.110: Loss = 0.757187
Epoch 4.111: Loss = 0.716049
Epoch 4.112: Loss = 0.665451
Epoch 4.113: Loss = 0.667023
Epoch 4.114: Loss = 0.658966
Epoch 4.115: Loss = 0.727753
Epoch 4.116: Loss = 0.56572
Epoch 4.117: Loss = 0.667755
Epoch 4.118: Loss = 0.845032
Epoch 4.119: Loss = 0.700378
Epoch 4.120: Loss = 0.627014
TRAIN LOSS = 0.679306
TRAIN ACC = 79.3701 % (47624/60000)
Loss = 0.5914
Loss = 0.789673
Loss = 0.666412
Loss = 0.589844
Loss = 0.669754
Loss = 0.814377
Loss = 0.842239
Loss = 0.845413
Loss = 0.696487
Loss = 0.648849
Loss = 0.824326
Loss = 0.848175
Loss = 0.782166
Loss = 0.733322
Loss = 0.72113
Loss = 0.794678
Loss = 0.657516
Loss = 0.744247
Loss = 0.784927
Loss = 0.730148
TEST LOSS = 0.738754
TEST ACC = 476.239 % (7848/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.703049
Epoch 5.2: Loss = 0.715729
Epoch 5.3: Loss = 0.790466
Epoch 5.4: Loss = 0.539597
Epoch 5.5: Loss = 0.771164
Epoch 5.6: Loss = 0.765289
Epoch 5.7: Loss = 0.633453
Epoch 5.8: Loss = 0.710342
Epoch 5.9: Loss = 0.805115
Epoch 5.10: Loss = 0.658234
Epoch 5.11: Loss = 0.698715
Epoch 5.12: Loss = 0.723938
Epoch 5.13: Loss = 0.750214
Epoch 5.14: Loss = 0.688431
Epoch 5.15: Loss = 0.725937
Epoch 5.16: Loss = 0.636307
Epoch 5.17: Loss = 0.696167
Epoch 5.18: Loss = 0.632309
Epoch 5.19: Loss = 0.66478
Epoch 5.20: Loss = 0.653809
Epoch 5.21: Loss = 0.681107
Epoch 5.22: Loss = 0.715057
Epoch 5.23: Loss = 0.63855
Epoch 5.24: Loss = 0.618576
Epoch 5.25: Loss = 0.717102
Epoch 5.26: Loss = 0.764267
Epoch 5.27: Loss = 0.646042
Epoch 5.28: Loss = 0.544662
Epoch 5.29: Loss = 0.810608
Epoch 5.30: Loss = 0.769257
Epoch 5.31: Loss = 0.784042
Epoch 5.32: Loss = 0.812775
Epoch 5.33: Loss = 0.720718
Epoch 5.34: Loss = 0.772202
Epoch 5.35: Loss = 0.57077
Epoch 5.36: Loss = 0.612167
Epoch 5.37: Loss = 0.631927
Epoch 5.38: Loss = 0.647934
Epoch 5.39: Loss = 0.671143
Epoch 5.40: Loss = 0.723999
Epoch 5.41: Loss = 0.718491
Epoch 5.42: Loss = 0.715576
Epoch 5.43: Loss = 0.6633
Epoch 5.44: Loss = 0.666443
Epoch 5.45: Loss = 0.711014
Epoch 5.46: Loss = 0.655411
Epoch 5.47: Loss = 0.602325
Epoch 5.48: Loss = 0.574097
Epoch 5.49: Loss = 0.731918
Epoch 5.50: Loss = 0.785065
Epoch 5.51: Loss = 0.7173
Epoch 5.52: Loss = 0.715652
Epoch 5.53: Loss = 0.674408
Epoch 5.54: Loss = 0.711716
Epoch 5.55: Loss = 0.698685
Epoch 5.56: Loss = 0.699829
Epoch 5.57: Loss = 0.67247
Epoch 5.58: Loss = 0.60202
Epoch 5.59: Loss = 0.772141
Epoch 5.60: Loss = 0.57608
Epoch 5.61: Loss = 0.724548
Epoch 5.62: Loss = 0.710449
Epoch 5.63: Loss = 0.702087
Epoch 5.64: Loss = 0.632874
Epoch 5.65: Loss = 0.636276
Epoch 5.66: Loss = 0.703659
Epoch 5.67: Loss = 0.645538
Epoch 5.68: Loss = 0.719147
Epoch 5.69: Loss = 0.776688
Epoch 5.70: Loss = 0.710037
Epoch 5.71: Loss = 0.609161
Epoch 5.72: Loss = 0.709915
Epoch 5.73: Loss = 0.670502
Epoch 5.74: Loss = 0.702972
Epoch 5.75: Loss = 0.694214
Epoch 5.76: Loss = 0.716385
Epoch 5.77: Loss = 0.568512
Epoch 5.78: Loss = 0.612473
Epoch 5.79: Loss = 0.722122
Epoch 5.80: Loss = 0.705948
Epoch 5.81: Loss = 0.751831
Epoch 5.82: Loss = 0.772598
Epoch 5.83: Loss = 0.634521
Epoch 5.84: Loss = 0.738922
Epoch 5.85: Loss = 0.876297
Epoch 5.86: Loss = 0.611313
Epoch 5.87: Loss = 0.712677
Epoch 5.88: Loss = 0.805786
Epoch 5.89: Loss = 0.766342
Epoch 5.90: Loss = 0.651779
Epoch 5.91: Loss = 0.731461
Epoch 5.92: Loss = 0.568283
Epoch 5.93: Loss = 0.619827
Epoch 5.94: Loss = 0.741394
Epoch 5.95: Loss = 0.742706
Epoch 5.96: Loss = 0.741837
Epoch 5.97: Loss = 0.658112
Epoch 5.98: Loss = 0.715851
Epoch 5.99: Loss = 0.762344
Epoch 5.100: Loss = 0.844894
Epoch 5.101: Loss = 0.806229
Epoch 5.102: Loss = 0.583176
Epoch 5.103: Loss = 0.648987
Epoch 5.104: Loss = 0.74971
Epoch 5.105: Loss = 0.697296
Epoch 5.106: Loss = 0.625916
Epoch 5.107: Loss = 0.843613
Epoch 5.108: Loss = 0.665619
Epoch 5.109: Loss = 0.757751
Epoch 5.110: Loss = 0.881454
Epoch 5.111: Loss = 0.80188
Epoch 5.112: Loss = 0.629517
Epoch 5.113: Loss = 0.814011
Epoch 5.114: Loss = 0.794846
Epoch 5.115: Loss = 0.664886
Epoch 5.116: Loss = 0.674835
Epoch 5.117: Loss = 0.714508
Epoch 5.118: Loss = 0.724472
Epoch 5.119: Loss = 0.669205
Epoch 5.120: Loss = 0.637619
TRAIN LOSS = 0.699905
TRAIN ACC = 79.7791 % (47869/60000)
Loss = 0.593018
Loss = 0.797989
Loss = 0.649567
Loss = 0.605042
Loss = 0.674774
Loss = 0.817276
Loss = 0.862518
Loss = 0.806091
Loss = 0.705078
Loss = 0.649658
Loss = 0.826752
Loss = 0.845306
Loss = 0.808426
Loss = 0.76297
Loss = 0.698898
Loss = 0.796982
Loss = 0.654083
Loss = 0.739563
Loss = 0.757553
Loss = 0.715195
TEST LOSS = 0.738337
TEST ACC = 478.69 % (7905/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.656677
Epoch 6.2: Loss = 0.727036
Epoch 6.3: Loss = 0.760422
Epoch 6.4: Loss = 0.827805
Epoch 6.5: Loss = 0.723099
Epoch 6.6: Loss = 0.800995
Epoch 6.7: Loss = 0.651596
Epoch 6.8: Loss = 0.812469
Epoch 6.9: Loss = 0.821869
Epoch 6.10: Loss = 0.723785
Epoch 6.11: Loss = 0.742889
Epoch 6.12: Loss = 0.634521
Epoch 6.13: Loss = 0.714691
Epoch 6.14: Loss = 0.736969
Epoch 6.15: Loss = 0.548447
Epoch 6.16: Loss = 0.69101
Epoch 6.17: Loss = 0.788452
Epoch 6.18: Loss = 0.596878
Epoch 6.19: Loss = 0.74588
Epoch 6.20: Loss = 0.631866
Epoch 6.21: Loss = 0.605896
Epoch 6.22: Loss = 0.719864
Epoch 6.23: Loss = 0.710907
Epoch 6.24: Loss = 0.731934
Epoch 6.25: Loss = 0.581604
Epoch 6.26: Loss = 0.598251
Epoch 6.27: Loss = 0.662323
Epoch 6.28: Loss = 0.688217
Epoch 6.29: Loss = 0.803482
Epoch 6.30: Loss = 0.74942
Epoch 6.31: Loss = 0.605423
Epoch 6.32: Loss = 0.721664
Epoch 6.33: Loss = 0.760406
Epoch 6.34: Loss = 0.778717
Epoch 6.35: Loss = 0.577209
Epoch 6.36: Loss = 0.667343
Epoch 6.37: Loss = 0.605621
Epoch 6.38: Loss = 0.621063
Epoch 6.39: Loss = 0.691833
Epoch 6.40: Loss = 0.557343
Epoch 6.41: Loss = 0.675018
Epoch 6.42: Loss = 0.699966
Epoch 6.43: Loss = 0.679138
Epoch 6.44: Loss = 0.689423
Epoch 6.45: Loss = 0.673859
Epoch 6.46: Loss = 0.525085
Epoch 6.47: Loss = 0.672348
Epoch 6.48: Loss = 0.727478
Epoch 6.49: Loss = 0.62915
Epoch 6.50: Loss = 0.597626
Epoch 6.51: Loss = 0.66008
Epoch 6.52: Loss = 0.75882
Epoch 6.53: Loss = 0.783981
Epoch 6.54: Loss = 0.693253
Epoch 6.55: Loss = 0.716415
Epoch 6.56: Loss = 0.75766
Epoch 6.57: Loss = 0.682419
Epoch 6.58: Loss = 0.78244
Epoch 6.59: Loss = 0.729538
Epoch 6.60: Loss = 0.62822
Epoch 6.61: Loss = 0.79158
Epoch 6.62: Loss = 0.686172
Epoch 6.63: Loss = 0.683319
Epoch 6.64: Loss = 0.635376
Epoch 6.65: Loss = 0.812744
Epoch 6.66: Loss = 0.760239
Epoch 6.67: Loss = 0.680298
Epoch 6.68: Loss = 0.714874
Epoch 6.69: Loss = 0.708206
Epoch 6.70: Loss = 0.609131
Epoch 6.71: Loss = 0.665802
Epoch 6.72: Loss = 0.816711
Epoch 6.73: Loss = 0.643677
Epoch 6.74: Loss = 0.703278
Epoch 6.75: Loss = 0.817657
Epoch 6.76: Loss = 0.700897
Epoch 6.77: Loss = 0.584015
Epoch 6.78: Loss = 0.728867
Epoch 6.79: Loss = 0.61087
Epoch 6.80: Loss = 0.666748
Epoch 6.81: Loss = 0.721863
Epoch 6.82: Loss = 0.766754
Epoch 6.83: Loss = 0.772491
Epoch 6.84: Loss = 0.727753
Epoch 6.85: Loss = 0.717331
Epoch 6.86: Loss = 0.806488
Epoch 6.87: Loss = 0.630829
Epoch 6.88: Loss = 0.650665
Epoch 6.89: Loss = 0.811554
Epoch 6.90: Loss = 0.923096
Epoch 6.91: Loss = 0.713562
Epoch 6.92: Loss = 0.657166
Epoch 6.93: Loss = 0.790421
Epoch 6.94: Loss = 0.720856
Epoch 6.95: Loss = 0.712738
Epoch 6.96: Loss = 0.765259
Epoch 6.97: Loss = 0.695313
Epoch 6.98: Loss = 0.818146
Epoch 6.99: Loss = 0.743301
Epoch 6.100: Loss = 0.745407
Epoch 6.101: Loss = 0.743988
Epoch 6.102: Loss = 0.648819
Epoch 6.103: Loss = 0.749298
Epoch 6.104: Loss = 0.61586
Epoch 6.105: Loss = 0.63887
Epoch 6.106: Loss = 0.876923
Epoch 6.107: Loss = 0.713409
Epoch 6.108: Loss = 0.657715
Epoch 6.109: Loss = 0.752411
Epoch 6.110: Loss = 0.795639
Epoch 6.111: Loss = 0.805374
Epoch 6.112: Loss = 0.64978
Epoch 6.113: Loss = 0.706284
Epoch 6.114: Loss = 0.731232
Epoch 6.115: Loss = 0.760208
Epoch 6.116: Loss = 0.763443
Epoch 6.117: Loss = 0.788345
Epoch 6.118: Loss = 0.870422
Epoch 6.119: Loss = 0.689087
Epoch 6.120: Loss = 0.797913
TRAIN LOSS = 0.710052
TRAIN ACC = 80.2856 % (48174/60000)
Loss = 0.602982
Loss = 0.751312
Loss = 0.652954
Loss = 0.580048
Loss = 0.658051
Loss = 0.836334
Loss = 0.862091
Loss = 0.772354
Loss = 0.714417
Loss = 0.67511
Loss = 0.853882
Loss = 0.82254
Loss = 0.806503
Loss = 0.780548
Loss = 0.686386
Loss = 0.774933
Loss = 0.680527
Loss = 0.71199
Loss = 0.730133
Loss = 0.72139
TEST LOSS = 0.733724
TEST ACC = 481.74 % (7936/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.522247
Epoch 7.2: Loss = 0.759842
Epoch 7.3: Loss = 0.721741
Epoch 7.4: Loss = 0.718063
Epoch 7.5: Loss = 0.553116
Epoch 7.6: Loss = 0.738739
Epoch 7.7: Loss = 0.73407
Epoch 7.8: Loss = 0.634171
Epoch 7.9: Loss = 0.681259
Epoch 7.10: Loss = 0.747543
Epoch 7.11: Loss = 0.62088
Epoch 7.12: Loss = 0.738739
Epoch 7.13: Loss = 0.727203
Epoch 7.14: Loss = 0.785126
Epoch 7.15: Loss = 0.786453
Epoch 7.16: Loss = 0.487732
Epoch 7.17: Loss = 0.580261
Epoch 7.18: Loss = 0.89595
Epoch 7.19: Loss = 0.773483
Epoch 7.20: Loss = 0.673615
Epoch 7.21: Loss = 0.67662
Epoch 7.22: Loss = 0.785049
Epoch 7.23: Loss = 0.761154
Epoch 7.24: Loss = 0.746964
Epoch 7.25: Loss = 0.782837
Epoch 7.26: Loss = 0.787155
Epoch 7.27: Loss = 0.654846
Epoch 7.28: Loss = 0.698486
Epoch 7.29: Loss = 0.76001
Epoch 7.30: Loss = 0.806183
Epoch 7.31: Loss = 0.660843
Epoch 7.32: Loss = 0.726791
Epoch 7.33: Loss = 0.648895
Epoch 7.34: Loss = 0.648087
Epoch 7.35: Loss = 0.734619
Epoch 7.36: Loss = 0.718109
Epoch 7.37: Loss = 0.78154
Epoch 7.38: Loss = 0.721848
Epoch 7.39: Loss = 0.800903
Epoch 7.40: Loss = 0.594711
Epoch 7.41: Loss = 0.665344
Epoch 7.42: Loss = 0.818649
Epoch 7.43: Loss = 0.71402
Epoch 7.44: Loss = 0.715118
Epoch 7.45: Loss = 0.654053
Epoch 7.46: Loss = 0.826828
Epoch 7.47: Loss = 0.665176
Epoch 7.48: Loss = 0.635544
Epoch 7.49: Loss = 0.594406
Epoch 7.50: Loss = 0.752884
Epoch 7.51: Loss = 0.736298
Epoch 7.52: Loss = 0.721909
Epoch 7.53: Loss = 0.687454
Epoch 7.54: Loss = 0.651459
Epoch 7.55: Loss = 0.830093
Epoch 7.56: Loss = 0.739182
Epoch 7.57: Loss = 0.756165
Epoch 7.58: Loss = 0.677475
Epoch 7.59: Loss = 0.767838
Epoch 7.60: Loss = 0.697937
Epoch 7.61: Loss = 0.662216
Epoch 7.62: Loss = 0.719116
Epoch 7.63: Loss = 0.703568
Epoch 7.64: Loss = 0.755966
Epoch 7.65: Loss = 0.751465
Epoch 7.66: Loss = 0.531235
Epoch 7.67: Loss = 0.83873
Epoch 7.68: Loss = 0.733765
Epoch 7.69: Loss = 0.67926
Epoch 7.70: Loss = 0.880615
Epoch 7.71: Loss = 0.767517
Epoch 7.72: Loss = 0.680054
Epoch 7.73: Loss = 0.56012
Epoch 7.74: Loss = 0.572357
Epoch 7.75: Loss = 0.895767
Epoch 7.76: Loss = 0.652191
Epoch 7.77: Loss = 0.672333
Epoch 7.78: Loss = 0.85318
Epoch 7.79: Loss = 0.745255
Epoch 7.80: Loss = 0.868698
Epoch 7.81: Loss = 0.905014
Epoch 7.82: Loss = 0.832382
Epoch 7.83: Loss = 0.617889
Epoch 7.84: Loss = 0.586716
Epoch 7.85: Loss = 0.685547
Epoch 7.86: Loss = 0.790894
Epoch 7.87: Loss = 0.531082
Epoch 7.88: Loss = 0.633972
Epoch 7.89: Loss = 0.582397
Epoch 7.90: Loss = 0.809082
Epoch 7.91: Loss = 0.637421
Epoch 7.92: Loss = 0.718323
Epoch 7.93: Loss = 0.748886
Epoch 7.94: Loss = 0.708588
Epoch 7.95: Loss = 0.869843
Epoch 7.96: Loss = 0.689346
Epoch 7.97: Loss = 0.830841
Epoch 7.98: Loss = 0.745331
Epoch 7.99: Loss = 0.723969
Epoch 7.100: Loss = 0.796326
Epoch 7.101: Loss = 0.546585
Epoch 7.102: Loss = 0.665009
Epoch 7.103: Loss = 0.568069
Epoch 7.104: Loss = 0.742599
Epoch 7.105: Loss = 0.800613
Epoch 7.106: Loss = 0.753906
Epoch 7.107: Loss = 0.747681
Epoch 7.108: Loss = 0.649445
Epoch 7.109: Loss = 0.66539
Epoch 7.110: Loss = 0.840683
Epoch 7.111: Loss = 0.690109
Epoch 7.112: Loss = 0.741928
Epoch 7.113: Loss = 0.765411
Epoch 7.114: Loss = 0.713135
Epoch 7.115: Loss = 0.806122
Epoch 7.116: Loss = 0.617538
Epoch 7.117: Loss = 0.71257
Epoch 7.118: Loss = 0.768509
Epoch 7.119: Loss = 0.777939
Epoch 7.120: Loss = 0.721115
TRAIN LOSS = 0.716217
TRAIN ACC = 80.7953 % (48480/60000)
Loss = 0.612732
Loss = 0.799026
Loss = 0.696579
Loss = 0.582382
Loss = 0.682846
Loss = 0.847137
Loss = 0.894669
Loss = 0.803802
Loss = 0.74147
Loss = 0.723862
Loss = 0.867126
Loss = 0.848328
Loss = 0.825745
Loss = 0.800888
Loss = 0.707047
Loss = 0.795547
Loss = 0.676758
Loss = 0.740189
Loss = 0.770218
Loss = 0.698334
TEST LOSS = 0.755734
TEST ACC = 484.799 % (7978/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.666214
Epoch 8.2: Loss = 0.669937
Epoch 8.3: Loss = 0.788773
Epoch 8.4: Loss = 0.62001
Epoch 8.5: Loss = 0.687119
Epoch 8.6: Loss = 0.701401
Epoch 8.7: Loss = 0.777664
Epoch 8.8: Loss = 0.640228
Epoch 8.9: Loss = 0.603119
Epoch 8.10: Loss = 0.8936
Epoch 8.11: Loss = 0.819275
Epoch 8.12: Loss = 0.752289
Epoch 8.13: Loss = 0.610794
Epoch 8.14: Loss = 0.885376
Epoch 8.15: Loss = 0.899231
Epoch 8.16: Loss = 0.822495
Epoch 8.17: Loss = 0.611526
Epoch 8.18: Loss = 0.703384
Epoch 8.19: Loss = 0.68808
Epoch 8.20: Loss = 0.811432
Epoch 8.21: Loss = 0.94989
Epoch 8.22: Loss = 0.566147
Epoch 8.23: Loss = 0.734161
Epoch 8.24: Loss = 0.729355
Epoch 8.25: Loss = 33.9236
Epoch 8.26: Loss = 20.0484
Epoch 8.27: Loss = 7.80988
Epoch 8.28: Loss = 3.593
Epoch 8.29: Loss = 3.02428
Epoch 8.30: Loss = 2.65543
Epoch 8.31: Loss = 2.63661
Epoch 8.32: Loss = 1.87248
Epoch 8.33: Loss = 1.66075
Epoch 8.34: Loss = 1.7664
Epoch 8.35: Loss = 1.36571
Epoch 8.36: Loss = 1.04605
Epoch 8.37: Loss = 1.04538
Epoch 8.38: Loss = 1.05269
Epoch 8.39: Loss = 0.952728
Epoch 8.40: Loss = 0.764862
Epoch 8.41: Loss = 0.90773
Epoch 8.42: Loss = 0.72261
Epoch 8.43: Loss = 1.03563
Epoch 8.44: Loss = 0.842972
Epoch 8.45: Loss = 0.973724
Epoch 8.46: Loss = 0.865173
Epoch 8.47: Loss = 0.924698
Epoch 8.48: Loss = 0.877365
Epoch 8.49: Loss = 0.957184
Epoch 8.50: Loss = 0.877426
Epoch 8.51: Loss = 0.620895
Epoch 8.52: Loss = 0.880829
Epoch 8.53: Loss = 0.919846
Epoch 8.54: Loss = 0.995087
Epoch 8.55: Loss = 1.06021
Epoch 8.56: Loss = 0.780792
Epoch 8.57: Loss = 0.795349
Epoch 8.58: Loss = 0.910202
Epoch 8.59: Loss = 0.857727
Epoch 8.60: Loss = 0.829529
Epoch 8.61: Loss = 0.705109
Epoch 8.62: Loss = 0.758255
Epoch 8.63: Loss = 0.844788
Epoch 8.64: Loss = 0.760437
Epoch 8.65: Loss = 0.812042
Epoch 8.66: Loss = 0.775406
Epoch 8.67: Loss = 0.862732
Epoch 8.68: Loss = 0.573166
Epoch 8.69: Loss = 0.699112
Epoch 8.70: Loss = 0.799088
Epoch 8.71: Loss = 0.740143
Epoch 8.72: Loss = 0.849854
Epoch 8.73: Loss = 0.805283
Epoch 8.74: Loss = 0.709106
Epoch 8.75: Loss = 0.88913
Epoch 8.76: Loss = 0.789658
Epoch 8.77: Loss = 0.878677
Epoch 8.78: Loss = 0.600998
Epoch 8.79: Loss = 0.702148
Epoch 8.80: Loss = 0.7117
Epoch 8.81: Loss = 0.675385
Epoch 8.82: Loss = 0.828262
Epoch 8.83: Loss = 0.74234
Epoch 8.84: Loss = 0.800934
Epoch 8.85: Loss = 0.952484
Epoch 8.86: Loss = 0.86882
Epoch 8.87: Loss = 0.812057
Epoch 8.88: Loss = 0.78157
Epoch 8.89: Loss = 0.783447
Epoch 8.90: Loss = 0.794296
Epoch 8.91: Loss = 0.824127
Epoch 8.92: Loss = 0.641739
Epoch 8.93: Loss = 0.996994
Epoch 8.94: Loss = 0.87796
Epoch 8.95: Loss = 0.824768
Epoch 8.96: Loss = 0.797226
Epoch 8.97: Loss = 0.867462
Epoch 8.98: Loss = 0.856537
Epoch 8.99: Loss = 0.897339
Epoch 8.100: Loss = 0.816849
Epoch 8.101: Loss = 0.732315
Epoch 8.102: Loss = 0.811203
Epoch 8.103: Loss = 0.91394
Epoch 8.104: Loss = 0.822739
Epoch 8.105: Loss = 0.543442
Epoch 8.106: Loss = 0.699509
Epoch 8.107: Loss = 0.674438
Epoch 8.108: Loss = 0.705536
Epoch 8.109: Loss = 0.773132
Epoch 8.110: Loss = 0.766312
Epoch 8.111: Loss = 0.807236
Epoch 8.112: Loss = 0.712128
Epoch 8.113: Loss = 0.840591
Epoch 8.114: Loss = 0.846573
Epoch 8.115: Loss = 0.720428
Epoch 8.116: Loss = 0.717087
Epoch 8.117: Loss = 0.924744
Epoch 8.118: Loss = 0.730728
Epoch 8.119: Loss = 0.753693
Epoch 8.120: Loss = 0.689774
TRAIN LOSS = 1.39482
TRAIN ACC = 78.0029 % (46804/60000)
Loss = 0.633896
Loss = 0.859222
Loss = 0.753204
Loss = 0.633728
Loss = 0.703003
Loss = 0.968826
Loss = 0.987961
Loss = 0.964767
Loss = 0.781189
Loss = 0.757309
Loss = 0.89772
Loss = 0.95195
Loss = 0.866638
Loss = 0.840286
Loss = 0.770859
Loss = 0.847153
Loss = 0.790268
Loss = 0.826462
Loss = 0.874191
Loss = 0.897766
TEST LOSS = 0.83032
TEST ACC = 468.039 % (7952/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.688858
Epoch 9.2: Loss = 0.898819
Epoch 9.3: Loss = 0.750427
Epoch 9.4: Loss = 0.900131
Epoch 9.5: Loss = 0.771652
Epoch 9.6: Loss = 0.773193
Epoch 9.7: Loss = 0.806564
Epoch 9.8: Loss = 0.759109
Epoch 9.9: Loss = 0.902527
Epoch 9.10: Loss = 0.951675
Epoch 9.11: Loss = 0.598923
Epoch 9.12: Loss = 0.792496
Epoch 9.13: Loss = 0.978882
Epoch 9.14: Loss = 0.73468
Epoch 9.15: Loss = 0.771683
Epoch 9.16: Loss = 0.668655
Epoch 9.17: Loss = 0.844666
Epoch 9.18: Loss = 0.811615
Epoch 9.19: Loss = 0.720657
Epoch 9.20: Loss = 0.945023
Epoch 9.21: Loss = 0.631119
Epoch 9.22: Loss = 0.795166
Epoch 9.23: Loss = 0.835159
Epoch 9.24: Loss = 0.642181
Epoch 9.25: Loss = 0.901855
Epoch 9.26: Loss = 0.914673
Epoch 9.27: Loss = 0.822617
Epoch 9.28: Loss = 0.718796
Epoch 9.29: Loss = 0.729736
Epoch 9.30: Loss = 0.912521
Epoch 9.31: Loss = 0.639618
Epoch 9.32: Loss = 0.728897
Epoch 9.33: Loss = 0.676636
Epoch 9.34: Loss = 0.666031
Epoch 9.35: Loss = 0.691406
Epoch 9.36: Loss = 0.669846
Epoch 9.37: Loss = 0.868774
Epoch 9.38: Loss = 0.89238
Epoch 9.39: Loss = 0.782211
Epoch 9.40: Loss = 0.773071
Epoch 9.41: Loss = 0.822906
Epoch 9.42: Loss = 0.742706
Epoch 9.43: Loss = 0.864029
Epoch 9.44: Loss = 0.929169
Epoch 9.45: Loss = 0.920929
Epoch 9.46: Loss = 0.723923
Epoch 9.47: Loss = 0.925903
Epoch 9.48: Loss = 0.941132
Epoch 9.49: Loss = 0.593933
Epoch 9.50: Loss = 0.906525
Epoch 9.51: Loss = 1.02856
Epoch 9.52: Loss = 0.898849
Epoch 9.53: Loss = 0.770477
Epoch 9.54: Loss = 0.856415
Epoch 9.55: Loss = 0.783691
Epoch 9.56: Loss = 0.813339
Epoch 9.57: Loss = 0.741913
Epoch 9.58: Loss = 0.740402
Epoch 9.59: Loss = 0.756012
Epoch 9.60: Loss = 0.627762
Epoch 9.61: Loss = 0.587753
Epoch 9.62: Loss = 0.779221
Epoch 9.63: Loss = 0.772202
Epoch 9.64: Loss = 0.71991
Epoch 9.65: Loss = 0.766022
Epoch 9.66: Loss = 0.714981
Epoch 9.67: Loss = 0.67749
Epoch 9.68: Loss = 0.75058
Epoch 9.69: Loss = 0.783005
Epoch 9.70: Loss = 0.796432
Epoch 9.71: Loss = 0.777054
Epoch 9.72: Loss = 0.753525
Epoch 9.73: Loss = 0.676331
Epoch 9.74: Loss = 0.724396
Epoch 9.75: Loss = 0.665955
Epoch 9.76: Loss = 0.730194
Epoch 9.77: Loss = 0.679092
Epoch 9.78: Loss = 0.735794
Epoch 9.79: Loss = 0.68956
Epoch 9.80: Loss = 0.768906
Epoch 9.81: Loss = 1.04691
Epoch 9.82: Loss = 0.722458
Epoch 9.83: Loss = 0.867264
Epoch 9.84: Loss = 0.831009
Epoch 9.85: Loss = 0.715195
Epoch 9.86: Loss = 0.782791
Epoch 9.87: Loss = 0.696945
Epoch 9.88: Loss = 0.782104
Epoch 9.89: Loss = 0.743027
Epoch 9.90: Loss = 0.622665
Epoch 9.91: Loss = 0.685074
Epoch 9.92: Loss = 0.794083
Epoch 9.93: Loss = 0.697891
Epoch 9.94: Loss = 0.71785
Epoch 9.95: Loss = 0.887451
Epoch 9.96: Loss = 0.709778
Epoch 9.97: Loss = 0.681931
Epoch 9.98: Loss = 0.714905
Epoch 9.99: Loss = 0.919296
Epoch 9.100: Loss = 0.809525
Epoch 9.101: Loss = 0.722626
Epoch 9.102: Loss = 0.752167
Epoch 9.103: Loss = 0.703445
Epoch 9.104: Loss = 0.676453
Epoch 9.105: Loss = 0.701859
Epoch 9.106: Loss = 0.682159
Epoch 9.107: Loss = 0.752457
Epoch 9.108: Loss = 0.593231
Epoch 9.109: Loss = 0.816223
Epoch 9.110: Loss = 0.936188
Epoch 9.111: Loss = 0.695801
Epoch 9.112: Loss = 0.678284
Epoch 9.113: Loss = 0.817459
Epoch 9.114: Loss = 0.663727
Epoch 9.115: Loss = 0.744492
Epoch 9.116: Loss = 0.929108
Epoch 9.117: Loss = 0.8927
Epoch 9.118: Loss = 0.682037
Epoch 9.119: Loss = 0.760681
Epoch 9.120: Loss = 0.653137
TRAIN LOSS = 0.77153
TRAIN ACC = 80.7007 % (48423/60000)
Loss = 0.639679
Loss = 0.915131
Loss = 0.693146
Loss = 0.665268
Loss = 0.709412
Loss = 0.907028
Loss = 0.95488
Loss = 0.907822
Loss = 0.764847
Loss = 0.723068
Loss = 0.859512
Loss = 0.931335
Loss = 0.808044
Loss = 0.831223
Loss = 0.736267
Loss = 0.807175
Loss = 0.790619
Loss = 0.796509
Loss = 0.811081
Loss = 0.806778
TEST LOSS = 0.802941
TEST ACC = 484.229 % (8029/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.763474
Epoch 10.2: Loss = 0.816223
Epoch 10.3: Loss = 0.668045
Epoch 10.4: Loss = 0.697479
Epoch 10.5: Loss = 0.760956
Epoch 10.6: Loss = 0.865829
Epoch 10.7: Loss = 0.657486
Epoch 10.8: Loss = 0.779678
Epoch 10.9: Loss = 0.819977
Epoch 10.10: Loss = 0.637909
Epoch 10.11: Loss = 0.69014
Epoch 10.12: Loss = 0.805298
Epoch 10.13: Loss = 0.77037
Epoch 10.14: Loss = 0.879669
Epoch 10.15: Loss = 0.89679
Epoch 10.16: Loss = 0.784256
Epoch 10.17: Loss = 0.717194
Epoch 10.18: Loss = 0.88649
Epoch 10.19: Loss = 0.577484
Epoch 10.20: Loss = 0.855957
Epoch 10.21: Loss = 0.659348
Epoch 10.22: Loss = 0.764679
Epoch 10.23: Loss = 0.746262
Epoch 10.24: Loss = 0.876099
Epoch 10.25: Loss = 0.697479
Epoch 10.26: Loss = 0.736435
Epoch 10.27: Loss = 0.737076
Epoch 10.28: Loss = 0.78923
Epoch 10.29: Loss = 0.877182
Epoch 10.30: Loss = 0.837021
Epoch 10.31: Loss = 0.777481
Epoch 10.32: Loss = 0.759781
Epoch 10.33: Loss = 0.666885
Epoch 10.34: Loss = 0.819397
Epoch 10.35: Loss = 0.638931
Epoch 10.36: Loss = 0.663528
Epoch 10.37: Loss = 0.759903
Epoch 10.38: Loss = 0.671295
Epoch 10.39: Loss = 0.810562
Epoch 10.40: Loss = 0.793594
Epoch 10.41: Loss = 0.634293
Epoch 10.42: Loss = 0.739227
Epoch 10.43: Loss = 0.89003
Epoch 10.44: Loss = 0.705383
Epoch 10.45: Loss = 0.756653
Epoch 10.46: Loss = 0.741745
Epoch 10.47: Loss = 0.678406
Epoch 10.48: Loss = 0.718643
Epoch 10.49: Loss = 0.655502
Epoch 10.50: Loss = 0.804321
Epoch 10.51: Loss = 0.847809
Epoch 10.52: Loss = 0.716721
Epoch 10.53: Loss = 0.673599
Epoch 10.54: Loss = 0.65976
Epoch 10.55: Loss = 0.760666
Epoch 10.56: Loss = 0.798569
Epoch 10.57: Loss = 0.7453
Epoch 10.58: Loss = 0.922104
Epoch 10.59: Loss = 0.621841
Epoch 10.60: Loss = 0.612442
Epoch 10.61: Loss = 0.727707
Epoch 10.62: Loss = 0.822189
Epoch 10.63: Loss = 0.761948
Epoch 10.64: Loss = 0.865265
Epoch 10.65: Loss = 1.03656
Epoch 10.66: Loss = 0.77533
Epoch 10.67: Loss = 0.908417
Epoch 10.68: Loss = 0.807465
Epoch 10.69: Loss = 0.782318
Epoch 10.70: Loss = 0.743668
Epoch 10.71: Loss = 0.718597
Epoch 10.72: Loss = 0.832169
Epoch 10.73: Loss = 0.789047
Epoch 10.74: Loss = 0.816849
Epoch 10.75: Loss = 0.821457
Epoch 10.76: Loss = 0.762375
Epoch 10.77: Loss = 0.702393
Epoch 10.78: Loss = 0.725479
Epoch 10.79: Loss = 0.900284
Epoch 10.80: Loss = 0.755005
Epoch 10.81: Loss = 0.810104
Epoch 10.82: Loss = 0.837509
Epoch 10.83: Loss = 0.705795
Epoch 10.84: Loss = 0.794266
Epoch 10.85: Loss = 0.77388
Epoch 10.86: Loss = 0.788895
Epoch 10.87: Loss = 0.790665
Epoch 10.88: Loss = 0.740677
Epoch 10.89: Loss = 0.842346
Epoch 10.90: Loss = 0.998962
Epoch 10.91: Loss = 0.90007
Epoch 10.92: Loss = 0.827103
Epoch 10.93: Loss = 0.74884
Epoch 10.94: Loss = 0.802383
Epoch 10.95: Loss = 0.606598
Epoch 10.96: Loss = 0.771561
Epoch 10.97: Loss = 0.755417
Epoch 10.98: Loss = 0.656723
Epoch 10.99: Loss = 0.664841
Epoch 10.100: Loss = 0.743179
Epoch 10.101: Loss = 0.910217
Epoch 10.102: Loss = 0.874985
Epoch 10.103: Loss = 0.730087
Epoch 10.104: Loss = 0.821457
Epoch 10.105: Loss = 0.699554
Epoch 10.106: Loss = 0.775833
Epoch 10.107: Loss = 0.747635
Epoch 10.108: Loss = 0.782349
Epoch 10.109: Loss = 0.711777
Epoch 10.110: Loss = 0.799805
Epoch 10.111: Loss = 0.782074
Epoch 10.112: Loss = 0.800964
Epoch 10.113: Loss = 0.806107
Epoch 10.114: Loss = 0.801941
Epoch 10.115: Loss = 0.67366
Epoch 10.116: Loss = 0.723724
Epoch 10.117: Loss = 0.801392
Epoch 10.118: Loss = 0.786057
Epoch 10.119: Loss = 0.841492
Epoch 10.120: Loss = 0.588943
TRAIN LOSS = 0.768097
TRAIN ACC = 80.9631 % (48580/60000)
Loss = 0.669006
Loss = 0.912964
Loss = 0.710312
Loss = 0.652206
Loss = 0.701569
Loss = 0.936661
Loss = 0.985184
Loss = 0.939209
Loss = 0.838669
Loss = 0.722412
Loss = 0.952118
Loss = 0.934189
Loss = 0.863037
Loss = 0.876724
Loss = 0.77388
Loss = 0.782623
Loss = 0.774994
Loss = 0.849075
Loss = 0.792236
Loss = 0.776031
TEST LOSS = 0.822155
TEST ACC = 485.799 % (8023/10000)

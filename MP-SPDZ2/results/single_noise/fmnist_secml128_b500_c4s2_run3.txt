Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.36923
Epoch 1.2: Loss = 2.27829
Epoch 1.3: Loss = 2.2363
Epoch 1.4: Loss = 2.19388
Epoch 1.5: Loss = 2.12341
Epoch 1.6: Loss = 2.09146
Epoch 1.7: Loss = 2.01892
Epoch 1.8: Loss = 1.99191
Epoch 1.9: Loss = 1.94434
Epoch 1.10: Loss = 1.93198
Epoch 1.11: Loss = 1.86993
Epoch 1.12: Loss = 1.80286
Epoch 1.13: Loss = 1.78879
Epoch 1.14: Loss = 1.75317
Epoch 1.15: Loss = 1.69926
Epoch 1.16: Loss = 1.67061
Epoch 1.17: Loss = 1.60535
Epoch 1.18: Loss = 1.63596
Epoch 1.19: Loss = 1.57178
Epoch 1.20: Loss = 1.5056
Epoch 1.21: Loss = 1.49527
Epoch 1.22: Loss = 1.42455
Epoch 1.23: Loss = 1.51175
Epoch 1.24: Loss = 1.43053
Epoch 1.25: Loss = 1.34839
Epoch 1.26: Loss = 1.37495
Epoch 1.27: Loss = 1.34082
Epoch 1.28: Loss = 1.35921
Epoch 1.29: Loss = 1.36177
Epoch 1.30: Loss = 1.26207
Epoch 1.31: Loss = 1.233
Epoch 1.32: Loss = 1.26758
Epoch 1.33: Loss = 1.26308
Epoch 1.34: Loss = 1.15639
Epoch 1.35: Loss = 1.16579
Epoch 1.36: Loss = 1.16383
Epoch 1.37: Loss = 1.20903
Epoch 1.38: Loss = 1.09908
Epoch 1.39: Loss = 1.16689
Epoch 1.40: Loss = 1.14851
Epoch 1.41: Loss = 1.10982
Epoch 1.42: Loss = 1.07555
Epoch 1.43: Loss = 1.08479
Epoch 1.44: Loss = 1.00682
Epoch 1.45: Loss = 1.08301
Epoch 1.46: Loss = 1.04085
Epoch 1.47: Loss = 1.0247
Epoch 1.48: Loss = 1.01338
Epoch 1.49: Loss = 0.966751
Epoch 1.50: Loss = 0.977982
Epoch 1.51: Loss = 1.05809
Epoch 1.52: Loss = 1.0425
Epoch 1.53: Loss = 1.02127
Epoch 1.54: Loss = 0.973267
Epoch 1.55: Loss = 0.96785
Epoch 1.56: Loss = 0.910904
Epoch 1.57: Loss = 0.960403
Epoch 1.58: Loss = 0.955383
Epoch 1.59: Loss = 0.930634
Epoch 1.60: Loss = 0.922653
Epoch 1.61: Loss = 0.957062
Epoch 1.62: Loss = 0.89183
Epoch 1.63: Loss = 0.861877
Epoch 1.64: Loss = 0.890518
Epoch 1.65: Loss = 0.888489
Epoch 1.66: Loss = 0.915771
Epoch 1.67: Loss = 0.935471
Epoch 1.68: Loss = 0.927322
Epoch 1.69: Loss = 0.876938
Epoch 1.70: Loss = 0.849533
Epoch 1.71: Loss = 0.839523
Epoch 1.72: Loss = 0.880127
Epoch 1.73: Loss = 0.923645
Epoch 1.74: Loss = 0.873413
Epoch 1.75: Loss = 0.863693
Epoch 1.76: Loss = 0.891403
Epoch 1.77: Loss = 0.837585
Epoch 1.78: Loss = 0.828049
Epoch 1.79: Loss = 0.885696
Epoch 1.80: Loss = 0.839859
Epoch 1.81: Loss = 0.861526
Epoch 1.82: Loss = 0.789291
Epoch 1.83: Loss = 0.819672
Epoch 1.84: Loss = 0.808197
Epoch 1.85: Loss = 0.855072
Epoch 1.86: Loss = 0.794464
Epoch 1.87: Loss = 0.863754
Epoch 1.88: Loss = 0.845566
Epoch 1.89: Loss = 0.804489
Epoch 1.90: Loss = 0.799011
Epoch 1.91: Loss = 0.817169
Epoch 1.92: Loss = 0.758514
Epoch 1.93: Loss = 0.770203
Epoch 1.94: Loss = 0.830887
Epoch 1.95: Loss = 0.862961
Epoch 1.96: Loss = 0.792908
Epoch 1.97: Loss = 0.823517
Epoch 1.98: Loss = 0.751984
Epoch 1.99: Loss = 0.825455
Epoch 1.100: Loss = 0.784897
Epoch 1.101: Loss = 0.728424
Epoch 1.102: Loss = 0.818848
Epoch 1.103: Loss = 0.755615
Epoch 1.104: Loss = 0.772842
Epoch 1.105: Loss = 0.730072
Epoch 1.106: Loss = 0.782639
Epoch 1.107: Loss = 0.868607
Epoch 1.108: Loss = 0.82988
Epoch 1.109: Loss = 0.741806
Epoch 1.110: Loss = 0.781067
Epoch 1.111: Loss = 0.827438
Epoch 1.112: Loss = 0.762634
Epoch 1.113: Loss = 0.793182
Epoch 1.114: Loss = 0.745209
Epoch 1.115: Loss = 0.830597
Epoch 1.116: Loss = 0.749802
Epoch 1.117: Loss = 0.813217
Epoch 1.118: Loss = 0.752319
Epoch 1.119: Loss = 0.763824
Epoch 1.120: Loss = 0.813675
TRAIN LOSS = 1.11311
TRAIN ACC = 63.0219 % (37815/60000)
Loss = 0.692276
Loss = 0.785858
Loss = 0.800995
Loss = 0.707153
Loss = 0.705536
Loss = 0.84288
Loss = 0.86322
Loss = 0.841125
Loss = 0.732605
Loss = 0.691635
Loss = 0.825851
Loss = 0.766693
Loss = 0.781647
Loss = 0.788589
Loss = 0.733902
Loss = 0.804932
Loss = 0.697891
Loss = 0.744583
Loss = 0.782043
Loss = 0.792252
TEST LOSS = 0.769083
TEST ACC = 378.149 % (7177/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.715179
Epoch 2.2: Loss = 0.846527
Epoch 2.3: Loss = 0.771515
Epoch 2.4: Loss = 0.733246
Epoch 2.5: Loss = 0.721115
Epoch 2.6: Loss = 0.749741
Epoch 2.7: Loss = 0.702438
Epoch 2.8: Loss = 0.722504
Epoch 2.9: Loss = 0.752502
Epoch 2.10: Loss = 0.713943
Epoch 2.11: Loss = 0.756668
Epoch 2.12: Loss = 0.789673
Epoch 2.13: Loss = 0.749496
Epoch 2.14: Loss = 0.780685
Epoch 2.15: Loss = 0.691696
Epoch 2.16: Loss = 0.73291
Epoch 2.17: Loss = 0.738647
Epoch 2.18: Loss = 0.739639
Epoch 2.19: Loss = 0.806519
Epoch 2.20: Loss = 0.671326
Epoch 2.21: Loss = 0.697052
Epoch 2.22: Loss = 0.766556
Epoch 2.23: Loss = 0.787872
Epoch 2.24: Loss = 0.743103
Epoch 2.25: Loss = 0.710831
Epoch 2.26: Loss = 0.727005
Epoch 2.27: Loss = 0.808258
Epoch 2.28: Loss = 0.701111
Epoch 2.29: Loss = 0.694061
Epoch 2.30: Loss = 0.74527
Epoch 2.31: Loss = 0.725647
Epoch 2.32: Loss = 0.721481
Epoch 2.33: Loss = 0.724045
Epoch 2.34: Loss = 0.617905
Epoch 2.35: Loss = 0.828293
Epoch 2.36: Loss = 0.67186
Epoch 2.37: Loss = 0.714447
Epoch 2.38: Loss = 0.664886
Epoch 2.39: Loss = 0.646271
Epoch 2.40: Loss = 0.631424
Epoch 2.41: Loss = 0.697525
Epoch 2.42: Loss = 0.751572
Epoch 2.43: Loss = 0.780746
Epoch 2.44: Loss = 0.68219
Epoch 2.45: Loss = 0.695663
Epoch 2.46: Loss = 0.701859
Epoch 2.47: Loss = 0.679535
Epoch 2.48: Loss = 0.85817
Epoch 2.49: Loss = 0.775253
Epoch 2.50: Loss = 0.793671
Epoch 2.51: Loss = 0.689438
Epoch 2.52: Loss = 0.701126
Epoch 2.53: Loss = 0.715775
Epoch 2.54: Loss = 0.740295
Epoch 2.55: Loss = 0.685577
Epoch 2.56: Loss = 0.714493
Epoch 2.57: Loss = 0.707336
Epoch 2.58: Loss = 0.723068
Epoch 2.59: Loss = 0.743668
Epoch 2.60: Loss = 0.721207
Epoch 2.61: Loss = 0.765808
Epoch 2.62: Loss = 0.736465
Epoch 2.63: Loss = 0.763885
Epoch 2.64: Loss = 0.715149
Epoch 2.65: Loss = 0.674255
Epoch 2.66: Loss = 0.76268
Epoch 2.67: Loss = 0.645874
Epoch 2.68: Loss = 0.761246
Epoch 2.69: Loss = 0.673386
Epoch 2.70: Loss = 0.748856
Epoch 2.71: Loss = 0.714554
Epoch 2.72: Loss = 0.748413
Epoch 2.73: Loss = 0.695801
Epoch 2.74: Loss = 0.658127
Epoch 2.75: Loss = 0.701782
Epoch 2.76: Loss = 0.706238
Epoch 2.77: Loss = 0.628265
Epoch 2.78: Loss = 0.65506
Epoch 2.79: Loss = 0.701035
Epoch 2.80: Loss = 0.73555
Epoch 2.81: Loss = 0.6698
Epoch 2.82: Loss = 0.637802
Epoch 2.83: Loss = 0.725189
Epoch 2.84: Loss = 0.796616
Epoch 2.85: Loss = 0.663376
Epoch 2.86: Loss = 0.770218
Epoch 2.87: Loss = 0.687531
Epoch 2.88: Loss = 0.622742
Epoch 2.89: Loss = 0.74202
Epoch 2.90: Loss = 0.80246
Epoch 2.91: Loss = 0.738678
Epoch 2.92: Loss = 0.708221
Epoch 2.93: Loss = 0.668655
Epoch 2.94: Loss = 0.661789
Epoch 2.95: Loss = 0.728683
Epoch 2.96: Loss = 0.624496
Epoch 2.97: Loss = 0.63356
Epoch 2.98: Loss = 0.710876
Epoch 2.99: Loss = 0.802139
Epoch 2.100: Loss = 0.645782
Epoch 2.101: Loss = 0.760254
Epoch 2.102: Loss = 0.691544
Epoch 2.103: Loss = 0.821823
Epoch 2.104: Loss = 0.657211
Epoch 2.105: Loss = 0.60437
Epoch 2.106: Loss = 0.688721
Epoch 2.107: Loss = 0.73349
Epoch 2.108: Loss = 0.669785
Epoch 2.109: Loss = 0.632523
Epoch 2.110: Loss = 0.631943
Epoch 2.111: Loss = 0.698044
Epoch 2.112: Loss = 0.691116
Epoch 2.113: Loss = 0.595901
Epoch 2.114: Loss = 0.608734
Epoch 2.115: Loss = 0.68959
Epoch 2.116: Loss = 0.642365
Epoch 2.117: Loss = 0.668274
Epoch 2.118: Loss = 0.596024
Epoch 2.119: Loss = 0.695389
Epoch 2.120: Loss = 0.646896
TRAIN LOSS = 0.711914
TRAIN ACC = 75.3448 % (45209/60000)
Loss = 0.615768
Loss = 0.738754
Loss = 0.71843
Loss = 0.616135
Loss = 0.635895
Loss = 0.801239
Loss = 0.816238
Loss = 0.795883
Loss = 0.664795
Loss = 0.629471
Loss = 0.790924
Loss = 0.721725
Loss = 0.723892
Loss = 0.715363
Loss = 0.671478
Loss = 0.753967
Loss = 0.635101
Loss = 0.710266
Loss = 0.732559
Loss = 0.744171
TEST LOSS = 0.711603
TEST ACC = 452.089 % (7546/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.78035
Epoch 3.2: Loss = 0.714813
Epoch 3.3: Loss = 0.64241
Epoch 3.4: Loss = 0.627274
Epoch 3.5: Loss = 0.663437
Epoch 3.6: Loss = 0.749664
Epoch 3.7: Loss = 0.58783
Epoch 3.8: Loss = 0.631027
Epoch 3.9: Loss = 0.68985
Epoch 3.10: Loss = 0.671326
Epoch 3.11: Loss = 0.642273
Epoch 3.12: Loss = 0.816696
Epoch 3.13: Loss = 0.700317
Epoch 3.14: Loss = 0.692169
Epoch 3.15: Loss = 0.698929
Epoch 3.16: Loss = 0.680298
Epoch 3.17: Loss = 0.677612
Epoch 3.18: Loss = 0.759705
Epoch 3.19: Loss = 0.736954
Epoch 3.20: Loss = 0.755356
Epoch 3.21: Loss = 0.72583
Epoch 3.22: Loss = 0.737823
Epoch 3.23: Loss = 0.705063
Epoch 3.24: Loss = 0.635254
Epoch 3.25: Loss = 0.667465
Epoch 3.26: Loss = 0.540405
Epoch 3.27: Loss = 0.634567
Epoch 3.28: Loss = 0.750534
Epoch 3.29: Loss = 0.64122
Epoch 3.30: Loss = 0.709503
Epoch 3.31: Loss = 0.711655
Epoch 3.32: Loss = 0.743835
Epoch 3.33: Loss = 0.676956
Epoch 3.34: Loss = 0.622818
Epoch 3.35: Loss = 0.635086
Epoch 3.36: Loss = 0.671326
Epoch 3.37: Loss = 0.599136
Epoch 3.38: Loss = 0.53685
Epoch 3.39: Loss = 0.719604
Epoch 3.40: Loss = 0.677017
Epoch 3.41: Loss = 0.764252
Epoch 3.42: Loss = 0.638885
Epoch 3.43: Loss = 0.611359
Epoch 3.44: Loss = 0.655014
Epoch 3.45: Loss = 0.575241
Epoch 3.46: Loss = 0.557266
Epoch 3.47: Loss = 0.647308
Epoch 3.48: Loss = 0.653351
Epoch 3.49: Loss = 0.727051
Epoch 3.50: Loss = 0.785004
Epoch 3.51: Loss = 0.608673
Epoch 3.52: Loss = 0.693512
Epoch 3.53: Loss = 0.710449
Epoch 3.54: Loss = 0.692474
Epoch 3.55: Loss = 0.661575
Epoch 3.56: Loss = 0.608383
Epoch 3.57: Loss = 0.549606
Epoch 3.58: Loss = 0.696945
Epoch 3.59: Loss = 0.660126
Epoch 3.60: Loss = 0.628113
Epoch 3.61: Loss = 0.76796
Epoch 3.62: Loss = 0.605453
Epoch 3.63: Loss = 0.652588
Epoch 3.64: Loss = 0.694824
Epoch 3.65: Loss = 0.744415
Epoch 3.66: Loss = 0.589691
Epoch 3.67: Loss = 0.608215
Epoch 3.68: Loss = 0.66478
Epoch 3.69: Loss = 0.786133
Epoch 3.70: Loss = 0.608536
Epoch 3.71: Loss = 0.68573
Epoch 3.72: Loss = 0.677063
Epoch 3.73: Loss = 0.648926
Epoch 3.74: Loss = 0.639267
Epoch 3.75: Loss = 0.741882
Epoch 3.76: Loss = 0.65686
Epoch 3.77: Loss = 0.624741
Epoch 3.78: Loss = 0.597427
Epoch 3.79: Loss = 0.714874
Epoch 3.80: Loss = 0.692276
Epoch 3.81: Loss = 0.774673
Epoch 3.82: Loss = 0.562683
Epoch 3.83: Loss = 0.679352
Epoch 3.84: Loss = 0.643829
Epoch 3.85: Loss = 0.762482
Epoch 3.86: Loss = 0.624359
Epoch 3.87: Loss = 0.66188
Epoch 3.88: Loss = 0.698608
Epoch 3.89: Loss = 0.687637
Epoch 3.90: Loss = 0.652786
Epoch 3.91: Loss = 0.585602
Epoch 3.92: Loss = 0.617035
Epoch 3.93: Loss = 0.739655
Epoch 3.94: Loss = 0.697586
Epoch 3.95: Loss = 0.657227
Epoch 3.96: Loss = 0.793411
Epoch 3.97: Loss = 0.703903
Epoch 3.98: Loss = 0.774063
Epoch 3.99: Loss = 0.614227
Epoch 3.100: Loss = 0.598663
Epoch 3.101: Loss = 0.628387
Epoch 3.102: Loss = 0.727753
Epoch 3.103: Loss = 0.579636
Epoch 3.104: Loss = 0.690689
Epoch 3.105: Loss = 0.664368
Epoch 3.106: Loss = 0.577942
Epoch 3.107: Loss = 0.768463
Epoch 3.108: Loss = 0.782516
Epoch 3.109: Loss = 0.557251
Epoch 3.110: Loss = 0.735428
Epoch 3.111: Loss = 0.690659
Epoch 3.112: Loss = 0.72934
Epoch 3.113: Loss = 0.702469
Epoch 3.114: Loss = 0.772552
Epoch 3.115: Loss = 0.701355
Epoch 3.116: Loss = 0.613892
Epoch 3.117: Loss = 0.764084
Epoch 3.118: Loss = 0.715759
Epoch 3.119: Loss = 0.648483
Epoch 3.120: Loss = 0.899765
TRAIN LOSS = 0.677246
TRAIN ACC = 77.8748 % (46727/60000)
Loss = 0.598373
Loss = 0.72113
Loss = 0.690628
Loss = 0.589981
Loss = 0.609863
Loss = 0.776764
Loss = 0.808594
Loss = 0.780426
Loss = 0.648666
Loss = 0.623779
Loss = 0.786957
Loss = 0.732758
Loss = 0.720062
Loss = 0.695236
Loss = 0.65358
Loss = 0.731171
Loss = 0.611191
Loss = 0.70311
Loss = 0.72171
Loss = 0.727859
TEST LOSS = 0.696592
TEST ACC = 467.27 % (7742/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.715149
Epoch 4.2: Loss = 0.695816
Epoch 4.3: Loss = 0.629425
Epoch 4.4: Loss = 0.580627
Epoch 4.5: Loss = 0.729553
Epoch 4.6: Loss = 0.713654
Epoch 4.7: Loss = 0.712067
Epoch 4.8: Loss = 0.707352
Epoch 4.9: Loss = 0.716324
Epoch 4.10: Loss = 0.664154
Epoch 4.11: Loss = 0.653229
Epoch 4.12: Loss = 0.790848
Epoch 4.13: Loss = 0.59729
Epoch 4.14: Loss = 0.603577
Epoch 4.15: Loss = 0.571777
Epoch 4.16: Loss = 0.593552
Epoch 4.17: Loss = 0.664261
Epoch 4.18: Loss = 0.666229
Epoch 4.19: Loss = 0.738663
Epoch 4.20: Loss = 0.771866
Epoch 4.21: Loss = 0.69487
Epoch 4.22: Loss = 0.601471
Epoch 4.23: Loss = 0.597
Epoch 4.24: Loss = 0.649033
Epoch 4.25: Loss = 0.733749
Epoch 4.26: Loss = 0.714661
Epoch 4.27: Loss = 0.676147
Epoch 4.28: Loss = 0.57338
Epoch 4.29: Loss = 0.642532
Epoch 4.30: Loss = 0.702118
Epoch 4.31: Loss = 0.676514
Epoch 4.32: Loss = 0.70813
Epoch 4.33: Loss = 0.604263
Epoch 4.34: Loss = 0.646713
Epoch 4.35: Loss = 0.69783
Epoch 4.36: Loss = 0.664246
Epoch 4.37: Loss = 0.579437
Epoch 4.38: Loss = 0.831131
Epoch 4.39: Loss = 0.573074
Epoch 4.40: Loss = 0.650452
Epoch 4.41: Loss = 0.562485
Epoch 4.42: Loss = 0.663696
Epoch 4.43: Loss = 0.618454
Epoch 4.44: Loss = 0.679047
Epoch 4.45: Loss = 0.733566
Epoch 4.46: Loss = 0.568527
Epoch 4.47: Loss = 0.857788
Epoch 4.48: Loss = 0.807449
Epoch 4.49: Loss = 0.618683
Epoch 4.50: Loss = 0.665802
Epoch 4.51: Loss = 0.738358
Epoch 4.52: Loss = 0.651062
Epoch 4.53: Loss = 0.610458
Epoch 4.54: Loss = 0.580841
Epoch 4.55: Loss = 0.654312
Epoch 4.56: Loss = 0.656525
Epoch 4.57: Loss = 0.597046
Epoch 4.58: Loss = 0.745193
Epoch 4.59: Loss = 0.694717
Epoch 4.60: Loss = 0.722519
Epoch 4.61: Loss = 0.622955
Epoch 4.62: Loss = 0.621399
Epoch 4.63: Loss = 0.68718
Epoch 4.64: Loss = 0.726334
Epoch 4.65: Loss = 0.701736
Epoch 4.66: Loss = 0.646301
Epoch 4.67: Loss = 0.652847
Epoch 4.68: Loss = 0.606308
Epoch 4.69: Loss = 0.654358
Epoch 4.70: Loss = 0.599411
Epoch 4.71: Loss = 0.686844
Epoch 4.72: Loss = 0.724609
Epoch 4.73: Loss = 0.702576
Epoch 4.74: Loss = 0.538925
Epoch 4.75: Loss = 0.801987
Epoch 4.76: Loss = 0.695297
Epoch 4.77: Loss = 0.673218
Epoch 4.78: Loss = 0.682236
Epoch 4.79: Loss = 0.725693
Epoch 4.80: Loss = 0.6931
Epoch 4.81: Loss = 0.698227
Epoch 4.82: Loss = 0.574554
Epoch 4.83: Loss = 0.686111
Epoch 4.84: Loss = 0.791443
Epoch 4.85: Loss = 0.625946
Epoch 4.86: Loss = 0.663574
Epoch 4.87: Loss = 0.70723
Epoch 4.88: Loss = 0.655914
Epoch 4.89: Loss = 0.638062
Epoch 4.90: Loss = 0.712997
Epoch 4.91: Loss = 0.54425
Epoch 4.92: Loss = 0.660278
Epoch 4.93: Loss = 0.589661
Epoch 4.94: Loss = 0.634766
Epoch 4.95: Loss = 0.530731
Epoch 4.96: Loss = 0.647247
Epoch 4.97: Loss = 0.711914
Epoch 4.98: Loss = 0.676056
Epoch 4.99: Loss = 0.713776
Epoch 4.100: Loss = 0.669632
Epoch 4.101: Loss = 0.776428
Epoch 4.102: Loss = 0.697159
Epoch 4.103: Loss = 0.707001
Epoch 4.104: Loss = 0.600647
Epoch 4.105: Loss = 0.676331
Epoch 4.106: Loss = 0.640427
Epoch 4.107: Loss = 0.601334
Epoch 4.108: Loss = 0.629532
Epoch 4.109: Loss = 0.733978
Epoch 4.110: Loss = 0.710022
Epoch 4.111: Loss = 0.647446
Epoch 4.112: Loss = 0.73642
Epoch 4.113: Loss = 0.705444
Epoch 4.114: Loss = 0.676163
Epoch 4.115: Loss = 0.621689
Epoch 4.116: Loss = 0.672897
Epoch 4.117: Loss = 0.622696
Epoch 4.118: Loss = 0.593445
Epoch 4.119: Loss = 0.779083
Epoch 4.120: Loss = 0.643021
TRAIN LOSS = 0.668396
TRAIN ACC = 78.8818 % (47331/60000)
Loss = 0.57753
Loss = 0.701004
Loss = 0.664169
Loss = 0.575119
Loss = 0.596436
Loss = 0.781113
Loss = 0.809967
Loss = 0.776993
Loss = 0.640091
Loss = 0.617157
Loss = 0.796051
Loss = 0.753647
Loss = 0.718323
Loss = 0.697083
Loss = 0.654373
Loss = 0.715088
Loss = 0.621796
Loss = 0.693115
Loss = 0.733307
Loss = 0.713715
TEST LOSS = 0.691804
TEST ACC = 473.309 % (7838/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.721909
Epoch 5.2: Loss = 0.522629
Epoch 5.3: Loss = 0.673569
Epoch 5.4: Loss = 0.57988
Epoch 5.5: Loss = 0.690109
Epoch 5.6: Loss = 0.732483
Epoch 5.7: Loss = 0.650299
Epoch 5.8: Loss = 0.74498
Epoch 5.9: Loss = 0.598679
Epoch 5.10: Loss = 0.539017
Epoch 5.11: Loss = 0.649979
Epoch 5.12: Loss = 0.808578
Epoch 5.13: Loss = 0.692337
Epoch 5.14: Loss = 0.632813
Epoch 5.15: Loss = 0.757095
Epoch 5.16: Loss = 0.717422
Epoch 5.17: Loss = 0.525452
Epoch 5.18: Loss = 0.678802
Epoch 5.19: Loss = 0.601364
Epoch 5.20: Loss = 0.700714
Epoch 5.21: Loss = 0.654648
Epoch 5.22: Loss = 0.746017
Epoch 5.23: Loss = 0.76944
Epoch 5.24: Loss = 0.706436
Epoch 5.25: Loss = 0.676163
Epoch 5.26: Loss = 0.559967
Epoch 5.27: Loss = 0.760025
Epoch 5.28: Loss = 0.724258
Epoch 5.29: Loss = 0.606018
Epoch 5.30: Loss = 0.665131
Epoch 5.31: Loss = 0.595428
Epoch 5.32: Loss = 0.587097
Epoch 5.33: Loss = 0.739944
Epoch 5.34: Loss = 0.690155
Epoch 5.35: Loss = 0.691498
Epoch 5.36: Loss = 0.712555
Epoch 5.37: Loss = 0.711884
Epoch 5.38: Loss = 0.593353
Epoch 5.39: Loss = 0.647232
Epoch 5.40: Loss = 0.739243
Epoch 5.41: Loss = 0.658188
Epoch 5.42: Loss = 0.734131
Epoch 5.43: Loss = 0.65181
Epoch 5.44: Loss = 0.627747
Epoch 5.45: Loss = 0.637161
Epoch 5.46: Loss = 0.762451
Epoch 5.47: Loss = 0.624664
Epoch 5.48: Loss = 0.750153
Epoch 5.49: Loss = 0.701965
Epoch 5.50: Loss = 0.696625
Epoch 5.51: Loss = 0.662109
Epoch 5.52: Loss = 0.710052
Epoch 5.53: Loss = 0.741776
Epoch 5.54: Loss = 0.653854
Epoch 5.55: Loss = 0.638046
Epoch 5.56: Loss = 0.627518
Epoch 5.57: Loss = 0.635498
Epoch 5.58: Loss = 0.670349
Epoch 5.59: Loss = 0.58786
Epoch 5.60: Loss = 0.620819
Epoch 5.61: Loss = 0.773361
Epoch 5.62: Loss = 0.585724
Epoch 5.63: Loss = 0.705368
Epoch 5.64: Loss = 0.599686
Epoch 5.65: Loss = 0.531784
Epoch 5.66: Loss = 0.568863
Epoch 5.67: Loss = 0.730499
Epoch 5.68: Loss = 0.494247
Epoch 5.69: Loss = 0.710434
Epoch 5.70: Loss = 0.634567
Epoch 5.71: Loss = 0.626373
Epoch 5.72: Loss = 0.817947
Epoch 5.73: Loss = 0.63208
Epoch 5.74: Loss = 0.767471
Epoch 5.75: Loss = 0.775375
Epoch 5.76: Loss = 0.641922
Epoch 5.77: Loss = 0.658707
Epoch 5.78: Loss = 0.672043
Epoch 5.79: Loss = 0.577728
Epoch 5.80: Loss = 0.582504
Epoch 5.81: Loss = 0.727509
Epoch 5.82: Loss = 0.703049
Epoch 5.83: Loss = 0.641388
Epoch 5.84: Loss = 0.670776
Epoch 5.85: Loss = 0.571823
Epoch 5.86: Loss = 0.556244
Epoch 5.87: Loss = 0.634491
Epoch 5.88: Loss = 0.604523
Epoch 5.89: Loss = 0.597992
Epoch 5.90: Loss = 0.647064
Epoch 5.91: Loss = 0.763016
Epoch 5.92: Loss = 0.634811
Epoch 5.93: Loss = 0.67424
Epoch 5.94: Loss = 0.638535
Epoch 5.95: Loss = 0.737717
Epoch 5.96: Loss = 0.687241
Epoch 5.97: Loss = 0.722015
Epoch 5.98: Loss = 0.670059
Epoch 5.99: Loss = 0.701904
Epoch 5.100: Loss = 0.689423
Epoch 5.101: Loss = 0.758514
Epoch 5.102: Loss = 0.681152
Epoch 5.103: Loss = 0.700684
Epoch 5.104: Loss = 0.611176
Epoch 5.105: Loss = 0.680862
Epoch 5.106: Loss = 0.566132
Epoch 5.107: Loss = 0.527313
Epoch 5.108: Loss = 0.568756
Epoch 5.109: Loss = 0.770523
Epoch 5.110: Loss = 0.652634
Epoch 5.111: Loss = 0.593628
Epoch 5.112: Loss = 0.715576
Epoch 5.113: Loss = 0.619629
Epoch 5.114: Loss = 0.663208
Epoch 5.115: Loss = 0.582001
Epoch 5.116: Loss = 0.647095
Epoch 5.117: Loss = 0.514374
Epoch 5.118: Loss = 0.57933
Epoch 5.119: Loss = 0.604752
Epoch 5.120: Loss = 0.607681
TRAIN LOSS = 0.659958
TRAIN ACC = 79.7592 % (47857/60000)
Loss = 0.573227
Loss = 0.695511
Loss = 0.658707
Loss = 0.564987
Loss = 0.612427
Loss = 0.771469
Loss = 0.839737
Loss = 0.790024
Loss = 0.644714
Loss = 0.629227
Loss = 0.821548
Loss = 0.786041
Loss = 0.739105
Loss = 0.701935
Loss = 0.665573
Loss = 0.717407
Loss = 0.635391
Loss = 0.707855
Loss = 0.732025
Loss = 0.72493
TEST LOSS = 0.700592
TEST ACC = 478.569 % (7905/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.654434
Epoch 6.2: Loss = 0.73909
Epoch 6.3: Loss = 0.656876
Epoch 6.4: Loss = 0.687119
Epoch 6.5: Loss = 0.543869
Epoch 6.6: Loss = 0.589966
Epoch 6.7: Loss = 0.584717
Epoch 6.8: Loss = 0.748993
Epoch 6.9: Loss = 0.664078
Epoch 6.10: Loss = 0.51564
Epoch 6.11: Loss = 0.621613
Epoch 6.12: Loss = 0.659683
Epoch 6.13: Loss = 0.62442
Epoch 6.14: Loss = 0.573425
Epoch 6.15: Loss = 0.770142
Epoch 6.16: Loss = 0.760391
Epoch 6.17: Loss = 0.728317
Epoch 6.18: Loss = 0.661652
Epoch 6.19: Loss = 0.606934
Epoch 6.20: Loss = 0.602448
Epoch 6.21: Loss = 0.702271
Epoch 6.22: Loss = 0.613281
Epoch 6.23: Loss = 0.659241
Epoch 6.24: Loss = 0.645096
Epoch 6.25: Loss = 0.578491
Epoch 6.26: Loss = 0.575195
Epoch 6.27: Loss = 0.63649
Epoch 6.28: Loss = 0.702957
Epoch 6.29: Loss = 0.8246
Epoch 6.30: Loss = 0.648605
Epoch 6.31: Loss = 0.63649
Epoch 6.32: Loss = 0.652008
Epoch 6.33: Loss = 0.644775
Epoch 6.34: Loss = 0.674744
Epoch 6.35: Loss = 0.603592
Epoch 6.36: Loss = 0.645477
Epoch 6.37: Loss = 0.674255
Epoch 6.38: Loss = 0.574371
Epoch 6.39: Loss = 0.684753
Epoch 6.40: Loss = 0.64975
Epoch 6.41: Loss = 0.692581
Epoch 6.42: Loss = 0.645111
Epoch 6.43: Loss = 0.670074
Epoch 6.44: Loss = 0.561447
Epoch 6.45: Loss = 0.603851
Epoch 6.46: Loss = 0.632813
Epoch 6.47: Loss = 0.814514
Epoch 6.48: Loss = 0.801804
Epoch 6.49: Loss = 0.647766
Epoch 6.50: Loss = 0.656631
Epoch 6.51: Loss = 0.633316
Epoch 6.52: Loss = 0.560394
Epoch 6.53: Loss = 0.76712
Epoch 6.54: Loss = 0.649216
Epoch 6.55: Loss = 0.591431
Epoch 6.56: Loss = 0.794205
Epoch 6.57: Loss = 0.677963
Epoch 6.58: Loss = 0.594955
Epoch 6.59: Loss = 0.597702
Epoch 6.60: Loss = 0.545517
Epoch 6.61: Loss = 0.763367
Epoch 6.62: Loss = 0.622574
Epoch 6.63: Loss = 0.685974
Epoch 6.64: Loss = 0.666199
Epoch 6.65: Loss = 0.651016
Epoch 6.66: Loss = 0.59021
Epoch 6.67: Loss = 0.630692
Epoch 6.68: Loss = 0.757126
Epoch 6.69: Loss = 0.66684
Epoch 6.70: Loss = 0.63533
Epoch 6.71: Loss = 0.721863
Epoch 6.72: Loss = 0.673111
Epoch 6.73: Loss = 0.585403
Epoch 6.74: Loss = 0.71167
Epoch 6.75: Loss = 0.685928
Epoch 6.76: Loss = 0.632919
Epoch 6.77: Loss = 0.674332
Epoch 6.78: Loss = 0.500992
Epoch 6.79: Loss = 0.557205
Epoch 6.80: Loss = 0.649506
Epoch 6.81: Loss = 0.639801
Epoch 6.82: Loss = 0.612137
Epoch 6.83: Loss = 0.611694
Epoch 6.84: Loss = 0.68399
Epoch 6.85: Loss = 0.675537
Epoch 6.86: Loss = 0.610443
Epoch 6.87: Loss = 0.555283
Epoch 6.88: Loss = 0.715576
Epoch 6.89: Loss = 0.632904
Epoch 6.90: Loss = 0.650391
Epoch 6.91: Loss = 0.726471
Epoch 6.92: Loss = 0.740555
Epoch 6.93: Loss = 0.637466
Epoch 6.94: Loss = 0.645126
Epoch 6.95: Loss = 0.669601
Epoch 6.96: Loss = 0.559113
Epoch 6.97: Loss = 0.73735
Epoch 6.98: Loss = 0.582535
Epoch 6.99: Loss = 0.821213
Epoch 6.100: Loss = 0.716873
Epoch 6.101: Loss = 0.645584
Epoch 6.102: Loss = 0.764557
Epoch 6.103: Loss = 0.705612
Epoch 6.104: Loss = 0.61821
Epoch 6.105: Loss = 0.702347
Epoch 6.106: Loss = 0.624588
Epoch 6.107: Loss = 0.774353
Epoch 6.108: Loss = 0.599304
Epoch 6.109: Loss = 0.675095
Epoch 6.110: Loss = 0.548355
Epoch 6.111: Loss = 0.57901
Epoch 6.112: Loss = 0.585281
Epoch 6.113: Loss = 0.626175
Epoch 6.114: Loss = 0.716248
Epoch 6.115: Loss = 0.669327
Epoch 6.116: Loss = 0.737534
Epoch 6.117: Loss = 0.815033
Epoch 6.118: Loss = 0.895569
Epoch 6.119: Loss = 0.78566
Epoch 6.120: Loss = 0.518433
TRAIN LOSS = 0.658615
TRAIN ACC = 80.5206 % (48315/60000)
Loss = 0.56012
Loss = 0.677917
Loss = 0.643463
Loss = 0.555206
Loss = 0.622086
Loss = 0.754089
Loss = 0.826492
Loss = 0.765549
Loss = 0.633423
Loss = 0.625641
Loss = 0.820602
Loss = 0.786987
Loss = 0.724762
Loss = 0.691849
Loss = 0.651199
Loss = 0.710678
Loss = 0.622452
Loss = 0.690262
Loss = 0.726166
Loss = 0.704803
TEST LOSS = 0.689687
TEST ACC = 483.15 % (7974/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.721954
Epoch 7.2: Loss = 0.573395
Epoch 7.3: Loss = 0.786926
Epoch 7.4: Loss = 0.661133
Epoch 7.5: Loss = 0.603577
Epoch 7.6: Loss = 0.596237
Epoch 7.7: Loss = 0.62767
Epoch 7.8: Loss = 0.691223
Epoch 7.9: Loss = 0.741638
Epoch 7.10: Loss = 0.625961
Epoch 7.11: Loss = 0.729202
Epoch 7.12: Loss = 0.777069
Epoch 7.13: Loss = 0.699966
Epoch 7.14: Loss = 0.660568
Epoch 7.15: Loss = 0.696762
Epoch 7.16: Loss = 0.698608
Epoch 7.17: Loss = 0.567032
Epoch 7.18: Loss = 0.644241
Epoch 7.19: Loss = 0.742813
Epoch 7.20: Loss = 0.64621
Epoch 7.21: Loss = 0.772812
Epoch 7.22: Loss = 0.69426
Epoch 7.23: Loss = 0.674042
Epoch 7.24: Loss = 0.615845
Epoch 7.25: Loss = 0.608139
Epoch 7.26: Loss = 0.597336
Epoch 7.27: Loss = 0.663101
Epoch 7.28: Loss = 0.775253
Epoch 7.29: Loss = 0.638855
Epoch 7.30: Loss = 0.612366
Epoch 7.31: Loss = 0.685532
Epoch 7.32: Loss = 0.618851
Epoch 7.33: Loss = 0.627106
Epoch 7.34: Loss = 0.656448
Epoch 7.35: Loss = 0.687256
Epoch 7.36: Loss = 0.494019
Epoch 7.37: Loss = 0.699295
Epoch 7.38: Loss = 0.645264
Epoch 7.39: Loss = 0.72377
Epoch 7.40: Loss = 0.594406
Epoch 7.41: Loss = 0.645065
Epoch 7.42: Loss = 0.666306
Epoch 7.43: Loss = 0.553986
Epoch 7.44: Loss = 0.642578
Epoch 7.45: Loss = 0.651581
Epoch 7.46: Loss = 0.609161
Epoch 7.47: Loss = 0.636963
Epoch 7.48: Loss = 0.562302
Epoch 7.49: Loss = 0.684555
Epoch 7.50: Loss = 0.906143
Epoch 7.51: Loss = 0.583191
Epoch 7.52: Loss = 0.65947
Epoch 7.53: Loss = 0.685867
Epoch 7.54: Loss = 0.629074
Epoch 7.55: Loss = 0.676376
Epoch 7.56: Loss = 0.795303
Epoch 7.57: Loss = 0.675171
Epoch 7.58: Loss = 0.720627
Epoch 7.59: Loss = 0.6884
Epoch 7.60: Loss = 0.744019
Epoch 7.61: Loss = 0.809982
Epoch 7.62: Loss = 0.639908
Epoch 7.63: Loss = 0.606018
Epoch 7.64: Loss = 0.580475
Epoch 7.65: Loss = 0.515793
Epoch 7.66: Loss = 0.658005
Epoch 7.67: Loss = 0.652771
Epoch 7.68: Loss = 0.676971
Epoch 7.69: Loss = 0.683548
Epoch 7.70: Loss = 0.543228
Epoch 7.71: Loss = 0.743729
Epoch 7.72: Loss = 0.626862
Epoch 7.73: Loss = 0.666733
Epoch 7.74: Loss = 0.57843
Epoch 7.75: Loss = 0.581436
Epoch 7.76: Loss = 0.657608
Epoch 7.77: Loss = 0.646973
Epoch 7.78: Loss = 0.635956
Epoch 7.79: Loss = 0.639374
Epoch 7.80: Loss = 0.742767
Epoch 7.81: Loss = 0.636841
Epoch 7.82: Loss = 0.521133
Epoch 7.83: Loss = 0.718338
Epoch 7.84: Loss = 0.681839
Epoch 7.85: Loss = 0.558304
Epoch 7.86: Loss = 0.683746
Epoch 7.87: Loss = 0.722397
Epoch 7.88: Loss = 0.769379
Epoch 7.89: Loss = 0.629257
Epoch 7.90: Loss = 0.655823
Epoch 7.91: Loss = 0.616577
Epoch 7.92: Loss = 0.646988
Epoch 7.93: Loss = 0.698898
Epoch 7.94: Loss = 0.788025
Epoch 7.95: Loss = 0.610336
Epoch 7.96: Loss = 0.706421
Epoch 7.97: Loss = 0.662277
Epoch 7.98: Loss = 0.481094
Epoch 7.99: Loss = 0.713028
Epoch 7.100: Loss = 0.580002
Epoch 7.101: Loss = 0.72879
Epoch 7.102: Loss = 0.634705
Epoch 7.103: Loss = 0.580673
Epoch 7.104: Loss = 0.698349
Epoch 7.105: Loss = 0.568176
Epoch 7.106: Loss = 0.507629
Epoch 7.107: Loss = 0.620056
Epoch 7.108: Loss = 0.597076
Epoch 7.109: Loss = 0.575317
Epoch 7.110: Loss = 0.7043
Epoch 7.111: Loss = 0.873917
Epoch 7.112: Loss = 0.732971
Epoch 7.113: Loss = 0.780045
Epoch 7.114: Loss = 0.768143
Epoch 7.115: Loss = 0.620407
Epoch 7.116: Loss = 0.722717
Epoch 7.117: Loss = 0.66629
Epoch 7.118: Loss = 0.660782
Epoch 7.119: Loss = 0.604004
Epoch 7.120: Loss = 0.487015
TRAIN LOSS = 0.659119
TRAIN ACC = 80.957 % (48577/60000)
Loss = 0.565247
Loss = 0.692551
Loss = 0.649658
Loss = 0.571198
Loss = 0.649033
Loss = 0.752548
Loss = 0.833084
Loss = 0.793198
Loss = 0.675827
Loss = 0.64856
Loss = 0.850525
Loss = 0.821274
Loss = 0.747009
Loss = 0.71582
Loss = 0.687271
Loss = 0.714798
Loss = 0.653198
Loss = 0.718857
Loss = 0.743347
Loss = 0.729538
TEST LOSS = 0.710627
TEST ACC = 485.77 % (8006/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.725769
Epoch 8.2: Loss = 0.670685
Epoch 8.3: Loss = 0.635498
Epoch 8.4: Loss = 0.750717
Epoch 8.5: Loss = 0.602386
Epoch 8.6: Loss = 0.601105
Epoch 8.7: Loss = 0.638641
Epoch 8.8: Loss = 0.630188
Epoch 8.9: Loss = 0.469757
Epoch 8.10: Loss = 0.584579
Epoch 8.11: Loss = 0.710434
Epoch 8.12: Loss = 0.669586
Epoch 8.13: Loss = 0.556427
Epoch 8.14: Loss = 0.634064
Epoch 8.15: Loss = 0.513565
Epoch 8.16: Loss = 0.668274
Epoch 8.17: Loss = 0.680832
Epoch 8.18: Loss = 0.691971
Epoch 8.19: Loss = 0.720947
Epoch 8.20: Loss = 0.577545
Epoch 8.21: Loss = 0.70311
Epoch 8.22: Loss = 0.67514
Epoch 8.23: Loss = 0.608215
Epoch 8.24: Loss = 0.660522
Epoch 8.25: Loss = 0.736099
Epoch 8.26: Loss = 0.773712
Epoch 8.27: Loss = 0.716537
Epoch 8.28: Loss = 0.753662
Epoch 8.29: Loss = 0.818069
Epoch 8.30: Loss = 0.625732
Epoch 8.31: Loss = 0.787582
Epoch 8.32: Loss = 0.645889
Epoch 8.33: Loss = 0.607742
Epoch 8.34: Loss = 0.70517
Epoch 8.35: Loss = 0.637894
Epoch 8.36: Loss = 0.566635
Epoch 8.37: Loss = 0.766907
Epoch 8.38: Loss = 0.606216
Epoch 8.39: Loss = 0.695541
Epoch 8.40: Loss = 0.626892
Epoch 8.41: Loss = 0.590424
Epoch 8.42: Loss = 0.693848
Epoch 8.43: Loss = 0.597366
Epoch 8.44: Loss = 0.693283
Epoch 8.45: Loss = 0.656769
Epoch 8.46: Loss = 0.507568
Epoch 8.47: Loss = 0.758133
Epoch 8.48: Loss = 0.710159
Epoch 8.49: Loss = 0.620575
Epoch 8.50: Loss = 0.777771
Epoch 8.51: Loss = 0.773193
Epoch 8.52: Loss = 0.760193
Epoch 8.53: Loss = 0.554382
Epoch 8.54: Loss = 0.649643
Epoch 8.55: Loss = 0.637131
Epoch 8.56: Loss = 0.564636
Epoch 8.57: Loss = 0.843369
Epoch 8.58: Loss = 0.660614
Epoch 8.59: Loss = 0.660706
Epoch 8.60: Loss = 0.704025
Epoch 8.61: Loss = 0.622589
Epoch 8.62: Loss = 0.634598
Epoch 8.63: Loss = 0.726837
Epoch 8.64: Loss = 0.719757
Epoch 8.65: Loss = 0.75708
Epoch 8.66: Loss = 0.597931
Epoch 8.67: Loss = 0.634445
Epoch 8.68: Loss = 0.734787
Epoch 8.69: Loss = 0.761902
Epoch 8.70: Loss = 0.500793
Epoch 8.71: Loss = 0.64534
Epoch 8.72: Loss = 0.684097
Epoch 8.73: Loss = 0.656387
Epoch 8.74: Loss = 0.580582
Epoch 8.75: Loss = 0.547211
Epoch 8.76: Loss = 0.687424
Epoch 8.77: Loss = 0.681351
Epoch 8.78: Loss = 0.69278
Epoch 8.79: Loss = 0.731812
Epoch 8.80: Loss = 0.6017
Epoch 8.81: Loss = 0.736877
Epoch 8.82: Loss = 0.671463
Epoch 8.83: Loss = 0.578659
Epoch 8.84: Loss = 0.732788
Epoch 8.85: Loss = 0.617233
Epoch 8.86: Loss = 0.656738
Epoch 8.87: Loss = 0.6586
Epoch 8.88: Loss = 0.588348
Epoch 8.89: Loss = 0.603668
Epoch 8.90: Loss = 0.774399
Epoch 8.91: Loss = 0.658249
Epoch 8.92: Loss = 0.700775
Epoch 8.93: Loss = 0.606705
Epoch 8.94: Loss = 0.666718
Epoch 8.95: Loss = 0.791809
Epoch 8.96: Loss = 0.608948
Epoch 8.97: Loss = 0.630234
Epoch 8.98: Loss = 0.605331
Epoch 8.99: Loss = 0.651947
Epoch 8.100: Loss = 0.71965
Epoch 8.101: Loss = 0.615875
Epoch 8.102: Loss = 0.538132
Epoch 8.103: Loss = 0.484436
Epoch 8.104: Loss = 0.699036
Epoch 8.105: Loss = 0.472122
Epoch 8.106: Loss = 0.556763
Epoch 8.107: Loss = 0.659378
Epoch 8.108: Loss = 0.716507
Epoch 8.109: Loss = 0.618393
Epoch 8.110: Loss = 0.592712
Epoch 8.111: Loss = 0.638885
Epoch 8.112: Loss = 0.575974
Epoch 8.113: Loss = 0.584641
Epoch 8.114: Loss = 0.665222
Epoch 8.115: Loss = 0.616959
Epoch 8.116: Loss = 0.654556
Epoch 8.117: Loss = 0.714615
Epoch 8.118: Loss = 0.528351
Epoch 8.119: Loss = 0.442184
Epoch 8.120: Loss = 0.664108
TRAIN LOSS = 0.653015
TRAIN ACC = 81.2851 % (48773/60000)
Loss = 0.550537
Loss = 0.659607
Loss = 0.626068
Loss = 0.560989
Loss = 0.627945
Loss = 0.734528
Loss = 0.817719
Loss = 0.756912
Loss = 0.642654
Loss = 0.631592
Loss = 0.840958
Loss = 0.793808
Loss = 0.730103
Loss = 0.696014
Loss = 0.65564
Loss = 0.689072
Loss = 0.648621
Loss = 0.695358
Loss = 0.715637
Loss = 0.704834
TEST LOSS = 0.68893
TEST ACC = 487.729 % (8052/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.681641
Epoch 9.2: Loss = 0.66803
Epoch 9.3: Loss = 0.538376
Epoch 9.4: Loss = 0.743927
Epoch 9.5: Loss = 0.622894
Epoch 9.6: Loss = 0.68457
Epoch 9.7: Loss = 0.677414
Epoch 9.8: Loss = 0.863129
Epoch 9.9: Loss = 0.579712
Epoch 9.10: Loss = 0.698029
Epoch 9.11: Loss = 0.70195
Epoch 9.12: Loss = 0.689514
Epoch 9.13: Loss = 0.638962
Epoch 9.14: Loss = 0.689545
Epoch 9.15: Loss = 0.548569
Epoch 9.16: Loss = 0.531723
Epoch 9.17: Loss = 0.772842
Epoch 9.18: Loss = 0.607315
Epoch 9.19: Loss = 0.677231
Epoch 9.20: Loss = 0.707596
Epoch 9.21: Loss = 0.680435
Epoch 9.22: Loss = 0.706451
Epoch 9.23: Loss = 0.667099
Epoch 9.24: Loss = 0.62999
Epoch 9.25: Loss = 0.566605
Epoch 9.26: Loss = 0.668381
Epoch 9.27: Loss = 0.703064
Epoch 9.28: Loss = 0.747223
Epoch 9.29: Loss = 0.604858
Epoch 9.30: Loss = 0.681412
Epoch 9.31: Loss = 0.714661
Epoch 9.32: Loss = 0.648895
Epoch 9.33: Loss = 0.642471
Epoch 9.34: Loss = 0.866562
Epoch 9.35: Loss = 0.642609
Epoch 9.36: Loss = 0.640045
Epoch 9.37: Loss = 0.589066
Epoch 9.38: Loss = 0.661591
Epoch 9.39: Loss = 0.767685
Epoch 9.40: Loss = 0.625305
Epoch 9.41: Loss = 0.636734
Epoch 9.42: Loss = 0.748947
Epoch 9.43: Loss = 0.765411
Epoch 9.44: Loss = 0.59816
Epoch 9.45: Loss = 0.648773
Epoch 9.46: Loss = 0.665009
Epoch 9.47: Loss = 0.700653
Epoch 9.48: Loss = 0.61702
Epoch 9.49: Loss = 0.527466
Epoch 9.50: Loss = 0.701477
Epoch 9.51: Loss = 0.614761
Epoch 9.52: Loss = 0.629745
Epoch 9.53: Loss = 0.648483
Epoch 9.54: Loss = 0.814621
Epoch 9.55: Loss = 0.754761
Epoch 9.56: Loss = 0.66272
Epoch 9.57: Loss = 0.667877
Epoch 9.58: Loss = 0.672867
Epoch 9.59: Loss = 0.562912
Epoch 9.60: Loss = 0.517487
Epoch 9.61: Loss = 0.624115
Epoch 9.62: Loss = 0.637268
Epoch 9.63: Loss = 0.672913
Epoch 9.64: Loss = 0.671204
Epoch 9.65: Loss = 0.584244
Epoch 9.66: Loss = 0.645279
Epoch 9.67: Loss = 0.605133
Epoch 9.68: Loss = 0.619644
Epoch 9.69: Loss = 0.57811
Epoch 9.70: Loss = 0.615005
Epoch 9.71: Loss = 0.604126
Epoch 9.72: Loss = 0.726501
Epoch 9.73: Loss = 0.724686
Epoch 9.74: Loss = 0.675507
Epoch 9.75: Loss = 0.599213
Epoch 9.76: Loss = 0.643173
Epoch 9.77: Loss = 0.732147
Epoch 9.78: Loss = 0.637726
Epoch 9.79: Loss = 0.611847
Epoch 9.80: Loss = 0.562012
Epoch 9.81: Loss = 0.574478
Epoch 9.82: Loss = 0.528885
Epoch 9.83: Loss = 0.610916
Epoch 9.84: Loss = 0.738831
Epoch 9.85: Loss = 0.658264
Epoch 9.86: Loss = 0.581696
Epoch 9.87: Loss = 0.711716
Epoch 9.88: Loss = 0.663361
Epoch 9.89: Loss = 0.622452
Epoch 9.90: Loss = 0.610672
Epoch 9.91: Loss = 0.647003
Epoch 9.92: Loss = 0.606323
Epoch 9.93: Loss = 0.688019
Epoch 9.94: Loss = 0.703033
Epoch 9.95: Loss = 0.61879
Epoch 9.96: Loss = 0.577713
Epoch 9.97: Loss = 0.614059
Epoch 9.98: Loss = 0.702484
Epoch 9.99: Loss = 0.651428
Epoch 9.100: Loss = 0.62825
Epoch 9.101: Loss = 0.568253
Epoch 9.102: Loss = 0.502029
Epoch 9.103: Loss = 0.677307
Epoch 9.104: Loss = 0.579193
Epoch 9.105: Loss = 0.734619
Epoch 9.106: Loss = 0.750015
Epoch 9.107: Loss = 0.639542
Epoch 9.108: Loss = 0.723419
Epoch 9.109: Loss = 0.704559
Epoch 9.110: Loss = 0.653076
Epoch 9.111: Loss = 0.588699
Epoch 9.112: Loss = 0.649979
Epoch 9.113: Loss = 0.710037
Epoch 9.114: Loss = 0.773148
Epoch 9.115: Loss = 0.727112
Epoch 9.116: Loss = 0.590332
Epoch 9.117: Loss = 0.697876
Epoch 9.118: Loss = 0.681503
Epoch 9.119: Loss = 0.56691
Epoch 9.120: Loss = 0.617355
TRAIN LOSS = 0.654816
TRAIN ACC = 81.6116 % (48969/60000)
Loss = 0.554199
Loss = 0.664948
Loss = 0.638229
Loss = 0.573318
Loss = 0.625336
Loss = 0.735001
Loss = 0.834106
Loss = 0.758484
Loss = 0.646057
Loss = 0.642883
Loss = 0.841415
Loss = 0.783524
Loss = 0.729233
Loss = 0.699829
Loss = 0.657791
Loss = 0.684555
Loss = 0.634171
Loss = 0.700378
Loss = 0.695709
Loss = 0.716507
TEST LOSS = 0.690783
TEST ACC = 489.69 % (8055/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.761932
Epoch 10.2: Loss = 0.910995
Epoch 10.3: Loss = 0.70459
Epoch 10.4: Loss = 0.600296
Epoch 10.5: Loss = 0.632599
Epoch 10.6: Loss = 0.639252
Epoch 10.7: Loss = 0.628952
Epoch 10.8: Loss = 0.592407
Epoch 10.9: Loss = 0.64357
Epoch 10.10: Loss = 0.704651
Epoch 10.11: Loss = 0.688309
Epoch 10.12: Loss = 0.569016
Epoch 10.13: Loss = 0.732864
Epoch 10.14: Loss = 0.648926
Epoch 10.15: Loss = 0.538818
Epoch 10.16: Loss = 0.700775
Epoch 10.17: Loss = 0.702164
Epoch 10.18: Loss = 0.663254
Epoch 10.19: Loss = 0.575104
Epoch 10.20: Loss = 0.584229
Epoch 10.21: Loss = 0.506454
Epoch 10.22: Loss = 0.637268
Epoch 10.23: Loss = 0.742661
Epoch 10.24: Loss = 0.621414
Epoch 10.25: Loss = 0.630966
Epoch 10.26: Loss = 0.791855
Epoch 10.27: Loss = 0.523666
Epoch 10.28: Loss = 0.627792
Epoch 10.29: Loss = 0.641296
Epoch 10.30: Loss = 0.690506
Epoch 10.31: Loss = 0.662857
Epoch 10.32: Loss = 0.702591
Epoch 10.33: Loss = 0.761627
Epoch 10.34: Loss = 0.614609
Epoch 10.35: Loss = 0.672668
Epoch 10.36: Loss = 0.753555
Epoch 10.37: Loss = 0.579712
Epoch 10.38: Loss = 0.578476
Epoch 10.39: Loss = 0.638718
Epoch 10.40: Loss = 0.572632
Epoch 10.41: Loss = 0.623734
Epoch 10.42: Loss = 0.75499
Epoch 10.43: Loss = 0.692017
Epoch 10.44: Loss = 0.750504
Epoch 10.45: Loss = 0.647491
Epoch 10.46: Loss = 0.624054
Epoch 10.47: Loss = 0.635132
Epoch 10.48: Loss = 0.582687
Epoch 10.49: Loss = 0.497711
Epoch 10.50: Loss = 0.737961
Epoch 10.51: Loss = 0.705475
Epoch 10.52: Loss = 0.507614
Epoch 10.53: Loss = 0.754944
Epoch 10.54: Loss = 0.696442
Epoch 10.55: Loss = 0.650543
Epoch 10.56: Loss = 0.655731
Epoch 10.57: Loss = 0.577118
Epoch 10.58: Loss = 0.594131
Epoch 10.59: Loss = 0.606598
Epoch 10.60: Loss = 0.618866
Epoch 10.61: Loss = 0.583893
Epoch 10.62: Loss = 0.74324
Epoch 10.63: Loss = 0.619247
Epoch 10.64: Loss = 0.629044
Epoch 10.65: Loss = 0.557404
Epoch 10.66: Loss = 0.616196
Epoch 10.67: Loss = 0.62738
Epoch 10.68: Loss = 0.712357
Epoch 10.69: Loss = 0.741196
Epoch 10.70: Loss = 0.558578
Epoch 10.71: Loss = 0.686493
Epoch 10.72: Loss = 0.613266
Epoch 10.73: Loss = 0.591278
Epoch 10.74: Loss = 0.531113
Epoch 10.75: Loss = 0.737534
Epoch 10.76: Loss = 0.631638
Epoch 10.77: Loss = 0.781509
Epoch 10.78: Loss = 0.636871
Epoch 10.79: Loss = 0.741943
Epoch 10.80: Loss = 0.739151
Epoch 10.81: Loss = 0.739822
Epoch 10.82: Loss = 0.685455
Epoch 10.83: Loss = 0.703018
Epoch 10.84: Loss = 0.657791
Epoch 10.85: Loss = 0.62645
Epoch 10.86: Loss = 0.635025
Epoch 10.87: Loss = 0.693832
Epoch 10.88: Loss = 0.622894
Epoch 10.89: Loss = 0.564545
Epoch 10.90: Loss = 0.517349
Epoch 10.91: Loss = 0.799927
Epoch 10.92: Loss = 0.608093
Epoch 10.93: Loss = 0.649307
Epoch 10.94: Loss = 0.822662
Epoch 10.95: Loss = 0.602203
Epoch 10.96: Loss = 0.651688
Epoch 10.97: Loss = 0.663101
Epoch 10.98: Loss = 0.606415
Epoch 10.99: Loss = 0.548889
Epoch 10.100: Loss = 0.60022
Epoch 10.101: Loss = 0.680496
Epoch 10.102: Loss = 0.549728
Epoch 10.103: Loss = 0.513519
Epoch 10.104: Loss = 0.705719
Epoch 10.105: Loss = 0.751724
Epoch 10.106: Loss = 0.741638
Epoch 10.107: Loss = 0.52829
Epoch 10.108: Loss = 0.565994
Epoch 10.109: Loss = 0.66153
Epoch 10.110: Loss = 0.565536
Epoch 10.111: Loss = 0.590805
Epoch 10.112: Loss = 0.749268
Epoch 10.113: Loss = 0.692215
Epoch 10.114: Loss = 0.574814
Epoch 10.115: Loss = 0.720657
Epoch 10.116: Loss = 0.566986
Epoch 10.117: Loss = 0.544693
Epoch 10.118: Loss = 0.727432
Epoch 10.119: Loss = 0.609299
Epoch 10.120: Loss = 0.63237
TRAIN LOSS = 0.648666
TRAIN ACC = 81.723 % (49036/60000)
Loss = 0.561035
Loss = 0.659683
Loss = 0.636276
Loss = 0.568024
Loss = 0.629517
Loss = 0.739792
Loss = 0.8591
Loss = 0.779663
Loss = 0.65567
Loss = 0.643448
Loss = 0.852676
Loss = 0.818024
Loss = 0.738754
Loss = 0.709473
Loss = 0.686142
Loss = 0.680328
Loss = 0.646133
Loss = 0.704147
Loss = 0.726242
Loss = 0.723572
TEST LOSS = 0.700885
TEST ACC = 490.359 % (8071/10000)

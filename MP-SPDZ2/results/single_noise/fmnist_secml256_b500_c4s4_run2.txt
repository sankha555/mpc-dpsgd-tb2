Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.31902
Epoch 1.2: Loss = 2.27481
Epoch 1.3: Loss = 2.21297
Epoch 1.4: Loss = 2.14612
Epoch 1.5: Loss = 2.10652
Epoch 1.6: Loss = 2.06369
Epoch 1.7: Loss = 2.0051
Epoch 1.8: Loss = 1.96552
Epoch 1.9: Loss = 1.89314
Epoch 1.10: Loss = 1.87721
Epoch 1.11: Loss = 1.86337
Epoch 1.12: Loss = 1.75679
Epoch 1.13: Loss = 1.76344
Epoch 1.14: Loss = 1.71857
Epoch 1.15: Loss = 1.6929
Epoch 1.16: Loss = 1.65205
Epoch 1.17: Loss = 1.62332
Epoch 1.18: Loss = 1.60728
Epoch 1.19: Loss = 1.52492
Epoch 1.20: Loss = 1.55443
Epoch 1.21: Loss = 1.48976
Epoch 1.22: Loss = 1.43163
Epoch 1.23: Loss = 1.38116
Epoch 1.24: Loss = 1.29782
Epoch 1.25: Loss = 1.3663
Epoch 1.26: Loss = 1.37505
Epoch 1.27: Loss = 1.34369
Epoch 1.28: Loss = 1.3167
Epoch 1.29: Loss = 1.24846
Epoch 1.30: Loss = 1.26282
Epoch 1.31: Loss = 1.2258
Epoch 1.32: Loss = 1.22914
Epoch 1.33: Loss = 1.18378
Epoch 1.34: Loss = 1.1953
Epoch 1.35: Loss = 1.11771
Epoch 1.36: Loss = 1.20569
Epoch 1.37: Loss = 1.16431
Epoch 1.38: Loss = 1.10271
Epoch 1.39: Loss = 1.13567
Epoch 1.40: Loss = 1.16476
Epoch 1.41: Loss = 1.09146
Epoch 1.42: Loss = 1.11115
Epoch 1.43: Loss = 1.00589
Epoch 1.44: Loss = 1.04195
Epoch 1.45: Loss = 1.07579
Epoch 1.46: Loss = 1.05203
Epoch 1.47: Loss = 1.02626
Epoch 1.48: Loss = 0.965073
Epoch 1.49: Loss = 1.01039
Epoch 1.50: Loss = 1.0312
Epoch 1.51: Loss = 1.04558
Epoch 1.52: Loss = 0.978104
Epoch 1.53: Loss = 0.981232
Epoch 1.54: Loss = 0.936493
Epoch 1.55: Loss = 1.0188
Epoch 1.56: Loss = 0.974091
Epoch 1.57: Loss = 0.891068
Epoch 1.58: Loss = 0.947495
Epoch 1.59: Loss = 0.939148
Epoch 1.60: Loss = 0.945724
Epoch 1.61: Loss = 0.881973
Epoch 1.62: Loss = 0.916763
Epoch 1.63: Loss = 0.845627
Epoch 1.64: Loss = 0.932281
Epoch 1.65: Loss = 0.909622
Epoch 1.66: Loss = 0.90564
Epoch 1.67: Loss = 0.912018
Epoch 1.68: Loss = 0.881729
Epoch 1.69: Loss = 0.908249
Epoch 1.70: Loss = 0.744965
Epoch 1.71: Loss = 0.833344
Epoch 1.72: Loss = 0.881943
Epoch 1.73: Loss = 0.837036
Epoch 1.74: Loss = 0.855438
Epoch 1.75: Loss = 0.797134
Epoch 1.76: Loss = 0.807022
Epoch 1.77: Loss = 0.884079
Epoch 1.78: Loss = 0.787537
Epoch 1.79: Loss = 0.821777
Epoch 1.80: Loss = 0.748016
Epoch 1.81: Loss = 0.794769
Epoch 1.82: Loss = 0.85582
Epoch 1.83: Loss = 0.85585
Epoch 1.84: Loss = 0.828644
Epoch 1.85: Loss = 0.820938
Epoch 1.86: Loss = 0.792099
Epoch 1.87: Loss = 0.754684
Epoch 1.88: Loss = 0.773911
Epoch 1.89: Loss = 0.790817
Epoch 1.90: Loss = 0.798691
Epoch 1.91: Loss = 0.727707
Epoch 1.92: Loss = 0.791229
Epoch 1.93: Loss = 0.818787
Epoch 1.94: Loss = 0.812424
Epoch 1.95: Loss = 0.772064
Epoch 1.96: Loss = 0.819672
Epoch 1.97: Loss = 0.709747
Epoch 1.98: Loss = 0.832169
Epoch 1.99: Loss = 0.793686
Epoch 1.100: Loss = 0.816116
Epoch 1.101: Loss = 0.725632
Epoch 1.102: Loss = 0.801697
Epoch 1.103: Loss = 0.791153
Epoch 1.104: Loss = 0.838623
Epoch 1.105: Loss = 0.811188
Epoch 1.106: Loss = 0.806732
Epoch 1.107: Loss = 0.715714
Epoch 1.108: Loss = 0.750153
Epoch 1.109: Loss = 0.729752
Epoch 1.110: Loss = 0.70108
Epoch 1.111: Loss = 0.769302
Epoch 1.112: Loss = 0.770309
Epoch 1.113: Loss = 0.797699
Epoch 1.114: Loss = 0.79628
Epoch 1.115: Loss = 0.777985
Epoch 1.116: Loss = 0.745316
Epoch 1.117: Loss = 0.771881
Epoch 1.118: Loss = 0.794388
Epoch 1.119: Loss = 0.770325
Epoch 1.120: Loss = 0.727463
TRAIN LOSS = 1.0948
TRAIN ACC = 63.7451 % (38249/60000)
Loss = 0.683929
Loss = 0.77565
Loss = 0.747452
Loss = 0.689545
Loss = 0.671844
Loss = 0.818741
Loss = 0.869583
Loss = 0.80275
Loss = 0.732193
Loss = 0.700912
Loss = 0.795105
Loss = 0.768173
Loss = 0.771881
Loss = 0.768478
Loss = 0.709641
Loss = 0.789978
Loss = 0.69696
Loss = 0.747452
Loss = 0.780106
Loss = 0.735229
TEST LOSS = 0.75278
TEST ACC = 382.489 % (7421/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.70047
Epoch 2.2: Loss = 0.692123
Epoch 2.3: Loss = 0.719589
Epoch 2.4: Loss = 0.812485
Epoch 2.5: Loss = 0.63736
Epoch 2.6: Loss = 0.727737
Epoch 2.7: Loss = 0.757217
Epoch 2.8: Loss = 0.723938
Epoch 2.9: Loss = 0.804031
Epoch 2.10: Loss = 0.756683
Epoch 2.11: Loss = 0.739151
Epoch 2.12: Loss = 0.693726
Epoch 2.13: Loss = 0.704849
Epoch 2.14: Loss = 0.810349
Epoch 2.15: Loss = 0.739914
Epoch 2.16: Loss = 0.650909
Epoch 2.17: Loss = 0.666458
Epoch 2.18: Loss = 0.703629
Epoch 2.19: Loss = 0.695969
Epoch 2.20: Loss = 0.679352
Epoch 2.21: Loss = 0.717041
Epoch 2.22: Loss = 0.744644
Epoch 2.23: Loss = 0.782211
Epoch 2.24: Loss = 0.664185
Epoch 2.25: Loss = 0.761566
Epoch 2.26: Loss = 0.658142
Epoch 2.27: Loss = 0.630692
Epoch 2.28: Loss = 0.817841
Epoch 2.29: Loss = 0.684418
Epoch 2.30: Loss = 0.805344
Epoch 2.31: Loss = 0.786972
Epoch 2.32: Loss = 0.808441
Epoch 2.33: Loss = 0.63327
Epoch 2.34: Loss = 0.668289
Epoch 2.35: Loss = 0.712112
Epoch 2.36: Loss = 0.727554
Epoch 2.37: Loss = 0.805984
Epoch 2.38: Loss = 0.739151
Epoch 2.39: Loss = 0.704529
Epoch 2.40: Loss = 0.747604
Epoch 2.41: Loss = 0.710632
Epoch 2.42: Loss = 0.597992
Epoch 2.43: Loss = 0.700409
Epoch 2.44: Loss = 0.60437
Epoch 2.45: Loss = 0.722244
Epoch 2.46: Loss = 0.846191
Epoch 2.47: Loss = 0.701233
Epoch 2.48: Loss = 0.695984
Epoch 2.49: Loss = 0.632156
Epoch 2.50: Loss = 0.653748
Epoch 2.51: Loss = 0.71756
Epoch 2.52: Loss = 0.609772
Epoch 2.53: Loss = 0.691864
Epoch 2.54: Loss = 0.683014
Epoch 2.55: Loss = 0.752777
Epoch 2.56: Loss = 0.561447
Epoch 2.57: Loss = 0.743073
Epoch 2.58: Loss = 0.628189
Epoch 2.59: Loss = 0.67453
Epoch 2.60: Loss = 0.733688
Epoch 2.61: Loss = 0.784225
Epoch 2.62: Loss = 0.762222
Epoch 2.63: Loss = 0.546204
Epoch 2.64: Loss = 0.740112
Epoch 2.65: Loss = 0.702179
Epoch 2.66: Loss = 0.57724
Epoch 2.67: Loss = 0.615738
Epoch 2.68: Loss = 0.726257
Epoch 2.69: Loss = 0.576935
Epoch 2.70: Loss = 0.651642
Epoch 2.71: Loss = 0.828598
Epoch 2.72: Loss = 0.714554
Epoch 2.73: Loss = 0.659637
Epoch 2.74: Loss = 0.649475
Epoch 2.75: Loss = 0.731689
Epoch 2.76: Loss = 0.709427
Epoch 2.77: Loss = 0.614365
Epoch 2.78: Loss = 0.622284
Epoch 2.79: Loss = 0.668976
Epoch 2.80: Loss = 0.770081
Epoch 2.81: Loss = 0.810959
Epoch 2.82: Loss = 0.775238
Epoch 2.83: Loss = 0.625977
Epoch 2.84: Loss = 0.778107
Epoch 2.85: Loss = 0.623138
Epoch 2.86: Loss = 0.666992
Epoch 2.87: Loss = 0.702698
Epoch 2.88: Loss = 0.660919
Epoch 2.89: Loss = 0.729294
Epoch 2.90: Loss = 0.722977
Epoch 2.91: Loss = 0.678589
Epoch 2.92: Loss = 0.667465
Epoch 2.93: Loss = 0.681488
Epoch 2.94: Loss = 0.726349
Epoch 2.95: Loss = 0.736115
Epoch 2.96: Loss = 0.71666
Epoch 2.97: Loss = 0.686737
Epoch 2.98: Loss = 0.797836
Epoch 2.99: Loss = 0.67514
Epoch 2.100: Loss = 0.756332
Epoch 2.101: Loss = 0.612061
Epoch 2.102: Loss = 0.738632
Epoch 2.103: Loss = 0.758545
Epoch 2.104: Loss = 0.625412
Epoch 2.105: Loss = 0.696945
Epoch 2.106: Loss = 0.785431
Epoch 2.107: Loss = 0.707901
Epoch 2.108: Loss = 0.738724
Epoch 2.109: Loss = 0.663666
Epoch 2.110: Loss = 0.641647
Epoch 2.111: Loss = 0.694824
Epoch 2.112: Loss = 0.697433
Epoch 2.113: Loss = 0.633118
Epoch 2.114: Loss = 0.764221
Epoch 2.115: Loss = 0.659409
Epoch 2.116: Loss = 0.723206
Epoch 2.117: Loss = 0.717407
Epoch 2.118: Loss = 0.621109
Epoch 2.119: Loss = 0.730972
Epoch 2.120: Loss = 0.742432
TRAIN LOSS = 0.704178
TRAIN ACC = 76.4877 % (45895/60000)
Loss = 0.624817
Loss = 0.763107
Loss = 0.67717
Loss = 0.621735
Loss = 0.630188
Loss = 0.775772
Loss = 0.822311
Loss = 0.763214
Loss = 0.699966
Loss = 0.646759
Loss = 0.802002
Loss = 0.783264
Loss = 0.729202
Loss = 0.709
Loss = 0.708054
Loss = 0.759506
Loss = 0.653366
Loss = 0.723358
Loss = 0.749283
Loss = 0.670303
TEST LOSS = 0.715619
TEST ACC = 458.949 % (7637/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.680817
Epoch 3.2: Loss = 0.595642
Epoch 3.3: Loss = 0.735367
Epoch 3.4: Loss = 0.615295
Epoch 3.5: Loss = 0.698212
Epoch 3.6: Loss = 0.663254
Epoch 3.7: Loss = 0.682907
Epoch 3.8: Loss = 0.623306
Epoch 3.9: Loss = 0.759384
Epoch 3.10: Loss = 0.732086
Epoch 3.11: Loss = 0.664795
Epoch 3.12: Loss = 0.848862
Epoch 3.13: Loss = 0.692062
Epoch 3.14: Loss = 0.698776
Epoch 3.15: Loss = 0.698944
Epoch 3.16: Loss = 0.692703
Epoch 3.17: Loss = 0.659836
Epoch 3.18: Loss = 0.568832
Epoch 3.19: Loss = 0.750244
Epoch 3.20: Loss = 0.689423
Epoch 3.21: Loss = 0.574341
Epoch 3.22: Loss = 0.621124
Epoch 3.23: Loss = 0.721848
Epoch 3.24: Loss = 0.758347
Epoch 3.25: Loss = 0.640915
Epoch 3.26: Loss = 0.708466
Epoch 3.27: Loss = 0.681137
Epoch 3.28: Loss = 0.749832
Epoch 3.29: Loss = 0.796692
Epoch 3.30: Loss = 0.643311
Epoch 3.31: Loss = 0.682205
Epoch 3.32: Loss = 0.65773
Epoch 3.33: Loss = 0.684143
Epoch 3.34: Loss = 0.703064
Epoch 3.35: Loss = 0.756485
Epoch 3.36: Loss = 0.608597
Epoch 3.37: Loss = 0.661377
Epoch 3.38: Loss = 0.670883
Epoch 3.39: Loss = 0.664383
Epoch 3.40: Loss = 0.765778
Epoch 3.41: Loss = 0.616409
Epoch 3.42: Loss = 0.674835
Epoch 3.43: Loss = 0.644089
Epoch 3.44: Loss = 0.693741
Epoch 3.45: Loss = 0.641937
Epoch 3.46: Loss = 0.781799
Epoch 3.47: Loss = 0.793655
Epoch 3.48: Loss = 0.57048
Epoch 3.49: Loss = 0.795761
Epoch 3.50: Loss = 0.668564
Epoch 3.51: Loss = 0.70575
Epoch 3.52: Loss = 0.735352
Epoch 3.53: Loss = 0.637329
Epoch 3.54: Loss = 0.661911
Epoch 3.55: Loss = 0.692596
Epoch 3.56: Loss = 0.621887
Epoch 3.57: Loss = 0.673126
Epoch 3.58: Loss = 0.709747
Epoch 3.59: Loss = 0.637772
Epoch 3.60: Loss = 0.701004
Epoch 3.61: Loss = 0.765442
Epoch 3.62: Loss = 0.66861
Epoch 3.63: Loss = 0.645096
Epoch 3.64: Loss = 0.781677
Epoch 3.65: Loss = 0.741241
Epoch 3.66: Loss = 0.743561
Epoch 3.67: Loss = 0.694305
Epoch 3.68: Loss = 0.765839
Epoch 3.69: Loss = 0.626358
Epoch 3.70: Loss = 0.724548
Epoch 3.71: Loss = 0.60556
Epoch 3.72: Loss = 0.753983
Epoch 3.73: Loss = 0.624771
Epoch 3.74: Loss = 0.759003
Epoch 3.75: Loss = 0.702759
Epoch 3.76: Loss = 0.747925
Epoch 3.77: Loss = 0.696503
Epoch 3.78: Loss = 0.587921
Epoch 3.79: Loss = 0.598099
Epoch 3.80: Loss = 0.697754
Epoch 3.81: Loss = 0.614639
Epoch 3.82: Loss = 0.63118
Epoch 3.83: Loss = 0.671478
Epoch 3.84: Loss = 0.625504
Epoch 3.85: Loss = 0.662033
Epoch 3.86: Loss = 0.786942
Epoch 3.87: Loss = 0.658875
Epoch 3.88: Loss = 0.584259
Epoch 3.89: Loss = 0.758087
Epoch 3.90: Loss = 0.721985
Epoch 3.91: Loss = 0.576447
Epoch 3.92: Loss = 0.74086
Epoch 3.93: Loss = 0.72226
Epoch 3.94: Loss = 0.641464
Epoch 3.95: Loss = 0.698044
Epoch 3.96: Loss = 0.749664
Epoch 3.97: Loss = 0.697601
Epoch 3.98: Loss = 0.709747
Epoch 3.99: Loss = 0.658157
Epoch 3.100: Loss = 0.695969
Epoch 3.101: Loss = 0.597641
Epoch 3.102: Loss = 0.657867
Epoch 3.103: Loss = 0.696304
Epoch 3.104: Loss = 0.58786
Epoch 3.105: Loss = 0.643875
Epoch 3.106: Loss = 0.536575
Epoch 3.107: Loss = 0.701401
Epoch 3.108: Loss = 0.610382
Epoch 3.109: Loss = 0.731567
Epoch 3.110: Loss = 0.652069
Epoch 3.111: Loss = 0.75473
Epoch 3.112: Loss = 0.667892
Epoch 3.113: Loss = 0.588318
Epoch 3.114: Loss = 0.703949
Epoch 3.115: Loss = 0.697128
Epoch 3.116: Loss = 0.781601
Epoch 3.117: Loss = 0.702148
Epoch 3.118: Loss = 0.694412
Epoch 3.119: Loss = 0.636993
Epoch 3.120: Loss = 0.71106
TRAIN LOSS = 0.683762
TRAIN ACC = 78.244 % (46948/60000)
Loss = 0.603363
Loss = 0.750931
Loss = 0.680542
Loss = 0.602844
Loss = 0.611969
Loss = 0.752075
Loss = 0.803528
Loss = 0.750931
Loss = 0.694489
Loss = 0.604095
Loss = 0.816193
Loss = 0.789368
Loss = 0.718063
Loss = 0.677094
Loss = 0.687958
Loss = 0.729141
Loss = 0.645111
Loss = 0.709671
Loss = 0.717865
Loss = 0.661835
TEST LOSS = 0.700353
TEST ACC = 469.479 % (7768/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.646255
Epoch 4.2: Loss = 0.643204
Epoch 4.3: Loss = 0.628723
Epoch 4.4: Loss = 0.714844
Epoch 4.5: Loss = 0.638107
Epoch 4.6: Loss = 0.598999
Epoch 4.7: Loss = 0.575439
Epoch 4.8: Loss = 0.679367
Epoch 4.9: Loss = 0.736282
Epoch 4.10: Loss = 0.739838
Epoch 4.11: Loss = 0.652023
Epoch 4.12: Loss = 0.697632
Epoch 4.13: Loss = 0.664993
Epoch 4.14: Loss = 0.771332
Epoch 4.15: Loss = 0.644272
Epoch 4.16: Loss = 0.720047
Epoch 4.17: Loss = 0.722763
Epoch 4.18: Loss = 0.659409
Epoch 4.19: Loss = 0.536362
Epoch 4.20: Loss = 0.586273
Epoch 4.21: Loss = 0.698242
Epoch 4.22: Loss = 0.706635
Epoch 4.23: Loss = 0.681305
Epoch 4.24: Loss = 0.81041
Epoch 4.25: Loss = 0.745224
Epoch 4.26: Loss = 0.697327
Epoch 4.27: Loss = 0.639557
Epoch 4.28: Loss = 0.838089
Epoch 4.29: Loss = 0.675262
Epoch 4.30: Loss = 0.65863
Epoch 4.31: Loss = 0.558121
Epoch 4.32: Loss = 0.689835
Epoch 4.33: Loss = 0.834457
Epoch 4.34: Loss = 0.740479
Epoch 4.35: Loss = 0.568787
Epoch 4.36: Loss = 0.662415
Epoch 4.37: Loss = 0.659851
Epoch 4.38: Loss = 0.63559
Epoch 4.39: Loss = 0.675858
Epoch 4.40: Loss = 0.79184
Epoch 4.41: Loss = 0.631714
Epoch 4.42: Loss = 0.617584
Epoch 4.43: Loss = 0.652542
Epoch 4.44: Loss = 0.691147
Epoch 4.45: Loss = 0.553482
Epoch 4.46: Loss = 0.763947
Epoch 4.47: Loss = 0.630646
Epoch 4.48: Loss = 0.647934
Epoch 4.49: Loss = 0.662735
Epoch 4.50: Loss = 0.725174
Epoch 4.51: Loss = 0.590942
Epoch 4.52: Loss = 0.609055
Epoch 4.53: Loss = 0.670868
Epoch 4.54: Loss = 0.615402
Epoch 4.55: Loss = 0.567886
Epoch 4.56: Loss = 0.67601
Epoch 4.57: Loss = 0.6922
Epoch 4.58: Loss = 0.775085
Epoch 4.59: Loss = 0.655945
Epoch 4.60: Loss = 0.909927
Epoch 4.61: Loss = 0.733414
Epoch 4.62: Loss = 0.648621
Epoch 4.63: Loss = 0.591019
Epoch 4.64: Loss = 0.622528
Epoch 4.65: Loss = 0.725769
Epoch 4.66: Loss = 0.647888
Epoch 4.67: Loss = 0.604156
Epoch 4.68: Loss = 0.747025
Epoch 4.69: Loss = 0.708191
Epoch 4.70: Loss = 0.588669
Epoch 4.71: Loss = 0.641815
Epoch 4.72: Loss = 0.696182
Epoch 4.73: Loss = 0.730484
Epoch 4.74: Loss = 0.705521
Epoch 4.75: Loss = 0.63559
Epoch 4.76: Loss = 0.627686
Epoch 4.77: Loss = 0.79454
Epoch 4.78: Loss = 0.690628
Epoch 4.79: Loss = 0.59964
Epoch 4.80: Loss = 0.720703
Epoch 4.81: Loss = 0.658737
Epoch 4.82: Loss = 0.605865
Epoch 4.83: Loss = 0.737411
Epoch 4.84: Loss = 0.719635
Epoch 4.85: Loss = 0.541504
Epoch 4.86: Loss = 0.681396
Epoch 4.87: Loss = 0.632263
Epoch 4.88: Loss = 0.717117
Epoch 4.89: Loss = 0.701782
Epoch 4.90: Loss = 0.74408
Epoch 4.91: Loss = 0.653809
Epoch 4.92: Loss = 0.814972
Epoch 4.93: Loss = 0.660751
Epoch 4.94: Loss = 0.63559
Epoch 4.95: Loss = 0.658279
Epoch 4.96: Loss = 0.711105
Epoch 4.97: Loss = 0.67395
Epoch 4.98: Loss = 0.70491
Epoch 4.99: Loss = 0.658783
Epoch 4.100: Loss = 0.885559
Epoch 4.101: Loss = 0.71553
Epoch 4.102: Loss = 0.5569
Epoch 4.103: Loss = 0.546646
Epoch 4.104: Loss = 0.669724
Epoch 4.105: Loss = 0.683212
Epoch 4.106: Loss = 0.564926
Epoch 4.107: Loss = 0.721298
Epoch 4.108: Loss = 0.758545
Epoch 4.109: Loss = 0.582169
Epoch 4.110: Loss = 0.694931
Epoch 4.111: Loss = 0.634369
Epoch 4.112: Loss = 0.590866
Epoch 4.113: Loss = 0.633804
Epoch 4.114: Loss = 0.732178
Epoch 4.115: Loss = 0.747665
Epoch 4.116: Loss = 0.663055
Epoch 4.117: Loss = 0.664536
Epoch 4.118: Loss = 0.725677
Epoch 4.119: Loss = 0.737091
Epoch 4.120: Loss = 0.650085
TRAIN LOSS = 0.675812
TRAIN ACC = 79.3182 % (47593/60000)
Loss = 0.59935
Loss = 0.7612
Loss = 0.669434
Loss = 0.619522
Loss = 0.624771
Loss = 0.786423
Loss = 0.841904
Loss = 0.772156
Loss = 0.698639
Loss = 0.640778
Loss = 0.828049
Loss = 0.800644
Loss = 0.715912
Loss = 0.687424
Loss = 0.677505
Loss = 0.734665
Loss = 0.667374
Loss = 0.70253
Loss = 0.736908
Loss = 0.68454
TEST LOSS = 0.712486
TEST ACC = 475.929 % (7852/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.690979
Epoch 5.2: Loss = 0.689255
Epoch 5.3: Loss = 0.733459
Epoch 5.4: Loss = 0.724625
Epoch 5.5: Loss = 0.672394
Epoch 5.6: Loss = 0.772629
Epoch 5.7: Loss = 0.662735
Epoch 5.8: Loss = 0.753952
Epoch 5.9: Loss = 0.719467
Epoch 5.10: Loss = 0.784241
Epoch 5.11: Loss = 0.715836
Epoch 5.12: Loss = 0.753754
Epoch 5.13: Loss = 0.729324
Epoch 5.14: Loss = 0.793625
Epoch 5.15: Loss = 0.560287
Epoch 5.16: Loss = 0.580109
Epoch 5.17: Loss = 0.72908
Epoch 5.18: Loss = 0.670944
Epoch 5.19: Loss = 0.716141
Epoch 5.20: Loss = 0.725357
Epoch 5.21: Loss = 0.678421
Epoch 5.22: Loss = 0.645264
Epoch 5.23: Loss = 0.625397
Epoch 5.24: Loss = 0.771011
Epoch 5.25: Loss = 0.637741
Epoch 5.26: Loss = 0.572769
Epoch 5.27: Loss = 0.690079
Epoch 5.28: Loss = 0.659088
Epoch 5.29: Loss = 0.680466
Epoch 5.30: Loss = 0.621185
Epoch 5.31: Loss = 0.616943
Epoch 5.32: Loss = 0.756378
Epoch 5.33: Loss = 0.759842
Epoch 5.34: Loss = 0.669434
Epoch 5.35: Loss = 0.733795
Epoch 5.36: Loss = 0.619904
Epoch 5.37: Loss = 0.679428
Epoch 5.38: Loss = 0.757706
Epoch 5.39: Loss = 0.713531
Epoch 5.40: Loss = 0.607574
Epoch 5.41: Loss = 0.751038
Epoch 5.42: Loss = 0.636292
Epoch 5.43: Loss = 0.699783
Epoch 5.44: Loss = 0.624771
Epoch 5.45: Loss = 0.682098
Epoch 5.46: Loss = 0.602203
Epoch 5.47: Loss = 0.641525
Epoch 5.48: Loss = 0.712891
Epoch 5.49: Loss = 0.69249
Epoch 5.50: Loss = 0.704453
Epoch 5.51: Loss = 0.654694
Epoch 5.52: Loss = 0.678635
Epoch 5.53: Loss = 0.602997
Epoch 5.54: Loss = 0.697189
Epoch 5.55: Loss = 0.697662
Epoch 5.56: Loss = 0.882217
Epoch 5.57: Loss = 0.643005
Epoch 5.58: Loss = 0.788116
Epoch 5.59: Loss = 0.675201
Epoch 5.60: Loss = 0.653732
Epoch 5.61: Loss = 0.773361
Epoch 5.62: Loss = 0.752792
Epoch 5.63: Loss = 0.690231
Epoch 5.64: Loss = 0.7742
Epoch 5.65: Loss = 0.706467
Epoch 5.66: Loss = 0.606186
Epoch 5.67: Loss = 0.560928
Epoch 5.68: Loss = 0.683914
Epoch 5.69: Loss = 0.687622
Epoch 5.70: Loss = 0.666458
Epoch 5.71: Loss = 0.710663
Epoch 5.72: Loss = 0.65538
Epoch 5.73: Loss = 0.653458
Epoch 5.74: Loss = 0.722702
Epoch 5.75: Loss = 0.623856
Epoch 5.76: Loss = 0.585602
Epoch 5.77: Loss = 0.649002
Epoch 5.78: Loss = 0.721237
Epoch 5.79: Loss = 0.554001
Epoch 5.80: Loss = 0.637466
Epoch 5.81: Loss = 0.732864
Epoch 5.82: Loss = 0.641785
Epoch 5.83: Loss = 0.623932
Epoch 5.84: Loss = 0.767105
Epoch 5.85: Loss = 0.557709
Epoch 5.86: Loss = 0.595932
Epoch 5.87: Loss = 0.575424
Epoch 5.88: Loss = 0.61615
Epoch 5.89: Loss = 0.641464
Epoch 5.90: Loss = 0.666275
Epoch 5.91: Loss = 0.682068
Epoch 5.92: Loss = 0.620346
Epoch 5.93: Loss = 0.650299
Epoch 5.94: Loss = 0.748306
Epoch 5.95: Loss = 0.663452
Epoch 5.96: Loss = 0.539444
Epoch 5.97: Loss = 0.591003
Epoch 5.98: Loss = 0.633118
Epoch 5.99: Loss = 0.519989
Epoch 5.100: Loss = 0.783356
Epoch 5.101: Loss = 0.54863
Epoch 5.102: Loss = 0.727814
Epoch 5.103: Loss = 0.774979
Epoch 5.104: Loss = 0.685318
Epoch 5.105: Loss = 0.580933
Epoch 5.106: Loss = 0.775513
Epoch 5.107: Loss = 0.758301
Epoch 5.108: Loss = 0.571503
Epoch 5.109: Loss = 0.766113
Epoch 5.110: Loss = 0.722168
Epoch 5.111: Loss = 0.683731
Epoch 5.112: Loss = 0.858826
Epoch 5.113: Loss = 0.705475
Epoch 5.114: Loss = 0.683365
Epoch 5.115: Loss = 0.697357
Epoch 5.116: Loss = 0.74707
Epoch 5.117: Loss = 0.740112
Epoch 5.118: Loss = 0.687775
Epoch 5.119: Loss = 0.621414
Epoch 5.120: Loss = 0.670883
TRAIN LOSS = 0.680588
TRAIN ACC = 80.1178 % (48073/60000)
Loss = 0.58046
Loss = 0.757462
Loss = 0.676193
Loss = 0.58403
Loss = 0.672882
Loss = 0.779221
Loss = 0.853424
Loss = 0.767014
Loss = 0.694977
Loss = 0.619659
Loss = 0.856262
Loss = 0.799347
Loss = 0.712357
Loss = 0.694717
Loss = 0.707825
Loss = 0.709091
Loss = 0.686874
Loss = 0.717407
Loss = 0.704269
Loss = 0.685272
TEST LOSS = 0.712937
TEST ACC = 480.73 % (7923/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.671967
Epoch 6.2: Loss = 0.656311
Epoch 6.3: Loss = 0.537628
Epoch 6.4: Loss = 0.68042
Epoch 6.5: Loss = 0.72934
Epoch 6.6: Loss = 0.762665
Epoch 6.7: Loss = 0.620834
Epoch 6.8: Loss = 0.696091
Epoch 6.9: Loss = 0.726898
Epoch 6.10: Loss = 0.613464
Epoch 6.11: Loss = 0.753174
Epoch 6.12: Loss = 0.592865
Epoch 6.13: Loss = 0.61203
Epoch 6.14: Loss = 0.695999
Epoch 6.15: Loss = 0.632339
Epoch 6.16: Loss = 0.743546
Epoch 6.17: Loss = 0.599655
Epoch 6.18: Loss = 0.545502
Epoch 6.19: Loss = 0.663651
Epoch 6.20: Loss = 0.725494
Epoch 6.21: Loss = 0.721924
Epoch 6.22: Loss = 0.693878
Epoch 6.23: Loss = 0.580948
Epoch 6.24: Loss = 0.71933
Epoch 6.25: Loss = 0.685822
Epoch 6.26: Loss = 0.772736
Epoch 6.27: Loss = 0.698257
Epoch 6.28: Loss = 0.6633
Epoch 6.29: Loss = 0.685287
Epoch 6.30: Loss = 0.677643
Epoch 6.31: Loss = 0.657913
Epoch 6.32: Loss = 0.652573
Epoch 6.33: Loss = 0.694702
Epoch 6.34: Loss = 0.706406
Epoch 6.35: Loss = 0.66272
Epoch 6.36: Loss = 0.664917
Epoch 6.37: Loss = 0.673111
Epoch 6.38: Loss = 0.767624
Epoch 6.39: Loss = 0.775894
Epoch 6.40: Loss = 0.737564
Epoch 6.41: Loss = 0.662857
Epoch 6.42: Loss = 0.599747
Epoch 6.43: Loss = 0.669052
Epoch 6.44: Loss = 0.649292
Epoch 6.45: Loss = 0.693649
Epoch 6.46: Loss = 0.650558
Epoch 6.47: Loss = 0.708603
Epoch 6.48: Loss = 0.721695
Epoch 6.49: Loss = 0.760818
Epoch 6.50: Loss = 0.699783
Epoch 6.51: Loss = 0.647995
Epoch 6.52: Loss = 0.591202
Epoch 6.53: Loss = 0.699509
Epoch 6.54: Loss = 0.626511
Epoch 6.55: Loss = 0.509247
Epoch 6.56: Loss = 0.598282
Epoch 6.57: Loss = 0.632172
Epoch 6.58: Loss = 0.593231
Epoch 6.59: Loss = 0.832352
Epoch 6.60: Loss = 0.677658
Epoch 6.61: Loss = 0.691406
Epoch 6.62: Loss = 0.61525
Epoch 6.63: Loss = 0.610672
Epoch 6.64: Loss = 0.698441
Epoch 6.65: Loss = 0.632538
Epoch 6.66: Loss = 0.640427
Epoch 6.67: Loss = 0.695877
Epoch 6.68: Loss = 0.788345
Epoch 6.69: Loss = 0.741638
Epoch 6.70: Loss = 0.734772
Epoch 6.71: Loss = 0.771301
Epoch 6.72: Loss = 0.851669
Epoch 6.73: Loss = 0.724136
Epoch 6.74: Loss = 0.661835
Epoch 6.75: Loss = 0.829575
Epoch 6.76: Loss = 0.735855
Epoch 6.77: Loss = 0.79129
Epoch 6.78: Loss = 0.704575
Epoch 6.79: Loss = 0.617584
Epoch 6.80: Loss = 0.677032
Epoch 6.81: Loss = 0.615067
Epoch 6.82: Loss = 0.665802
Epoch 6.83: Loss = 0.627579
Epoch 6.84: Loss = 0.639786
Epoch 6.85: Loss = 0.558838
Epoch 6.86: Loss = 0.619888
Epoch 6.87: Loss = 0.570236
Epoch 6.88: Loss = 0.687134
Epoch 6.89: Loss = 0.68544
Epoch 6.90: Loss = 0.703384
Epoch 6.91: Loss = 0.732361
Epoch 6.92: Loss = 0.667801
Epoch 6.93: Loss = 0.735657
Epoch 6.94: Loss = 0.636368
Epoch 6.95: Loss = 0.600647
Epoch 6.96: Loss = 0.519333
Epoch 6.97: Loss = 0.732208
Epoch 6.98: Loss = 0.84256
Epoch 6.99: Loss = 0.80719
Epoch 6.100: Loss = 0.673157
Epoch 6.101: Loss = 0.61792
Epoch 6.102: Loss = 0.649429
Epoch 6.103: Loss = 0.721298
Epoch 6.104: Loss = 0.605362
Epoch 6.105: Loss = 0.779053
Epoch 6.106: Loss = 0.720139
Epoch 6.107: Loss = 0.614594
Epoch 6.108: Loss = 0.582275
Epoch 6.109: Loss = 0.767075
Epoch 6.110: Loss = 0.703262
Epoch 6.111: Loss = 0.641647
Epoch 6.112: Loss = 0.75322
Epoch 6.113: Loss = 0.766647
Epoch 6.114: Loss = 0.69751
Epoch 6.115: Loss = 0.690903
Epoch 6.116: Loss = 0.622833
Epoch 6.117: Loss = 0.884918
Epoch 6.118: Loss = 0.680481
Epoch 6.119: Loss = 0.681702
Epoch 6.120: Loss = 0.661194
TRAIN LOSS = 0.68129
TRAIN ACC = 80.2124 % (48129/60000)
Loss = 0.565552
Loss = 0.731537
Loss = 0.643845
Loss = 0.589783
Loss = 0.66951
Loss = 0.792236
Loss = 0.844345
Loss = 0.775879
Loss = 0.709244
Loss = 0.643784
Loss = 0.875412
Loss = 0.818481
Loss = 0.701965
Loss = 0.719116
Loss = 0.730728
Loss = 0.705811
Loss = 0.668839
Loss = 0.756577
Loss = 0.694427
Loss = 0.70079
TEST LOSS = 0.716893
TEST ACC = 481.29 % (7964/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.751389
Epoch 7.2: Loss = 0.641098
Epoch 7.3: Loss = 0.596466
Epoch 7.4: Loss = 0.667633
Epoch 7.5: Loss = 0.663605
Epoch 7.6: Loss = 0.67363
Epoch 7.7: Loss = 0.660675
Epoch 7.8: Loss = 0.596588
Epoch 7.9: Loss = 0.641388
Epoch 7.10: Loss = 0.692719
Epoch 7.11: Loss = 0.704407
Epoch 7.12: Loss = 0.71611
Epoch 7.13: Loss = 0.726151
Epoch 7.14: Loss = 0.724243
Epoch 7.15: Loss = 0.584381
Epoch 7.16: Loss = 0.676117
Epoch 7.17: Loss = 0.667221
Epoch 7.18: Loss = 0.752197
Epoch 7.19: Loss = 0.560654
Epoch 7.20: Loss = 0.806183
Epoch 7.21: Loss = 0.703613
Epoch 7.22: Loss = 0.624084
Epoch 7.23: Loss = 0.720917
Epoch 7.24: Loss = 0.699127
Epoch 7.25: Loss = 0.832138
Epoch 7.26: Loss = 0.723175
Epoch 7.27: Loss = 0.773941
Epoch 7.28: Loss = 0.72905
Epoch 7.29: Loss = 0.772308
Epoch 7.30: Loss = 0.674576
Epoch 7.31: Loss = 0.616257
Epoch 7.32: Loss = 0.674942
Epoch 7.33: Loss = 0.664001
Epoch 7.34: Loss = 0.797211
Epoch 7.35: Loss = 0.655258
Epoch 7.36: Loss = 0.812408
Epoch 7.37: Loss = 0.778763
Epoch 7.38: Loss = 0.61496
Epoch 7.39: Loss = 0.681702
Epoch 7.40: Loss = 0.703125
Epoch 7.41: Loss = 0.796173
Epoch 7.42: Loss = 0.682983
Epoch 7.43: Loss = 0.709824
Epoch 7.44: Loss = 0.675354
Epoch 7.45: Loss = 0.636734
Epoch 7.46: Loss = 0.696091
Epoch 7.47: Loss = 0.65686
Epoch 7.48: Loss = 0.725952
Epoch 7.49: Loss = 0.696289
Epoch 7.50: Loss = 0.543152
Epoch 7.51: Loss = 0.707443
Epoch 7.52: Loss = 0.814499
Epoch 7.53: Loss = 0.667114
Epoch 7.54: Loss = 0.80658
Epoch 7.55: Loss = 0.669266
Epoch 7.56: Loss = 0.730484
Epoch 7.57: Loss = 0.689911
Epoch 7.58: Loss = 0.727646
Epoch 7.59: Loss = 0.796844
Epoch 7.60: Loss = 0.677551
Epoch 7.61: Loss = 0.643127
Epoch 7.62: Loss = 0.798141
Epoch 7.63: Loss = 0.542526
Epoch 7.64: Loss = 0.616455
Epoch 7.65: Loss = 0.741211
Epoch 7.66: Loss = 0.71489
Epoch 7.67: Loss = 0.784622
Epoch 7.68: Loss = 0.633331
Epoch 7.69: Loss = 0.753036
Epoch 7.70: Loss = 0.702225
Epoch 7.71: Loss = 0.701813
Epoch 7.72: Loss = 0.764771
Epoch 7.73: Loss = 0.609726
Epoch 7.74: Loss = 0.646667
Epoch 7.75: Loss = 0.701172
Epoch 7.76: Loss = 0.714035
Epoch 7.77: Loss = 0.706161
Epoch 7.78: Loss = 0.632751
Epoch 7.79: Loss = 0.795273
Epoch 7.80: Loss = 0.767731
Epoch 7.81: Loss = 0.721146
Epoch 7.82: Loss = 0.715698
Epoch 7.83: Loss = 0.705124
Epoch 7.84: Loss = 0.77652
Epoch 7.85: Loss = 0.706558
Epoch 7.86: Loss = 0.714966
Epoch 7.87: Loss = 0.768433
Epoch 7.88: Loss = 0.703979
Epoch 7.89: Loss = 0.616486
Epoch 7.90: Loss = 0.690247
Epoch 7.91: Loss = 0.610596
Epoch 7.92: Loss = 0.682739
Epoch 7.93: Loss = 0.777924
Epoch 7.94: Loss = 0.7742
Epoch 7.95: Loss = 0.561707
Epoch 7.96: Loss = 0.625153
Epoch 7.97: Loss = 0.664047
Epoch 7.98: Loss = 0.657379
Epoch 7.99: Loss = 0.955856
Epoch 7.100: Loss = 0.671692
Epoch 7.101: Loss = 0.689926
Epoch 7.102: Loss = 0.793213
Epoch 7.103: Loss = 0.641953
Epoch 7.104: Loss = 0.625122
Epoch 7.105: Loss = 0.688812
Epoch 7.106: Loss = 0.849548
Epoch 7.107: Loss = 0.694412
Epoch 7.108: Loss = 0.621155
Epoch 7.109: Loss = 0.69725
Epoch 7.110: Loss = 0.749939
Epoch 7.111: Loss = 0.661255
Epoch 7.112: Loss = 0.577347
Epoch 7.113: Loss = 0.822357
Epoch 7.114: Loss = 0.753403
Epoch 7.115: Loss = 0.685608
Epoch 7.116: Loss = 0.62204
Epoch 7.117: Loss = 0.803665
Epoch 7.118: Loss = 0.655792
Epoch 7.119: Loss = 0.774414
Epoch 7.120: Loss = 0.736954
TRAIN LOSS = 0.700638
TRAIN ACC = 80.481 % (48291/60000)
Loss = 0.61795
Loss = 0.750381
Loss = 0.696701
Loss = 0.615005
Loss = 0.736282
Loss = 0.816742
Loss = 0.893524
Loss = 0.776199
Loss = 0.745804
Loss = 0.691513
Loss = 0.941696
Loss = 0.855011
Loss = 0.765594
Loss = 0.749298
Loss = 0.765427
Loss = 0.750595
Loss = 0.664215
Loss = 0.797745
Loss = 0.753082
Loss = 0.720016
TEST LOSS = 0.755139
TEST ACC = 482.909 % (7938/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.760025
Epoch 8.2: Loss = 0.627548
Epoch 8.3: Loss = 0.638916
Epoch 8.4: Loss = 0.702606
Epoch 8.5: Loss = 0.658035
Epoch 8.6: Loss = 0.722504
Epoch 8.7: Loss = 0.647293
Epoch 8.8: Loss = 0.673111
Epoch 8.9: Loss = 0.710693
Epoch 8.10: Loss = 0.60817
Epoch 8.11: Loss = 0.726593
Epoch 8.12: Loss = 0.63353
Epoch 8.13: Loss = 0.728333
Epoch 8.14: Loss = 0.723236
Epoch 8.15: Loss = 0.724258
Epoch 8.16: Loss = 0.696106
Epoch 8.17: Loss = 0.629303
Epoch 8.18: Loss = 0.794281
Epoch 8.19: Loss = 0.793121
Epoch 8.20: Loss = 0.572159
Epoch 8.21: Loss = 0.707886
Epoch 8.22: Loss = 0.833435
Epoch 8.23: Loss = 0.839981
Epoch 8.24: Loss = 0.613632
Epoch 8.25: Loss = 0.722748
Epoch 8.26: Loss = 0.710419
Epoch 8.27: Loss = 0.737991
Epoch 8.28: Loss = 0.716751
Epoch 8.29: Loss = 0.67926
Epoch 8.30: Loss = 0.727081
Epoch 8.31: Loss = 0.739578
Epoch 8.32: Loss = 0.780365
Epoch 8.33: Loss = 0.60997
Epoch 8.34: Loss = 0.621399
Epoch 8.35: Loss = 0.78624
Epoch 8.36: Loss = 0.747864
Epoch 8.37: Loss = 0.672745
Epoch 8.38: Loss = 0.615173
Epoch 8.39: Loss = 0.758362
Epoch 8.40: Loss = 0.720856
Epoch 8.41: Loss = 0.648575
Epoch 8.42: Loss = 0.822754
Epoch 8.43: Loss = 0.773163
Epoch 8.44: Loss = 0.813812
Epoch 8.45: Loss = 0.645294
Epoch 8.46: Loss = 0.74501
Epoch 8.47: Loss = 0.897934
Epoch 8.48: Loss = 0.734467
Epoch 8.49: Loss = 0.654831
Epoch 8.50: Loss = 0.783096
Epoch 8.51: Loss = 0.726807
Epoch 8.52: Loss = 28.6678
Epoch 8.53: Loss = 15.1073
Epoch 8.54: Loss = 5.46147
Epoch 8.55: Loss = 4.19986
Epoch 8.56: Loss = 4.29825
Epoch 8.57: Loss = 3.50911
Epoch 8.58: Loss = 4.21777
Epoch 8.59: Loss = 3.07808
Epoch 8.60: Loss = 3.75301
Epoch 8.61: Loss = 3.77495
Epoch 8.62: Loss = 2.66025
Epoch 8.63: Loss = 3.05109
Epoch 8.64: Loss = 2.74786
Epoch 8.65: Loss = 1.78693
Epoch 8.66: Loss = 2.34627
Epoch 8.67: Loss = 1.57185
Epoch 8.68: Loss = 1.44751
Epoch 8.69: Loss = 1.22617
Epoch 8.70: Loss = 1.30397
Epoch 8.71: Loss = 1.03008
Epoch 8.72: Loss = 0.897415
Epoch 8.73: Loss = 1.03441
Epoch 8.74: Loss = 0.790436
Epoch 8.75: Loss = 0.843323
Epoch 8.76: Loss = 0.737335
Epoch 8.77: Loss = 0.914551
Epoch 8.78: Loss = 0.679703
Epoch 8.79: Loss = 0.765518
Epoch 8.80: Loss = 0.869186
Epoch 8.81: Loss = 0.617355
Epoch 8.82: Loss = 0.734756
Epoch 8.83: Loss = 0.770966
Epoch 8.84: Loss = 0.778656
Epoch 8.85: Loss = 0.649948
Epoch 8.86: Loss = 0.810913
Epoch 8.87: Loss = 0.79184
Epoch 8.88: Loss = 0.695557
Epoch 8.89: Loss = 0.806046
Epoch 8.90: Loss = 0.767502
Epoch 8.91: Loss = 0.857101
Epoch 8.92: Loss = 0.810669
Epoch 8.93: Loss = 0.727432
Epoch 8.94: Loss = 0.584778
Epoch 8.95: Loss = 0.788925
Epoch 8.96: Loss = 0.692093
Epoch 8.97: Loss = 0.657379
Epoch 8.98: Loss = 0.625412
Epoch 8.99: Loss = 0.657532
Epoch 8.100: Loss = 0.895966
Epoch 8.101: Loss = 0.642044
Epoch 8.102: Loss = 0.68689
Epoch 8.103: Loss = 0.594299
Epoch 8.104: Loss = 0.5522
Epoch 8.105: Loss = 0.714951
Epoch 8.106: Loss = 0.786469
Epoch 8.107: Loss = 0.72226
Epoch 8.108: Loss = 0.738373
Epoch 8.109: Loss = 0.805466
Epoch 8.110: Loss = 0.836639
Epoch 8.111: Loss = 0.707382
Epoch 8.112: Loss = 0.82048
Epoch 8.113: Loss = 0.729111
Epoch 8.114: Loss = 1.03348
Epoch 8.115: Loss = 0.975815
Epoch 8.116: Loss = 0.842865
Epoch 8.117: Loss = 0.649673
Epoch 8.118: Loss = 0.769516
Epoch 8.119: Loss = 0.838577
Epoch 8.120: Loss = 0.62001
TRAIN LOSS = 1.40765
TRAIN ACC = 78.3463 % (47010/60000)
Loss = 0.616608
Loss = 0.758118
Loss = 0.693268
Loss = 0.649765
Loss = 0.757904
Loss = 0.867844
Loss = 1.0097
Loss = 0.776169
Loss = 0.738937
Loss = 0.718307
Loss = 0.95665
Loss = 0.934219
Loss = 0.777298
Loss = 0.762726
Loss = 0.813797
Loss = 0.783661
Loss = 0.63945
Loss = 0.836548
Loss = 0.832001
Loss = 0.749557
TEST LOSS = 0.783626
TEST ACC = 470.099 % (7947/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.729477
Epoch 9.2: Loss = 0.653473
Epoch 9.3: Loss = 0.827728
Epoch 9.4: Loss = 0.585419
Epoch 9.5: Loss = 0.797684
Epoch 9.6: Loss = 0.787292
Epoch 9.7: Loss = 0.712479
Epoch 9.8: Loss = 0.682938
Epoch 9.9: Loss = 0.650543
Epoch 9.10: Loss = 0.613708
Epoch 9.11: Loss = 0.883133
Epoch 9.12: Loss = 0.750305
Epoch 9.13: Loss = 0.819839
Epoch 9.14: Loss = 0.741806
Epoch 9.15: Loss = 0.74292
Epoch 9.16: Loss = 0.697205
Epoch 9.17: Loss = 0.696899
Epoch 9.18: Loss = 0.886856
Epoch 9.19: Loss = 0.704575
Epoch 9.20: Loss = 0.759384
Epoch 9.21: Loss = 0.650742
Epoch 9.22: Loss = 0.735413
Epoch 9.23: Loss = 0.654297
Epoch 9.24: Loss = 0.722672
Epoch 9.25: Loss = 0.741638
Epoch 9.26: Loss = 0.655518
Epoch 9.27: Loss = 0.816086
Epoch 9.28: Loss = 0.667328
Epoch 9.29: Loss = 0.673309
Epoch 9.30: Loss = 0.740051
Epoch 9.31: Loss = 0.730499
Epoch 9.32: Loss = 0.687088
Epoch 9.33: Loss = 0.726913
Epoch 9.34: Loss = 0.915451
Epoch 9.35: Loss = 0.81163
Epoch 9.36: Loss = 0.758682
Epoch 9.37: Loss = 0.982269
Epoch 9.38: Loss = 0.708252
Epoch 9.39: Loss = 0.687347
Epoch 9.40: Loss = 0.804489
Epoch 9.41: Loss = 0.767914
Epoch 9.42: Loss = 0.755722
Epoch 9.43: Loss = 0.645691
Epoch 9.44: Loss = 0.800842
Epoch 9.45: Loss = 0.713211
Epoch 9.46: Loss = 0.763351
Epoch 9.47: Loss = 0.863281
Epoch 9.48: Loss = 0.794266
Epoch 9.49: Loss = 0.752609
Epoch 9.50: Loss = 0.791504
Epoch 9.51: Loss = 0.640503
Epoch 9.52: Loss = 0.686096
Epoch 9.53: Loss = 0.615967
Epoch 9.54: Loss = 0.755096
Epoch 9.55: Loss = 0.796524
Epoch 9.56: Loss = 0.637436
Epoch 9.57: Loss = 0.804199
Epoch 9.58: Loss = 0.850311
Epoch 9.59: Loss = 0.746185
Epoch 9.60: Loss = 0.689926
Epoch 9.61: Loss = 0.688751
Epoch 9.62: Loss = 0.779114
Epoch 9.63: Loss = 0.616165
Epoch 9.64: Loss = 0.738113
Epoch 9.65: Loss = 0.77327
Epoch 9.66: Loss = 0.768158
Epoch 9.67: Loss = 0.737717
Epoch 9.68: Loss = 0.737671
Epoch 9.69: Loss = 0.564209
Epoch 9.70: Loss = 0.773193
Epoch 9.71: Loss = 0.745911
Epoch 9.72: Loss = 0.708511
Epoch 9.73: Loss = 0.688766
Epoch 9.74: Loss = 0.772644
Epoch 9.75: Loss = 0.664307
Epoch 9.76: Loss = 0.852356
Epoch 9.77: Loss = 0.703186
Epoch 9.78: Loss = 0.758057
Epoch 9.79: Loss = 0.608047
Epoch 9.80: Loss = 0.745422
Epoch 9.81: Loss = 0.800903
Epoch 9.82: Loss = 0.588898
Epoch 9.83: Loss = 0.821457
Epoch 9.84: Loss = 0.657227
Epoch 9.85: Loss = 0.689072
Epoch 9.86: Loss = 0.732742
Epoch 9.87: Loss = 0.759491
Epoch 9.88: Loss = 0.536819
Epoch 9.89: Loss = 0.922958
Epoch 9.90: Loss = 0.659439
Epoch 9.91: Loss = 0.727539
Epoch 9.92: Loss = 0.729034
Epoch 9.93: Loss = 0.676163
Epoch 9.94: Loss = 0.848129
Epoch 9.95: Loss = 0.663651
Epoch 9.96: Loss = 0.789627
Epoch 9.97: Loss = 0.846909
Epoch 9.98: Loss = 0.79744
Epoch 9.99: Loss = 0.719482
Epoch 9.100: Loss = 0.785324
Epoch 9.101: Loss = 0.865997
Epoch 9.102: Loss = 0.600296
Epoch 9.103: Loss = 0.65625
Epoch 9.104: Loss = 0.652237
Epoch 9.105: Loss = 0.827972
Epoch 9.106: Loss = 0.876175
Epoch 9.107: Loss = 0.798172
Epoch 9.108: Loss = 0.63353
Epoch 9.109: Loss = 0.682831
Epoch 9.110: Loss = 0.846893
Epoch 9.111: Loss = 0.691986
Epoch 9.112: Loss = 0.692169
Epoch 9.113: Loss = 0.61911
Epoch 9.114: Loss = 0.685165
Epoch 9.115: Loss = 0.987381
Epoch 9.116: Loss = 0.806549
Epoch 9.117: Loss = 0.838577
Epoch 9.118: Loss = 0.693481
Epoch 9.119: Loss = 0.695297
Epoch 9.120: Loss = 0.652664
TRAIN LOSS = 0.737045
TRAIN ACC = 80.7724 % (48466/60000)
Loss = 0.615097
Loss = 0.731155
Loss = 0.698303
Loss = 0.658493
Loss = 0.725342
Loss = 0.905029
Loss = 0.989532
Loss = 0.760422
Loss = 0.744019
Loss = 0.694016
Loss = 0.997772
Loss = 0.880157
Loss = 0.793961
Loss = 0.784332
Loss = 0.816055
Loss = 0.730225
Loss = 0.712555
Loss = 0.850571
Loss = 0.808548
Loss = 0.771469
TEST LOSS = 0.783352
TEST ACC = 484.659 % (7990/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.684418
Epoch 10.2: Loss = 0.719864
Epoch 10.3: Loss = 0.860489
Epoch 10.4: Loss = 0.647797
Epoch 10.5: Loss = 0.799225
Epoch 10.6: Loss = 0.736618
Epoch 10.7: Loss = 0.671082
Epoch 10.8: Loss = 0.820496
Epoch 10.9: Loss = 0.755585
Epoch 10.10: Loss = 0.750168
Epoch 10.11: Loss = 0.808121
Epoch 10.12: Loss = 0.700119
Epoch 10.13: Loss = 0.619995
Epoch 10.14: Loss = 0.746765
Epoch 10.15: Loss = 0.827499
Epoch 10.16: Loss = 0.779053
Epoch 10.17: Loss = 0.73497
Epoch 10.18: Loss = 0.846481
Epoch 10.19: Loss = 0.691742
Epoch 10.20: Loss = 0.785797
Epoch 10.21: Loss = 0.735992
Epoch 10.22: Loss = 0.710358
Epoch 10.23: Loss = 0.710861
Epoch 10.24: Loss = 0.726074
Epoch 10.25: Loss = 0.736862
Epoch 10.26: Loss = 0.757919
Epoch 10.27: Loss = 0.886612
Epoch 10.28: Loss = 0.8013
Epoch 10.29: Loss = 0.843445
Epoch 10.30: Loss = 1.01233
Epoch 10.31: Loss = 0.853561
Epoch 10.32: Loss = 0.904541
Epoch 10.33: Loss = 0.722458
Epoch 10.34: Loss = 0.824631
Epoch 10.35: Loss = 0.984528
Epoch 10.36: Loss = 0.931885
Epoch 10.37: Loss = 0.780258
Epoch 10.38: Loss = 0.74054
Epoch 10.39: Loss = 32.7543
Epoch 10.40: Loss = 13.0322
Epoch 10.41: Loss = 7.40221
Epoch 10.42: Loss = 5.33649
Epoch 10.43: Loss = 3.79346
Epoch 10.44: Loss = 4.38168
Epoch 10.45: Loss = 4.02126
Epoch 10.46: Loss = 3.8558
Epoch 10.47: Loss = 3.40456
Epoch 10.48: Loss = 3.30392
Epoch 10.49: Loss = 2.34073
Epoch 10.50: Loss = 2.15636
Epoch 10.51: Loss = 1.91612
Epoch 10.52: Loss = 2.43933
Epoch 10.53: Loss = 1.88416
Epoch 10.54: Loss = 2.02014
Epoch 10.55: Loss = 1.44524
Epoch 10.56: Loss = 1.40729
Epoch 10.57: Loss = 0.931229
Epoch 10.58: Loss = 1.30424
Epoch 10.59: Loss = 1.03084
Epoch 10.60: Loss = 0.799347
Epoch 10.61: Loss = 0.964417
Epoch 10.62: Loss = 0.905853
Epoch 10.63: Loss = 1.0687
Epoch 10.64: Loss = 0.937546
Epoch 10.65: Loss = 0.843811
Epoch 10.66: Loss = 0.674988
Epoch 10.67: Loss = 0.802658
Epoch 10.68: Loss = 0.890518
Epoch 10.69: Loss = 0.679413
Epoch 10.70: Loss = 0.845703
Epoch 10.71: Loss = 0.97702
Epoch 10.72: Loss = 0.749237
Epoch 10.73: Loss = 0.685501
Epoch 10.74: Loss = 0.773804
Epoch 10.75: Loss = 0.978729
Epoch 10.76: Loss = 0.912888
Epoch 10.77: Loss = 0.696457
Epoch 10.78: Loss = 0.606628
Epoch 10.79: Loss = 0.803574
Epoch 10.80: Loss = 0.653046
Epoch 10.81: Loss = 0.662964
Epoch 10.82: Loss = 0.71579
Epoch 10.83: Loss = 0.965744
Epoch 10.84: Loss = 0.810623
Epoch 10.85: Loss = 0.783371
Epoch 10.86: Loss = 0.846512
Epoch 10.87: Loss = 0.753204
Epoch 10.88: Loss = 0.728897
Epoch 10.89: Loss = 0.988281
Epoch 10.90: Loss = 0.679672
Epoch 10.91: Loss = 0.858948
Epoch 10.92: Loss = 0.689178
Epoch 10.93: Loss = 0.789566
Epoch 10.94: Loss = 0.714249
Epoch 10.95: Loss = 0.728867
Epoch 10.96: Loss = 0.633163
Epoch 10.97: Loss = 1.00015
Epoch 10.98: Loss = 0.71196
Epoch 10.99: Loss = 0.775085
Epoch 10.100: Loss = 0.67485
Epoch 10.101: Loss = 0.889832
Epoch 10.102: Loss = 0.617188
Epoch 10.103: Loss = 0.757233
Epoch 10.104: Loss = 0.670837
Epoch 10.105: Loss = 1.05
Epoch 10.106: Loss = 0.819809
Epoch 10.107: Loss = 0.672913
Epoch 10.108: Loss = 0.946762
Epoch 10.109: Loss = 0.702805
Epoch 10.110: Loss = 0.63707
Epoch 10.111: Loss = 1.0202
Epoch 10.112: Loss = 0.777252
Epoch 10.113: Loss = 0.8703
Epoch 10.114: Loss = 0.752365
Epoch 10.115: Loss = 0.692795
Epoch 10.116: Loss = 0.756332
Epoch 10.117: Loss = 0.72641
Epoch 10.118: Loss = 0.763809
Epoch 10.119: Loss = 0.712418
Epoch 10.120: Loss = 0.787384
TRAIN LOSS = 1.48499
TRAIN ACC = 78.5782 % (47149/60000)
Loss = 0.624237
Loss = 0.748062
Loss = 0.711472
Loss = 0.698593
Loss = 0.774704
Loss = 0.896408
Loss = 1.03236
Loss = 0.842148
Loss = 0.773346
Loss = 0.674744
Loss = 1.03127
Loss = 0.987274
Loss = 0.847992
Loss = 0.884247
Loss = 0.868256
Loss = 0.787399
Loss = 0.710968
Loss = 0.93219
Loss = 0.873245
Loss = 0.803848
TEST LOSS = 0.825138
TEST ACC = 471.489 % (7955/10000)

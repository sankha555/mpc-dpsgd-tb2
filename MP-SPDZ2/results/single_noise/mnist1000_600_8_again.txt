Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 1000]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 15
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.30704
Epoch 1.2: Loss = 2.27734
Epoch 1.3: Loss = 2.21931
Epoch 1.4: Loss = 2.17377
Epoch 1.5: Loss = 2.12643
Epoch 1.6: Loss = 2.07726
Epoch 1.7: Loss = 2.04561
Epoch 1.8: Loss = 2.00989
Epoch 1.9: Loss = 1.97098
Epoch 1.10: Loss = 1.92415
Epoch 1.11: Loss = 1.8506
Epoch 1.12: Loss = 1.85182
Epoch 1.13: Loss = 1.8241
Epoch 1.14: Loss = 1.73737
Epoch 1.15: Loss = 1.74072
Epoch 1.16: Loss = 1.71332
Epoch 1.17: Loss = 1.66115
Epoch 1.18: Loss = 1.59125
Epoch 1.19: Loss = 1.57376
Epoch 1.20: Loss = 1.56866
Epoch 1.21: Loss = 1.51686
Epoch 1.22: Loss = 1.46721
Epoch 1.23: Loss = 1.43045
Epoch 1.24: Loss = 1.42624
Epoch 1.25: Loss = 1.38574
Epoch 1.26: Loss = 1.3297
Epoch 1.27: Loss = 1.39655
Epoch 1.28: Loss = 1.36453
Epoch 1.29: Loss = 1.31184
Epoch 1.30: Loss = 1.30504
Epoch 1.31: Loss = 1.23915
Epoch 1.32: Loss = 1.23912
Epoch 1.33: Loss = 1.22125
Epoch 1.34: Loss = 1.18571
Epoch 1.35: Loss = 1.18874
Epoch 1.36: Loss = 1.18961
Epoch 1.37: Loss = 1.11639
Epoch 1.38: Loss = 1.06641
Epoch 1.39: Loss = 1.10565
Epoch 1.40: Loss = 1.05023
Epoch 1.41: Loss = 1.03114
Epoch 1.42: Loss = 1.03252
Epoch 1.43: Loss = 1.02242
Epoch 1.44: Loss = 0.987335
Epoch 1.45: Loss = 0.985336
Epoch 1.46: Loss = 0.966248
Epoch 1.47: Loss = 0.919632
Epoch 1.48: Loss = 0.983917
Epoch 1.49: Loss = 0.939407
Epoch 1.50: Loss = 0.986282
Epoch 1.51: Loss = 0.907257
Epoch 1.52: Loss = 0.934998
Epoch 1.53: Loss = 0.87056
Epoch 1.54: Loss = 0.891541
Epoch 1.55: Loss = 0.893036
Epoch 1.56: Loss = 0.857498
Epoch 1.57: Loss = 0.805603
Epoch 1.58: Loss = 0.82402
Epoch 1.59: Loss = 0.815948
Epoch 1.60: Loss = 0.762405
Epoch 1.61: Loss = 0.798935
Epoch 1.62: Loss = 0.781464
Epoch 1.63: Loss = 0.819427
Epoch 1.64: Loss = 0.729782
Epoch 1.65: Loss = 0.688171
Epoch 1.66: Loss = 0.755127
Epoch 1.67: Loss = 0.77832
Epoch 1.68: Loss = 0.698975
Epoch 1.69: Loss = 0.808533
Epoch 1.70: Loss = 0.762527
Epoch 1.71: Loss = 0.741852
Epoch 1.72: Loss = 0.737442
Epoch 1.73: Loss = 0.74588
Epoch 1.74: Loss = 0.809586
Epoch 1.75: Loss = 0.664474
Epoch 1.76: Loss = 0.643692
Epoch 1.77: Loss = 0.686417
Epoch 1.78: Loss = 0.693985
Epoch 1.79: Loss = 0.66391
Epoch 1.80: Loss = 0.648087
Epoch 1.81: Loss = 0.67746
Epoch 1.82: Loss = 0.64325
Epoch 1.83: Loss = 0.623444
Epoch 1.84: Loss = 0.614838
Epoch 1.85: Loss = 0.622757
Epoch 1.86: Loss = 0.658875
Epoch 1.87: Loss = 0.615189
Epoch 1.88: Loss = 0.656754
Epoch 1.89: Loss = 0.641861
Epoch 1.90: Loss = 0.683487
Epoch 1.91: Loss = 0.678726
Epoch 1.92: Loss = 0.556015
Epoch 1.93: Loss = 0.632278
Epoch 1.94: Loss = 0.625031
Epoch 1.95: Loss = 0.576584
Epoch 1.96: Loss = 0.584702
Epoch 1.97: Loss = 0.619614
Epoch 1.98: Loss = 0.600784
Epoch 1.99: Loss = 0.630981
Epoch 1.100: Loss = 0.5354
TRAIN LOSS = 1.09334
TRAIN ACC = 72.4396 % (43465/60000)
Loss = 0.64064
Loss = 0.645065
Loss = 0.768173
Loss = 0.712372
Loss = 0.630905
Loss = 0.632217
Loss = 0.705338
Loss = 0.695801
Loss = 0.5224
Loss = 0.46846
Loss = 0.416245
Loss = 0.504044
Loss = 0.454117
Loss = 0.452072
Loss = 0.271439
Loss = 0.43187
Loss = 0.762558
TEST LOSS = 0.567572
TEST ACC = 434.65 % (8444/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.606934
Epoch 2.2: Loss = 0.600388
Epoch 2.3: Loss = 0.578232
Epoch 2.4: Loss = 0.567337
Epoch 2.5: Loss = 0.543976
Epoch 2.6: Loss = 0.596802
Epoch 2.7: Loss = 0.554474
Epoch 2.8: Loss = 0.581024
Epoch 2.9: Loss = 0.578476
Epoch 2.10: Loss = 0.570709
Epoch 2.11: Loss = 0.594543
Epoch 2.12: Loss = 0.598083
Epoch 2.13: Loss = 0.593338
Epoch 2.14: Loss = 0.514557
Epoch 2.15: Loss = 0.566269
Epoch 2.16: Loss = 0.55806
Epoch 2.17: Loss = 0.546112
Epoch 2.18: Loss = 0.584885
Epoch 2.19: Loss = 0.538101
Epoch 2.20: Loss = 0.472961
Epoch 2.21: Loss = 0.460007
Epoch 2.22: Loss = 0.545959
Epoch 2.23: Loss = 0.605667
Epoch 2.24: Loss = 0.577408
Epoch 2.25: Loss = 0.547729
Epoch 2.26: Loss = 0.554581
Epoch 2.27: Loss = 0.535522
Epoch 2.28: Loss = 0.556412
Epoch 2.29: Loss = 0.530823
Epoch 2.30: Loss = 0.528473
Epoch 2.31: Loss = 0.530884
Epoch 2.32: Loss = 0.495621
Epoch 2.33: Loss = 0.512253
Epoch 2.34: Loss = 0.493042
Epoch 2.35: Loss = 0.51123
Epoch 2.36: Loss = 0.490936
Epoch 2.37: Loss = 0.458023
Epoch 2.38: Loss = 0.510712
Epoch 2.39: Loss = 0.503952
Epoch 2.40: Loss = 0.573776
Epoch 2.41: Loss = 0.514465
Epoch 2.42: Loss = 0.562668
Epoch 2.43: Loss = 0.544479
Epoch 2.44: Loss = 0.52179
Epoch 2.45: Loss = 0.554916
Epoch 2.46: Loss = 0.48967
Epoch 2.47: Loss = 0.497223
Epoch 2.48: Loss = 0.451935
Epoch 2.49: Loss = 0.521057
Epoch 2.50: Loss = 0.477585
Epoch 2.51: Loss = 0.530548
Epoch 2.52: Loss = 0.482407
Epoch 2.53: Loss = 0.483154
Epoch 2.54: Loss = 0.456299
Epoch 2.55: Loss = 0.485382
Epoch 2.56: Loss = 0.486465
Epoch 2.57: Loss = 0.490616
Epoch 2.58: Loss = 0.502243
Epoch 2.59: Loss = 0.497772
Epoch 2.60: Loss = 0.543915
Epoch 2.61: Loss = 0.461288
Epoch 2.62: Loss = 0.481827
Epoch 2.63: Loss = 0.530502
Epoch 2.64: Loss = 0.485367
Epoch 2.65: Loss = 0.52359
Epoch 2.66: Loss = 0.492874
Epoch 2.67: Loss = 0.463074
Epoch 2.68: Loss = 0.466354
Epoch 2.69: Loss = 0.509338
Epoch 2.70: Loss = 0.462326
Epoch 2.71: Loss = 0.445114
Epoch 2.72: Loss = 0.457504
Epoch 2.73: Loss = 0.487259
Epoch 2.74: Loss = 0.494125
Epoch 2.75: Loss = 0.459686
Epoch 2.76: Loss = 0.449509
Epoch 2.77: Loss = 0.460403
Epoch 2.78: Loss = 0.511765
Epoch 2.79: Loss = 0.480698
Epoch 2.80: Loss = 0.442383
Epoch 2.81: Loss = 0.41684
Epoch 2.82: Loss = 0.46489
Epoch 2.83: Loss = 0.462799
Epoch 2.84: Loss = 0.498108
Epoch 2.85: Loss = 0.454071
Epoch 2.86: Loss = 0.504257
Epoch 2.87: Loss = 0.450943
Epoch 2.88: Loss = 0.411896
Epoch 2.89: Loss = 0.423965
Epoch 2.90: Loss = 0.411972
Epoch 2.91: Loss = 0.46315
Epoch 2.92: Loss = 0.466293
Epoch 2.93: Loss = 0.461761
Epoch 2.94: Loss = 0.495972
Epoch 2.95: Loss = 0.44487
Epoch 2.96: Loss = 0.414185
Epoch 2.97: Loss = 0.42955
Epoch 2.98: Loss = 0.426285
Epoch 2.99: Loss = 0.493744
Epoch 2.100: Loss = 0.435944
TRAIN LOSS = 0.5056
TRAIN ACC = 85.4279 % (51259/60000)
Loss = 0.470352
Loss = 0.491852
Loss = 0.621368
Loss = 0.573547
Loss = 0.454819
Loss = 0.467178
Loss = 0.57457
Loss = 0.533066
Loss = 0.386444
Loss = 0.331924
Loss = 0.31456
Loss = 0.350266
Loss = 0.288025
Loss = 0.341049
Loss = 0.146347
Loss = 0.279938
Loss = 0.636337
TEST LOSS = 0.422972
TEST ACC = 512.589 % (8780/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.436646
Epoch 3.2: Loss = 0.468552
Epoch 3.3: Loss = 0.4655
Epoch 3.4: Loss = 0.47142
Epoch 3.5: Loss = 0.497986
Epoch 3.6: Loss = 0.490784
Epoch 3.7: Loss = 0.478973
Epoch 3.8: Loss = 0.421356
Epoch 3.9: Loss = 0.434921
Epoch 3.10: Loss = 0.433304
Epoch 3.11: Loss = 0.480453
Epoch 3.12: Loss = 0.465897
Epoch 3.13: Loss = 0.509201
Epoch 3.14: Loss = 0.412796
Epoch 3.15: Loss = 0.362396
Epoch 3.16: Loss = 0.429398
Epoch 3.17: Loss = 0.411102
Epoch 3.18: Loss = 0.420456
Epoch 3.19: Loss = 0.401688
Epoch 3.20: Loss = 0.491745
Epoch 3.21: Loss = 0.414886
Epoch 3.22: Loss = 0.465637
Epoch 3.23: Loss = 0.40863
Epoch 3.24: Loss = 0.494476
Epoch 3.25: Loss = 0.39389
Epoch 3.26: Loss = 0.408966
Epoch 3.27: Loss = 0.433334
Epoch 3.28: Loss = 0.439194
Epoch 3.29: Loss = 0.405853
Epoch 3.30: Loss = 0.461578
Epoch 3.31: Loss = 0.408997
Epoch 3.32: Loss = 0.391586
Epoch 3.33: Loss = 0.46138
Epoch 3.34: Loss = 0.403763
Epoch 3.35: Loss = 0.355789
Epoch 3.36: Loss = 0.492126
Epoch 3.37: Loss = 0.4543
Epoch 3.38: Loss = 0.386566
Epoch 3.39: Loss = 0.418976
Epoch 3.40: Loss = 0.487595
Epoch 3.41: Loss = 0.420502
Epoch 3.42: Loss = 0.351395
Epoch 3.43: Loss = 0.388351
Epoch 3.44: Loss = 0.45491
Epoch 3.45: Loss = 0.500061
Epoch 3.46: Loss = 0.400833
Epoch 3.47: Loss = 0.4207
Epoch 3.48: Loss = 0.451782
Epoch 3.49: Loss = 0.394104
Epoch 3.50: Loss = 0.438004
Epoch 3.51: Loss = 0.422943
Epoch 3.52: Loss = 0.380325
Epoch 3.53: Loss = 0.395493
Epoch 3.54: Loss = 0.437317
Epoch 3.55: Loss = 0.402252
Epoch 3.56: Loss = 0.451553
Epoch 3.57: Loss = 0.337173
Epoch 3.58: Loss = 0.350815
Epoch 3.59: Loss = 0.473404
Epoch 3.60: Loss = 0.398529
Epoch 3.61: Loss = 0.449905
Epoch 3.62: Loss = 0.411774
Epoch 3.63: Loss = 0.374786
Epoch 3.64: Loss = 0.454163
Epoch 3.65: Loss = 0.424118
Epoch 3.66: Loss = 0.42572
Epoch 3.67: Loss = 0.432388
Epoch 3.68: Loss = 0.384048
Epoch 3.69: Loss = 0.432831
Epoch 3.70: Loss = 0.38121
Epoch 3.71: Loss = 0.484299
Epoch 3.72: Loss = 0.391922
Epoch 3.73: Loss = 0.326477
Epoch 3.74: Loss = 0.357346
Epoch 3.75: Loss = 0.396759
Epoch 3.76: Loss = 0.401962
Epoch 3.77: Loss = 0.524719
Epoch 3.78: Loss = 0.387787
Epoch 3.79: Loss = 0.491592
Epoch 3.80: Loss = 0.446518
Epoch 3.81: Loss = 0.418182
Epoch 3.82: Loss = 0.428482
Epoch 3.83: Loss = 0.392807
Epoch 3.84: Loss = 0.436935
Epoch 3.85: Loss = 0.377426
Epoch 3.86: Loss = 0.3759
Epoch 3.87: Loss = 0.387817
Epoch 3.88: Loss = 0.371307
Epoch 3.89: Loss = 0.399216
Epoch 3.90: Loss = 0.380188
Epoch 3.91: Loss = 0.443954
Epoch 3.92: Loss = 0.423279
Epoch 3.93: Loss = 0.368744
Epoch 3.94: Loss = 0.39299
Epoch 3.95: Loss = 0.419312
Epoch 3.96: Loss = 0.384918
Epoch 3.97: Loss = 0.434998
Epoch 3.98: Loss = 0.413513
Epoch 3.99: Loss = 0.404831
Epoch 3.100: Loss = 0.424667
TRAIN LOSS = 0.423035
TRAIN ACC = 87.4344 % (52463/60000)
Loss = 0.413162
Loss = 0.443451
Loss = 0.571655
Loss = 0.531509
Loss = 0.394104
Loss = 0.407669
Loss = 0.531052
Loss = 0.478882
Loss = 0.345215
Loss = 0.275635
Loss = 0.299591
Loss = 0.300079
Loss = 0.237457
Loss = 0.323746
Loss = 0.110443
Loss = 0.241913
Loss = 0.595978
TEST LOSS = 0.378173
TEST ACC = 524.629 % (8899/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.321259
Epoch 4.2: Loss = 0.400833
Epoch 4.3: Loss = 0.363098
Epoch 4.4: Loss = 0.375046
Epoch 4.5: Loss = 0.470032
Epoch 4.6: Loss = 0.432724
Epoch 4.7: Loss = 0.389694
Epoch 4.8: Loss = 0.395447
Epoch 4.9: Loss = 0.391785
Epoch 4.10: Loss = 0.442093
Epoch 4.11: Loss = 0.383072
Epoch 4.12: Loss = 0.419739
Epoch 4.13: Loss = 0.384811
Epoch 4.14: Loss = 0.413757
Epoch 4.15: Loss = 0.400742
Epoch 4.16: Loss = 0.41304
Epoch 4.17: Loss = 0.450745
Epoch 4.18: Loss = 0.44342
Epoch 4.19: Loss = 0.368515
Epoch 4.20: Loss = 0.390213
Epoch 4.21: Loss = 0.401733
Epoch 4.22: Loss = 0.43219
Epoch 4.23: Loss = 0.416565
Epoch 4.24: Loss = 0.38942
Epoch 4.25: Loss = 0.335754
Epoch 4.26: Loss = 0.389435
Epoch 4.27: Loss = 0.346481
Epoch 4.28: Loss = 0.394089
Epoch 4.29: Loss = 0.407959
Epoch 4.30: Loss = 0.393295
Epoch 4.31: Loss = 0.393875
Epoch 4.32: Loss = 0.352737
Epoch 4.33: Loss = 0.459976
Epoch 4.34: Loss = 0.344849
Epoch 4.35: Loss = 0.392166
Epoch 4.36: Loss = 0.350159
Epoch 4.37: Loss = 0.400253
Epoch 4.38: Loss = 0.487762
Epoch 4.39: Loss = 0.433884
Epoch 4.40: Loss = 0.370529
Epoch 4.41: Loss = 0.447388
Epoch 4.42: Loss = 0.363724
Epoch 4.43: Loss = 0.395203
Epoch 4.44: Loss = 0.371475
Epoch 4.45: Loss = 0.462753
Epoch 4.46: Loss = 0.412552
Epoch 4.47: Loss = 0.370468
Epoch 4.48: Loss = 0.413528
Epoch 4.49: Loss = 0.346573
Epoch 4.50: Loss = 0.378784
Epoch 4.51: Loss = 0.33905
Epoch 4.52: Loss = 0.377838
Epoch 4.53: Loss = 0.440369
Epoch 4.54: Loss = 0.342514
Epoch 4.55: Loss = 0.397644
Epoch 4.56: Loss = 0.351257
Epoch 4.57: Loss = 0.38765
Epoch 4.58: Loss = 0.436401
Epoch 4.59: Loss = 0.377884
Epoch 4.60: Loss = 0.434204
Epoch 4.61: Loss = 0.332962
Epoch 4.62: Loss = 0.394043
Epoch 4.63: Loss = 0.356659
Epoch 4.64: Loss = 0.44191
Epoch 4.65: Loss = 0.357651
Epoch 4.66: Loss = 0.344086
Epoch 4.67: Loss = 0.339828
Epoch 4.68: Loss = 0.416626
Epoch 4.69: Loss = 0.377594
Epoch 4.70: Loss = 0.362518
Epoch 4.71: Loss = 0.332596
Epoch 4.72: Loss = 0.385941
Epoch 4.73: Loss = 0.437958
Epoch 4.74: Loss = 0.428726
Epoch 4.75: Loss = 0.412247
Epoch 4.76: Loss = 0.356827
Epoch 4.77: Loss = 0.377335
Epoch 4.78: Loss = 0.323166
Epoch 4.79: Loss = 0.46814
Epoch 4.80: Loss = 0.398758
Epoch 4.81: Loss = 0.346161
Epoch 4.82: Loss = 0.317841
Epoch 4.83: Loss = 0.409393
Epoch 4.84: Loss = 0.364471
Epoch 4.85: Loss = 0.405853
Epoch 4.86: Loss = 0.374771
Epoch 4.87: Loss = 0.350357
Epoch 4.88: Loss = 0.422897
Epoch 4.89: Loss = 0.43985
Epoch 4.90: Loss = 0.404587
Epoch 4.91: Loss = 0.427887
Epoch 4.92: Loss = 0.33432
Epoch 4.93: Loss = 0.39328
Epoch 4.94: Loss = 0.327484
Epoch 4.95: Loss = 0.403137
Epoch 4.96: Loss = 0.421402
Epoch 4.97: Loss = 0.334396
Epoch 4.98: Loss = 0.375336
Epoch 4.99: Loss = 0.394424
Epoch 4.100: Loss = 0.381989
TRAIN LOSS = 0.390656
TRAIN ACC = 88.3774 % (53029/60000)
Loss = 0.38858
Loss = 0.421112
Loss = 0.550369
Loss = 0.522446
Loss = 0.36557
Loss = 0.379883
Loss = 0.517502
Loss = 0.458328
Loss = 0.326813
Loss = 0.259949
Loss = 0.282898
Loss = 0.280472
Loss = 0.211548
Loss = 0.300385
Loss = 0.0906525
Loss = 0.218323
Loss = 0.576614
TEST LOSS = 0.357554
TEST ACC = 530.289 % (8948/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.433853
Epoch 5.2: Loss = 0.419022
Epoch 5.3: Loss = 0.386368
Epoch 5.4: Loss = 0.396606
Epoch 5.5: Loss = 0.359863
Epoch 5.6: Loss = 0.334229
Epoch 5.7: Loss = 0.365723
Epoch 5.8: Loss = 0.339279
Epoch 5.9: Loss = 0.394196
Epoch 5.10: Loss = 0.386032
Epoch 5.11: Loss = 0.408798
Epoch 5.12: Loss = 0.333755
Epoch 5.13: Loss = 0.363235
Epoch 5.14: Loss = 0.389969
Epoch 5.15: Loss = 0.358597
Epoch 5.16: Loss = 0.401306
Epoch 5.17: Loss = 0.425552
Epoch 5.18: Loss = 0.303101
Epoch 5.19: Loss = 0.364746
Epoch 5.20: Loss = 0.354218
Epoch 5.21: Loss = 0.393097
Epoch 5.22: Loss = 0.327881
Epoch 5.23: Loss = 0.362946
Epoch 5.24: Loss = 0.339645
Epoch 5.25: Loss = 0.357254
Epoch 5.26: Loss = 0.39119
Epoch 5.27: Loss = 0.360245
Epoch 5.28: Loss = 0.427124
Epoch 5.29: Loss = 0.420624
Epoch 5.30: Loss = 0.419922
Epoch 5.31: Loss = 0.467361
Epoch 5.32: Loss = 0.318741
Epoch 5.33: Loss = 0.396454
Epoch 5.34: Loss = 0.40509
Epoch 5.35: Loss = 0.377792
Epoch 5.36: Loss = 0.331116
Epoch 5.37: Loss = 0.354462
Epoch 5.38: Loss = 0.416962
Epoch 5.39: Loss = 0.345535
Epoch 5.40: Loss = 0.402267
Epoch 5.41: Loss = 0.376892
Epoch 5.42: Loss = 0.351471
Epoch 5.43: Loss = 0.393616
Epoch 5.44: Loss = 0.368088
Epoch 5.45: Loss = 0.331284
Epoch 5.46: Loss = 0.341843
Epoch 5.47: Loss = 0.353149
Epoch 5.48: Loss = 0.41835
Epoch 5.49: Loss = 0.363144
Epoch 5.50: Loss = 0.362091
Epoch 5.51: Loss = 0.389038
Epoch 5.52: Loss = 0.398666
Epoch 5.53: Loss = 0.379211
Epoch 5.54: Loss = 0.324112
Epoch 5.55: Loss = 0.409912
Epoch 5.56: Loss = 0.417053
Epoch 5.57: Loss = 0.347519
Epoch 5.58: Loss = 0.344604
Epoch 5.59: Loss = 0.370667
Epoch 5.60: Loss = 0.384933
Epoch 5.61: Loss = 0.353271
Epoch 5.62: Loss = 0.339432
Epoch 5.63: Loss = 0.382385
Epoch 5.64: Loss = 0.378891
Epoch 5.65: Loss = 0.315643
Epoch 5.66: Loss = 0.419434
Epoch 5.67: Loss = 0.38884
Epoch 5.68: Loss = 0.337708
Epoch 5.69: Loss = 0.421829
Epoch 5.70: Loss = 0.381027
Epoch 5.71: Loss = 0.435684
Epoch 5.72: Loss = 0.338181
Epoch 5.73: Loss = 0.40007
Epoch 5.74: Loss = 0.410599
Epoch 5.75: Loss = 0.344711
Epoch 5.76: Loss = 0.401367
Epoch 5.77: Loss = 0.34729
Epoch 5.78: Loss = 0.354645
Epoch 5.79: Loss = 0.453079
Epoch 5.80: Loss = 0.314026
Epoch 5.81: Loss = 0.364365
Epoch 5.82: Loss = 0.301483
Epoch 5.83: Loss = 0.300613
Epoch 5.84: Loss = 0.375946
Epoch 5.85: Loss = 0.339661
Epoch 5.86: Loss = 0.425781
Epoch 5.87: Loss = 0.424805
Epoch 5.88: Loss = 0.401672
Epoch 5.89: Loss = 0.35672
Epoch 5.90: Loss = 0.382065
Epoch 5.91: Loss = 0.356537
Epoch 5.92: Loss = 0.361374
Epoch 5.93: Loss = 0.398331
Epoch 5.94: Loss = 0.421051
Epoch 5.95: Loss = 0.45018
Epoch 5.96: Loss = 0.35907
Epoch 5.97: Loss = 0.454437
Epoch 5.98: Loss = 0.369293
Epoch 5.99: Loss = 0.360336
Epoch 5.100: Loss = 0.33165
TRAIN LOSS = 0.376129
TRAIN ACC = 88.8107 % (53288/60000)
Loss = 0.37616
Loss = 0.416809
Loss = 0.543671
Loss = 0.514969
Loss = 0.35144
Loss = 0.367584
Loss = 0.511169
Loss = 0.446106
Loss = 0.319443
Loss = 0.254578
Loss = 0.288086
Loss = 0.263718
Loss = 0.192444
Loss = 0.291061
Loss = 0.0819702
Loss = 0.21051
Loss = 0.562775
TEST LOSS = 0.348294
TEST ACC = 532.88 % (8987/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.357224
Epoch 6.2: Loss = 0.394928
Epoch 6.3: Loss = 0.351135
Epoch 6.4: Loss = 0.344254
Epoch 6.5: Loss = 0.330963
Epoch 6.6: Loss = 0.374207
Epoch 6.7: Loss = 0.387894
Epoch 6.8: Loss = 0.464264
Epoch 6.9: Loss = 0.417389
Epoch 6.10: Loss = 0.409485
Epoch 6.11: Loss = 0.442444
Epoch 6.12: Loss = 0.372208
Epoch 6.13: Loss = 0.351135
Epoch 6.14: Loss = 0.304886
Epoch 6.15: Loss = 0.376022
Epoch 6.16: Loss = 0.313919
Epoch 6.17: Loss = 0.332809
Epoch 6.18: Loss = 0.396423
Epoch 6.19: Loss = 0.314728
Epoch 6.20: Loss = 0.421982
Epoch 6.21: Loss = 0.32724
Epoch 6.22: Loss = 0.377365
Epoch 6.23: Loss = 0.365814
Epoch 6.24: Loss = 0.437927
Epoch 6.25: Loss = 0.411163
Epoch 6.26: Loss = 0.379471
Epoch 6.27: Loss = 0.385666
Epoch 6.28: Loss = 0.361481
Epoch 6.29: Loss = 0.342896
Epoch 6.30: Loss = 0.439865
Epoch 6.31: Loss = 0.378036
Epoch 6.32: Loss = 0.356903
Epoch 6.33: Loss = 0.371918
Epoch 6.34: Loss = 0.373505
Epoch 6.35: Loss = 0.304352
Epoch 6.36: Loss = 0.325897
Epoch 6.37: Loss = 0.280182
Epoch 6.38: Loss = 0.430161
Epoch 6.39: Loss = 0.397614
Epoch 6.40: Loss = 0.345825
Epoch 6.41: Loss = 0.297699
Epoch 6.42: Loss = 0.380737
Epoch 6.43: Loss = 0.33017
Epoch 6.44: Loss = 0.419266
Epoch 6.45: Loss = 0.361862
Epoch 6.46: Loss = 0.323914
Epoch 6.47: Loss = 0.342026
Epoch 6.48: Loss = 0.333267
Epoch 6.49: Loss = 0.341278
Epoch 6.50: Loss = 0.41687
Epoch 6.51: Loss = 0.378967
Epoch 6.52: Loss = 0.307892
Epoch 6.53: Loss = 0.358337
Epoch 6.54: Loss = 0.320328
Epoch 6.55: Loss = 0.438278
Epoch 6.56: Loss = 0.418991
Epoch 6.57: Loss = 0.364899
Epoch 6.58: Loss = 0.370331
Epoch 6.59: Loss = 0.323868
Epoch 6.60: Loss = 0.386047
Epoch 6.61: Loss = 0.349228
Epoch 6.62: Loss = 0.438385
Epoch 6.63: Loss = 0.380203
Epoch 6.64: Loss = 0.366394
Epoch 6.65: Loss = 0.349533
Epoch 6.66: Loss = 0.32312
Epoch 6.67: Loss = 0.444336
Epoch 6.68: Loss = 0.304321
Epoch 6.69: Loss = 0.436829
Epoch 6.70: Loss = 0.36113
Epoch 6.71: Loss = 0.355621
Epoch 6.72: Loss = 0.328003
Epoch 6.73: Loss = 0.322006
Epoch 6.74: Loss = 0.370972
Epoch 6.75: Loss = 0.34227
Epoch 6.76: Loss = 0.349045
Epoch 6.77: Loss = 0.403519
Epoch 6.78: Loss = 0.395355
Epoch 6.79: Loss = 0.346161
Epoch 6.80: Loss = 0.322723
Epoch 6.81: Loss = 0.386963
Epoch 6.82: Loss = 0.338959
Epoch 6.83: Loss = 0.402481
Epoch 6.84: Loss = 0.344742
Epoch 6.85: Loss = 0.358032
Epoch 6.86: Loss = 0.379135
Epoch 6.87: Loss = 0.419647
Epoch 6.88: Loss = 0.363022
Epoch 6.89: Loss = 0.370544
Epoch 6.90: Loss = 0.330032
Epoch 6.91: Loss = 0.290756
Epoch 6.92: Loss = 0.367767
Epoch 6.93: Loss = 0.401703
Epoch 6.94: Loss = 0.379639
Epoch 6.95: Loss = 0.303268
Epoch 6.96: Loss = 0.368713
Epoch 6.97: Loss = 0.361557
Epoch 6.98: Loss = 0.324829
Epoch 6.99: Loss = 0.376266
Epoch 6.100: Loss = 0.312378
TRAIN LOSS = 0.365372
TRAIN ACC = 89.2761 % (53568/60000)
Loss = 0.363876
Loss = 0.403595
Loss = 0.53685
Loss = 0.509537
Loss = 0.340652
Loss = 0.359085
Loss = 0.509445
Loss = 0.436111
Loss = 0.313416
Loss = 0.247482
Loss = 0.281845
Loss = 0.255997
Loss = 0.182709
Loss = 0.282135
Loss = 0.0763397
Loss = 0.203705
Loss = 0.555099
TEST LOSS = 0.340371
TEST ACC = 535.68 % (9025/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.37915
Epoch 7.2: Loss = 0.33165
Epoch 7.3: Loss = 0.319992
Epoch 7.4: Loss = 0.377502
Epoch 7.5: Loss = 0.351334
Epoch 7.6: Loss = 0.379349
Epoch 7.7: Loss = 0.398621
Epoch 7.8: Loss = 0.329697
Epoch 7.9: Loss = 0.401047
Epoch 7.10: Loss = 0.440033
Epoch 7.11: Loss = 0.305649
Epoch 7.12: Loss = 0.359375
Epoch 7.13: Loss = 0.376038
Epoch 7.14: Loss = 0.354736
Epoch 7.15: Loss = 0.433273
Epoch 7.16: Loss = 0.433731
Epoch 7.17: Loss = 0.302597
Epoch 7.18: Loss = 0.320435
Epoch 7.19: Loss = 0.334351
Epoch 7.20: Loss = 0.341461
Epoch 7.21: Loss = 0.336823
Epoch 7.22: Loss = 0.449936
Epoch 7.23: Loss = 0.40657
Epoch 7.24: Loss = 0.360153
Epoch 7.25: Loss = 0.337814
Epoch 7.26: Loss = 0.38324
Epoch 7.27: Loss = 0.389496
Epoch 7.28: Loss = 0.332779
Epoch 7.29: Loss = 0.364502
Epoch 7.30: Loss = 0.343338
Epoch 7.31: Loss = 0.345551
Epoch 7.32: Loss = 0.395981
Epoch 7.33: Loss = 0.298645
Epoch 7.34: Loss = 0.395966
Epoch 7.35: Loss = 0.409851
Epoch 7.36: Loss = 0.387512
Epoch 7.37: Loss = 0.363068
Epoch 7.38: Loss = 0.389038
Epoch 7.39: Loss = 0.353851
Epoch 7.40: Loss = 0.330688
Epoch 7.41: Loss = 0.306152
Epoch 7.42: Loss = 0.36882
Epoch 7.43: Loss = 0.368408
Epoch 7.44: Loss = 0.403442
Epoch 7.45: Loss = 0.359314
Epoch 7.46: Loss = 0.350159
Epoch 7.47: Loss = 0.35083
Epoch 7.48: Loss = 0.357407
Epoch 7.49: Loss = 0.366913
Epoch 7.50: Loss = 0.354935
Epoch 7.51: Loss = 0.328934
Epoch 7.52: Loss = 0.338028
Epoch 7.53: Loss = 0.326004
Epoch 7.54: Loss = 0.312881
Epoch 7.55: Loss = 0.399612
Epoch 7.56: Loss = 0.312851
Epoch 7.57: Loss = 0.351913
Epoch 7.58: Loss = 0.415588
Epoch 7.59: Loss = 0.388794
Epoch 7.60: Loss = 0.413498
Epoch 7.61: Loss = 0.409149
Epoch 7.62: Loss = 0.373993
Epoch 7.63: Loss = 0.35881
Epoch 7.64: Loss = 0.370285
Epoch 7.65: Loss = 0.300385
Epoch 7.66: Loss = 0.370239
Epoch 7.67: Loss = 0.297714
Epoch 7.68: Loss = 0.303177
Epoch 7.69: Loss = 0.364182
Epoch 7.70: Loss = 0.407028
Epoch 7.71: Loss = 0.266541
Epoch 7.72: Loss = 0.321747
Epoch 7.73: Loss = 0.336685
Epoch 7.74: Loss = 0.338516
Epoch 7.75: Loss = 0.396805
Epoch 7.76: Loss = 0.372864
Epoch 7.77: Loss = 0.348587
Epoch 7.78: Loss = 0.356705
Epoch 7.79: Loss = 0.316559
Epoch 7.80: Loss = 0.376312
Epoch 7.81: Loss = 0.307632
Epoch 7.82: Loss = 0.347015
Epoch 7.83: Loss = 0.353333
Epoch 7.84: Loss = 0.285812
Epoch 7.85: Loss = 0.433624
Epoch 7.86: Loss = 0.407166
Epoch 7.87: Loss = 0.357437
Epoch 7.88: Loss = 0.247223
Epoch 7.89: Loss = 0.348877
Epoch 7.90: Loss = 0.313843
Epoch 7.91: Loss = 0.297226
Epoch 7.92: Loss = 0.363876
Epoch 7.93: Loss = 0.296082
Epoch 7.94: Loss = 0.456253
Epoch 7.95: Loss = 0.335022
Epoch 7.96: Loss = 0.318069
Epoch 7.97: Loss = 0.341064
Epoch 7.98: Loss = 0.33432
Epoch 7.99: Loss = 0.373398
Epoch 7.100: Loss = 0.336578
TRAIN LOSS = 0.356598
TRAIN ACC = 89.5554 % (53736/60000)
Loss = 0.35051
Loss = 0.392868
Loss = 0.529892
Loss = 0.497696
Loss = 0.334
Loss = 0.347824
Loss = 0.5065
Loss = 0.427902
Loss = 0.308624
Loss = 0.235336
Loss = 0.282135
Loss = 0.249969
Loss = 0.173553
Loss = 0.272598
Loss = 0.0679321
Loss = 0.198288
Loss = 0.540314
TEST LOSS = 0.33215
TEST ACC = 537.36 % (9067/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.330154
Epoch 8.2: Loss = 0.397247
Epoch 8.3: Loss = 0.407013
Epoch 8.4: Loss = 0.374222
Epoch 8.5: Loss = 0.329819
Epoch 8.6: Loss = 0.343811
Epoch 8.7: Loss = 0.348785
Epoch 8.8: Loss = 0.355026
Epoch 8.9: Loss = 0.28212
Epoch 8.10: Loss = 0.306961
Epoch 8.11: Loss = 0.324738
Epoch 8.12: Loss = 0.31279
Epoch 8.13: Loss = 0.342545
Epoch 8.14: Loss = 0.346985
Epoch 8.15: Loss = 0.430862
Epoch 8.16: Loss = 0.382721
Epoch 8.17: Loss = 0.311188
Epoch 8.18: Loss = 0.350418
Epoch 8.19: Loss = 0.336273
Epoch 8.20: Loss = 0.289734
Epoch 8.21: Loss = 0.312988
Epoch 8.22: Loss = 0.300018
Epoch 8.23: Loss = 0.357849
Epoch 8.24: Loss = 0.313095
Epoch 8.25: Loss = 0.328766
Epoch 8.26: Loss = 0.283112
Epoch 8.27: Loss = 0.350601
Epoch 8.28: Loss = 0.330582
Epoch 8.29: Loss = 0.328354
Epoch 8.30: Loss = 0.375687
Epoch 8.31: Loss = 0.349792
Epoch 8.32: Loss = 0.382568
Epoch 8.33: Loss = 0.369186
Epoch 8.34: Loss = 0.280075
Epoch 8.35: Loss = 0.389297
Epoch 8.36: Loss = 0.406372
Epoch 8.37: Loss = 0.376465
Epoch 8.38: Loss = 0.338013
Epoch 8.39: Loss = 0.342239
Epoch 8.40: Loss = 0.378342
Epoch 8.41: Loss = 0.431824
Epoch 8.42: Loss = 0.374802
Epoch 8.43: Loss = 0.419739
Epoch 8.44: Loss = 0.394257
Epoch 8.45: Loss = 0.445694
Epoch 8.46: Loss = 0.4021
Epoch 8.47: Loss = 0.402252
Epoch 8.48: Loss = 0.320862
Epoch 8.49: Loss = 0.277725
Epoch 8.50: Loss = 0.349548
Epoch 8.51: Loss = 0.357727
Epoch 8.52: Loss = 0.287338
Epoch 8.53: Loss = 0.278152
Epoch 8.54: Loss = 0.354355
Epoch 8.55: Loss = 0.354218
Epoch 8.56: Loss = 0.395386
Epoch 8.57: Loss = 0.446457
Epoch 8.58: Loss = 0.390442
Epoch 8.59: Loss = 0.279846
Epoch 8.60: Loss = 0.361053
Epoch 8.61: Loss = 0.305679
Epoch 8.62: Loss = 0.336197
Epoch 8.63: Loss = 0.289673
Epoch 8.64: Loss = 0.305374
Epoch 8.65: Loss = 0.284454
Epoch 8.66: Loss = 0.408493
Epoch 8.67: Loss = 0.349548
Epoch 8.68: Loss = 0.34198
Epoch 8.69: Loss = 0.364822
Epoch 8.70: Loss = 0.307938
Epoch 8.71: Loss = 0.400787
Epoch 8.72: Loss = 0.408752
Epoch 8.73: Loss = 0.342422
Epoch 8.74: Loss = 0.310211
Epoch 8.75: Loss = 0.422684
Epoch 8.76: Loss = 0.330795
Epoch 8.77: Loss = 0.381088
Epoch 8.78: Loss = 0.346039
Epoch 8.79: Loss = 0.301254
Epoch 8.80: Loss = 0.299026
Epoch 8.81: Loss = 0.363388
Epoch 8.82: Loss = 0.417801
Epoch 8.83: Loss = 0.43718
Epoch 8.84: Loss = 0.308517
Epoch 8.85: Loss = 0.408218
Epoch 8.86: Loss = 0.334091
Epoch 8.87: Loss = 0.301819
Epoch 8.88: Loss = 0.333939
Epoch 8.89: Loss = 0.429703
Epoch 8.90: Loss = 0.315018
Epoch 8.91: Loss = 0.365936
Epoch 8.92: Loss = 0.377716
Epoch 8.93: Loss = 0.356888
Epoch 8.94: Loss = 0.385895
Epoch 8.95: Loss = 0.316788
Epoch 8.96: Loss = 0.299576
Epoch 8.97: Loss = 0.346268
Epoch 8.98: Loss = 0.310089
Epoch 8.99: Loss = 0.310501
Epoch 8.100: Loss = 0.344711
TRAIN LOSS = 0.35025
TRAIN ACC = 89.8132 % (53891/60000)
Loss = 0.346863
Loss = 0.390747
Loss = 0.524948
Loss = 0.498001
Loss = 0.33165
Loss = 0.343628
Loss = 0.501907
Loss = 0.416824
Loss = 0.296127
Loss = 0.238602
Loss = 0.27861
Loss = 0.241196
Loss = 0.166733
Loss = 0.270081
Loss = 0.0638733
Loss = 0.192215
Loss = 0.527481
TEST LOSS = 0.327219
TEST ACC = 538.908 % (9070/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.374054
Epoch 9.2: Loss = 0.264511
Epoch 9.3: Loss = 0.30748
Epoch 9.4: Loss = 0.329514
Epoch 9.5: Loss = 0.399658
Epoch 9.6: Loss = 0.276306
Epoch 9.7: Loss = 0.388077
Epoch 9.8: Loss = 0.41655
Epoch 9.9: Loss = 0.333984
Epoch 9.10: Loss = 0.379333
Epoch 9.11: Loss = 0.356812
Epoch 9.12: Loss = 0.321045
Epoch 9.13: Loss = 0.258392
Epoch 9.14: Loss = 0.383926
Epoch 9.15: Loss = 0.336212
Epoch 9.16: Loss = 0.297653
Epoch 9.17: Loss = 0.301468
Epoch 9.18: Loss = 0.305496
Epoch 9.19: Loss = 0.339661
Epoch 9.20: Loss = 0.276184
Epoch 9.21: Loss = 0.313995
Epoch 9.22: Loss = 0.308899
Epoch 9.23: Loss = 0.34964
Epoch 9.24: Loss = 0.3255
Epoch 9.25: Loss = 0.329498
Epoch 9.26: Loss = 0.353958
Epoch 9.27: Loss = 0.379349
Epoch 9.28: Loss = 0.263855
Epoch 9.29: Loss = 0.396347
Epoch 9.30: Loss = 0.299118
Epoch 9.31: Loss = 0.287903
Epoch 9.32: Loss = 0.4646
Epoch 9.33: Loss = 0.24852
Epoch 9.34: Loss = 0.416382
Epoch 9.35: Loss = 0.311523
Epoch 9.36: Loss = 0.375336
Epoch 9.37: Loss = 0.296906
Epoch 9.38: Loss = 0.319244
Epoch 9.39: Loss = 0.383759
Epoch 9.40: Loss = 0.345215
Epoch 9.41: Loss = 0.330536
Epoch 9.42: Loss = 0.293121
Epoch 9.43: Loss = 0.328644
Epoch 9.44: Loss = 0.333405
Epoch 9.45: Loss = 0.293045
Epoch 9.46: Loss = 0.366364
Epoch 9.47: Loss = 0.344421
Epoch 9.48: Loss = 0.351898
Epoch 9.49: Loss = 0.371292
Epoch 9.50: Loss = 0.326477
Epoch 9.51: Loss = 0.37088
Epoch 9.52: Loss = 0.370285
Epoch 9.53: Loss = 0.362137
Epoch 9.54: Loss = 0.33345
Epoch 9.55: Loss = 0.307846
Epoch 9.56: Loss = 0.401108
Epoch 9.57: Loss = 0.406357
Epoch 9.58: Loss = 0.381104
Epoch 9.59: Loss = 0.319534
Epoch 9.60: Loss = 0.348892
Epoch 9.61: Loss = 0.377426
Epoch 9.62: Loss = 0.319946
Epoch 9.63: Loss = 0.355545
Epoch 9.64: Loss = 0.379379
Epoch 9.65: Loss = 0.272659
Epoch 9.66: Loss = 0.438446
Epoch 9.67: Loss = 0.418701
Epoch 9.68: Loss = 0.355255
Epoch 9.69: Loss = 0.336624
Epoch 9.70: Loss = 0.395645
Epoch 9.71: Loss = 0.379807
Epoch 9.72: Loss = 0.307556
Epoch 9.73: Loss = 0.344269
Epoch 9.74: Loss = 0.416794
Epoch 9.75: Loss = 0.415161
Epoch 9.76: Loss = 0.309784
Epoch 9.77: Loss = 0.325302
Epoch 9.78: Loss = 0.351852
Epoch 9.79: Loss = 0.326645
Epoch 9.80: Loss = 0.282349
Epoch 9.81: Loss = 0.341156
Epoch 9.82: Loss = 0.308456
Epoch 9.83: Loss = 0.442108
Epoch 9.84: Loss = 0.306076
Epoch 9.85: Loss = 0.301529
Epoch 9.86: Loss = 0.394928
Epoch 9.87: Loss = 0.308395
Epoch 9.88: Loss = 0.414459
Epoch 9.89: Loss = 0.369598
Epoch 9.90: Loss = 0.349167
Epoch 9.91: Loss = 0.371887
Epoch 9.92: Loss = 0.320297
Epoch 9.93: Loss = 0.258759
Epoch 9.94: Loss = 0.387833
Epoch 9.95: Loss = 0.350174
Epoch 9.96: Loss = 0.37999
Epoch 9.97: Loss = 0.3992
Epoch 9.98: Loss = 0.412445
Epoch 9.99: Loss = 0.327927
Epoch 9.100: Loss = 0.363266
TRAIN LOSS = 0.345734
TRAIN ACC = 90.126 % (54078/60000)
Loss = 0.339935
Loss = 0.384766
Loss = 0.519485
Loss = 0.495468
Loss = 0.320251
Loss = 0.335785
Loss = 0.50296
Loss = 0.409561
Loss = 0.2957
Loss = 0.229324
Loss = 0.282318
Loss = 0.23703
Loss = 0.158112
Loss = 0.268188
Loss = 0.061676
Loss = 0.189194
Loss = 0.517853
TEST LOSS = 0.322499
TEST ACC = 540.779 % (9103/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.300644
Epoch 10.2: Loss = 0.420471
Epoch 10.3: Loss = 0.373276
Epoch 10.4: Loss = 0.337738
Epoch 10.5: Loss = 0.364792
Epoch 10.6: Loss = 0.301605
Epoch 10.7: Loss = 0.323639
Epoch 10.8: Loss = 0.437073
Epoch 10.9: Loss = 0.313751
Epoch 10.10: Loss = 0.324356
Epoch 10.11: Loss = 0.337952
Epoch 10.12: Loss = 0.319016
Epoch 10.13: Loss = 0.323029
Epoch 10.14: Loss = 0.263641
Epoch 10.15: Loss = 0.359558
Epoch 10.16: Loss = 0.393585
Epoch 10.17: Loss = 0.34079
Epoch 10.18: Loss = 0.328415
Epoch 10.19: Loss = 0.327179
Epoch 10.20: Loss = 0.375427
Epoch 10.21: Loss = 0.367203
Epoch 10.22: Loss = 0.367722
Epoch 10.23: Loss = 0.349884
Epoch 10.24: Loss = 0.350571
Epoch 10.25: Loss = 0.409302
Epoch 10.26: Loss = 0.301361
Epoch 10.27: Loss = 0.359039
Epoch 10.28: Loss = 0.332764
Epoch 10.29: Loss = 0.292099
Epoch 10.30: Loss = 0.335602
Epoch 10.31: Loss = 0.290161
Epoch 10.32: Loss = 0.235138
Epoch 10.33: Loss = 0.282974
Epoch 10.34: Loss = 0.341125
Epoch 10.35: Loss = 0.354568
Epoch 10.36: Loss = 0.386383
Epoch 10.37: Loss = 0.429291
Epoch 10.38: Loss = 0.281464
Epoch 10.39: Loss = 0.35791
Epoch 10.40: Loss = 0.394653
Epoch 10.41: Loss = 0.331757
Epoch 10.42: Loss = 0.322083
Epoch 10.43: Loss = 0.327225
Epoch 10.44: Loss = 0.342087
Epoch 10.45: Loss = 0.359863
Epoch 10.46: Loss = 0.339752
Epoch 10.47: Loss = 0.410263
Epoch 10.48: Loss = 0.325424
Epoch 10.49: Loss = 0.304474
Epoch 10.50: Loss = 0.39299
Epoch 10.51: Loss = 0.313812
Epoch 10.52: Loss = 0.34613
Epoch 10.53: Loss = 0.350723
Epoch 10.54: Loss = 0.357147
Epoch 10.55: Loss = 0.389893
Epoch 10.56: Loss = 0.257065
Epoch 10.57: Loss = 0.378952
Epoch 10.58: Loss = 0.375992
Epoch 10.59: Loss = 0.310654
Epoch 10.60: Loss = 0.333984
Epoch 10.61: Loss = 0.38237
Epoch 10.62: Loss = 0.243851
Epoch 10.63: Loss = 0.358429
Epoch 10.64: Loss = 0.268463
Epoch 10.65: Loss = 0.381699
Epoch 10.66: Loss = 0.347778
Epoch 10.67: Loss = 0.298874
Epoch 10.68: Loss = 0.414398
Epoch 10.69: Loss = 0.324875
Epoch 10.70: Loss = 0.380569
Epoch 10.71: Loss = 0.296234
Epoch 10.72: Loss = 0.318771
Epoch 10.73: Loss = 0.308258
Epoch 10.74: Loss = 0.347885
Epoch 10.75: Loss = 0.340973
Epoch 10.76: Loss = 0.276489
Epoch 10.77: Loss = 0.397491
Epoch 10.78: Loss = 0.385345
Epoch 10.79: Loss = 0.281555
Epoch 10.80: Loss = 0.332153
Epoch 10.81: Loss = 0.449158
Epoch 10.82: Loss = 0.31485
Epoch 10.83: Loss = 0.302933
Epoch 10.84: Loss = 0.340836
Epoch 10.85: Loss = 0.431259
Epoch 10.86: Loss = 0.304474
Epoch 10.87: Loss = 0.260391
Epoch 10.88: Loss = 0.270325
Epoch 10.89: Loss = 0.355515
Epoch 10.90: Loss = 0.286575
Epoch 10.91: Loss = 0.382751
Epoch 10.92: Loss = 0.376633
Epoch 10.93: Loss = 0.34819
Epoch 10.94: Loss = 0.359619
Epoch 10.95: Loss = 0.378311
Epoch 10.96: Loss = 0.35228
Epoch 10.97: Loss = 0.339752
Epoch 10.98: Loss = 0.342453
Epoch 10.99: Loss = 0.361099
Epoch 10.100: Loss = 0.348679
TRAIN LOSS = 0.341461
TRAIN ACC = 90.2695 % (54164/60000)
Loss = 0.336838
Loss = 0.381516
Loss = 0.517014
Loss = 0.491608
Loss = 0.317764
Loss = 0.331421
Loss = 0.497635
Loss = 0.405182
Loss = 0.292664
Loss = 0.226379
Loss = 0.283554
Loss = 0.230377
Loss = 0.152588
Loss = 0.266541
Loss = 0.0595856
Loss = 0.18692
Loss = 0.518616
TEST LOSS = 0.3194
TEST ACC = 541.64 % (9111/10000)
Epoch 11.1: Loss = 0.419006
Epoch 11.2: Loss = 0.326675
Epoch 11.3: Loss = 0.292953
Epoch 11.4: Loss = 0.342041
Epoch 11.5: Loss = 0.384064
Epoch 11.6: Loss = 0.370804
Epoch 11.7: Loss = 0.285645
Epoch 11.8: Loss = 0.25824
Epoch 11.9: Loss = 0.368637
Epoch 11.10: Loss = 0.322998
Epoch 11.11: Loss = 0.355499
Epoch 11.12: Loss = 0.378525
Epoch 11.13: Loss = 0.388733
Epoch 11.14: Loss = 0.391357
Epoch 11.15: Loss = 0.36821
Epoch 11.16: Loss = 0.402802
Epoch 11.17: Loss = 0.399338
Epoch 11.18: Loss = 0.372864
Epoch 11.19: Loss = 0.306931
Epoch 11.20: Loss = 0.293594
Epoch 11.21: Loss = 0.259247
Epoch 11.22: Loss = 0.385742
Epoch 11.23: Loss = 0.307526
Epoch 11.24: Loss = 0.307785
Epoch 11.25: Loss = 0.229004
Epoch 11.26: Loss = 0.320572
Epoch 11.27: Loss = 0.330841
Epoch 11.28: Loss = 0.342896
Epoch 11.29: Loss = 0.303085
Epoch 11.30: Loss = 0.362091
Epoch 11.31: Loss = 0.339645
Epoch 11.32: Loss = 0.33992
Epoch 11.33: Loss = 0.252258
Epoch 11.34: Loss = 0.320053
Epoch 11.35: Loss = 0.233383
Epoch 11.36: Loss = 0.341568
Epoch 11.37: Loss = 0.353668
Epoch 11.38: Loss = 0.320068
Epoch 11.39: Loss = 0.328125
Epoch 11.40: Loss = 0.468185
Epoch 11.41: Loss = 0.371231
Epoch 11.42: Loss = 0.346298
Epoch 11.43: Loss = 0.371552
Epoch 11.44: Loss = 0.37413
Epoch 11.45: Loss = 0.279282
Epoch 11.46: Loss = 0.45787
Epoch 11.47: Loss = 0.354752
Epoch 11.48: Loss = 0.36615
Epoch 11.49: Loss = 0.253723
Epoch 11.50: Loss = 0.311172
Epoch 11.51: Loss = 0.316437
Epoch 11.52: Loss = 0.320984
Epoch 11.53: Loss = 0.259399
Epoch 11.54: Loss = 0.285477
Epoch 11.55: Loss = 0.398972
Epoch 11.56: Loss = 0.273727
Epoch 11.57: Loss = 0.289337
Epoch 11.58: Loss = 0.350906
Epoch 11.59: Loss = 0.413925
Epoch 11.60: Loss = 0.347397
Epoch 11.61: Loss = 0.357895
Epoch 11.62: Loss = 0.303696
Epoch 11.63: Loss = 0.362549
Epoch 11.64: Loss = 0.417175
Epoch 11.65: Loss = 0.32489
Epoch 11.66: Loss = 0.345901
Epoch 11.67: Loss = 0.307373
Epoch 11.68: Loss = 0.412399
Epoch 11.69: Loss = 0.337479
Epoch 11.70: Loss = 0.354218
Epoch 11.71: Loss = 0.345169
Epoch 11.72: Loss = 0.32637
Epoch 11.73: Loss = 0.405609
Epoch 11.74: Loss = 0.359558
Epoch 11.75: Loss = 0.383682
Epoch 11.76: Loss = 0.416885
Epoch 11.77: Loss = 0.251007
Epoch 11.78: Loss = 0.330246
Epoch 11.79: Loss = 0.290192
Epoch 11.80: Loss = 0.361679
Epoch 11.81: Loss = 0.290665
Epoch 11.82: Loss = 0.310684
Epoch 11.83: Loss = 0.283783
Epoch 11.84: Loss = 0.360153
Epoch 11.85: Loss = 0.33345
Epoch 11.86: Loss = 0.310516
Epoch 11.87: Loss = 0.340485
Epoch 11.88: Loss = 0.45607
Epoch 11.89: Loss = 0.369537
Epoch 11.90: Loss = 0.280685
Epoch 11.91: Loss = 0.316193
Epoch 11.92: Loss = 0.31337
Epoch 11.93: Loss = 0.382126
Epoch 11.94: Loss = 0.315308
Epoch 11.95: Loss = 0.371796
Epoch 11.96: Loss = 0.251907
Epoch 11.97: Loss = 0.342972
Epoch 11.98: Loss = 0.329102
Epoch 11.99: Loss = 0.335327
Epoch 11.100: Loss = 0.331177
TRAIN LOSS = 0.338379
TRAIN ACC = 90.4037 % (54245/60000)
Loss = 0.333954
Loss = 0.37999
Loss = 0.513702
Loss = 0.489136
Loss = 0.314651
Loss = 0.332764
Loss = 0.500992
Loss = 0.403046
Loss = 0.292358
Loss = 0.223465
Loss = 0.287033
Loss = 0.225479
Loss = 0.150803
Loss = 0.262115
Loss = 0.0579987
Loss = 0.186798
Loss = 0.513153
TEST LOSS = 0.317783
TEST ACC = 542.448 % (9121/10000)
Epoch 12.1: Loss = 0.36792
Epoch 12.2: Loss = 0.338364
Epoch 12.3: Loss = 0.283554
Epoch 12.4: Loss = 0.371078
Epoch 12.5: Loss = 0.299362
Epoch 12.6: Loss = 0.377777
Epoch 12.7: Loss = 0.35437
Epoch 12.8: Loss = 0.239441
Epoch 12.9: Loss = 0.280838
Epoch 12.10: Loss = 0.399597
Epoch 12.11: Loss = 0.283539
Epoch 12.12: Loss = 0.333344
Epoch 12.13: Loss = 0.280151
Epoch 12.14: Loss = 0.297836
Epoch 12.15: Loss = 0.28653
Epoch 12.16: Loss = 0.436646
Epoch 12.17: Loss = 0.384186
Epoch 12.18: Loss = 0.354675
Epoch 12.19: Loss = 0.356979
Epoch 12.20: Loss = 0.30484
Epoch 12.21: Loss = 0.347092
Epoch 12.22: Loss = 0.307236
Epoch 12.23: Loss = 0.340988
Epoch 12.24: Loss = 0.361816
Epoch 12.25: Loss = 0.409332
Epoch 12.26: Loss = 0.373688
Epoch 12.27: Loss = 0.27507
Epoch 12.28: Loss = 0.379272
Epoch 12.29: Loss = 0.297745
Epoch 12.30: Loss = 0.361389
Epoch 12.31: Loss = 0.349609
Epoch 12.32: Loss = 0.342178
Epoch 12.33: Loss = 0.330582
Epoch 12.34: Loss = 0.354004
Epoch 12.35: Loss = 0.317703
Epoch 12.36: Loss = 0.367859
Epoch 12.37: Loss = 0.364777
Epoch 12.38: Loss = 0.302551
Epoch 12.39: Loss = 0.312317
Epoch 12.40: Loss = 0.243896
Epoch 12.41: Loss = 0.258896
Epoch 12.42: Loss = 0.333435
Epoch 12.43: Loss = 0.311157
Epoch 12.44: Loss = 0.338348
Epoch 12.45: Loss = 0.302124
Epoch 12.46: Loss = 0.342499
Epoch 12.47: Loss = 0.311844
Epoch 12.48: Loss = 0.303436
Epoch 12.49: Loss = 0.364212
Epoch 12.50: Loss = 0.421219
Epoch 12.51: Loss = 0.258804
Epoch 12.52: Loss = 0.356659
Epoch 12.53: Loss = 0.283813
Epoch 12.54: Loss = 0.34874
Epoch 12.55: Loss = 0.389877
Epoch 12.56: Loss = 0.401276
Epoch 12.57: Loss = 0.341736
Epoch 12.58: Loss = 0.360077
Epoch 12.59: Loss = 0.340637
Epoch 12.60: Loss = 0.330933
Epoch 12.61: Loss = 0.380051
Epoch 12.62: Loss = 0.335037
Epoch 12.63: Loss = 0.397171
Epoch 12.64: Loss = 0.243057
Epoch 12.65: Loss = 0.325577
Epoch 12.66: Loss = 0.302231
Epoch 12.67: Loss = 0.335342
Epoch 12.68: Loss = 0.390961
Epoch 12.69: Loss = 0.361969
Epoch 12.70: Loss = 0.327423
Epoch 12.71: Loss = 0.37056
Epoch 12.72: Loss = 0.319214
Epoch 12.73: Loss = 0.349258
Epoch 12.74: Loss = 0.327774
Epoch 12.75: Loss = 0.261353
Epoch 12.76: Loss = 0.387131
Epoch 12.77: Loss = 0.326187
Epoch 12.78: Loss = 0.327194
Epoch 12.79: Loss = 0.250366
Epoch 12.80: Loss = 0.302246
Epoch 12.81: Loss = 0.344849
Epoch 12.82: Loss = 0.343704
Epoch 12.83: Loss = 0.391953
Epoch 12.84: Loss = 0.280273
Epoch 12.85: Loss = 0.325729
Epoch 12.86: Loss = 0.340714
Epoch 12.87: Loss = 0.317688
Epoch 12.88: Loss = 0.311096
Epoch 12.89: Loss = 0.414764
Epoch 12.90: Loss = 0.281357
Epoch 12.91: Loss = 0.393906
Epoch 12.92: Loss = 0.28125
Epoch 12.93: Loss = 0.453415
Epoch 12.94: Loss = 0.308441
Epoch 12.95: Loss = 0.304459
Epoch 12.96: Loss = 0.34761
Epoch 12.97: Loss = 0.35054
Epoch 12.98: Loss = 0.3927
Epoch 12.99: Loss = 0.322403
Epoch 12.100: Loss = 0.339783
TRAIN LOSS = 0.335388
TRAIN ACC = 90.5014 % (54303/60000)
Loss = 0.330063
Loss = 0.378189
Loss = 0.512283
Loss = 0.484421
Loss = 0.310394
Loss = 0.32756
Loss = 0.500595
Loss = 0.396301
Loss = 0.28389
Loss = 0.224762
Loss = 0.286545
Loss = 0.220764
Loss = 0.149261
Loss = 0.262787
Loss = 0.0567932
Loss = 0.183823
Loss = 0.51001
TEST LOSS = 0.314906
TEST ACC = 543.03 % (9128/10000)
Epoch 13.1: Loss = 0.314316
Epoch 13.2: Loss = 0.359329
Epoch 13.3: Loss = 0.335403
Epoch 13.4: Loss = 0.328735
Epoch 13.5: Loss = 0.251633
Epoch 13.6: Loss = 0.315964
Epoch 13.7: Loss = 0.364487
Epoch 13.8: Loss = 0.368332
Epoch 13.9: Loss = 0.474075
Epoch 13.10: Loss = 0.346069
Epoch 13.11: Loss = 0.277863
Epoch 13.12: Loss = 0.276932
Epoch 13.13: Loss = 0.296463
Epoch 13.14: Loss = 0.314209
Epoch 13.15: Loss = 0.307205
Epoch 13.16: Loss = 0.355423
Epoch 13.17: Loss = 0.311829
Epoch 13.18: Loss = 0.331894
Epoch 13.19: Loss = 0.297699
Epoch 13.20: Loss = 0.369308
Epoch 13.21: Loss = 0.238312
Epoch 13.22: Loss = 0.344559
Epoch 13.23: Loss = 0.33049
Epoch 13.24: Loss = 0.308105
Epoch 13.25: Loss = 0.297501
Epoch 13.26: Loss = 0.331314
Epoch 13.27: Loss = 0.39064
Epoch 13.28: Loss = 0.271362
Epoch 13.29: Loss = 0.314194
Epoch 13.30: Loss = 0.311188
Epoch 13.31: Loss = 0.29364
Epoch 13.32: Loss = 0.394394
Epoch 13.33: Loss = 0.337753
Epoch 13.34: Loss = 0.384537
Epoch 13.35: Loss = 0.291016
Epoch 13.36: Loss = 0.441055
Epoch 13.37: Loss = 0.377914
Epoch 13.38: Loss = 0.372528
Epoch 13.39: Loss = 0.322296
Epoch 13.40: Loss = 0.332687
Epoch 13.41: Loss = 0.411484
Epoch 13.42: Loss = 0.27124
Epoch 13.43: Loss = 0.321564
Epoch 13.44: Loss = 0.326385
Epoch 13.45: Loss = 0.296326
Epoch 13.46: Loss = 0.364075
Epoch 13.47: Loss = 0.366074
Epoch 13.48: Loss = 0.281769
Epoch 13.49: Loss = 0.316452
Epoch 13.50: Loss = 0.346527
Epoch 13.51: Loss = 0.395309
Epoch 13.52: Loss = 0.30722
Epoch 13.53: Loss = 0.339172
Epoch 13.54: Loss = 0.280197
Epoch 13.55: Loss = 0.298645
Epoch 13.56: Loss = 0.305527
Epoch 13.57: Loss = 0.337524
Epoch 13.58: Loss = 0.319794
Epoch 13.59: Loss = 0.345795
Epoch 13.60: Loss = 0.268127
Epoch 13.61: Loss = 0.298996
Epoch 13.62: Loss = 0.304306
Epoch 13.63: Loss = 0.366974
Epoch 13.64: Loss = 0.426208
Epoch 13.65: Loss = 0.366226
Epoch 13.66: Loss = 0.380493
Epoch 13.67: Loss = 0.359329
Epoch 13.68: Loss = 0.283386
Epoch 13.69: Loss = 0.301041
Epoch 13.70: Loss = 0.328384
Epoch 13.71: Loss = 0.378204
Epoch 13.72: Loss = 0.28801
Epoch 13.73: Loss = 0.33139
Epoch 13.74: Loss = 0.386261
Epoch 13.75: Loss = 0.353317
Epoch 13.76: Loss = 0.33873
Epoch 13.77: Loss = 0.320908
Epoch 13.78: Loss = 0.314285
Epoch 13.79: Loss = 0.290955
Epoch 13.80: Loss = 0.407761
Epoch 13.81: Loss = 0.30603
Epoch 13.82: Loss = 0.326584
Epoch 13.83: Loss = 0.318069
Epoch 13.84: Loss = 0.369171
Epoch 13.85: Loss = 0.293488
Epoch 13.86: Loss = 0.374023
Epoch 13.87: Loss = 0.314392
Epoch 13.88: Loss = 0.311386
Epoch 13.89: Loss = 0.270767
Epoch 13.90: Loss = 0.423416
Epoch 13.91: Loss = 0.373611
Epoch 13.92: Loss = 0.308578
Epoch 13.93: Loss = 0.376526
Epoch 13.94: Loss = 0.360703
Epoch 13.95: Loss = 0.322098
Epoch 13.96: Loss = 0.286545
Epoch 13.97: Loss = 0.331451
Epoch 13.98: Loss = 0.436172
Epoch 13.99: Loss = 0.284103
Epoch 13.100: Loss = 0.323776
TRAIN LOSS = 0.333176
TRAIN ACC = 90.6174 % (54373/60000)
Loss = 0.328949
Loss = 0.373871
Loss = 0.509995
Loss = 0.481689
Loss = 0.303497
Loss = 0.323212
Loss = 0.493729
Loss = 0.391861
Loss = 0.282684
Loss = 0.221832
Loss = 0.286774
Loss = 0.21788
Loss = 0.1474
Loss = 0.26207
Loss = 0.0533752
Loss = 0.18251
Loss = 0.511414
TEST LOSS = 0.312136
TEST ACC = 543.729 % (9134/10000)
Epoch 14.1: Loss = 0.351044
Epoch 14.2: Loss = 0.304993
Epoch 14.3: Loss = 0.352356
Epoch 14.4: Loss = 0.289001
Epoch 14.5: Loss = 0.344528
Epoch 14.6: Loss = 0.349487
Epoch 14.7: Loss = 0.399963
Epoch 14.8: Loss = 0.375977
Epoch 14.9: Loss = 0.341873
Epoch 14.10: Loss = 0.386215
Epoch 14.11: Loss = 0.433182
Epoch 14.12: Loss = 0.337372
Epoch 14.13: Loss = 0.339249
Epoch 14.14: Loss = 0.323212
Epoch 14.15: Loss = 0.275375
Epoch 14.16: Loss = 0.324036
Epoch 14.17: Loss = 0.39592
Epoch 14.18: Loss = 0.315735
Epoch 14.19: Loss = 0.274109
Epoch 14.20: Loss = 0.348648
Epoch 14.21: Loss = 0.306671
Epoch 14.22: Loss = 0.31073
Epoch 14.23: Loss = 0.32872
Epoch 14.24: Loss = 0.30011
Epoch 14.25: Loss = 0.333267
Epoch 14.26: Loss = 0.272202
Epoch 14.27: Loss = 0.29567
Epoch 14.28: Loss = 0.25824
Epoch 14.29: Loss = 0.338638
Epoch 14.30: Loss = 0.346054
Epoch 14.31: Loss = 0.370987
Epoch 14.32: Loss = 0.365585
Epoch 14.33: Loss = 0.310349
Epoch 14.34: Loss = 0.320297
Epoch 14.35: Loss = 0.287079
Epoch 14.36: Loss = 0.289612
Epoch 14.37: Loss = 0.291992
Epoch 14.38: Loss = 0.399185
Epoch 14.39: Loss = 0.33168
Epoch 14.40: Loss = 0.423264
Epoch 14.41: Loss = 0.287262
Epoch 14.42: Loss = 0.314255
Epoch 14.43: Loss = 0.344589
Epoch 14.44: Loss = 0.338882
Epoch 14.45: Loss = 0.262833
Epoch 14.46: Loss = 0.372284
Epoch 14.47: Loss = 0.332474
Epoch 14.48: Loss = 0.308563
Epoch 14.49: Loss = 0.352936
Epoch 14.50: Loss = 0.320038
Epoch 14.51: Loss = 0.324615
Epoch 14.52: Loss = 0.36171
Epoch 14.53: Loss = 0.328598
Epoch 14.54: Loss = 0.329102
Epoch 14.55: Loss = 0.20752
Epoch 14.56: Loss = 0.243103
Epoch 14.57: Loss = 0.347214
Epoch 14.58: Loss = 0.395203
Epoch 14.59: Loss = 0.286102
Epoch 14.60: Loss = 0.32959
Epoch 14.61: Loss = 0.281754
Epoch 14.62: Loss = 0.363861
Epoch 14.63: Loss = 0.287613
Epoch 14.64: Loss = 0.278503
Epoch 14.65: Loss = 0.286331
Epoch 14.66: Loss = 0.254425
Epoch 14.67: Loss = 0.33577
Epoch 14.68: Loss = 0.303101
Epoch 14.69: Loss = 0.396011
Epoch 14.70: Loss = 0.353897
Epoch 14.71: Loss = 0.31546
Epoch 14.72: Loss = 0.288773
Epoch 14.73: Loss = 0.345413
Epoch 14.74: Loss = 0.3311
Epoch 14.75: Loss = 0.317139
Epoch 14.76: Loss = 0.330551
Epoch 14.77: Loss = 0.310013
Epoch 14.78: Loss = 0.42598
Epoch 14.79: Loss = 0.371155
Epoch 14.80: Loss = 0.290222
Epoch 14.81: Loss = 0.295425
Epoch 14.82: Loss = 0.370621
Epoch 14.83: Loss = 0.392715
Epoch 14.84: Loss = 0.305206
Epoch 14.85: Loss = 0.386673
Epoch 14.86: Loss = 0.413818
Epoch 14.87: Loss = 0.320267
Epoch 14.88: Loss = 0.316055
Epoch 14.89: Loss = 0.306625
Epoch 14.90: Loss = 0.375549
Epoch 14.91: Loss = 0.377548
Epoch 14.92: Loss = 0.357483
Epoch 14.93: Loss = 0.318512
Epoch 14.94: Loss = 0.393631
Epoch 14.95: Loss = 0.396774
Epoch 14.96: Loss = 0.278564
Epoch 14.97: Loss = 0.345459
Epoch 14.98: Loss = 0.320206
Epoch 14.99: Loss = 0.327469
Epoch 14.100: Loss = 0.300217
TRAIN LOSS = 0.330963
TRAIN ACC = 90.7288 % (54440/60000)
Loss = 0.324692
Loss = 0.373047
Loss = 0.510101
Loss = 0.478104
Loss = 0.306152
Loss = 0.322479
Loss = 0.494156
Loss = 0.391541
Loss = 0.280045
Loss = 0.219299
Loss = 0.291306
Loss = 0.21434
Loss = 0.146072
Loss = 0.252792
Loss = 0.0530701
Loss = 0.184265
Loss = 0.511093
TEST LOSS = 0.310931
TEST ACC = 544.398 % (9140/10000)
Epoch 15.1: Loss = 0.309341
Epoch 15.2: Loss = 0.339874
Epoch 15.3: Loss = 0.326996
Epoch 15.4: Loss = 0.259537
Epoch 15.5: Loss = 0.291473
Epoch 15.6: Loss = 0.384521
Epoch 15.7: Loss = 0.29628
Epoch 15.8: Loss = 0.333252
Epoch 15.9: Loss = 0.409363
Epoch 15.10: Loss = 0.29892
Epoch 15.11: Loss = 0.24614
Epoch 15.12: Loss = 0.320389
Epoch 15.13: Loss = 0.271225
Epoch 15.14: Loss = 0.331284
Epoch 15.15: Loss = 0.356796
Epoch 15.16: Loss = 0.292297
Epoch 15.17: Loss = 0.317368
Epoch 15.18: Loss = 0.335052
Epoch 15.19: Loss = 0.375595
Epoch 15.20: Loss = 0.371918
Epoch 15.21: Loss = 0.387207
Epoch 15.22: Loss = 0.328354
Epoch 15.23: Loss = 0.307449
Epoch 15.24: Loss = 0.302811
Epoch 15.25: Loss = 0.333786
Epoch 15.26: Loss = 0.302322
Epoch 15.27: Loss = 0.284668
Epoch 15.28: Loss = 0.302643
Epoch 15.29: Loss = 0.352615
Epoch 15.30: Loss = 0.384445
Epoch 15.31: Loss = 0.379333
Epoch 15.32: Loss = 0.351059
Epoch 15.33: Loss = 0.300415
Epoch 15.34: Loss = 0.398743
Epoch 15.35: Loss = 0.266769
Epoch 15.36: Loss = 0.29982
Epoch 15.37: Loss = 0.346558
Epoch 15.38: Loss = 0.34523
Epoch 15.39: Loss = 0.336212
Epoch 15.40: Loss = 0.320557
Epoch 15.41: Loss = 0.402786
Epoch 15.42: Loss = 0.346695
Epoch 15.43: Loss = 0.355469
Epoch 15.44: Loss = 0.313354
Epoch 15.45: Loss = 0.319092
Epoch 15.46: Loss = 0.227188
Epoch 15.47: Loss = 0.336746
Epoch 15.48: Loss = 0.292313
Epoch 15.49: Loss = 0.288559
Epoch 15.50: Loss = 0.363022
Epoch 15.51: Loss = 0.347275
Epoch 15.52: Loss = 0.302109
Epoch 15.53: Loss = 0.326309
Epoch 15.54: Loss = 0.418076
Epoch 15.55: Loss = 0.300064
Epoch 15.56: Loss = 0.36882
Epoch 15.57: Loss = 0.379089
Epoch 15.58: Loss = 0.309677
Epoch 15.59: Loss = 0.32222
Epoch 15.60: Loss = 0.336761
Epoch 15.61: Loss = 0.326721
Epoch 15.62: Loss = 0.334366
Epoch 15.63: Loss = 0.27533
Epoch 15.64: Loss = 0.292053
Epoch 15.65: Loss = 0.288513
Epoch 15.66: Loss = 0.304977
Epoch 15.67: Loss = 0.349655
Epoch 15.68: Loss = 0.332565
Epoch 15.69: Loss = 0.310455
Epoch 15.70: Loss = 0.357468
Epoch 15.71: Loss = 0.359024
Epoch 15.72: Loss = 0.32666
Epoch 15.73: Loss = 0.328339
Epoch 15.74: Loss = 0.32515
Epoch 15.75: Loss = 0.349335
Epoch 15.76: Loss = 0.406693
Epoch 15.77: Loss = 0.249115
Epoch 15.78: Loss = 0.310318
Epoch 15.79: Loss = 0.283951
Epoch 15.80: Loss = 0.409027
Epoch 15.81: Loss = 0.348541
Epoch 15.82: Loss = 0.300812
Epoch 15.83: Loss = 0.402954
Epoch 15.84: Loss = 0.310349
Epoch 15.85: Loss = 0.314819
Epoch 15.86: Loss = 0.347855
Epoch 15.87: Loss = 0.373718
Epoch 15.88: Loss = 0.312958
Epoch 15.89: Loss = 0.321671
Epoch 15.90: Loss = 0.321411
Epoch 15.91: Loss = 0.416138
Epoch 15.92: Loss = 0.381729
Epoch 15.93: Loss = 0.266129
Epoch 15.94: Loss = 0.3311
Epoch 15.95: Loss = 0.287567
Epoch 15.96: Loss = 0.299622
Epoch 15.97: Loss = 0.256454
Epoch 15.98: Loss = 0.273819
Epoch 15.99: Loss = 0.273453
Epoch 15.100: Loss = 0.359116
TRAIN LOSS = 0.327744
TRAIN ACC = 90.8829 % (54533/60000)
Loss = 0.321091
Loss = 0.368195
Loss = 0.503708
Loss = 0.471252
Loss = 0.301331
Loss = 0.314682
Loss = 0.489731
Loss = 0.383377
Loss = 0.278351
Loss = 0.214874
Loss = 0.292648
Loss = 0.212143
Loss = 0.14151
Loss = 0.25592
Loss = 0.0508881
Loss = 0.184097
Loss = 0.504105
TEST LOSS = 0.307192
TEST ACC = 545.329 % (9161/10000)
The following benchmarks are including preprocessing (offline phase).
Time = 58532 seconds 
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True

Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.41644
Epoch 1.2: Loss = 2.33946
Epoch 1.3: Loss = 2.2498
Epoch 1.4: Loss = 2.21373
Epoch 1.5: Loss = 2.147
Epoch 1.6: Loss = 2.09268
Epoch 1.7: Loss = 2.04037
Epoch 1.8: Loss = 2.00519
Epoch 1.9: Loss = 1.98157
Epoch 1.10: Loss = 1.90564
Epoch 1.11: Loss = 1.85802
Epoch 1.12: Loss = 1.81126
Epoch 1.13: Loss = 1.79091
Epoch 1.14: Loss = 1.74742
Epoch 1.15: Loss = 1.69711
Epoch 1.16: Loss = 1.64394
Epoch 1.17: Loss = 1.64342
Epoch 1.18: Loss = 1.58162
Epoch 1.19: Loss = 1.56644
Epoch 1.20: Loss = 1.50821
Epoch 1.21: Loss = 1.50786
Epoch 1.22: Loss = 1.46939
Epoch 1.23: Loss = 1.45638
Epoch 1.24: Loss = 1.39096
Epoch 1.25: Loss = 1.41911
Epoch 1.26: Loss = 1.35239
Epoch 1.27: Loss = 1.32745
Epoch 1.28: Loss = 1.27527
Epoch 1.29: Loss = 1.27078
Epoch 1.30: Loss = 1.23265
Epoch 1.31: Loss = 1.25142
Epoch 1.32: Loss = 1.23787
Epoch 1.33: Loss = 1.16139
Epoch 1.34: Loss = 1.21857
Epoch 1.35: Loss = 1.16856
Epoch 1.36: Loss = 1.11546
Epoch 1.37: Loss = 1.08813
Epoch 1.38: Loss = 1.10062
Epoch 1.39: Loss = 1.15395
Epoch 1.40: Loss = 1.06281
Epoch 1.41: Loss = 1.04745
Epoch 1.42: Loss = 1.04831
Epoch 1.43: Loss = 1.07857
Epoch 1.44: Loss = 1.01712
Epoch 1.45: Loss = 0.963638
Epoch 1.46: Loss = 1.0325
Epoch 1.47: Loss = 1.01263
Epoch 1.48: Loss = 1.00775
Epoch 1.49: Loss = 0.949982
Epoch 1.50: Loss = 1.0251
Epoch 1.51: Loss = 0.948853
Epoch 1.52: Loss = 0.973953
Epoch 1.53: Loss = 0.947433
Epoch 1.54: Loss = 0.94426
Epoch 1.55: Loss = 0.956573
Epoch 1.56: Loss = 0.944351
Epoch 1.57: Loss = 0.950012
Epoch 1.58: Loss = 0.984589
Epoch 1.59: Loss = 0.95488
Epoch 1.60: Loss = 0.939987
Epoch 1.61: Loss = 0.87291
Epoch 1.62: Loss = 0.833908
Epoch 1.63: Loss = 0.954666
Epoch 1.64: Loss = 0.959091
Epoch 1.65: Loss = 0.881393
Epoch 1.66: Loss = 0.888428
Epoch 1.67: Loss = 0.838791
Epoch 1.68: Loss = 0.930115
Epoch 1.69: Loss = 0.922668
Epoch 1.70: Loss = 0.864914
Epoch 1.71: Loss = 0.890076
Epoch 1.72: Loss = 0.869629
Epoch 1.73: Loss = 0.844223
Epoch 1.74: Loss = 0.839478
Epoch 1.75: Loss = 0.832291
Epoch 1.76: Loss = 0.834503
Epoch 1.77: Loss = 0.692261
Epoch 1.78: Loss = 0.810318
Epoch 1.79: Loss = 0.859818
Epoch 1.80: Loss = 0.87088
Epoch 1.81: Loss = 0.830994
Epoch 1.82: Loss = 0.838745
Epoch 1.83: Loss = 0.838608
Epoch 1.84: Loss = 0.745667
Epoch 1.85: Loss = 0.849762
Epoch 1.86: Loss = 0.80513
Epoch 1.87: Loss = 0.839111
Epoch 1.88: Loss = 0.80629
Epoch 1.89: Loss = 0.780426
Epoch 1.90: Loss = 0.805359
Epoch 1.91: Loss = 0.828674
Epoch 1.92: Loss = 0.76329
Epoch 1.93: Loss = 0.771942
Epoch 1.94: Loss = 0.823883
Epoch 1.95: Loss = 0.773407
Epoch 1.96: Loss = 0.824936
Epoch 1.97: Loss = 0.754059
Epoch 1.98: Loss = 0.835281
Epoch 1.99: Loss = 0.744553
Epoch 1.100: Loss = 0.830917
Epoch 1.101: Loss = 0.790619
Epoch 1.102: Loss = 0.753769
Epoch 1.103: Loss = 0.804291
Epoch 1.104: Loss = 0.833817
Epoch 1.105: Loss = 0.752899
Epoch 1.106: Loss = 0.800186
Epoch 1.107: Loss = 0.758438
Epoch 1.108: Loss = 0.784973
Epoch 1.109: Loss = 0.812439
Epoch 1.110: Loss = 0.77681
Epoch 1.111: Loss = 0.794495
Epoch 1.112: Loss = 0.757889
Epoch 1.113: Loss = 0.837097
Epoch 1.114: Loss = 0.728928
Epoch 1.115: Loss = 0.735901
Epoch 1.116: Loss = 0.785614
Epoch 1.117: Loss = 0.70813
Epoch 1.118: Loss = 0.755646
Epoch 1.119: Loss = 0.782104
Epoch 1.120: Loss = 0.729874
TRAIN LOSS = 1.10081
TRAIN ACC = 63.6047 % (38164/60000)
Loss = 0.684113
Loss = 0.797775
Loss = 0.790634
Loss = 0.696503
Loss = 0.693558
Loss = 0.863281
Loss = 0.883347
Loss = 0.826889
Loss = 0.746277
Loss = 0.711456
Loss = 0.82193
Loss = 0.784393
Loss = 0.765472
Loss = 0.781982
Loss = 0.746017
Loss = 0.807526
Loss = 0.718369
Loss = 0.773331
Loss = 0.815674
Loss = 0.742096
TEST LOSS = 0.772531
TEST ACC = 381.639 % (7233/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.789398
Epoch 2.2: Loss = 0.660919
Epoch 2.3: Loss = 0.724548
Epoch 2.4: Loss = 0.842621
Epoch 2.5: Loss = 0.767502
Epoch 2.6: Loss = 0.755569
Epoch 2.7: Loss = 0.710846
Epoch 2.8: Loss = 0.71579
Epoch 2.9: Loss = 0.698807
Epoch 2.10: Loss = 0.810196
Epoch 2.11: Loss = 0.769913
Epoch 2.12: Loss = 0.797714
Epoch 2.13: Loss = 0.746002
Epoch 2.14: Loss = 0.770111
Epoch 2.15: Loss = 0.717255
Epoch 2.16: Loss = 0.671844
Epoch 2.17: Loss = 0.837845
Epoch 2.18: Loss = 0.748672
Epoch 2.19: Loss = 0.69014
Epoch 2.20: Loss = 0.726151
Epoch 2.21: Loss = 0.758408
Epoch 2.22: Loss = 0.790512
Epoch 2.23: Loss = 0.697372
Epoch 2.24: Loss = 0.74617
Epoch 2.25: Loss = 0.744156
Epoch 2.26: Loss = 0.688705
Epoch 2.27: Loss = 0.759216
Epoch 2.28: Loss = 0.733932
Epoch 2.29: Loss = 0.646973
Epoch 2.30: Loss = 0.70755
Epoch 2.31: Loss = 0.761185
Epoch 2.32: Loss = 0.687363
Epoch 2.33: Loss = 0.676926
Epoch 2.34: Loss = 0.710175
Epoch 2.35: Loss = 0.796585
Epoch 2.36: Loss = 0.690094
Epoch 2.37: Loss = 0.795197
Epoch 2.38: Loss = 0.737595
Epoch 2.39: Loss = 0.696579
Epoch 2.40: Loss = 0.720184
Epoch 2.41: Loss = 0.693283
Epoch 2.42: Loss = 0.703217
Epoch 2.43: Loss = 0.748306
Epoch 2.44: Loss = 0.72464
Epoch 2.45: Loss = 0.763184
Epoch 2.46: Loss = 0.718964
Epoch 2.47: Loss = 0.618225
Epoch 2.48: Loss = 0.819656
Epoch 2.49: Loss = 0.760376
Epoch 2.50: Loss = 0.614517
Epoch 2.51: Loss = 0.652924
Epoch 2.52: Loss = 0.694092
Epoch 2.53: Loss = 0.662811
Epoch 2.54: Loss = 0.765472
Epoch 2.55: Loss = 0.72641
Epoch 2.56: Loss = 0.684647
Epoch 2.57: Loss = 0.741501
Epoch 2.58: Loss = 0.647949
Epoch 2.59: Loss = 0.707062
Epoch 2.60: Loss = 0.651794
Epoch 2.61: Loss = 0.676193
Epoch 2.62: Loss = 0.645569
Epoch 2.63: Loss = 0.743591
Epoch 2.64: Loss = 0.607346
Epoch 2.65: Loss = 0.757278
Epoch 2.66: Loss = 0.641449
Epoch 2.67: Loss = 0.742599
Epoch 2.68: Loss = 0.64653
Epoch 2.69: Loss = 0.769608
Epoch 2.70: Loss = 0.687943
Epoch 2.71: Loss = 0.593597
Epoch 2.72: Loss = 0.754059
Epoch 2.73: Loss = 0.578552
Epoch 2.74: Loss = 0.631821
Epoch 2.75: Loss = 0.708588
Epoch 2.76: Loss = 0.760834
Epoch 2.77: Loss = 0.73996
Epoch 2.78: Loss = 0.804871
Epoch 2.79: Loss = 0.683228
Epoch 2.80: Loss = 0.704559
Epoch 2.81: Loss = 0.621109
Epoch 2.82: Loss = 0.714218
Epoch 2.83: Loss = 0.731903
Epoch 2.84: Loss = 0.579559
Epoch 2.85: Loss = 0.654221
Epoch 2.86: Loss = 0.764999
Epoch 2.87: Loss = 0.68367
Epoch 2.88: Loss = 0.691238
Epoch 2.89: Loss = 0.758301
Epoch 2.90: Loss = 0.771332
Epoch 2.91: Loss = 0.739136
Epoch 2.92: Loss = 0.787506
Epoch 2.93: Loss = 0.750793
Epoch 2.94: Loss = 0.783783
Epoch 2.95: Loss = 0.637085
Epoch 2.96: Loss = 0.68692
Epoch 2.97: Loss = 0.727936
Epoch 2.98: Loss = 0.778412
Epoch 2.99: Loss = 0.700119
Epoch 2.100: Loss = 0.677231
Epoch 2.101: Loss = 0.662338
Epoch 2.102: Loss = 0.65654
Epoch 2.103: Loss = 0.747757
Epoch 2.104: Loss = 0.753586
Epoch 2.105: Loss = 0.716187
Epoch 2.106: Loss = 0.728302
Epoch 2.107: Loss = 0.643173
Epoch 2.108: Loss = 0.670944
Epoch 2.109: Loss = 0.676773
Epoch 2.110: Loss = 0.698547
Epoch 2.111: Loss = 0.746658
Epoch 2.112: Loss = 0.758896
Epoch 2.113: Loss = 0.810791
Epoch 2.114: Loss = 0.592072
Epoch 2.115: Loss = 0.706451
Epoch 2.116: Loss = 0.677704
Epoch 2.117: Loss = 0.656845
Epoch 2.118: Loss = 0.702988
Epoch 2.119: Loss = 0.674454
Epoch 2.120: Loss = 0.667725
TRAIN LOSS = 0.713867
TRAIN ACC = 75.7889 % (45476/60000)
Loss = 0.630707
Loss = 0.761627
Loss = 0.72081
Loss = 0.62355
Loss = 0.638916
Loss = 0.825851
Loss = 0.846375
Loss = 0.795013
Loss = 0.714035
Loss = 0.66272
Loss = 0.801987
Loss = 0.775665
Loss = 0.712357
Loss = 0.72728
Loss = 0.709763
Loss = 0.770996
Loss = 0.665649
Loss = 0.737885
Loss = 0.783844
Loss = 0.701355
TEST LOSS = 0.730319
TEST ACC = 454.759 % (7588/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.703094
Epoch 3.2: Loss = 0.608871
Epoch 3.3: Loss = 0.773849
Epoch 3.4: Loss = 0.731827
Epoch 3.5: Loss = 0.666412
Epoch 3.6: Loss = 0.732544
Epoch 3.7: Loss = 0.75853
Epoch 3.8: Loss = 0.79039
Epoch 3.9: Loss = 0.667282
Epoch 3.10: Loss = 0.684265
Epoch 3.11: Loss = 0.736191
Epoch 3.12: Loss = 0.642212
Epoch 3.13: Loss = 0.632568
Epoch 3.14: Loss = 0.682831
Epoch 3.15: Loss = 0.765091
Epoch 3.16: Loss = 0.660828
Epoch 3.17: Loss = 0.672653
Epoch 3.18: Loss = 0.800018
Epoch 3.19: Loss = 0.672119
Epoch 3.20: Loss = 0.624191
Epoch 3.21: Loss = 0.738693
Epoch 3.22: Loss = 0.751434
Epoch 3.23: Loss = 0.632355
Epoch 3.24: Loss = 0.718018
Epoch 3.25: Loss = 0.572449
Epoch 3.26: Loss = 0.66684
Epoch 3.27: Loss = 0.674728
Epoch 3.28: Loss = 0.65387
Epoch 3.29: Loss = 0.644608
Epoch 3.30: Loss = 0.728058
Epoch 3.31: Loss = 0.669708
Epoch 3.32: Loss = 0.77272
Epoch 3.33: Loss = 0.605789
Epoch 3.34: Loss = 0.703201
Epoch 3.35: Loss = 0.649994
Epoch 3.36: Loss = 0.630371
Epoch 3.37: Loss = 0.720901
Epoch 3.38: Loss = 0.828247
Epoch 3.39: Loss = 0.73584
Epoch 3.40: Loss = 0.738358
Epoch 3.41: Loss = 0.656174
Epoch 3.42: Loss = 0.653366
Epoch 3.43: Loss = 0.733429
Epoch 3.44: Loss = 0.631821
Epoch 3.45: Loss = 0.705185
Epoch 3.46: Loss = 0.692261
Epoch 3.47: Loss = 0.666031
Epoch 3.48: Loss = 0.574631
Epoch 3.49: Loss = 0.744873
Epoch 3.50: Loss = 0.575897
Epoch 3.51: Loss = 0.662949
Epoch 3.52: Loss = 0.584396
Epoch 3.53: Loss = 0.737595
Epoch 3.54: Loss = 0.638458
Epoch 3.55: Loss = 0.726227
Epoch 3.56: Loss = 0.572556
Epoch 3.57: Loss = 0.7444
Epoch 3.58: Loss = 0.665512
Epoch 3.59: Loss = 0.725662
Epoch 3.60: Loss = 0.767426
Epoch 3.61: Loss = 0.744186
Epoch 3.62: Loss = 0.68277
Epoch 3.63: Loss = 0.637436
Epoch 3.64: Loss = 0.619781
Epoch 3.65: Loss = 0.707016
Epoch 3.66: Loss = 0.662155
Epoch 3.67: Loss = 0.699905
Epoch 3.68: Loss = 0.745605
Epoch 3.69: Loss = 0.653351
Epoch 3.70: Loss = 0.692703
Epoch 3.71: Loss = 0.659332
Epoch 3.72: Loss = 0.655411
Epoch 3.73: Loss = 0.826675
Epoch 3.74: Loss = 0.636322
Epoch 3.75: Loss = 0.729919
Epoch 3.76: Loss = 0.608688
Epoch 3.77: Loss = 0.675171
Epoch 3.78: Loss = 0.669403
Epoch 3.79: Loss = 0.75325
Epoch 3.80: Loss = 0.730331
Epoch 3.81: Loss = 0.683029
Epoch 3.82: Loss = 0.718994
Epoch 3.83: Loss = 0.630722
Epoch 3.84: Loss = 0.593216
Epoch 3.85: Loss = 0.704987
Epoch 3.86: Loss = 0.708664
Epoch 3.87: Loss = 0.583115
Epoch 3.88: Loss = 0.58989
Epoch 3.89: Loss = 0.716705
Epoch 3.90: Loss = 0.617981
Epoch 3.91: Loss = 0.673523
Epoch 3.92: Loss = 0.693405
Epoch 3.93: Loss = 0.636124
Epoch 3.94: Loss = 0.703995
Epoch 3.95: Loss = 0.679977
Epoch 3.96: Loss = 0.757477
Epoch 3.97: Loss = 0.788528
Epoch 3.98: Loss = 0.666473
Epoch 3.99: Loss = 0.697495
Epoch 3.100: Loss = 0.670471
Epoch 3.101: Loss = 0.653061
Epoch 3.102: Loss = 0.618423
Epoch 3.103: Loss = 0.630951
Epoch 3.104: Loss = 0.718903
Epoch 3.105: Loss = 0.579163
Epoch 3.106: Loss = 0.6465
Epoch 3.107: Loss = 0.673141
Epoch 3.108: Loss = 0.656525
Epoch 3.109: Loss = 0.614563
Epoch 3.110: Loss = 0.560089
Epoch 3.111: Loss = 0.624664
Epoch 3.112: Loss = 0.638611
Epoch 3.113: Loss = 0.73938
Epoch 3.114: Loss = 0.658813
Epoch 3.115: Loss = 0.664642
Epoch 3.116: Loss = 0.706375
Epoch 3.117: Loss = 0.672424
Epoch 3.118: Loss = 0.544327
Epoch 3.119: Loss = 0.622375
Epoch 3.120: Loss = 0.683884
TRAIN LOSS = 0.679276
TRAIN ACC = 77.9556 % (46776/60000)
Loss = 0.598007
Loss = 0.73259
Loss = 0.6698
Loss = 0.584641
Loss = 0.613937
Loss = 0.77684
Loss = 0.828751
Loss = 0.760254
Loss = 0.689926
Loss = 0.631134
Loss = 0.780533
Loss = 0.768936
Loss = 0.676453
Loss = 0.707855
Loss = 0.673386
Loss = 0.730865
Loss = 0.634842
Loss = 0.712692
Loss = 0.735504
Loss = 0.674042
TEST LOSS = 0.699049
TEST ACC = 467.76 % (7776/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.699417
Epoch 4.2: Loss = 0.611298
Epoch 4.3: Loss = 0.624344
Epoch 4.4: Loss = 0.757324
Epoch 4.5: Loss = 0.839874
Epoch 4.6: Loss = 0.663376
Epoch 4.7: Loss = 0.610016
Epoch 4.8: Loss = 0.584351
Epoch 4.9: Loss = 0.585495
Epoch 4.10: Loss = 0.656158
Epoch 4.11: Loss = 0.642609
Epoch 4.12: Loss = 0.702347
Epoch 4.13: Loss = 0.797119
Epoch 4.14: Loss = 0.669037
Epoch 4.15: Loss = 0.622452
Epoch 4.16: Loss = 0.684784
Epoch 4.17: Loss = 0.609055
Epoch 4.18: Loss = 0.695114
Epoch 4.19: Loss = 0.718292
Epoch 4.20: Loss = 0.619797
Epoch 4.21: Loss = 0.646301
Epoch 4.22: Loss = 0.663452
Epoch 4.23: Loss = 0.601257
Epoch 4.24: Loss = 0.788895
Epoch 4.25: Loss = 0.615295
Epoch 4.26: Loss = 0.727356
Epoch 4.27: Loss = 0.658768
Epoch 4.28: Loss = 0.66925
Epoch 4.29: Loss = 0.612167
Epoch 4.30: Loss = 0.744904
Epoch 4.31: Loss = 0.730362
Epoch 4.32: Loss = 0.681625
Epoch 4.33: Loss = 0.555969
Epoch 4.34: Loss = 0.623459
Epoch 4.35: Loss = 0.726181
Epoch 4.36: Loss = 0.70871
Epoch 4.37: Loss = 0.692322
Epoch 4.38: Loss = 0.740891
Epoch 4.39: Loss = 0.682831
Epoch 4.40: Loss = 0.724701
Epoch 4.41: Loss = 0.651871
Epoch 4.42: Loss = 0.692734
Epoch 4.43: Loss = 0.698059
Epoch 4.44: Loss = 0.668518
Epoch 4.45: Loss = 0.65155
Epoch 4.46: Loss = 0.596832
Epoch 4.47: Loss = 0.644318
Epoch 4.48: Loss = 0.660828
Epoch 4.49: Loss = 0.645309
Epoch 4.50: Loss = 0.622742
Epoch 4.51: Loss = 0.64711
Epoch 4.52: Loss = 0.530289
Epoch 4.53: Loss = 0.641098
Epoch 4.54: Loss = 0.594803
Epoch 4.55: Loss = 0.629196
Epoch 4.56: Loss = 0.687057
Epoch 4.57: Loss = 0.62384
Epoch 4.58: Loss = 0.665039
Epoch 4.59: Loss = 0.655396
Epoch 4.60: Loss = 0.679092
Epoch 4.61: Loss = 0.683182
Epoch 4.62: Loss = 0.69603
Epoch 4.63: Loss = 0.738342
Epoch 4.64: Loss = 0.627289
Epoch 4.65: Loss = 0.773346
Epoch 4.66: Loss = 0.710983
Epoch 4.67: Loss = 0.628021
Epoch 4.68: Loss = 0.635193
Epoch 4.69: Loss = 0.661697
Epoch 4.70: Loss = 0.609818
Epoch 4.71: Loss = 0.624542
Epoch 4.72: Loss = 0.64592
Epoch 4.73: Loss = 0.772217
Epoch 4.74: Loss = 0.593979
Epoch 4.75: Loss = 0.617371
Epoch 4.76: Loss = 0.682465
Epoch 4.77: Loss = 0.740646
Epoch 4.78: Loss = 0.612534
Epoch 4.79: Loss = 0.656525
Epoch 4.80: Loss = 0.662689
Epoch 4.81: Loss = 0.626923
Epoch 4.82: Loss = 0.527863
Epoch 4.83: Loss = 0.628143
Epoch 4.84: Loss = 0.726135
Epoch 4.85: Loss = 0.605377
Epoch 4.86: Loss = 0.758652
Epoch 4.87: Loss = 0.613037
Epoch 4.88: Loss = 0.531601
Epoch 4.89: Loss = 0.594177
Epoch 4.90: Loss = 0.662033
Epoch 4.91: Loss = 0.692535
Epoch 4.92: Loss = 0.640945
Epoch 4.93: Loss = 0.620865
Epoch 4.94: Loss = 0.697327
Epoch 4.95: Loss = 0.623276
Epoch 4.96: Loss = 0.637878
Epoch 4.97: Loss = 0.642029
Epoch 4.98: Loss = 0.649521
Epoch 4.99: Loss = 0.646484
Epoch 4.100: Loss = 0.648407
Epoch 4.101: Loss = 0.752884
Epoch 4.102: Loss = 0.715591
Epoch 4.103: Loss = 0.602798
Epoch 4.104: Loss = 0.710419
Epoch 4.105: Loss = 0.66954
Epoch 4.106: Loss = 0.67276
Epoch 4.107: Loss = 0.617584
Epoch 4.108: Loss = 0.720001
Epoch 4.109: Loss = 0.711288
Epoch 4.110: Loss = 0.736664
Epoch 4.111: Loss = 0.613144
Epoch 4.112: Loss = 0.749619
Epoch 4.113: Loss = 0.669327
Epoch 4.114: Loss = 0.695572
Epoch 4.115: Loss = 0.652924
Epoch 4.116: Loss = 0.674118
Epoch 4.117: Loss = 0.691116
Epoch 4.118: Loss = 0.722366
Epoch 4.119: Loss = 0.631104
Epoch 4.120: Loss = 0.553482
TRAIN LOSS = 0.663193
TRAIN ACC = 79.3564 % (47616/60000)
Loss = 0.578415
Loss = 0.714035
Loss = 0.65564
Loss = 0.567734
Loss = 0.621704
Loss = 0.773163
Loss = 0.825302
Loss = 0.742905
Loss = 0.683945
Loss = 0.629257
Loss = 0.803314
Loss = 0.767105
Loss = 0.687302
Loss = 0.701874
Loss = 0.666595
Loss = 0.723785
Loss = 0.625488
Loss = 0.709381
Loss = 0.72644
Loss = 0.66954
TEST LOSS = 0.693646
TEST ACC = 476.16 % (7915/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.71965
Epoch 5.2: Loss = 0.756287
Epoch 5.3: Loss = 0.641617
Epoch 5.4: Loss = 0.576553
Epoch 5.5: Loss = 0.713562
Epoch 5.6: Loss = 0.547943
Epoch 5.7: Loss = 0.54985
Epoch 5.8: Loss = 0.769547
Epoch 5.9: Loss = 0.664581
Epoch 5.10: Loss = 0.693497
Epoch 5.11: Loss = 0.640717
Epoch 5.12: Loss = 0.649078
Epoch 5.13: Loss = 0.651306
Epoch 5.14: Loss = 0.608932
Epoch 5.15: Loss = 0.712173
Epoch 5.16: Loss = 0.688553
Epoch 5.17: Loss = 0.625061
Epoch 5.18: Loss = 0.803192
Epoch 5.19: Loss = 0.602814
Epoch 5.20: Loss = 0.604065
Epoch 5.21: Loss = 0.644394
Epoch 5.22: Loss = 0.668701
Epoch 5.23: Loss = 0.69516
Epoch 5.24: Loss = 0.681564
Epoch 5.25: Loss = 0.612244
Epoch 5.26: Loss = 0.668854
Epoch 5.27: Loss = 0.680374
Epoch 5.28: Loss = 0.668991
Epoch 5.29: Loss = 0.61055
Epoch 5.30: Loss = 0.673431
Epoch 5.31: Loss = 0.590759
Epoch 5.32: Loss = 0.732132
Epoch 5.33: Loss = 0.601074
Epoch 5.34: Loss = 0.637238
Epoch 5.35: Loss = 0.72641
Epoch 5.36: Loss = 0.666351
Epoch 5.37: Loss = 0.757523
Epoch 5.38: Loss = 0.655945
Epoch 5.39: Loss = 0.656143
Epoch 5.40: Loss = 0.574326
Epoch 5.41: Loss = 0.681168
Epoch 5.42: Loss = 0.65863
Epoch 5.43: Loss = 0.684631
Epoch 5.44: Loss = 0.667862
Epoch 5.45: Loss = 0.607605
Epoch 5.46: Loss = 0.709091
Epoch 5.47: Loss = 0.601181
Epoch 5.48: Loss = 0.723145
Epoch 5.49: Loss = 0.568085
Epoch 5.50: Loss = 0.738235
Epoch 5.51: Loss = 0.636887
Epoch 5.52: Loss = 0.723862
Epoch 5.53: Loss = 0.645828
Epoch 5.54: Loss = 0.565125
Epoch 5.55: Loss = 0.564407
Epoch 5.56: Loss = 0.692322
Epoch 5.57: Loss = 0.724762
Epoch 5.58: Loss = 0.645401
Epoch 5.59: Loss = 0.732071
Epoch 5.60: Loss = 0.531281
Epoch 5.61: Loss = 0.532166
Epoch 5.62: Loss = 0.702133
Epoch 5.63: Loss = 0.6483
Epoch 5.64: Loss = 0.614227
Epoch 5.65: Loss = 0.673157
Epoch 5.66: Loss = 0.6427
Epoch 5.67: Loss = 0.564514
Epoch 5.68: Loss = 0.587219
Epoch 5.69: Loss = 0.647202
Epoch 5.70: Loss = 0.746078
Epoch 5.71: Loss = 0.658401
Epoch 5.72: Loss = 0.612228
Epoch 5.73: Loss = 0.663589
Epoch 5.74: Loss = 0.611893
Epoch 5.75: Loss = 0.600739
Epoch 5.76: Loss = 0.550674
Epoch 5.77: Loss = 0.650833
Epoch 5.78: Loss = 0.594421
Epoch 5.79: Loss = 0.696625
Epoch 5.80: Loss = 0.685623
Epoch 5.81: Loss = 0.675842
Epoch 5.82: Loss = 0.621399
Epoch 5.83: Loss = 0.729843
Epoch 5.84: Loss = 0.639297
Epoch 5.85: Loss = 0.563919
Epoch 5.86: Loss = 0.624832
Epoch 5.87: Loss = 0.742737
Epoch 5.88: Loss = 0.648804
Epoch 5.89: Loss = 0.743942
Epoch 5.90: Loss = 0.756714
Epoch 5.91: Loss = 0.618698
Epoch 5.92: Loss = 0.614944
Epoch 5.93: Loss = 0.634399
Epoch 5.94: Loss = 0.595139
Epoch 5.95: Loss = 0.743484
Epoch 5.96: Loss = 0.767151
Epoch 5.97: Loss = 0.612427
Epoch 5.98: Loss = 0.687561
Epoch 5.99: Loss = 0.705536
Epoch 5.100: Loss = 0.603516
Epoch 5.101: Loss = 0.696991
Epoch 5.102: Loss = 0.605865
Epoch 5.103: Loss = 0.582657
Epoch 5.104: Loss = 0.662964
Epoch 5.105: Loss = 0.601852
Epoch 5.106: Loss = 0.608536
Epoch 5.107: Loss = 0.691833
Epoch 5.108: Loss = 0.65535
Epoch 5.109: Loss = 0.752426
Epoch 5.110: Loss = 0.614853
Epoch 5.111: Loss = 0.629791
Epoch 5.112: Loss = 0.675629
Epoch 5.113: Loss = 0.560425
Epoch 5.114: Loss = 0.615509
Epoch 5.115: Loss = 0.750656
Epoch 5.116: Loss = 0.632706
Epoch 5.117: Loss = 0.660583
Epoch 5.118: Loss = 0.658936
Epoch 5.119: Loss = 0.492081
Epoch 5.120: Loss = 0.67305
TRAIN LOSS = 0.653
TRAIN ACC = 80.217 % (48132/60000)
Loss = 0.568863
Loss = 0.692383
Loss = 0.63324
Loss = 0.539597
Loss = 0.60022
Loss = 0.755264
Loss = 0.806747
Loss = 0.741852
Loss = 0.663055
Loss = 0.627853
Loss = 0.817139
Loss = 0.770782
Loss = 0.675064
Loss = 0.686172
Loss = 0.656937
Loss = 0.708588
Loss = 0.623596
Loss = 0.687973
Loss = 0.723495
Loss = 0.659973
TEST LOSS = 0.68194
TEST ACC = 481.319 % (7971/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.631241
Epoch 6.2: Loss = 0.648376
Epoch 6.3: Loss = 0.617203
Epoch 6.4: Loss = 0.656006
Epoch 6.5: Loss = 0.727036
Epoch 6.6: Loss = 0.511536
Epoch 6.7: Loss = 0.71228
Epoch 6.8: Loss = 0.725662
Epoch 6.9: Loss = 0.716202
Epoch 6.10: Loss = 0.549362
Epoch 6.11: Loss = 0.740356
Epoch 6.12: Loss = 0.602783
Epoch 6.13: Loss = 0.671356
Epoch 6.14: Loss = 0.599899
Epoch 6.15: Loss = 0.750931
Epoch 6.16: Loss = 0.655365
Epoch 6.17: Loss = 0.529404
Epoch 6.18: Loss = 0.629822
Epoch 6.19: Loss = 0.576172
Epoch 6.20: Loss = 0.67955
Epoch 6.21: Loss = 0.54126
Epoch 6.22: Loss = 0.531693
Epoch 6.23: Loss = 0.61644
Epoch 6.24: Loss = 0.658615
Epoch 6.25: Loss = 0.600632
Epoch 6.26: Loss = 0.687012
Epoch 6.27: Loss = 0.596573
Epoch 6.28: Loss = 0.721542
Epoch 6.29: Loss = 0.644196
Epoch 6.30: Loss = 0.661377
Epoch 6.31: Loss = 0.702377
Epoch 6.32: Loss = 0.53125
Epoch 6.33: Loss = 0.683762
Epoch 6.34: Loss = 0.639206
Epoch 6.35: Loss = 0.575668
Epoch 6.36: Loss = 0.689102
Epoch 6.37: Loss = 0.532761
Epoch 6.38: Loss = 0.634201
Epoch 6.39: Loss = 0.590103
Epoch 6.40: Loss = 0.580902
Epoch 6.41: Loss = 0.659637
Epoch 6.42: Loss = 0.608688
Epoch 6.43: Loss = 0.723038
Epoch 6.44: Loss = 0.727463
Epoch 6.45: Loss = 0.628479
Epoch 6.46: Loss = 0.67601
Epoch 6.47: Loss = 0.565811
Epoch 6.48: Loss = 0.629196
Epoch 6.49: Loss = 0.614899
Epoch 6.50: Loss = 0.71463
Epoch 6.51: Loss = 0.577011
Epoch 6.52: Loss = 0.71933
Epoch 6.53: Loss = 0.667145
Epoch 6.54: Loss = 0.689377
Epoch 6.55: Loss = 0.471085
Epoch 6.56: Loss = 0.649124
Epoch 6.57: Loss = 0.628769
Epoch 6.58: Loss = 0.70343
Epoch 6.59: Loss = 0.544876
Epoch 6.60: Loss = 0.635483
Epoch 6.61: Loss = 0.674713
Epoch 6.62: Loss = 0.783554
Epoch 6.63: Loss = 0.768631
Epoch 6.64: Loss = 0.658279
Epoch 6.65: Loss = 0.617188
Epoch 6.66: Loss = 0.624832
Epoch 6.67: Loss = 0.686218
Epoch 6.68: Loss = 0.637772
Epoch 6.69: Loss = 0.63385
Epoch 6.70: Loss = 0.7948
Epoch 6.71: Loss = 0.651108
Epoch 6.72: Loss = 0.718033
Epoch 6.73: Loss = 0.557861
Epoch 6.74: Loss = 0.642151
Epoch 6.75: Loss = 0.573822
Epoch 6.76: Loss = 0.698532
Epoch 6.77: Loss = 0.623123
Epoch 6.78: Loss = 0.746796
Epoch 6.79: Loss = 0.625687
Epoch 6.80: Loss = 0.689606
Epoch 6.81: Loss = 0.558212
Epoch 6.82: Loss = 0.552399
Epoch 6.83: Loss = 0.626984
Epoch 6.84: Loss = 0.570724
Epoch 6.85: Loss = 0.696152
Epoch 6.86: Loss = 0.66124
Epoch 6.87: Loss = 0.614777
Epoch 6.88: Loss = 0.54393
Epoch 6.89: Loss = 0.761749
Epoch 6.90: Loss = 0.585907
Epoch 6.91: Loss = 0.485825
Epoch 6.92: Loss = 0.641922
Epoch 6.93: Loss = 0.681641
Epoch 6.94: Loss = 0.715225
Epoch 6.95: Loss = 0.569351
Epoch 6.96: Loss = 0.659271
Epoch 6.97: Loss = 0.646133
Epoch 6.98: Loss = 0.702484
Epoch 6.99: Loss = 0.604538
Epoch 6.100: Loss = 0.729858
Epoch 6.101: Loss = 0.590286
Epoch 6.102: Loss = 0.627884
Epoch 6.103: Loss = 0.641968
Epoch 6.104: Loss = 0.513184
Epoch 6.105: Loss = 0.731003
Epoch 6.106: Loss = 0.678604
Epoch 6.107: Loss = 0.726608
Epoch 6.108: Loss = 0.59845
Epoch 6.109: Loss = 0.590912
Epoch 6.110: Loss = 0.758179
Epoch 6.111: Loss = 0.68898
Epoch 6.112: Loss = 0.751953
Epoch 6.113: Loss = 0.606888
Epoch 6.114: Loss = 0.76506
Epoch 6.115: Loss = 0.5802
Epoch 6.116: Loss = 0.598312
Epoch 6.117: Loss = 0.665115
Epoch 6.118: Loss = 0.621994
Epoch 6.119: Loss = 0.615448
Epoch 6.120: Loss = 0.755417
TRAIN LOSS = 0.644211
TRAIN ACC = 80.7037 % (48424/60000)
Loss = 0.565292
Loss = 0.670609
Loss = 0.622009
Loss = 0.538895
Loss = 0.597824
Loss = 0.736572
Loss = 0.818893
Loss = 0.744919
Loss = 0.651062
Loss = 0.618149
Loss = 0.799316
Loss = 0.77327
Loss = 0.676773
Loss = 0.695572
Loss = 0.641724
Loss = 0.69371
Loss = 0.626526
Loss = 0.679306
Loss = 0.698715
Loss = 0.667465
TEST LOSS = 0.67583
TEST ACC = 484.239 % (8024/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.641617
Epoch 7.2: Loss = 0.694138
Epoch 7.3: Loss = 0.559967
Epoch 7.4: Loss = 0.552841
Epoch 7.5: Loss = 0.595703
Epoch 7.6: Loss = 0.669891
Epoch 7.7: Loss = 0.662079
Epoch 7.8: Loss = 0.607697
Epoch 7.9: Loss = 0.693481
Epoch 7.10: Loss = 0.653687
Epoch 7.11: Loss = 0.605148
Epoch 7.12: Loss = 0.58934
Epoch 7.13: Loss = 0.64061
Epoch 7.14: Loss = 0.603287
Epoch 7.15: Loss = 0.709015
Epoch 7.16: Loss = 0.68187
Epoch 7.17: Loss = 0.576172
Epoch 7.18: Loss = 0.694412
Epoch 7.19: Loss = 0.611801
Epoch 7.20: Loss = 0.594849
Epoch 7.21: Loss = 0.528091
Epoch 7.22: Loss = 0.764572
Epoch 7.23: Loss = 0.635544
Epoch 7.24: Loss = 0.621887
Epoch 7.25: Loss = 0.653488
Epoch 7.26: Loss = 0.777481
Epoch 7.27: Loss = 0.586151
Epoch 7.28: Loss = 0.638565
Epoch 7.29: Loss = 0.697586
Epoch 7.30: Loss = 0.706512
Epoch 7.31: Loss = 0.561081
Epoch 7.32: Loss = 0.763947
Epoch 7.33: Loss = 0.610779
Epoch 7.34: Loss = 0.672043
Epoch 7.35: Loss = 0.715134
Epoch 7.36: Loss = 0.633469
Epoch 7.37: Loss = 0.699219
Epoch 7.38: Loss = 0.628632
Epoch 7.39: Loss = 0.530838
Epoch 7.40: Loss = 0.679977
Epoch 7.41: Loss = 0.622818
Epoch 7.42: Loss = 0.471069
Epoch 7.43: Loss = 0.719086
Epoch 7.44: Loss = 0.600784
Epoch 7.45: Loss = 0.615952
Epoch 7.46: Loss = 0.546707
Epoch 7.47: Loss = 0.588913
Epoch 7.48: Loss = 0.619003
Epoch 7.49: Loss = 0.637894
Epoch 7.50: Loss = 0.642105
Epoch 7.51: Loss = 0.619034
Epoch 7.52: Loss = 0.589737
Epoch 7.53: Loss = 0.651321
Epoch 7.54: Loss = 0.673325
Epoch 7.55: Loss = 0.602097
Epoch 7.56: Loss = 0.808578
Epoch 7.57: Loss = 0.603577
Epoch 7.58: Loss = 0.654236
Epoch 7.59: Loss = 0.615738
Epoch 7.60: Loss = 0.617706
Epoch 7.61: Loss = 0.647034
Epoch 7.62: Loss = 0.632202
Epoch 7.63: Loss = 0.649872
Epoch 7.64: Loss = 0.550812
Epoch 7.65: Loss = 0.69693
Epoch 7.66: Loss = 0.657593
Epoch 7.67: Loss = 0.647247
Epoch 7.68: Loss = 0.73378
Epoch 7.69: Loss = 0.576553
Epoch 7.70: Loss = 0.72348
Epoch 7.71: Loss = 0.574585
Epoch 7.72: Loss = 0.605484
Epoch 7.73: Loss = 0.731415
Epoch 7.74: Loss = 0.792084
Epoch 7.75: Loss = 0.638519
Epoch 7.76: Loss = 0.67038
Epoch 7.77: Loss = 0.782242
Epoch 7.78: Loss = 0.686844
Epoch 7.79: Loss = 0.661697
Epoch 7.80: Loss = 0.551849
Epoch 7.81: Loss = 0.568344
Epoch 7.82: Loss = 0.696182
Epoch 7.83: Loss = 0.629761
Epoch 7.84: Loss = 0.593399
Epoch 7.85: Loss = 0.602524
Epoch 7.86: Loss = 0.629456
Epoch 7.87: Loss = 0.643906
Epoch 7.88: Loss = 0.581177
Epoch 7.89: Loss = 0.569199
Epoch 7.90: Loss = 0.595184
Epoch 7.91: Loss = 0.803345
Epoch 7.92: Loss = 0.733612
Epoch 7.93: Loss = 0.684631
Epoch 7.94: Loss = 0.788254
Epoch 7.95: Loss = 0.748184
Epoch 7.96: Loss = 0.665573
Epoch 7.97: Loss = 0.549683
Epoch 7.98: Loss = 0.604507
Epoch 7.99: Loss = 0.469711
Epoch 7.100: Loss = 0.518143
Epoch 7.101: Loss = 0.637695
Epoch 7.102: Loss = 0.665924
Epoch 7.103: Loss = 0.656143
Epoch 7.104: Loss = 0.483261
Epoch 7.105: Loss = 0.67009
Epoch 7.106: Loss = 0.598587
Epoch 7.107: Loss = 0.537964
Epoch 7.108: Loss = 0.584137
Epoch 7.109: Loss = 0.809143
Epoch 7.110: Loss = 0.513504
Epoch 7.111: Loss = 0.690414
Epoch 7.112: Loss = 0.672913
Epoch 7.113: Loss = 0.669922
Epoch 7.114: Loss = 0.640869
Epoch 7.115: Loss = 0.7854
Epoch 7.116: Loss = 0.693329
Epoch 7.117: Loss = 0.68573
Epoch 7.118: Loss = 0.554733
Epoch 7.119: Loss = 0.549377
Epoch 7.120: Loss = 0.640411
TRAIN LOSS = 0.640564
TRAIN ACC = 81.1646 % (48701/60000)
Loss = 0.56517
Loss = 0.67659
Loss = 0.619415
Loss = 0.524979
Loss = 0.596817
Loss = 0.747162
Loss = 0.79715
Loss = 0.734589
Loss = 0.65715
Loss = 0.6026
Loss = 0.820633
Loss = 0.776169
Loss = 0.672073
Loss = 0.673386
Loss = 0.632416
Loss = 0.696747
Loss = 0.622101
Loss = 0.699524
Loss = 0.68718
Loss = 0.662231
TEST LOSS = 0.673204
TEST ACC = 487.009 % (8092/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.589539
Epoch 8.2: Loss = 0.681503
Epoch 8.3: Loss = 0.563293
Epoch 8.4: Loss = 0.61055
Epoch 8.5: Loss = 0.61467
Epoch 8.6: Loss = 0.615997
Epoch 8.7: Loss = 0.619553
Epoch 8.8: Loss = 0.670074
Epoch 8.9: Loss = 0.590347
Epoch 8.10: Loss = 0.619049
Epoch 8.11: Loss = 0.569244
Epoch 8.12: Loss = 0.645966
Epoch 8.13: Loss = 0.68602
Epoch 8.14: Loss = 0.620682
Epoch 8.15: Loss = 0.685593
Epoch 8.16: Loss = 0.821472
Epoch 8.17: Loss = 0.61824
Epoch 8.18: Loss = 0.633408
Epoch 8.19: Loss = 0.571289
Epoch 8.20: Loss = 0.593033
Epoch 8.21: Loss = 0.757507
Epoch 8.22: Loss = 0.633957
Epoch 8.23: Loss = 0.622528
Epoch 8.24: Loss = 0.626923
Epoch 8.25: Loss = 0.691971
Epoch 8.26: Loss = 0.467316
Epoch 8.27: Loss = 0.610596
Epoch 8.28: Loss = 0.7211
Epoch 8.29: Loss = 0.714294
Epoch 8.30: Loss = 0.715866
Epoch 8.31: Loss = 0.633194
Epoch 8.32: Loss = 0.637741
Epoch 8.33: Loss = 0.52713
Epoch 8.34: Loss = 0.600952
Epoch 8.35: Loss = 0.682953
Epoch 8.36: Loss = 0.656143
Epoch 8.37: Loss = 0.568176
Epoch 8.38: Loss = 0.630875
Epoch 8.39: Loss = 0.670471
Epoch 8.40: Loss = 0.542236
Epoch 8.41: Loss = 0.624496
Epoch 8.42: Loss = 0.664658
Epoch 8.43: Loss = 0.537491
Epoch 8.44: Loss = 0.63858
Epoch 8.45: Loss = 0.580383
Epoch 8.46: Loss = 0.701431
Epoch 8.47: Loss = 0.592255
Epoch 8.48: Loss = 0.605728
Epoch 8.49: Loss = 0.651764
Epoch 8.50: Loss = 0.562927
Epoch 8.51: Loss = 0.569794
Epoch 8.52: Loss = 0.678406
Epoch 8.53: Loss = 0.744125
Epoch 8.54: Loss = 0.714813
Epoch 8.55: Loss = 0.755493
Epoch 8.56: Loss = 0.773117
Epoch 8.57: Loss = 0.573105
Epoch 8.58: Loss = 0.584991
Epoch 8.59: Loss = 0.618088
Epoch 8.60: Loss = 0.577621
Epoch 8.61: Loss = 0.712204
Epoch 8.62: Loss = 0.558594
Epoch 8.63: Loss = 0.706696
Epoch 8.64: Loss = 0.538055
Epoch 8.65: Loss = 0.68837
Epoch 8.66: Loss = 0.519989
Epoch 8.67: Loss = 0.597504
Epoch 8.68: Loss = 0.592972
Epoch 8.69: Loss = 0.57402
Epoch 8.70: Loss = 0.580887
Epoch 8.71: Loss = 0.537186
Epoch 8.72: Loss = 0.78743
Epoch 8.73: Loss = 0.558792
Epoch 8.74: Loss = 0.754471
Epoch 8.75: Loss = 0.673569
Epoch 8.76: Loss = 0.76799
Epoch 8.77: Loss = 0.733246
Epoch 8.78: Loss = 0.61528
Epoch 8.79: Loss = 0.590897
Epoch 8.80: Loss = 0.684784
Epoch 8.81: Loss = 0.570969
Epoch 8.82: Loss = 0.689819
Epoch 8.83: Loss = 0.806168
Epoch 8.84: Loss = 0.657806
Epoch 8.85: Loss = 0.565735
Epoch 8.86: Loss = 0.578598
Epoch 8.87: Loss = 0.597595
Epoch 8.88: Loss = 0.645859
Epoch 8.89: Loss = 0.783493
Epoch 8.90: Loss = 0.669754
Epoch 8.91: Loss = 0.603821
Epoch 8.92: Loss = 0.675461
Epoch 8.93: Loss = 0.682114
Epoch 8.94: Loss = 0.597488
Epoch 8.95: Loss = 0.726852
Epoch 8.96: Loss = 0.687027
Epoch 8.97: Loss = 0.695709
Epoch 8.98: Loss = 0.550964
Epoch 8.99: Loss = 0.700607
Epoch 8.100: Loss = 0.710617
Epoch 8.101: Loss = 0.63942
Epoch 8.102: Loss = 0.6633
Epoch 8.103: Loss = 0.447311
Epoch 8.104: Loss = 0.583817
Epoch 8.105: Loss = 0.643112
Epoch 8.106: Loss = 0.51329
Epoch 8.107: Loss = 0.72551
Epoch 8.108: Loss = 0.594955
Epoch 8.109: Loss = 0.626495
Epoch 8.110: Loss = 0.656891
Epoch 8.111: Loss = 0.640884
Epoch 8.112: Loss = 0.628555
Epoch 8.113: Loss = 0.585724
Epoch 8.114: Loss = 0.637726
Epoch 8.115: Loss = 0.649734
Epoch 8.116: Loss = 0.67572
Epoch 8.117: Loss = 0.618927
Epoch 8.118: Loss = 0.662521
Epoch 8.119: Loss = 0.746338
Epoch 8.120: Loss = 0.665466
TRAIN LOSS = 0.638809
TRAIN ACC = 81.488 % (48895/60000)
Loss = 0.568588
Loss = 0.668762
Loss = 0.619446
Loss = 0.533127
Loss = 0.608292
Loss = 0.737137
Loss = 0.814041
Loss = 0.727386
Loss = 0.657104
Loss = 0.601364
Loss = 0.818893
Loss = 0.779816
Loss = 0.69014
Loss = 0.6707
Loss = 0.618347
Loss = 0.690781
Loss = 0.614212
Loss = 0.706055
Loss = 0.69577
Loss = 0.666977
TEST LOSS = 0.674347
TEST ACC = 488.95 % (8096/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.754425
Epoch 9.2: Loss = 0.683289
Epoch 9.3: Loss = 0.571487
Epoch 9.4: Loss = 0.524521
Epoch 9.5: Loss = 0.560699
Epoch 9.6: Loss = 0.660202
Epoch 9.7: Loss = 0.56485
Epoch 9.8: Loss = 0.663391
Epoch 9.9: Loss = 0.594193
Epoch 9.10: Loss = 0.550674
Epoch 9.11: Loss = 0.643219
Epoch 9.12: Loss = 0.677063
Epoch 9.13: Loss = 0.533722
Epoch 9.14: Loss = 0.698196
Epoch 9.15: Loss = 0.636765
Epoch 9.16: Loss = 0.569824
Epoch 9.17: Loss = 0.621124
Epoch 9.18: Loss = 0.636093
Epoch 9.19: Loss = 0.658707
Epoch 9.20: Loss = 0.750397
Epoch 9.21: Loss = 0.723083
Epoch 9.22: Loss = 0.674698
Epoch 9.23: Loss = 0.618484
Epoch 9.24: Loss = 0.573013
Epoch 9.25: Loss = 0.593674
Epoch 9.26: Loss = 0.707901
Epoch 9.27: Loss = 0.523911
Epoch 9.28: Loss = 0.657883
Epoch 9.29: Loss = 0.568848
Epoch 9.30: Loss = 0.628693
Epoch 9.31: Loss = 0.59079
Epoch 9.32: Loss = 0.516022
Epoch 9.33: Loss = 0.692291
Epoch 9.34: Loss = 0.608719
Epoch 9.35: Loss = 0.748123
Epoch 9.36: Loss = 0.676544
Epoch 9.37: Loss = 0.783249
Epoch 9.38: Loss = 0.60881
Epoch 9.39: Loss = 0.660141
Epoch 9.40: Loss = 0.644073
Epoch 9.41: Loss = 0.579544
Epoch 9.42: Loss = 0.758438
Epoch 9.43: Loss = 0.550308
Epoch 9.44: Loss = 0.551285
Epoch 9.45: Loss = 0.608673
Epoch 9.46: Loss = 0.58812
Epoch 9.47: Loss = 0.644608
Epoch 9.48: Loss = 0.640564
Epoch 9.49: Loss = 0.558365
Epoch 9.50: Loss = 0.647995
Epoch 9.51: Loss = 0.638885
Epoch 9.52: Loss = 0.610123
Epoch 9.53: Loss = 0.614792
Epoch 9.54: Loss = 0.654282
Epoch 9.55: Loss = 0.638687
Epoch 9.56: Loss = 0.574554
Epoch 9.57: Loss = 0.626251
Epoch 9.58: Loss = 0.790192
Epoch 9.59: Loss = 0.704941
Epoch 9.60: Loss = 0.589432
Epoch 9.61: Loss = 0.558838
Epoch 9.62: Loss = 0.540466
Epoch 9.63: Loss = 0.697784
Epoch 9.64: Loss = 0.658722
Epoch 9.65: Loss = 0.537018
Epoch 9.66: Loss = 0.629684
Epoch 9.67: Loss = 0.621979
Epoch 9.68: Loss = 0.701706
Epoch 9.69: Loss = 0.706406
Epoch 9.70: Loss = 0.621704
Epoch 9.71: Loss = 0.620941
Epoch 9.72: Loss = 0.589005
Epoch 9.73: Loss = 0.585617
Epoch 9.74: Loss = 0.624512
Epoch 9.75: Loss = 0.74144
Epoch 9.76: Loss = 0.612671
Epoch 9.77: Loss = 0.734222
Epoch 9.78: Loss = 0.730789
Epoch 9.79: Loss = 0.636597
Epoch 9.80: Loss = 0.621948
Epoch 9.81: Loss = 0.599167
Epoch 9.82: Loss = 0.606674
Epoch 9.83: Loss = 0.72522
Epoch 9.84: Loss = 0.657043
Epoch 9.85: Loss = 0.758392
Epoch 9.86: Loss = 0.535614
Epoch 9.87: Loss = 0.634094
Epoch 9.88: Loss = 0.499878
Epoch 9.89: Loss = 0.601532
Epoch 9.90: Loss = 0.656723
Epoch 9.91: Loss = 0.589355
Epoch 9.92: Loss = 0.69635
Epoch 9.93: Loss = 0.641144
Epoch 9.94: Loss = 0.652954
Epoch 9.95: Loss = 0.643707
Epoch 9.96: Loss = 0.701355
Epoch 9.97: Loss = 0.631607
Epoch 9.98: Loss = 0.683792
Epoch 9.99: Loss = 0.695053
Epoch 9.100: Loss = 0.577515
Epoch 9.101: Loss = 0.726913
Epoch 9.102: Loss = 0.602417
Epoch 9.103: Loss = 0.697678
Epoch 9.104: Loss = 0.563889
Epoch 9.105: Loss = 0.661911
Epoch 9.106: Loss = 0.638474
Epoch 9.107: Loss = 0.806854
Epoch 9.108: Loss = 0.690094
Epoch 9.109: Loss = 0.665375
Epoch 9.110: Loss = 0.652512
Epoch 9.111: Loss = 0.533646
Epoch 9.112: Loss = 0.690842
Epoch 9.113: Loss = 0.640579
Epoch 9.114: Loss = 0.676285
Epoch 9.115: Loss = 0.587875
Epoch 9.116: Loss = 0.63295
Epoch 9.117: Loss = 0.700836
Epoch 9.118: Loss = 0.690094
Epoch 9.119: Loss = 0.638245
Epoch 9.120: Loss = 0.7052
TRAIN LOSS = 0.639038
TRAIN ACC = 81.8451 % (49110/60000)
Loss = 0.581024
Loss = 0.657944
Loss = 0.628174
Loss = 0.532883
Loss = 0.601273
Loss = 0.735718
Loss = 0.817932
Loss = 0.742126
Loss = 0.660889
Loss = 0.618851
Loss = 0.851364
Loss = 0.797195
Loss = 0.675507
Loss = 0.676712
Loss = 0.619583
Loss = 0.678391
Loss = 0.617981
Loss = 0.70079
Loss = 0.705429
Loss = 0.680481
TEST LOSS = 0.679012
TEST ACC = 491.1 % (8135/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.738983
Epoch 10.2: Loss = 0.681213
Epoch 10.3: Loss = 0.501343
Epoch 10.4: Loss = 0.563065
Epoch 10.5: Loss = 0.592804
Epoch 10.6: Loss = 0.549866
Epoch 10.7: Loss = 0.729858
Epoch 10.8: Loss = 0.678253
Epoch 10.9: Loss = 0.594727
Epoch 10.10: Loss = 0.552887
Epoch 10.11: Loss = 0.750275
Epoch 10.12: Loss = 0.776108
Epoch 10.13: Loss = 0.641083
Epoch 10.14: Loss = 0.492996
Epoch 10.15: Loss = 0.739563
Epoch 10.16: Loss = 0.594254
Epoch 10.17: Loss = 0.686432
Epoch 10.18: Loss = 0.586258
Epoch 10.19: Loss = 0.643936
Epoch 10.20: Loss = 0.682571
Epoch 10.21: Loss = 0.575119
Epoch 10.22: Loss = 0.608292
Epoch 10.23: Loss = 0.627914
Epoch 10.24: Loss = 0.469894
Epoch 10.25: Loss = 0.573196
Epoch 10.26: Loss = 0.5728
Epoch 10.27: Loss = 0.740036
Epoch 10.28: Loss = 0.591888
Epoch 10.29: Loss = 0.669052
Epoch 10.30: Loss = 0.572281
Epoch 10.31: Loss = 0.601028
Epoch 10.32: Loss = 0.537842
Epoch 10.33: Loss = 0.746552
Epoch 10.34: Loss = 0.742508
Epoch 10.35: Loss = 0.547943
Epoch 10.36: Loss = 0.811996
Epoch 10.37: Loss = 0.660599
Epoch 10.38: Loss = 0.636826
Epoch 10.39: Loss = 0.628067
Epoch 10.40: Loss = 0.65567
Epoch 10.41: Loss = 0.566788
Epoch 10.42: Loss = 0.62471
Epoch 10.43: Loss = 0.723969
Epoch 10.44: Loss = 0.70163
Epoch 10.45: Loss = 0.505615
Epoch 10.46: Loss = 0.550797
Epoch 10.47: Loss = 0.65506
Epoch 10.48: Loss = 0.605225
Epoch 10.49: Loss = 0.627899
Epoch 10.50: Loss = 0.571838
Epoch 10.51: Loss = 0.56044
Epoch 10.52: Loss = 0.692154
Epoch 10.53: Loss = 0.687302
Epoch 10.54: Loss = 0.633163
Epoch 10.55: Loss = 0.667679
Epoch 10.56: Loss = 0.822525
Epoch 10.57: Loss = 0.677063
Epoch 10.58: Loss = 0.746704
Epoch 10.59: Loss = 0.535934
Epoch 10.60: Loss = 0.545822
Epoch 10.61: Loss = 0.549683
Epoch 10.62: Loss = 0.581894
Epoch 10.63: Loss = 0.671707
Epoch 10.64: Loss = 0.600937
Epoch 10.65: Loss = 0.584213
Epoch 10.66: Loss = 0.582687
Epoch 10.67: Loss = 0.713882
Epoch 10.68: Loss = 0.59816
Epoch 10.69: Loss = 0.550812
Epoch 10.70: Loss = 0.632904
Epoch 10.71: Loss = 0.908417
Epoch 10.72: Loss = 0.653992
Epoch 10.73: Loss = 0.490982
Epoch 10.74: Loss = 0.731628
Epoch 10.75: Loss = 0.507004
Epoch 10.76: Loss = 0.709579
Epoch 10.77: Loss = 0.59024
Epoch 10.78: Loss = 0.637787
Epoch 10.79: Loss = 0.672684
Epoch 10.80: Loss = 0.663712
Epoch 10.81: Loss = 0.680344
Epoch 10.82: Loss = 0.692978
Epoch 10.83: Loss = 0.689346
Epoch 10.84: Loss = 0.601761
Epoch 10.85: Loss = 0.588501
Epoch 10.86: Loss = 0.692047
Epoch 10.87: Loss = 0.75592
Epoch 10.88: Loss = 0.583252
Epoch 10.89: Loss = 0.583374
Epoch 10.90: Loss = 0.551254
Epoch 10.91: Loss = 0.605103
Epoch 10.92: Loss = 0.647827
Epoch 10.93: Loss = 0.706726
Epoch 10.94: Loss = 0.697235
Epoch 10.95: Loss = 0.634018
Epoch 10.96: Loss = 0.63385
Epoch 10.97: Loss = 0.645233
Epoch 10.98: Loss = 0.643158
Epoch 10.99: Loss = 0.816086
Epoch 10.100: Loss = 0.725052
Epoch 10.101: Loss = 0.627029
Epoch 10.102: Loss = 0.787628
Epoch 10.103: Loss = 0.754745
Epoch 10.104: Loss = 0.592758
Epoch 10.105: Loss = 0.668381
Epoch 10.106: Loss = 0.654648
Epoch 10.107: Loss = 0.641251
Epoch 10.108: Loss = 0.647598
Epoch 10.109: Loss = 0.564987
Epoch 10.110: Loss = 0.555313
Epoch 10.111: Loss = 0.624725
Epoch 10.112: Loss = 0.693466
Epoch 10.113: Loss = 0.541779
Epoch 10.114: Loss = 0.687469
Epoch 10.115: Loss = 0.716629
Epoch 10.116: Loss = 0.617493
Epoch 10.117: Loss = 0.612976
Epoch 10.118: Loss = 0.683823
Epoch 10.119: Loss = 0.592194
Epoch 10.120: Loss = 0.780197
TRAIN LOSS = 0.640839
TRAIN ACC = 82.2861 % (49374/60000)
Loss = 0.574997
Loss = 0.663666
Loss = 0.629532
Loss = 0.536301
Loss = 0.598557
Loss = 0.739105
Loss = 0.837067
Loss = 0.741211
Loss = 0.654434
Loss = 0.628845
Loss = 0.859344
Loss = 0.791046
Loss = 0.660492
Loss = 0.683884
Loss = 0.625641
Loss = 0.67804
Loss = 0.629959
Loss = 0.711624
Loss = 0.685974
Loss = 0.693649
TEST LOSS = 0.681168
TEST ACC = 493.739 % (8148/10000)

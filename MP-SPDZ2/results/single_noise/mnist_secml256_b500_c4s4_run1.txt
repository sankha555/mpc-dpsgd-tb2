Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.37697
Epoch 1.2: Loss = 2.31358
Epoch 1.3: Loss = 2.28726
Epoch 1.4: Loss = 2.239
Epoch 1.5: Loss = 2.23041
Epoch 1.6: Loss = 2.17415
Epoch 1.7: Loss = 2.17355
Epoch 1.8: Loss = 2.11012
Epoch 1.9: Loss = 2.10988
Epoch 1.10: Loss = 2.08826
Epoch 1.11: Loss = 2.0463
Epoch 1.12: Loss = 2.02457
Epoch 1.13: Loss = 1.98216
Epoch 1.14: Loss = 1.96883
Epoch 1.15: Loss = 1.95041
Epoch 1.16: Loss = 1.91495
Epoch 1.17: Loss = 1.88635
Epoch 1.18: Loss = 1.86189
Epoch 1.19: Loss = 1.8031
Epoch 1.20: Loss = 1.77434
Epoch 1.21: Loss = 1.78481
Epoch 1.22: Loss = 1.737
Epoch 1.23: Loss = 1.73915
Epoch 1.24: Loss = 1.66447
Epoch 1.25: Loss = 1.65073
Epoch 1.26: Loss = 1.61134
Epoch 1.27: Loss = 1.59521
Epoch 1.28: Loss = 1.57735
Epoch 1.29: Loss = 1.50079
Epoch 1.30: Loss = 1.47493
Epoch 1.31: Loss = 1.44881
Epoch 1.32: Loss = 1.48671
Epoch 1.33: Loss = 1.45248
Epoch 1.34: Loss = 1.39882
Epoch 1.35: Loss = 1.38821
Epoch 1.36: Loss = 1.35785
Epoch 1.37: Loss = 1.32819
Epoch 1.38: Loss = 1.31876
Epoch 1.39: Loss = 1.26215
Epoch 1.40: Loss = 1.26459
Epoch 1.41: Loss = 1.23291
Epoch 1.42: Loss = 1.17183
Epoch 1.43: Loss = 1.20001
Epoch 1.44: Loss = 1.15128
Epoch 1.45: Loss = 1.0674
Epoch 1.46: Loss = 1.02457
Epoch 1.47: Loss = 1.08731
Epoch 1.48: Loss = 1.15363
Epoch 1.49: Loss = 1.08557
Epoch 1.50: Loss = 1.06047
Epoch 1.51: Loss = 1.03242
Epoch 1.52: Loss = 0.99942
Epoch 1.53: Loss = 1.03139
Epoch 1.54: Loss = 1.04453
Epoch 1.55: Loss = 1.01317
Epoch 1.56: Loss = 0.951553
Epoch 1.57: Loss = 0.90918
Epoch 1.58: Loss = 0.90564
Epoch 1.59: Loss = 0.928558
Epoch 1.60: Loss = 0.863266
Epoch 1.61: Loss = 0.871506
Epoch 1.62: Loss = 0.857239
Epoch 1.63: Loss = 0.839508
Epoch 1.64: Loss = 0.909241
Epoch 1.65: Loss = 0.765793
Epoch 1.66: Loss = 0.819839
Epoch 1.67: Loss = 0.790482
Epoch 1.68: Loss = 0.804367
Epoch 1.69: Loss = 0.79483
Epoch 1.70: Loss = 0.826294
Epoch 1.71: Loss = 0.784103
Epoch 1.72: Loss = 0.685852
Epoch 1.73: Loss = 0.744415
Epoch 1.74: Loss = 0.717697
Epoch 1.75: Loss = 0.79776
Epoch 1.76: Loss = 0.743011
Epoch 1.77: Loss = 0.759735
Epoch 1.78: Loss = 0.735336
Epoch 1.79: Loss = 0.750763
Epoch 1.80: Loss = 0.685898
Epoch 1.81: Loss = 0.740021
Epoch 1.82: Loss = 0.732147
Epoch 1.83: Loss = 0.768051
Epoch 1.84: Loss = 0.748016
Epoch 1.85: Loss = 0.761108
Epoch 1.86: Loss = 0.623871
Epoch 1.87: Loss = 0.7276
Epoch 1.88: Loss = 0.590698
Epoch 1.89: Loss = 0.692535
Epoch 1.90: Loss = 0.689163
Epoch 1.91: Loss = 0.702072
Epoch 1.92: Loss = 0.727905
Epoch 1.93: Loss = 0.685013
Epoch 1.94: Loss = 0.70752
Epoch 1.95: Loss = 0.627655
Epoch 1.96: Loss = 0.619766
Epoch 1.97: Loss = 0.643631
Epoch 1.98: Loss = 0.581848
Epoch 1.99: Loss = 0.715363
Epoch 1.100: Loss = 0.632874
Epoch 1.101: Loss = 0.690033
Epoch 1.102: Loss = 0.570038
Epoch 1.103: Loss = 0.585876
Epoch 1.104: Loss = 0.631088
Epoch 1.105: Loss = 0.637619
Epoch 1.106: Loss = 0.652664
Epoch 1.107: Loss = 0.638901
Epoch 1.108: Loss = 0.617523
Epoch 1.109: Loss = 0.62735
Epoch 1.110: Loss = 0.681213
Epoch 1.111: Loss = 0.612915
Epoch 1.112: Loss = 0.624893
Epoch 1.113: Loss = 0.566147
Epoch 1.114: Loss = 0.65097
Epoch 1.115: Loss = 0.581543
Epoch 1.116: Loss = 0.51796
Epoch 1.117: Loss = 0.56958
Epoch 1.118: Loss = 0.574066
Epoch 1.119: Loss = 0.561401
Epoch 1.120: Loss = 0.597717
TRAIN LOSS = 1.11554
TRAIN ACC = 69.3832 % (41632/60000)
Loss = 0.584473
Loss = 0.616257
Loss = 0.71994
Loss = 0.668045
Loss = 0.720612
Loss = 0.611267
Loss = 0.578552
Loss = 0.75032
Loss = 0.708389
Loss = 0.644455
Loss = 0.323837
Loss = 0.465073
Loss = 0.378906
Loss = 0.544922
Loss = 0.421356
Loss = 0.421051
Loss = 0.382217
Loss = 0.202911
Loss = 0.394867
Loss = 0.676422
TEST LOSS = 0.540694
TEST ACC = 416.319 % (8370/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.601364
Epoch 2.2: Loss = 0.520645
Epoch 2.3: Loss = 0.596085
Epoch 2.4: Loss = 0.639175
Epoch 2.5: Loss = 0.567688
Epoch 2.6: Loss = 0.547699
Epoch 2.7: Loss = 0.566284
Epoch 2.8: Loss = 0.563675
Epoch 2.9: Loss = 0.480759
Epoch 2.10: Loss = 0.539642
Epoch 2.11: Loss = 0.551208
Epoch 2.12: Loss = 0.563171
Epoch 2.13: Loss = 0.618851
Epoch 2.14: Loss = 0.514603
Epoch 2.15: Loss = 0.589294
Epoch 2.16: Loss = 0.491684
Epoch 2.17: Loss = 0.541245
Epoch 2.18: Loss = 0.490631
Epoch 2.19: Loss = 0.496338
Epoch 2.20: Loss = 0.555069
Epoch 2.21: Loss = 0.464844
Epoch 2.22: Loss = 0.539856
Epoch 2.23: Loss = 0.595703
Epoch 2.24: Loss = 0.51651
Epoch 2.25: Loss = 0.500061
Epoch 2.26: Loss = 0.463196
Epoch 2.27: Loss = 0.465515
Epoch 2.28: Loss = 0.568604
Epoch 2.29: Loss = 0.504715
Epoch 2.30: Loss = 0.535751
Epoch 2.31: Loss = 0.489243
Epoch 2.32: Loss = 0.472031
Epoch 2.33: Loss = 0.458054
Epoch 2.34: Loss = 0.516769
Epoch 2.35: Loss = 0.525818
Epoch 2.36: Loss = 0.502441
Epoch 2.37: Loss = 0.505951
Epoch 2.38: Loss = 0.45285
Epoch 2.39: Loss = 0.519089
Epoch 2.40: Loss = 0.537704
Epoch 2.41: Loss = 0.519348
Epoch 2.42: Loss = 0.538345
Epoch 2.43: Loss = 0.461136
Epoch 2.44: Loss = 0.509979
Epoch 2.45: Loss = 0.565781
Epoch 2.46: Loss = 0.524216
Epoch 2.47: Loss = 0.536118
Epoch 2.48: Loss = 0.449585
Epoch 2.49: Loss = 0.440445
Epoch 2.50: Loss = 0.49176
Epoch 2.51: Loss = 0.461441
Epoch 2.52: Loss = 0.512833
Epoch 2.53: Loss = 0.54335
Epoch 2.54: Loss = 0.493378
Epoch 2.55: Loss = 0.519257
Epoch 2.56: Loss = 0.502609
Epoch 2.57: Loss = 0.459427
Epoch 2.58: Loss = 0.47673
Epoch 2.59: Loss = 0.432877
Epoch 2.60: Loss = 0.41774
Epoch 2.61: Loss = 0.500031
Epoch 2.62: Loss = 0.542908
Epoch 2.63: Loss = 0.516403
Epoch 2.64: Loss = 0.413086
Epoch 2.65: Loss = 0.453003
Epoch 2.66: Loss = 0.409546
Epoch 2.67: Loss = 0.511078
Epoch 2.68: Loss = 0.409073
Epoch 2.69: Loss = 0.582092
Epoch 2.70: Loss = 0.383423
Epoch 2.71: Loss = 0.552429
Epoch 2.72: Loss = 0.485397
Epoch 2.73: Loss = 0.409424
Epoch 2.74: Loss = 0.457092
Epoch 2.75: Loss = 0.609604
Epoch 2.76: Loss = 0.485397
Epoch 2.77: Loss = 0.484314
Epoch 2.78: Loss = 0.436783
Epoch 2.79: Loss = 0.561325
Epoch 2.80: Loss = 0.413528
Epoch 2.81: Loss = 0.440704
Epoch 2.82: Loss = 0.49791
Epoch 2.83: Loss = 0.448898
Epoch 2.84: Loss = 0.494492
Epoch 2.85: Loss = 0.410782
Epoch 2.86: Loss = 0.411743
Epoch 2.87: Loss = 0.49527
Epoch 2.88: Loss = 0.44455
Epoch 2.89: Loss = 0.561737
Epoch 2.90: Loss = 0.502731
Epoch 2.91: Loss = 0.614334
Epoch 2.92: Loss = 0.475296
Epoch 2.93: Loss = 0.456711
Epoch 2.94: Loss = 0.423889
Epoch 2.95: Loss = 0.491501
Epoch 2.96: Loss = 0.504242
Epoch 2.97: Loss = 0.486649
Epoch 2.98: Loss = 0.535629
Epoch 2.99: Loss = 0.434097
Epoch 2.100: Loss = 0.45607
Epoch 2.101: Loss = 0.425964
Epoch 2.102: Loss = 0.45433
Epoch 2.103: Loss = 0.51062
Epoch 2.104: Loss = 0.457687
Epoch 2.105: Loss = 0.487946
Epoch 2.106: Loss = 0.536346
Epoch 2.107: Loss = 0.543869
Epoch 2.108: Loss = 0.493225
Epoch 2.109: Loss = 0.425369
Epoch 2.110: Loss = 0.44574
Epoch 2.111: Loss = 0.421936
Epoch 2.112: Loss = 0.493271
Epoch 2.113: Loss = 0.515396
Epoch 2.114: Loss = 0.39743
Epoch 2.115: Loss = 0.521133
Epoch 2.116: Loss = 0.537323
Epoch 2.117: Loss = 0.493866
Epoch 2.118: Loss = 0.518143
Epoch 2.119: Loss = 0.431396
Epoch 2.120: Loss = 0.464828
TRAIN LOSS = 0.498383
TRAIN ACC = 84.6405 % (50787/60000)
Loss = 0.456375
Loss = 0.544525
Loss = 0.617035
Loss = 0.589111
Loss = 0.633713
Loss = 0.491364
Loss = 0.443771
Loss = 0.666473
Loss = 0.586472
Loss = 0.577301
Loss = 0.233536
Loss = 0.368103
Loss = 0.336487
Loss = 0.4254
Loss = 0.293121
Loss = 0.354492
Loss = 0.264343
Loss = 0.088913
Loss = 0.268845
Loss = 0.576294
TEST LOSS = 0.440784
TEST ACC = 507.869 % (8681/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.501083
Epoch 3.2: Loss = 0.502945
Epoch 3.3: Loss = 0.43161
Epoch 3.4: Loss = 0.457855
Epoch 3.5: Loss = 0.365204
Epoch 3.6: Loss = 0.473465
Epoch 3.7: Loss = 0.590881
Epoch 3.8: Loss = 0.522415
Epoch 3.9: Loss = 0.401428
Epoch 3.10: Loss = 0.567688
Epoch 3.11: Loss = 0.489105
Epoch 3.12: Loss = 0.414215
Epoch 3.13: Loss = 0.483582
Epoch 3.14: Loss = 0.434006
Epoch 3.15: Loss = 0.456528
Epoch 3.16: Loss = 0.468872
Epoch 3.17: Loss = 0.482071
Epoch 3.18: Loss = 0.471359
Epoch 3.19: Loss = 0.487549
Epoch 3.20: Loss = 0.441238
Epoch 3.21: Loss = 0.607666
Epoch 3.22: Loss = 0.394989
Epoch 3.23: Loss = 0.530212
Epoch 3.24: Loss = 0.417603
Epoch 3.25: Loss = 0.487381
Epoch 3.26: Loss = 0.396561
Epoch 3.27: Loss = 0.395569
Epoch 3.28: Loss = 0.437439
Epoch 3.29: Loss = 0.50322
Epoch 3.30: Loss = 0.395447
Epoch 3.31: Loss = 0.509003
Epoch 3.32: Loss = 0.410736
Epoch 3.33: Loss = 0.345428
Epoch 3.34: Loss = 0.472031
Epoch 3.35: Loss = 0.468842
Epoch 3.36: Loss = 0.439117
Epoch 3.37: Loss = 0.492554
Epoch 3.38: Loss = 0.380676
Epoch 3.39: Loss = 0.478973
Epoch 3.40: Loss = 0.436859
Epoch 3.41: Loss = 0.470901
Epoch 3.42: Loss = 0.506485
Epoch 3.43: Loss = 0.410019
Epoch 3.44: Loss = 0.479431
Epoch 3.45: Loss = 0.432236
Epoch 3.46: Loss = 0.446564
Epoch 3.47: Loss = 0.516495
Epoch 3.48: Loss = 0.373215
Epoch 3.49: Loss = 0.521866
Epoch 3.50: Loss = 0.527206
Epoch 3.51: Loss = 0.385223
Epoch 3.52: Loss = 0.48085
Epoch 3.53: Loss = 0.400818
Epoch 3.54: Loss = 0.539703
Epoch 3.55: Loss = 0.454758
Epoch 3.56: Loss = 0.420319
Epoch 3.57: Loss = 0.46907
Epoch 3.58: Loss = 0.532181
Epoch 3.59: Loss = 0.34787
Epoch 3.60: Loss = 0.490982
Epoch 3.61: Loss = 0.466125
Epoch 3.62: Loss = 0.401764
Epoch 3.63: Loss = 0.45752
Epoch 3.64: Loss = 0.414764
Epoch 3.65: Loss = 0.40065
Epoch 3.66: Loss = 0.430801
Epoch 3.67: Loss = 0.388428
Epoch 3.68: Loss = 0.496948
Epoch 3.69: Loss = 0.448685
Epoch 3.70: Loss = 0.475952
Epoch 3.71: Loss = 0.378891
Epoch 3.72: Loss = 0.37587
Epoch 3.73: Loss = 0.371094
Epoch 3.74: Loss = 0.412125
Epoch 3.75: Loss = 0.373718
Epoch 3.76: Loss = 0.300812
Epoch 3.77: Loss = 0.41806
Epoch 3.78: Loss = 0.337143
Epoch 3.79: Loss = 0.424622
Epoch 3.80: Loss = 0.427734
Epoch 3.81: Loss = 0.378174
Epoch 3.82: Loss = 0.509003
Epoch 3.83: Loss = 0.460037
Epoch 3.84: Loss = 0.547226
Epoch 3.85: Loss = 0.479706
Epoch 3.86: Loss = 0.390549
Epoch 3.87: Loss = 0.425583
Epoch 3.88: Loss = 0.505447
Epoch 3.89: Loss = 0.445847
Epoch 3.90: Loss = 0.510727
Epoch 3.91: Loss = 0.4664
Epoch 3.92: Loss = 0.415405
Epoch 3.93: Loss = 0.391525
Epoch 3.94: Loss = 0.484726
Epoch 3.95: Loss = 0.402893
Epoch 3.96: Loss = 0.442886
Epoch 3.97: Loss = 0.526855
Epoch 3.98: Loss = 0.480743
Epoch 3.99: Loss = 0.419678
Epoch 3.100: Loss = 0.374466
Epoch 3.101: Loss = 0.433197
Epoch 3.102: Loss = 0.421249
Epoch 3.103: Loss = 0.416702
Epoch 3.104: Loss = 0.383514
Epoch 3.105: Loss = 0.505173
Epoch 3.106: Loss = 0.513351
Epoch 3.107: Loss = 0.487885
Epoch 3.108: Loss = 0.396683
Epoch 3.109: Loss = 0.394104
Epoch 3.110: Loss = 0.464859
Epoch 3.111: Loss = 0.389771
Epoch 3.112: Loss = 0.401123
Epoch 3.113: Loss = 0.495255
Epoch 3.114: Loss = 0.43013
Epoch 3.115: Loss = 0.504349
Epoch 3.116: Loss = 0.496384
Epoch 3.117: Loss = 0.449631
Epoch 3.118: Loss = 0.334641
Epoch 3.119: Loss = 0.464401
Epoch 3.120: Loss = 0.421417
TRAIN LOSS = 0.447601
TRAIN ACC = 86.7523 % (52054/60000)
Loss = 0.408356
Loss = 0.518463
Loss = 0.56871
Loss = 0.574005
Loss = 0.582123
Loss = 0.464111
Loss = 0.39798
Loss = 0.669861
Loss = 0.553757
Loss = 0.517365
Loss = 0.205246
Loss = 0.307236
Loss = 0.323441
Loss = 0.388947
Loss = 0.246048
Loss = 0.299545
Loss = 0.200836
Loss = 0.0620728
Loss = 0.230392
Loss = 0.572739
TEST LOSS = 0.404562
TEST ACC = 520.54 % (8824/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.384521
Epoch 4.2: Loss = 0.418457
Epoch 4.3: Loss = 0.444336
Epoch 4.4: Loss = 0.342545
Epoch 4.5: Loss = 0.449875
Epoch 4.6: Loss = 0.409119
Epoch 4.7: Loss = 0.502747
Epoch 4.8: Loss = 0.476776
Epoch 4.9: Loss = 0.445251
Epoch 4.10: Loss = 0.389481
Epoch 4.11: Loss = 0.437958
Epoch 4.12: Loss = 0.373718
Epoch 4.13: Loss = 0.439209
Epoch 4.14: Loss = 0.543533
Epoch 4.15: Loss = 0.466309
Epoch 4.16: Loss = 0.415604
Epoch 4.17: Loss = 0.453476
Epoch 4.18: Loss = 0.463135
Epoch 4.19: Loss = 0.45224
Epoch 4.20: Loss = 0.380325
Epoch 4.21: Loss = 0.426788
Epoch 4.22: Loss = 0.308655
Epoch 4.23: Loss = 0.447845
Epoch 4.24: Loss = 0.491898
Epoch 4.25: Loss = 0.456467
Epoch 4.26: Loss = 0.500931
Epoch 4.27: Loss = 0.489594
Epoch 4.28: Loss = 0.424271
Epoch 4.29: Loss = 0.485535
Epoch 4.30: Loss = 0.417633
Epoch 4.31: Loss = 0.331818
Epoch 4.32: Loss = 0.348999
Epoch 4.33: Loss = 0.489243
Epoch 4.34: Loss = 0.425308
Epoch 4.35: Loss = 0.503906
Epoch 4.36: Loss = 0.495193
Epoch 4.37: Loss = 0.364319
Epoch 4.38: Loss = 0.380295
Epoch 4.39: Loss = 0.399872
Epoch 4.40: Loss = 0.426895
Epoch 4.41: Loss = 0.486923
Epoch 4.42: Loss = 0.409302
Epoch 4.43: Loss = 0.401962
Epoch 4.44: Loss = 0.479477
Epoch 4.45: Loss = 0.494095
Epoch 4.46: Loss = 0.442276
Epoch 4.47: Loss = 0.447937
Epoch 4.48: Loss = 0.404663
Epoch 4.49: Loss = 0.428177
Epoch 4.50: Loss = 0.445404
Epoch 4.51: Loss = 0.471451
Epoch 4.52: Loss = 0.488586
Epoch 4.53: Loss = 0.547318
Epoch 4.54: Loss = 0.335327
Epoch 4.55: Loss = 0.583588
Epoch 4.56: Loss = 0.404999
Epoch 4.57: Loss = 0.406265
Epoch 4.58: Loss = 0.434906
Epoch 4.59: Loss = 0.416367
Epoch 4.60: Loss = 0.431274
Epoch 4.61: Loss = 0.477737
Epoch 4.62: Loss = 0.380325
Epoch 4.63: Loss = 0.341263
Epoch 4.64: Loss = 0.434647
Epoch 4.65: Loss = 0.376266
Epoch 4.66: Loss = 0.461105
Epoch 4.67: Loss = 0.485779
Epoch 4.68: Loss = 0.556503
Epoch 4.69: Loss = 0.504852
Epoch 4.70: Loss = 0.520065
Epoch 4.71: Loss = 0.506195
Epoch 4.72: Loss = 0.366699
Epoch 4.73: Loss = 0.406189
Epoch 4.74: Loss = 0.404877
Epoch 4.75: Loss = 0.358871
Epoch 4.76: Loss = 0.513519
Epoch 4.77: Loss = 0.475601
Epoch 4.78: Loss = 0.429535
Epoch 4.79: Loss = 0.340012
Epoch 4.80: Loss = 0.407608
Epoch 4.81: Loss = 0.410278
Epoch 4.82: Loss = 0.4673
Epoch 4.83: Loss = 0.444885
Epoch 4.84: Loss = 0.340332
Epoch 4.85: Loss = 0.558136
Epoch 4.86: Loss = 0.427856
Epoch 4.87: Loss = 0.492615
Epoch 4.88: Loss = 0.510971
Epoch 4.89: Loss = 0.429916
Epoch 4.90: Loss = 0.451904
Epoch 4.91: Loss = 0.42189
Epoch 4.92: Loss = 0.381332
Epoch 4.93: Loss = 0.411819
Epoch 4.94: Loss = 0.438461
Epoch 4.95: Loss = 0.428146
Epoch 4.96: Loss = 0.344025
Epoch 4.97: Loss = 0.388779
Epoch 4.98: Loss = 0.371262
Epoch 4.99: Loss = 0.411926
Epoch 4.100: Loss = 0.400146
Epoch 4.101: Loss = 0.367874
Epoch 4.102: Loss = 0.501648
Epoch 4.103: Loss = 0.468384
Epoch 4.104: Loss = 0.445251
Epoch 4.105: Loss = 0.369003
Epoch 4.106: Loss = 0.409302
Epoch 4.107: Loss = 0.553177
Epoch 4.108: Loss = 0.397446
Epoch 4.109: Loss = 0.461914
Epoch 4.110: Loss = 0.368378
Epoch 4.111: Loss = 0.559998
Epoch 4.112: Loss = 0.42128
Epoch 4.113: Loss = 0.531387
Epoch 4.114: Loss = 0.497604
Epoch 4.115: Loss = 0.32576
Epoch 4.116: Loss = 0.455368
Epoch 4.117: Loss = 0.470093
Epoch 4.118: Loss = 0.447906
Epoch 4.119: Loss = 0.383606
Epoch 4.120: Loss = 0.429489
TRAIN LOSS = 0.436737
TRAIN ACC = 87.619 % (52574/60000)
Loss = 0.396866
Loss = 0.504791
Loss = 0.576935
Loss = 0.589615
Loss = 0.581192
Loss = 0.441589
Loss = 0.393845
Loss = 0.670792
Loss = 0.56694
Loss = 0.495682
Loss = 0.228912
Loss = 0.329834
Loss = 0.345657
Loss = 0.40596
Loss = 0.240341
Loss = 0.342377
Loss = 0.218231
Loss = 0.0511475
Loss = 0.223007
Loss = 0.591614
TEST LOSS = 0.409766
TEST ACC = 525.739 % (8858/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.449677
Epoch 5.2: Loss = 0.488052
Epoch 5.3: Loss = 0.418671
Epoch 5.4: Loss = 0.490234
Epoch 5.5: Loss = 0.42218
Epoch 5.6: Loss = 0.393448
Epoch 5.7: Loss = 0.451706
Epoch 5.8: Loss = 0.447388
Epoch 5.9: Loss = 0.491104
Epoch 5.10: Loss = 0.478836
Epoch 5.11: Loss = 0.590515
Epoch 5.12: Loss = 0.425034
Epoch 5.13: Loss = 0.450104
Epoch 5.14: Loss = 0.429626
Epoch 5.15: Loss = 0.496521
Epoch 5.16: Loss = 0.40361
Epoch 5.17: Loss = 0.397202
Epoch 5.18: Loss = 0.415787
Epoch 5.19: Loss = 0.529175
Epoch 5.20: Loss = 0.412842
Epoch 5.21: Loss = 0.463821
Epoch 5.22: Loss = 0.560837
Epoch 5.23: Loss = 0.386429
Epoch 5.24: Loss = 0.540588
Epoch 5.25: Loss = 0.425888
Epoch 5.26: Loss = 0.347366
Epoch 5.27: Loss = 0.513611
Epoch 5.28: Loss = 0.468292
Epoch 5.29: Loss = 0.492905
Epoch 5.30: Loss = 0.441055
Epoch 5.31: Loss = 0.395981
Epoch 5.32: Loss = 0.377197
Epoch 5.33: Loss = 0.319611
Epoch 5.34: Loss = 0.365021
Epoch 5.35: Loss = 0.346054
Epoch 5.36: Loss = 0.43158
Epoch 5.37: Loss = 0.405792
Epoch 5.38: Loss = 0.355164
Epoch 5.39: Loss = 0.401505
Epoch 5.40: Loss = 0.408279
Epoch 5.41: Loss = 0.370575
Epoch 5.42: Loss = 0.466156
Epoch 5.43: Loss = 0.468475
Epoch 5.44: Loss = 0.358444
Epoch 5.45: Loss = 0.355637
Epoch 5.46: Loss = 0.465927
Epoch 5.47: Loss = 0.539902
Epoch 5.48: Loss = 0.549088
Epoch 5.49: Loss = 0.445374
Epoch 5.50: Loss = 0.529739
Epoch 5.51: Loss = 0.440964
Epoch 5.52: Loss = 0.419647
Epoch 5.53: Loss = 0.382507
Epoch 5.54: Loss = 0.603333
Epoch 5.55: Loss = 0.427597
Epoch 5.56: Loss = 0.434052
Epoch 5.57: Loss = 0.365021
Epoch 5.58: Loss = 0.405975
Epoch 5.59: Loss = 0.431198
Epoch 5.60: Loss = 0.432892
Epoch 5.61: Loss = 0.423325
Epoch 5.62: Loss = 0.434311
Epoch 5.63: Loss = 0.377869
Epoch 5.64: Loss = 0.400742
Epoch 5.65: Loss = 0.502411
Epoch 5.66: Loss = 0.434433
Epoch 5.67: Loss = 0.446213
Epoch 5.68: Loss = 0.532333
Epoch 5.69: Loss = 0.455017
Epoch 5.70: Loss = 0.511017
Epoch 5.71: Loss = 0.313492
Epoch 5.72: Loss = 0.441879
Epoch 5.73: Loss = 0.493515
Epoch 5.74: Loss = 0.474838
Epoch 5.75: Loss = 0.5298
Epoch 5.76: Loss = 0.414291
Epoch 5.77: Loss = 0.506287
Epoch 5.78: Loss = 0.321014
Epoch 5.79: Loss = 0.381409
Epoch 5.80: Loss = 0.308136
Epoch 5.81: Loss = 0.5625
Epoch 5.82: Loss = 0.44371
Epoch 5.83: Loss = 0.458618
Epoch 5.84: Loss = 0.399094
Epoch 5.85: Loss = 0.399765
Epoch 5.86: Loss = 0.427643
Epoch 5.87: Loss = 0.440063
Epoch 5.88: Loss = 0.463882
Epoch 5.89: Loss = 0.437714
Epoch 5.90: Loss = 0.469864
Epoch 5.91: Loss = 0.418198
Epoch 5.92: Loss = 0.537933
Epoch 5.93: Loss = 0.358078
Epoch 5.94: Loss = 0.462082
Epoch 5.95: Loss = 0.532608
Epoch 5.96: Loss = 0.480194
Epoch 5.97: Loss = 0.400833
Epoch 5.98: Loss = 0.452332
Epoch 5.99: Loss = 0.491714
Epoch 5.100: Loss = 0.444885
Epoch 5.101: Loss = 0.532562
Epoch 5.102: Loss = 0.484085
Epoch 5.103: Loss = 0.422546
Epoch 5.104: Loss = 0.43573
Epoch 5.105: Loss = 0.496246
Epoch 5.106: Loss = 0.537292
Epoch 5.107: Loss = 0.56897
Epoch 5.108: Loss = 0.380722
Epoch 5.109: Loss = 0.473526
Epoch 5.110: Loss = 0.485229
Epoch 5.111: Loss = 0.496155
Epoch 5.112: Loss = 0.393143
Epoch 5.113: Loss = 0.428329
Epoch 5.114: Loss = 0.532394
Epoch 5.115: Loss = 0.439392
Epoch 5.116: Loss = 0.368881
Epoch 5.117: Loss = 0.415512
Epoch 5.118: Loss = 0.355042
Epoch 5.119: Loss = 0.463226
Epoch 5.120: Loss = 0.45578
TRAIN LOSS = 0.444748
TRAIN ACC = 87.8601 % (52719/60000)
Loss = 0.447021
Loss = 0.502823
Loss = 0.568192
Loss = 0.618774
Loss = 0.608414
Loss = 0.442947
Loss = 0.38205
Loss = 0.668274
Loss = 0.595795
Loss = 0.517334
Loss = 0.240677
Loss = 0.361801
Loss = 0.384338
Loss = 0.430222
Loss = 0.2668
Loss = 0.365158
Loss = 0.231354
Loss = 0.0457458
Loss = 0.227463
Loss = 0.68898
TEST LOSS = 0.429708
TEST ACC = 527.19 % (8871/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.419128
Epoch 6.2: Loss = 0.358261
Epoch 6.3: Loss = 0.363815
Epoch 6.4: Loss = 0.39238
Epoch 6.5: Loss = 0.49855
Epoch 6.6: Loss = 0.396606
Epoch 6.7: Loss = 0.370804
Epoch 6.8: Loss = 0.561981
Epoch 6.9: Loss = 0.42894
Epoch 6.10: Loss = 0.405991
Epoch 6.11: Loss = 0.356659
Epoch 6.12: Loss = 0.423019
Epoch 6.13: Loss = 0.443527
Epoch 6.14: Loss = 0.519012
Epoch 6.15: Loss = 0.415817
Epoch 6.16: Loss = 0.518265
Epoch 6.17: Loss = 0.542572
Epoch 6.18: Loss = 0.470123
Epoch 6.19: Loss = 0.532089
Epoch 6.20: Loss = 0.401031
Epoch 6.21: Loss = 0.3573
Epoch 6.22: Loss = 0.472427
Epoch 6.23: Loss = 0.403687
Epoch 6.24: Loss = 0.539993
Epoch 6.25: Loss = 0.469025
Epoch 6.26: Loss = 0.388672
Epoch 6.27: Loss = 0.475281
Epoch 6.28: Loss = 0.401733
Epoch 6.29: Loss = 0.400238
Epoch 6.30: Loss = 0.460114
Epoch 6.31: Loss = 0.450607
Epoch 6.32: Loss = 0.509735
Epoch 6.33: Loss = 0.49173
Epoch 6.34: Loss = 0.428345
Epoch 6.35: Loss = 0.380859
Epoch 6.36: Loss = 0.359894
Epoch 6.37: Loss = 0.495926
Epoch 6.38: Loss = 0.441864
Epoch 6.39: Loss = 0.418991
Epoch 6.40: Loss = 0.289871
Epoch 6.41: Loss = 0.468216
Epoch 6.42: Loss = 0.395279
Epoch 6.43: Loss = 0.422028
Epoch 6.44: Loss = 0.315842
Epoch 6.45: Loss = 0.52684
Epoch 6.46: Loss = 0.549149
Epoch 6.47: Loss = 0.463379
Epoch 6.48: Loss = 0.475723
Epoch 6.49: Loss = 0.590561
Epoch 6.50: Loss = 0.434448
Epoch 6.51: Loss = 0.464951
Epoch 6.52: Loss = 0.354843
Epoch 6.53: Loss = 0.623032
Epoch 6.54: Loss = 0.421234
Epoch 6.55: Loss = 0.428772
Epoch 6.56: Loss = 0.56778
Epoch 6.57: Loss = 0.379211
Epoch 6.58: Loss = 0.411224
Epoch 6.59: Loss = 0.52388
Epoch 6.60: Loss = 0.429428
Epoch 6.61: Loss = 0.489059
Epoch 6.62: Loss = 0.468491
Epoch 6.63: Loss = 0.422302
Epoch 6.64: Loss = 0.506058
Epoch 6.65: Loss = 0.443542
Epoch 6.66: Loss = 0.387115
Epoch 6.67: Loss = 0.406311
Epoch 6.68: Loss = 0.585526
Epoch 6.69: Loss = 0.420334
Epoch 6.70: Loss = 0.439957
Epoch 6.71: Loss = 0.416733
Epoch 6.72: Loss = 0.339371
Epoch 6.73: Loss = 0.45784
Epoch 6.74: Loss = 0.431351
Epoch 6.75: Loss = 0.466202
Epoch 6.76: Loss = 0.440842
Epoch 6.77: Loss = 0.616638
Epoch 6.78: Loss = 0.476044
Epoch 6.79: Loss = 0.409805
Epoch 6.80: Loss = 0.465393
Epoch 6.81: Loss = 0.519089
Epoch 6.82: Loss = 0.417221
Epoch 6.83: Loss = 0.500519
Epoch 6.84: Loss = 0.408066
Epoch 6.85: Loss = 0.49472
Epoch 6.86: Loss = 0.472794
Epoch 6.87: Loss = 0.511353
Epoch 6.88: Loss = 0.510529
Epoch 6.89: Loss = 0.534332
Epoch 6.90: Loss = 0.475204
Epoch 6.91: Loss = 0.422287
Epoch 6.92: Loss = 0.430649
Epoch 6.93: Loss = 0.446533
Epoch 6.94: Loss = 0.355072
Epoch 6.95: Loss = 0.528656
Epoch 6.96: Loss = 0.540512
Epoch 6.97: Loss = 0.476135
Epoch 6.98: Loss = 0.498734
Epoch 6.99: Loss = 0.55011
Epoch 6.100: Loss = 0.42598
Epoch 6.101: Loss = 0.439377
Epoch 6.102: Loss = 0.758957
Epoch 6.103: Loss = 0.452209
Epoch 6.104: Loss = 0.40947
Epoch 6.105: Loss = 0.422485
Epoch 6.106: Loss = 0.491272
Epoch 6.107: Loss = 0.402344
Epoch 6.108: Loss = 0.530838
Epoch 6.109: Loss = 0.426239
Epoch 6.110: Loss = 0.356796
Epoch 6.111: Loss = 0.422562
Epoch 6.112: Loss = 0.524933
Epoch 6.113: Loss = 0.356674
Epoch 6.114: Loss = 0.532196
Epoch 6.115: Loss = 0.428574
Epoch 6.116: Loss = 0.531464
Epoch 6.117: Loss = 0.44957
Epoch 6.118: Loss = 0.489182
Epoch 6.119: Loss = 0.441071
Epoch 6.120: Loss = 0.541153
TRAIN LOSS = 0.455963
TRAIN ACC = 88.0096 % (52808/60000)
Loss = 0.431076
Loss = 0.536041
Loss = 0.602432
Loss = 0.660736
Loss = 0.632385
Loss = 0.457764
Loss = 0.388306
Loss = 0.728745
Loss = 0.610992
Loss = 0.535767
Loss = 0.239456
Loss = 0.34668
Loss = 0.398483
Loss = 0.451981
Loss = 0.246078
Loss = 0.414383
Loss = 0.244614
Loss = 0.0487061
Loss = 0.263443
Loss = 0.607132
TEST LOSS = 0.44226
TEST ACC = 528.079 % (8883/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.422028
Epoch 7.2: Loss = 0.421555
Epoch 7.3: Loss = 0.65242
Epoch 7.4: Loss = 0.509598
Epoch 7.5: Loss = 0.35675
Epoch 7.6: Loss = 0.442566
Epoch 7.7: Loss = 0.376282
Epoch 7.8: Loss = 0.52301
Epoch 7.9: Loss = 0.445633
Epoch 7.10: Loss = 0.401123
Epoch 7.11: Loss = 0.358871
Epoch 7.12: Loss = 0.463242
Epoch 7.13: Loss = 0.449158
Epoch 7.14: Loss = 0.408264
Epoch 7.15: Loss = 0.574783
Epoch 7.16: Loss = 0.539169
Epoch 7.17: Loss = 0.504593
Epoch 7.18: Loss = 0.542679
Epoch 7.19: Loss = 0.535889
Epoch 7.20: Loss = 0.416565
Epoch 7.21: Loss = 0.489517
Epoch 7.22: Loss = 0.530518
Epoch 7.23: Loss = 0.458832
Epoch 7.24: Loss = 0.464371
Epoch 7.25: Loss = 0.377426
Epoch 7.26: Loss = 0.480682
Epoch 7.27: Loss = 0.486816
Epoch 7.28: Loss = 0.388992
Epoch 7.29: Loss = 0.445419
Epoch 7.30: Loss = 0.450562
Epoch 7.31: Loss = 0.491135
Epoch 7.32: Loss = 0.436981
Epoch 7.33: Loss = 0.472382
Epoch 7.34: Loss = 0.41243
Epoch 7.35: Loss = 0.490784
Epoch 7.36: Loss = 0.529266
Epoch 7.37: Loss = 0.529892
Epoch 7.38: Loss = 0.491806
Epoch 7.39: Loss = 0.459961
Epoch 7.40: Loss = 0.593536
Epoch 7.41: Loss = 0.566589
Epoch 7.42: Loss = 0.458664
Epoch 7.43: Loss = 0.358276
Epoch 7.44: Loss = 0.424545
Epoch 7.45: Loss = 0.377869
Epoch 7.46: Loss = 0.544357
Epoch 7.47: Loss = 0.606781
Epoch 7.48: Loss = 0.509689
Epoch 7.49: Loss = 0.469345
Epoch 7.50: Loss = 0.492661
Epoch 7.51: Loss = 0.430923
Epoch 7.52: Loss = 0.605133
Epoch 7.53: Loss = 0.472198
Epoch 7.54: Loss = 0.464325
Epoch 7.55: Loss = 0.401169
Epoch 7.56: Loss = 0.427353
Epoch 7.57: Loss = 0.443909
Epoch 7.58: Loss = 0.414276
Epoch 7.59: Loss = 0.61293
Epoch 7.60: Loss = 0.448807
Epoch 7.61: Loss = 0.508942
Epoch 7.62: Loss = 0.438461
Epoch 7.63: Loss = 0.352493
Epoch 7.64: Loss = 0.540787
Epoch 7.65: Loss = 0.558655
Epoch 7.66: Loss = 0.483932
Epoch 7.67: Loss = 0.571884
Epoch 7.68: Loss = 0.499954
Epoch 7.69: Loss = 0.562363
Epoch 7.70: Loss = 0.430283
Epoch 7.71: Loss = 0.332108
Epoch 7.72: Loss = 0.473404
Epoch 7.73: Loss = 0.450821
Epoch 7.74: Loss = 0.401321
Epoch 7.75: Loss = 0.454803
Epoch 7.76: Loss = 0.447311
Epoch 7.77: Loss = 0.560318
Epoch 7.78: Loss = 0.502228
Epoch 7.79: Loss = 0.569931
Epoch 7.80: Loss = 0.417145
Epoch 7.81: Loss = 0.439468
Epoch 7.82: Loss = 0.369034
Epoch 7.83: Loss = 0.477341
Epoch 7.84: Loss = 0.474915
Epoch 7.85: Loss = 0.431168
Epoch 7.86: Loss = 0.507843
Epoch 7.87: Loss = 0.449753
Epoch 7.88: Loss = 0.379074
Epoch 7.89: Loss = 0.532715
Epoch 7.90: Loss = 0.529373
Epoch 7.91: Loss = 0.4953
Epoch 7.92: Loss = 0.508652
Epoch 7.93: Loss = 0.626526
Epoch 7.94: Loss = 0.453644
Epoch 7.95: Loss = 0.427139
Epoch 7.96: Loss = 0.402466
Epoch 7.97: Loss = 0.361465
Epoch 7.98: Loss = 0.49736
Epoch 7.99: Loss = 0.414856
Epoch 7.100: Loss = 0.443832
Epoch 7.101: Loss = 0.518723
Epoch 7.102: Loss = 0.511749
Epoch 7.103: Loss = 0.42778
Epoch 7.104: Loss = 0.447861
Epoch 7.105: Loss = 0.541092
Epoch 7.106: Loss = 0.488815
Epoch 7.107: Loss = 0.491577
Epoch 7.108: Loss = 0.414413
Epoch 7.109: Loss = 0.397369
Epoch 7.110: Loss = 0.493378
Epoch 7.111: Loss = 0.433716
Epoch 7.112: Loss = 0.340561
Epoch 7.113: Loss = 0.484116
Epoch 7.114: Loss = 0.356354
Epoch 7.115: Loss = 0.390762
Epoch 7.116: Loss = 0.378891
Epoch 7.117: Loss = 0.503281
Epoch 7.118: Loss = 0.55777
Epoch 7.119: Loss = 0.358612
Epoch 7.120: Loss = 0.496933
TRAIN LOSS = 0.468063
TRAIN ACC = 88.0447 % (52829/60000)
Loss = 0.45163
Loss = 0.554489
Loss = 0.621063
Loss = 0.659332
Loss = 0.694565
Loss = 0.474838
Loss = 0.397736
Loss = 0.747101
Loss = 0.679459
Loss = 0.560028
Loss = 0.228394
Loss = 0.399826
Loss = 0.35762
Loss = 0.447632
Loss = 0.23407
Loss = 0.391449
Loss = 0.226425
Loss = 0.0464172
Loss = 0.240341
Loss = 0.690613
TEST LOSS = 0.455151
TEST ACC = 528.29 % (8872/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.500732
Epoch 8.2: Loss = 0.466187
Epoch 8.3: Loss = 0.606979
Epoch 8.4: Loss = 0.465424
Epoch 8.5: Loss = 0.389572
Epoch 8.6: Loss = 0.338959
Epoch 8.7: Loss = 0.53334
Epoch 8.8: Loss = 0.435638
Epoch 8.9: Loss = 0.495102
Epoch 8.10: Loss = 0.445206
Epoch 8.11: Loss = 0.606628
Epoch 8.12: Loss = 0.458847
Epoch 8.13: Loss = 0.636978
Epoch 8.14: Loss = 0.463425
Epoch 8.15: Loss = 0.494797
Epoch 8.16: Loss = 0.404327
Epoch 8.17: Loss = 0.42218
Epoch 8.18: Loss = 0.455963
Epoch 8.19: Loss = 0.436966
Epoch 8.20: Loss = 0.470154
Epoch 8.21: Loss = 0.415802
Epoch 8.22: Loss = 0.475861
Epoch 8.23: Loss = 0.473511
Epoch 8.24: Loss = 0.445663
Epoch 8.25: Loss = 0.480957
Epoch 8.26: Loss = 0.484222
Epoch 8.27: Loss = 0.463638
Epoch 8.28: Loss = 0.467133
Epoch 8.29: Loss = 0.413345
Epoch 8.30: Loss = 0.514664
Epoch 8.31: Loss = 0.530472
Epoch 8.32: Loss = 0.406815
Epoch 8.33: Loss = 0.322922
Epoch 8.34: Loss = 0.465973
Epoch 8.35: Loss = 0.437012
Epoch 8.36: Loss = 0.534958
Epoch 8.37: Loss = 0.404709
Epoch 8.38: Loss = 0.524521
Epoch 8.39: Loss = 0.556152
Epoch 8.40: Loss = 0.411194
Epoch 8.41: Loss = 0.453644
Epoch 8.42: Loss = 0.476013
Epoch 8.43: Loss = 0.431274
Epoch 8.44: Loss = 0.442703
Epoch 8.45: Loss = 0.40239
Epoch 8.46: Loss = 0.403748
Epoch 8.47: Loss = 0.324478
Epoch 8.48: Loss = 0.436661
Epoch 8.49: Loss = 0.44278
Epoch 8.50: Loss = 0.540497
Epoch 8.51: Loss = 0.488617
Epoch 8.52: Loss = 0.470596
Epoch 8.53: Loss = 0.422073
Epoch 8.54: Loss = 0.492752
Epoch 8.55: Loss = 0.440918
Epoch 8.56: Loss = 0.544983
Epoch 8.57: Loss = 0.519165
Epoch 8.58: Loss = 0.512329
Epoch 8.59: Loss = 0.47435
Epoch 8.60: Loss = 0.406235
Epoch 8.61: Loss = 0.449051
Epoch 8.62: Loss = 0.44278
Epoch 8.63: Loss = 0.422501
Epoch 8.64: Loss = 0.46199
Epoch 8.65: Loss = 0.499115
Epoch 8.66: Loss = 0.542892
Epoch 8.67: Loss = 0.495514
Epoch 8.68: Loss = 0.507629
Epoch 8.69: Loss = 0.529938
Epoch 8.70: Loss = 0.446854
Epoch 8.71: Loss = 0.476547
Epoch 8.72: Loss = 0.640564
Epoch 8.73: Loss = 0.469818
Epoch 8.74: Loss = 0.442596
Epoch 8.75: Loss = 0.625244
Epoch 8.76: Loss = 0.62059
Epoch 8.77: Loss = 0.522522
Epoch 8.78: Loss = 0.557877
Epoch 8.79: Loss = 0.531799
Epoch 8.80: Loss = 0.420883
Epoch 8.81: Loss = 0.543823
Epoch 8.82: Loss = 0.388489
Epoch 8.83: Loss = 0.48909
Epoch 8.84: Loss = 0.530609
Epoch 8.85: Loss = 0.526459
Epoch 8.86: Loss = 0.518845
Epoch 8.87: Loss = 0.490829
Epoch 8.88: Loss = 0.462952
Epoch 8.89: Loss = 0.400589
Epoch 8.90: Loss = 0.43663
Epoch 8.91: Loss = 0.526764
Epoch 8.92: Loss = 0.417191
Epoch 8.93: Loss = 0.347
Epoch 8.94: Loss = 0.540848
Epoch 8.95: Loss = 0.639908
Epoch 8.96: Loss = 0.468399
Epoch 8.97: Loss = 0.523209
Epoch 8.98: Loss = 0.339447
Epoch 8.99: Loss = 0.486053
Epoch 8.100: Loss = 0.479965
Epoch 8.101: Loss = 0.499664
Epoch 8.102: Loss = 0.567734
Epoch 8.103: Loss = 0.690994
Epoch 8.104: Loss = 0.435364
Epoch 8.105: Loss = 0.503052
Epoch 8.106: Loss = 0.546448
Epoch 8.107: Loss = 0.49823
Epoch 8.108: Loss = 0.425507
Epoch 8.109: Loss = 0.397278
Epoch 8.110: Loss = 0.5009
Epoch 8.111: Loss = 0.435883
Epoch 8.112: Loss = 0.531433
Epoch 8.113: Loss = 0.469696
Epoch 8.114: Loss = 0.51767
Epoch 8.115: Loss = 0.400406
Epoch 8.116: Loss = 0.453506
Epoch 8.117: Loss = 0.540298
Epoch 8.118: Loss = 0.670456
Epoch 8.119: Loss = 0.38298
Epoch 8.120: Loss = 0.570389
TRAIN LOSS = 0.479874
TRAIN ACC = 88.2339 % (52943/60000)
Loss = 0.493866
Loss = 0.552155
Loss = 0.611954
Loss = 0.665924
Loss = 0.736084
Loss = 0.512878
Loss = 0.401962
Loss = 0.814499
Loss = 0.718933
Loss = 0.597595
Loss = 0.219757
Loss = 0.417511
Loss = 0.385864
Loss = 0.466187
Loss = 0.222336
Loss = 0.41008
Loss = 0.248322
Loss = 0.0444336
Loss = 0.252762
Loss = 0.726013
TEST LOSS = 0.474956
TEST ACC = 529.43 % (8852/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.468384
Epoch 9.2: Loss = 0.560089
Epoch 9.3: Loss = 0.660797
Epoch 9.4: Loss = 0.485428
Epoch 9.5: Loss = 0.548798
Epoch 9.6: Loss = 0.466476
Epoch 9.7: Loss = 0.459641
Epoch 9.8: Loss = 0.538574
Epoch 9.9: Loss = 0.429199
Epoch 9.10: Loss = 0.500061
Epoch 9.11: Loss = 0.385452
Epoch 9.12: Loss = 0.476379
Epoch 9.13: Loss = 0.389847
Epoch 9.14: Loss = 0.453415
Epoch 9.15: Loss = 0.391724
Epoch 9.16: Loss = 0.477859
Epoch 9.17: Loss = 0.542557
Epoch 9.18: Loss = 0.500977
Epoch 9.19: Loss = 0.43515
Epoch 9.20: Loss = 0.471054
Epoch 9.21: Loss = 0.376984
Epoch 9.22: Loss = 0.476547
Epoch 9.23: Loss = 0.480362
Epoch 9.24: Loss = 0.519913
Epoch 9.25: Loss = 0.438431
Epoch 9.26: Loss = 0.496735
Epoch 9.27: Loss = 0.4711
Epoch 9.28: Loss = 0.475754
Epoch 9.29: Loss = 0.428482
Epoch 9.30: Loss = 0.515671
Epoch 9.31: Loss = 0.58432
Epoch 9.32: Loss = 0.378647
Epoch 9.33: Loss = 0.491531
Epoch 9.34: Loss = 0.425507
Epoch 9.35: Loss = 0.480637
Epoch 9.36: Loss = 0.346527
Epoch 9.37: Loss = 0.333237
Epoch 9.38: Loss = 0.496063
Epoch 9.39: Loss = 0.513901
Epoch 9.40: Loss = 0.384399
Epoch 9.41: Loss = 0.631805
Epoch 9.42: Loss = 0.49379
Epoch 9.43: Loss = 0.530197
Epoch 9.44: Loss = 0.377228
Epoch 9.45: Loss = 0.368103
Epoch 9.46: Loss = 0.536148
Epoch 9.47: Loss = 0.504883
Epoch 9.48: Loss = 0.52533
Epoch 9.49: Loss = 0.419662
Epoch 9.50: Loss = 0.622803
Epoch 9.51: Loss = 0.526932
Epoch 9.52: Loss = 0.498657
Epoch 9.53: Loss = 0.431183
Epoch 9.54: Loss = 0.456375
Epoch 9.55: Loss = 0.534698
Epoch 9.56: Loss = 0.430359
Epoch 9.57: Loss = 0.606232
Epoch 9.58: Loss = 0.478149
Epoch 9.59: Loss = 0.5401
Epoch 9.60: Loss = 0.669647
Epoch 9.61: Loss = 0.487595
Epoch 9.62: Loss = 0.479584
Epoch 9.63: Loss = 0.411804
Epoch 9.64: Loss = 0.510361
Epoch 9.65: Loss = 0.535934
Epoch 9.66: Loss = 0.497742
Epoch 9.67: Loss = 0.458923
Epoch 9.68: Loss = 0.491425
Epoch 9.69: Loss = 0.503204
Epoch 9.70: Loss = 0.567169
Epoch 9.71: Loss = 0.495224
Epoch 9.72: Loss = 0.49408
Epoch 9.73: Loss = 0.469162
Epoch 9.74: Loss = 0.54361
Epoch 9.75: Loss = 0.443756
Epoch 9.76: Loss = 0.648941
Epoch 9.77: Loss = 0.43689
Epoch 9.78: Loss = 0.579819
Epoch 9.79: Loss = 0.476593
Epoch 9.80: Loss = 0.5168
Epoch 9.81: Loss = 0.459015
Epoch 9.82: Loss = 0.493073
Epoch 9.83: Loss = 0.549759
Epoch 9.84: Loss = 0.516876
Epoch 9.85: Loss = 0.486252
Epoch 9.86: Loss = 0.501236
Epoch 9.87: Loss = 0.507584
Epoch 9.88: Loss = 0.514755
Epoch 9.89: Loss = 0.477753
Epoch 9.90: Loss = 0.555847
Epoch 9.91: Loss = 0.568665
Epoch 9.92: Loss = 0.59491
Epoch 9.93: Loss = 0.620377
Epoch 9.94: Loss = 0.611954
Epoch 9.95: Loss = 0.4151
Epoch 9.96: Loss = 0.534912
Epoch 9.97: Loss = 0.546707
Epoch 9.98: Loss = 0.429474
Epoch 9.99: Loss = 0.444717
Epoch 9.100: Loss = 0.399918
Epoch 9.101: Loss = 0.501511
Epoch 9.102: Loss = 0.506134
Epoch 9.103: Loss = 0.56813
Epoch 9.104: Loss = 0.461472
Epoch 9.105: Loss = 0.477417
Epoch 9.106: Loss = 0.540619
Epoch 9.107: Loss = 0.521408
Epoch 9.108: Loss = 0.411697
Epoch 9.109: Loss = 0.463425
Epoch 9.110: Loss = 0.507248
Epoch 9.111: Loss = 0.502258
Epoch 9.112: Loss = 0.507416
Epoch 9.113: Loss = 0.405899
Epoch 9.114: Loss = 0.510651
Epoch 9.115: Loss = 0.471207
Epoch 9.116: Loss = 0.504791
Epoch 9.117: Loss = 0.524384
Epoch 9.118: Loss = 0.641968
Epoch 9.119: Loss = 0.523407
Epoch 9.120: Loss = 0.552811
TRAIN LOSS = 0.494339
TRAIN ACC = 88.2339 % (52943/60000)
Loss = 0.482025
Loss = 0.565399
Loss = 0.678802
Loss = 0.723679
Loss = 0.769836
Loss = 0.506348
Loss = 0.407043
Loss = 0.799438
Loss = 0.674713
Loss = 0.592972
Loss = 0.223434
Loss = 0.426941
Loss = 0.410522
Loss = 0.453934
Loss = 0.220764
Loss = 0.489319
Loss = 0.23671
Loss = 0.0530701
Loss = 0.259201
Loss = 0.758072
TEST LOSS = 0.486611
TEST ACC = 529.43 % (8883/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.541306
Epoch 10.2: Loss = 0.507355
Epoch 10.3: Loss = 0.456802
Epoch 10.4: Loss = 0.454803
Epoch 10.5: Loss = 0.511642
Epoch 10.6: Loss = 0.540314
Epoch 10.7: Loss = 0.494934
Epoch 10.8: Loss = 0.5047
Epoch 10.9: Loss = 0.45166
Epoch 10.10: Loss = 0.498047
Epoch 10.11: Loss = 0.649094
Epoch 10.12: Loss = 0.5867
Epoch 10.13: Loss = 0.459274
Epoch 10.14: Loss = 0.462799
Epoch 10.15: Loss = 0.416077
Epoch 10.16: Loss = 0.463364
Epoch 10.17: Loss = 0.572128
Epoch 10.18: Loss = 0.481689
Epoch 10.19: Loss = 0.493423
Epoch 10.20: Loss = 0.593582
Epoch 10.21: Loss = 0.376358
Epoch 10.22: Loss = 0.462326
Epoch 10.23: Loss = 0.422287
Epoch 10.24: Loss = 0.597534
Epoch 10.25: Loss = 0.435699
Epoch 10.26: Loss = 0.468246
Epoch 10.27: Loss = 0.501251
Epoch 10.28: Loss = 0.54985
Epoch 10.29: Loss = 0.668976
Epoch 10.30: Loss = 0.397415
Epoch 10.31: Loss = 0.551743
Epoch 10.32: Loss = 0.477829
Epoch 10.33: Loss = 0.440842
Epoch 10.34: Loss = 0.470093
Epoch 10.35: Loss = 0.470093
Epoch 10.36: Loss = 0.631439
Epoch 10.37: Loss = 0.476898
Epoch 10.38: Loss = 0.565689
Epoch 10.39: Loss = 0.418427
Epoch 10.40: Loss = 0.419205
Epoch 10.41: Loss = 0.561249
Epoch 10.42: Loss = 0.371307
Epoch 10.43: Loss = 0.513687
Epoch 10.44: Loss = 0.474792
Epoch 10.45: Loss = 0.607422
Epoch 10.46: Loss = 0.494415
Epoch 10.47: Loss = 0.497192
Epoch 10.48: Loss = 0.589478
Epoch 10.49: Loss = 0.331146
Epoch 10.50: Loss = 0.528763
Epoch 10.51: Loss = 0.522781
Epoch 10.52: Loss = 0.487671
Epoch 10.53: Loss = 0.461655
Epoch 10.54: Loss = 0.557251
Epoch 10.55: Loss = 0.541626
Epoch 10.56: Loss = 0.535156
Epoch 10.57: Loss = 0.636993
Epoch 10.58: Loss = 0.521896
Epoch 10.59: Loss = 0.55899
Epoch 10.60: Loss = 0.542969
Epoch 10.61: Loss = 0.623779
Epoch 10.62: Loss = 0.407471
Epoch 10.63: Loss = 0.554138
Epoch 10.64: Loss = 0.508102
Epoch 10.65: Loss = 0.457993
Epoch 10.66: Loss = 0.586609
Epoch 10.67: Loss = 0.454742
Epoch 10.68: Loss = 0.604156
Epoch 10.69: Loss = 0.45163
Epoch 10.70: Loss = 0.449585
Epoch 10.71: Loss = 0.55426
Epoch 10.72: Loss = 0.513168
Epoch 10.73: Loss = 0.589203
Epoch 10.74: Loss = 0.44429
Epoch 10.75: Loss = 0.62915
Epoch 10.76: Loss = 0.413742
Epoch 10.77: Loss = 0.397202
Epoch 10.78: Loss = 0.599518
Epoch 10.79: Loss = 0.415375
Epoch 10.80: Loss = 0.369476
Epoch 10.81: Loss = 0.539948
Epoch 10.82: Loss = 0.411133
Epoch 10.83: Loss = 0.564056
Epoch 10.84: Loss = 0.446228
Epoch 10.85: Loss = 0.463013
Epoch 10.86: Loss = 0.498734
Epoch 10.87: Loss = 0.487854
Epoch 10.88: Loss = 0.516129
Epoch 10.89: Loss = 0.477448
Epoch 10.90: Loss = 0.456268
Epoch 10.91: Loss = 0.680954
Epoch 10.92: Loss = 0.498672
Epoch 10.93: Loss = 0.378586
Epoch 10.94: Loss = 0.453598
Epoch 10.95: Loss = 0.460602
Epoch 10.96: Loss = 0.472244
Epoch 10.97: Loss = 0.505905
Epoch 10.98: Loss = 0.659485
Epoch 10.99: Loss = 0.455124
Epoch 10.100: Loss = 0.590347
Epoch 10.101: Loss = 0.533112
Epoch 10.102: Loss = 0.707214
Epoch 10.103: Loss = 0.353241
Epoch 10.104: Loss = 0.571609
Epoch 10.105: Loss = 0.513046
Epoch 10.106: Loss = 0.523941
Epoch 10.107: Loss = 0.486313
Epoch 10.108: Loss = 0.429413
Epoch 10.109: Loss = 0.428009
Epoch 10.110: Loss = 0.508133
Epoch 10.111: Loss = 0.373657
Epoch 10.112: Loss = 0.525101
Epoch 10.113: Loss = 0.442902
Epoch 10.114: Loss = 0.450867
Epoch 10.115: Loss = 0.429199
Epoch 10.116: Loss = 0.513184
Epoch 10.117: Loss = 0.493301
Epoch 10.118: Loss = 0.554169
Epoch 10.119: Loss = 0.436325
Epoch 10.120: Loss = 0.479828
TRAIN LOSS = 0.50061
TRAIN ACC = 88.2996 % (52982/60000)
Loss = 0.459076
Loss = 0.59671
Loss = 0.656952
Loss = 0.709137
Loss = 0.751785
Loss = 0.502701
Loss = 0.430023
Loss = 0.795456
Loss = 0.716644
Loss = 0.579987
Loss = 0.219711
Loss = 0.394424
Loss = 0.3573
Loss = 0.448212
Loss = 0.246323
Loss = 0.486893
Loss = 0.24173
Loss = 0.0390472
Loss = 0.304718
Loss = 0.70845
TEST LOSS = 0.482264
TEST ACC = 529.819 % (8913/10000)

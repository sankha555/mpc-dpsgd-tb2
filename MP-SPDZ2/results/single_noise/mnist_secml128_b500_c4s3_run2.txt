Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.3246
Epoch 1.2: Loss = 2.29373
Epoch 1.3: Loss = 2.27263
Epoch 1.4: Loss = 2.24323
Epoch 1.5: Loss = 2.235
Epoch 1.6: Loss = 2.21648
Epoch 1.7: Loss = 2.20175
Epoch 1.8: Loss = 2.15163
Epoch 1.9: Loss = 2.1541
Epoch 1.10: Loss = 2.13074
Epoch 1.11: Loss = 2.10028
Epoch 1.12: Loss = 2.08725
Epoch 1.13: Loss = 2.07602
Epoch 1.14: Loss = 2.05447
Epoch 1.15: Loss = 2.01727
Epoch 1.16: Loss = 2.0042
Epoch 1.17: Loss = 1.98083
Epoch 1.18: Loss = 1.88512
Epoch 1.19: Loss = 1.87714
Epoch 1.20: Loss = 1.86519
Epoch 1.21: Loss = 1.86575
Epoch 1.22: Loss = 1.84746
Epoch 1.23: Loss = 1.7724
Epoch 1.24: Loss = 1.78265
Epoch 1.25: Loss = 1.77847
Epoch 1.26: Loss = 1.74594
Epoch 1.27: Loss = 1.69637
Epoch 1.28: Loss = 1.69383
Epoch 1.29: Loss = 1.61722
Epoch 1.30: Loss = 1.65567
Epoch 1.31: Loss = 1.55092
Epoch 1.32: Loss = 1.5589
Epoch 1.33: Loss = 1.56654
Epoch 1.34: Loss = 1.53554
Epoch 1.35: Loss = 1.46208
Epoch 1.36: Loss = 1.50453
Epoch 1.37: Loss = 1.44269
Epoch 1.38: Loss = 1.37608
Epoch 1.39: Loss = 1.33569
Epoch 1.40: Loss = 1.38771
Epoch 1.41: Loss = 1.36563
Epoch 1.42: Loss = 1.36307
Epoch 1.43: Loss = 1.30823
Epoch 1.44: Loss = 1.26047
Epoch 1.45: Loss = 1.20972
Epoch 1.46: Loss = 1.24956
Epoch 1.47: Loss = 1.20122
Epoch 1.48: Loss = 1.23264
Epoch 1.49: Loss = 1.14624
Epoch 1.50: Loss = 1.0674
Epoch 1.51: Loss = 1.12625
Epoch 1.52: Loss = 1.13374
Epoch 1.53: Loss = 1.10907
Epoch 1.54: Loss = 1.12169
Epoch 1.55: Loss = 1.05083
Epoch 1.56: Loss = 1.04321
Epoch 1.57: Loss = 1.04869
Epoch 1.58: Loss = 1.01675
Epoch 1.59: Loss = 1.00574
Epoch 1.60: Loss = 1.02379
Epoch 1.61: Loss = 0.958359
Epoch 1.62: Loss = 0.977936
Epoch 1.63: Loss = 0.964981
Epoch 1.64: Loss = 0.975586
Epoch 1.65: Loss = 0.918808
Epoch 1.66: Loss = 0.889664
Epoch 1.67: Loss = 0.92215
Epoch 1.68: Loss = 0.86734
Epoch 1.69: Loss = 0.926682
Epoch 1.70: Loss = 0.873505
Epoch 1.71: Loss = 0.857819
Epoch 1.72: Loss = 0.87146
Epoch 1.73: Loss = 0.817734
Epoch 1.74: Loss = 0.846756
Epoch 1.75: Loss = 0.843155
Epoch 1.76: Loss = 0.741913
Epoch 1.77: Loss = 0.836563
Epoch 1.78: Loss = 0.842224
Epoch 1.79: Loss = 0.91423
Epoch 1.80: Loss = 0.828217
Epoch 1.81: Loss = 0.785965
Epoch 1.82: Loss = 0.844818
Epoch 1.83: Loss = 0.770981
Epoch 1.84: Loss = 0.792831
Epoch 1.85: Loss = 0.733627
Epoch 1.86: Loss = 0.811172
Epoch 1.87: Loss = 0.746445
Epoch 1.88: Loss = 0.728409
Epoch 1.89: Loss = 0.723541
Epoch 1.90: Loss = 0.669678
Epoch 1.91: Loss = 0.684082
Epoch 1.92: Loss = 0.732193
Epoch 1.93: Loss = 0.714188
Epoch 1.94: Loss = 0.75621
Epoch 1.95: Loss = 0.730515
Epoch 1.96: Loss = 0.747833
Epoch 1.97: Loss = 0.750336
Epoch 1.98: Loss = 0.688675
Epoch 1.99: Loss = 0.691483
Epoch 1.100: Loss = 0.695877
Epoch 1.101: Loss = 0.613495
Epoch 1.102: Loss = 0.631424
Epoch 1.103: Loss = 0.616806
Epoch 1.104: Loss = 0.686066
Epoch 1.105: Loss = 0.643539
Epoch 1.106: Loss = 0.583069
Epoch 1.107: Loss = 0.635986
Epoch 1.108: Loss = 0.628662
Epoch 1.109: Loss = 0.676254
Epoch 1.110: Loss = 0.629471
Epoch 1.111: Loss = 0.641159
Epoch 1.112: Loss = 0.606796
Epoch 1.113: Loss = 0.595352
Epoch 1.114: Loss = 0.626389
Epoch 1.115: Loss = 0.691925
Epoch 1.116: Loss = 0.566177
Epoch 1.117: Loss = 0.519623
Epoch 1.118: Loss = 0.620529
Epoch 1.119: Loss = 0.571869
Epoch 1.120: Loss = 0.619049
TRAIN LOSS = 1.18593
TRAIN ACC = 67.3843 % (40433/60000)
Loss = 0.636536
Loss = 0.638092
Loss = 0.759827
Loss = 0.745605
Loss = 0.74118
Loss = 0.659729
Loss = 0.616394
Loss = 0.78952
Loss = 0.759079
Loss = 0.684494
Loss = 0.3535
Loss = 0.520721
Loss = 0.395172
Loss = 0.584106
Loss = 0.461761
Loss = 0.461731
Loss = 0.467239
Loss = 0.262207
Loss = 0.451843
Loss = 0.705963
TEST LOSS = 0.584735
TEST ACC = 404.329 % (8248/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.585144
Epoch 2.2: Loss = 0.583755
Epoch 2.3: Loss = 0.634583
Epoch 2.4: Loss = 0.5811
Epoch 2.5: Loss = 0.554916
Epoch 2.6: Loss = 0.507233
Epoch 2.7: Loss = 0.605042
Epoch 2.8: Loss = 0.619156
Epoch 2.9: Loss = 0.517319
Epoch 2.10: Loss = 0.595947
Epoch 2.11: Loss = 0.660965
Epoch 2.12: Loss = 0.591187
Epoch 2.13: Loss = 0.641205
Epoch 2.14: Loss = 0.570038
Epoch 2.15: Loss = 0.56929
Epoch 2.16: Loss = 0.52887
Epoch 2.17: Loss = 0.491638
Epoch 2.18: Loss = 0.541611
Epoch 2.19: Loss = 0.561264
Epoch 2.20: Loss = 0.568848
Epoch 2.21: Loss = 0.585129
Epoch 2.22: Loss = 0.590408
Epoch 2.23: Loss = 0.543594
Epoch 2.24: Loss = 0.545868
Epoch 2.25: Loss = 0.537918
Epoch 2.26: Loss = 0.586227
Epoch 2.27: Loss = 0.508041
Epoch 2.28: Loss = 0.572144
Epoch 2.29: Loss = 0.568146
Epoch 2.30: Loss = 0.516037
Epoch 2.31: Loss = 0.639648
Epoch 2.32: Loss = 0.478561
Epoch 2.33: Loss = 0.575394
Epoch 2.34: Loss = 0.472427
Epoch 2.35: Loss = 0.573837
Epoch 2.36: Loss = 0.51506
Epoch 2.37: Loss = 0.560669
Epoch 2.38: Loss = 0.543808
Epoch 2.39: Loss = 0.429398
Epoch 2.40: Loss = 0.466965
Epoch 2.41: Loss = 0.537079
Epoch 2.42: Loss = 0.631577
Epoch 2.43: Loss = 0.565018
Epoch 2.44: Loss = 0.541016
Epoch 2.45: Loss = 0.544006
Epoch 2.46: Loss = 0.502563
Epoch 2.47: Loss = 0.529175
Epoch 2.48: Loss = 0.484467
Epoch 2.49: Loss = 0.566437
Epoch 2.50: Loss = 0.49617
Epoch 2.51: Loss = 0.474121
Epoch 2.52: Loss = 0.558746
Epoch 2.53: Loss = 0.459015
Epoch 2.54: Loss = 0.530426
Epoch 2.55: Loss = 0.519821
Epoch 2.56: Loss = 0.476929
Epoch 2.57: Loss = 0.473892
Epoch 2.58: Loss = 0.459641
Epoch 2.59: Loss = 0.511597
Epoch 2.60: Loss = 0.453094
Epoch 2.61: Loss = 0.500076
Epoch 2.62: Loss = 0.418777
Epoch 2.63: Loss = 0.472656
Epoch 2.64: Loss = 0.417114
Epoch 2.65: Loss = 0.434616
Epoch 2.66: Loss = 0.438828
Epoch 2.67: Loss = 0.461578
Epoch 2.68: Loss = 0.468094
Epoch 2.69: Loss = 0.487534
Epoch 2.70: Loss = 0.480042
Epoch 2.71: Loss = 0.510223
Epoch 2.72: Loss = 0.453537
Epoch 2.73: Loss = 0.43956
Epoch 2.74: Loss = 0.423965
Epoch 2.75: Loss = 0.52182
Epoch 2.76: Loss = 0.469589
Epoch 2.77: Loss = 0.445129
Epoch 2.78: Loss = 0.510498
Epoch 2.79: Loss = 0.420029
Epoch 2.80: Loss = 0.456299
Epoch 2.81: Loss = 0.471985
Epoch 2.82: Loss = 0.464325
Epoch 2.83: Loss = 0.467087
Epoch 2.84: Loss = 0.381714
Epoch 2.85: Loss = 0.477036
Epoch 2.86: Loss = 0.514069
Epoch 2.87: Loss = 0.495987
Epoch 2.88: Loss = 0.450134
Epoch 2.89: Loss = 0.412842
Epoch 2.90: Loss = 0.42392
Epoch 2.91: Loss = 0.439163
Epoch 2.92: Loss = 0.481308
Epoch 2.93: Loss = 0.530518
Epoch 2.94: Loss = 0.477951
Epoch 2.95: Loss = 0.469528
Epoch 2.96: Loss = 0.486069
Epoch 2.97: Loss = 0.444839
Epoch 2.98: Loss = 0.425064
Epoch 2.99: Loss = 0.47818
Epoch 2.100: Loss = 0.45401
Epoch 2.101: Loss = 0.422379
Epoch 2.102: Loss = 0.531036
Epoch 2.103: Loss = 0.483322
Epoch 2.104: Loss = 0.394226
Epoch 2.105: Loss = 0.416214
Epoch 2.106: Loss = 0.539963
Epoch 2.107: Loss = 0.43927
Epoch 2.108: Loss = 0.484772
Epoch 2.109: Loss = 0.513596
Epoch 2.110: Loss = 0.45755
Epoch 2.111: Loss = 0.508408
Epoch 2.112: Loss = 0.418564
Epoch 2.113: Loss = 0.494965
Epoch 2.114: Loss = 0.434113
Epoch 2.115: Loss = 0.47998
Epoch 2.116: Loss = 0.439621
Epoch 2.117: Loss = 0.456543
Epoch 2.118: Loss = 0.478027
Epoch 2.119: Loss = 0.523392
Epoch 2.120: Loss = 0.403717
TRAIN LOSS = 0.503586
TRAIN ACC = 84.6008 % (50763/60000)
Loss = 0.443893
Loss = 0.499481
Loss = 0.571457
Loss = 0.580139
Loss = 0.594193
Loss = 0.476837
Loss = 0.432404
Loss = 0.659683
Loss = 0.586166
Loss = 0.534866
Loss = 0.242126
Loss = 0.349213
Loss = 0.304596
Loss = 0.422287
Loss = 0.27533
Loss = 0.328857
Loss = 0.301605
Loss = 0.116547
Loss = 0.295273
Loss = 0.616348
TEST LOSS = 0.431565
TEST ACC = 507.629 % (8715/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.424591
Epoch 3.2: Loss = 0.462845
Epoch 3.3: Loss = 0.42421
Epoch 3.4: Loss = 0.454895
Epoch 3.5: Loss = 0.416245
Epoch 3.6: Loss = 0.412674
Epoch 3.7: Loss = 0.494492
Epoch 3.8: Loss = 0.429413
Epoch 3.9: Loss = 0.414383
Epoch 3.10: Loss = 0.436737
Epoch 3.11: Loss = 0.392715
Epoch 3.12: Loss = 0.417099
Epoch 3.13: Loss = 0.449219
Epoch 3.14: Loss = 0.396011
Epoch 3.15: Loss = 0.452499
Epoch 3.16: Loss = 0.470474
Epoch 3.17: Loss = 0.392624
Epoch 3.18: Loss = 0.531525
Epoch 3.19: Loss = 0.445328
Epoch 3.20: Loss = 0.447647
Epoch 3.21: Loss = 0.559296
Epoch 3.22: Loss = 0.440002
Epoch 3.23: Loss = 0.415146
Epoch 3.24: Loss = 0.449051
Epoch 3.25: Loss = 0.429611
Epoch 3.26: Loss = 0.399689
Epoch 3.27: Loss = 0.407944
Epoch 3.28: Loss = 0.444229
Epoch 3.29: Loss = 0.429291
Epoch 3.30: Loss = 0.445419
Epoch 3.31: Loss = 0.45784
Epoch 3.32: Loss = 0.474457
Epoch 3.33: Loss = 0.435791
Epoch 3.34: Loss = 0.434631
Epoch 3.35: Loss = 0.516037
Epoch 3.36: Loss = 0.465805
Epoch 3.37: Loss = 0.38826
Epoch 3.38: Loss = 0.451553
Epoch 3.39: Loss = 0.344269
Epoch 3.40: Loss = 0.464523
Epoch 3.41: Loss = 0.407883
Epoch 3.42: Loss = 0.406937
Epoch 3.43: Loss = 0.427734
Epoch 3.44: Loss = 0.470551
Epoch 3.45: Loss = 0.460724
Epoch 3.46: Loss = 0.394669
Epoch 3.47: Loss = 0.475876
Epoch 3.48: Loss = 0.451675
Epoch 3.49: Loss = 0.530777
Epoch 3.50: Loss = 0.478317
Epoch 3.51: Loss = 0.502808
Epoch 3.52: Loss = 0.477005
Epoch 3.53: Loss = 0.416504
Epoch 3.54: Loss = 0.478912
Epoch 3.55: Loss = 0.409561
Epoch 3.56: Loss = 0.45726
Epoch 3.57: Loss = 0.548569
Epoch 3.58: Loss = 0.432663
Epoch 3.59: Loss = 0.554306
Epoch 3.60: Loss = 0.484039
Epoch 3.61: Loss = 0.429764
Epoch 3.62: Loss = 0.367401
Epoch 3.63: Loss = 0.401535
Epoch 3.64: Loss = 0.40152
Epoch 3.65: Loss = 0.367538
Epoch 3.66: Loss = 0.486954
Epoch 3.67: Loss = 0.427856
Epoch 3.68: Loss = 0.415131
Epoch 3.69: Loss = 0.527435
Epoch 3.70: Loss = 0.4646
Epoch 3.71: Loss = 0.571487
Epoch 3.72: Loss = 0.446426
Epoch 3.73: Loss = 0.369751
Epoch 3.74: Loss = 0.384415
Epoch 3.75: Loss = 0.378708
Epoch 3.76: Loss = 0.349396
Epoch 3.77: Loss = 0.387192
Epoch 3.78: Loss = 0.482834
Epoch 3.79: Loss = 0.501602
Epoch 3.80: Loss = 0.428986
Epoch 3.81: Loss = 0.398636
Epoch 3.82: Loss = 0.332642
Epoch 3.83: Loss = 0.379044
Epoch 3.84: Loss = 0.467896
Epoch 3.85: Loss = 0.410126
Epoch 3.86: Loss = 0.38974
Epoch 3.87: Loss = 0.466812
Epoch 3.88: Loss = 0.400772
Epoch 3.89: Loss = 0.415482
Epoch 3.90: Loss = 0.3853
Epoch 3.91: Loss = 0.390015
Epoch 3.92: Loss = 0.439102
Epoch 3.93: Loss = 0.565811
Epoch 3.94: Loss = 0.3797
Epoch 3.95: Loss = 0.545563
Epoch 3.96: Loss = 0.429779
Epoch 3.97: Loss = 0.406906
Epoch 3.98: Loss = 0.44162
Epoch 3.99: Loss = 0.408585
Epoch 3.100: Loss = 0.445404
Epoch 3.101: Loss = 0.455811
Epoch 3.102: Loss = 0.468979
Epoch 3.103: Loss = 0.474655
Epoch 3.104: Loss = 0.337448
Epoch 3.105: Loss = 0.417664
Epoch 3.106: Loss = 0.343414
Epoch 3.107: Loss = 0.370468
Epoch 3.108: Loss = 0.463562
Epoch 3.109: Loss = 0.391373
Epoch 3.110: Loss = 0.453445
Epoch 3.111: Loss = 0.406631
Epoch 3.112: Loss = 0.444672
Epoch 3.113: Loss = 0.449066
Epoch 3.114: Loss = 0.36528
Epoch 3.115: Loss = 0.391006
Epoch 3.116: Loss = 0.478912
Epoch 3.117: Loss = 0.417633
Epoch 3.118: Loss = 0.486969
Epoch 3.119: Loss = 0.356812
Epoch 3.120: Loss = 0.351547
TRAIN LOSS = 0.435837
TRAIN ACC = 86.9156 % (52152/60000)
Loss = 0.405579
Loss = 0.481613
Loss = 0.547607
Loss = 0.554382
Loss = 0.586227
Loss = 0.445404
Loss = 0.404633
Loss = 0.654449
Loss = 0.560944
Loss = 0.511627
Loss = 0.20433
Loss = 0.308731
Loss = 0.292068
Loss = 0.370224
Loss = 0.21019
Loss = 0.288727
Loss = 0.239197
Loss = 0.0775299
Loss = 0.248993
Loss = 0.557205
TEST LOSS = 0.397483
TEST ACC = 521.519 % (8873/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.500397
Epoch 4.2: Loss = 0.495682
Epoch 4.3: Loss = 0.365189
Epoch 4.4: Loss = 0.37175
Epoch 4.5: Loss = 0.346848
Epoch 4.6: Loss = 0.421707
Epoch 4.7: Loss = 0.364655
Epoch 4.8: Loss = 0.355896
Epoch 4.9: Loss = 0.431641
Epoch 4.10: Loss = 0.429108
Epoch 4.11: Loss = 0.342133
Epoch 4.12: Loss = 0.482422
Epoch 4.13: Loss = 0.439224
Epoch 4.14: Loss = 0.367416
Epoch 4.15: Loss = 0.458984
Epoch 4.16: Loss = 0.433456
Epoch 4.17: Loss = 0.397232
Epoch 4.18: Loss = 0.397034
Epoch 4.19: Loss = 0.304596
Epoch 4.20: Loss = 0.490738
Epoch 4.21: Loss = 0.469788
Epoch 4.22: Loss = 0.474319
Epoch 4.23: Loss = 0.457092
Epoch 4.24: Loss = 0.381531
Epoch 4.25: Loss = 0.411072
Epoch 4.26: Loss = 0.445847
Epoch 4.27: Loss = 0.46373
Epoch 4.28: Loss = 0.440582
Epoch 4.29: Loss = 0.386612
Epoch 4.30: Loss = 0.384216
Epoch 4.31: Loss = 0.421265
Epoch 4.32: Loss = 0.395889
Epoch 4.33: Loss = 0.482651
Epoch 4.34: Loss = 0.438339
Epoch 4.35: Loss = 0.433929
Epoch 4.36: Loss = 0.364059
Epoch 4.37: Loss = 0.439087
Epoch 4.38: Loss = 0.511673
Epoch 4.39: Loss = 0.399796
Epoch 4.40: Loss = 0.345871
Epoch 4.41: Loss = 0.430145
Epoch 4.42: Loss = 0.428955
Epoch 4.43: Loss = 0.388947
Epoch 4.44: Loss = 0.400101
Epoch 4.45: Loss = 0.472504
Epoch 4.46: Loss = 0.400909
Epoch 4.47: Loss = 0.341293
Epoch 4.48: Loss = 0.478058
Epoch 4.49: Loss = 0.373779
Epoch 4.50: Loss = 0.297455
Epoch 4.51: Loss = 0.360352
Epoch 4.52: Loss = 0.502075
Epoch 4.53: Loss = 0.416885
Epoch 4.54: Loss = 0.401947
Epoch 4.55: Loss = 0.441391
Epoch 4.56: Loss = 0.444046
Epoch 4.57: Loss = 0.454788
Epoch 4.58: Loss = 0.356079
Epoch 4.59: Loss = 0.387985
Epoch 4.60: Loss = 0.349899
Epoch 4.61: Loss = 0.410629
Epoch 4.62: Loss = 0.408081
Epoch 4.63: Loss = 0.420471
Epoch 4.64: Loss = 0.393051
Epoch 4.65: Loss = 0.303802
Epoch 4.66: Loss = 0.410461
Epoch 4.67: Loss = 0.428604
Epoch 4.68: Loss = 0.414398
Epoch 4.69: Loss = 0.430466
Epoch 4.70: Loss = 0.512695
Epoch 4.71: Loss = 0.536728
Epoch 4.72: Loss = 0.339554
Epoch 4.73: Loss = 0.360962
Epoch 4.74: Loss = 0.400513
Epoch 4.75: Loss = 0.435272
Epoch 4.76: Loss = 0.355042
Epoch 4.77: Loss = 0.447388
Epoch 4.78: Loss = 0.421478
Epoch 4.79: Loss = 0.372955
Epoch 4.80: Loss = 0.468903
Epoch 4.81: Loss = 0.382675
Epoch 4.82: Loss = 0.339462
Epoch 4.83: Loss = 0.367554
Epoch 4.84: Loss = 0.380722
Epoch 4.85: Loss = 0.378006
Epoch 4.86: Loss = 0.538559
Epoch 4.87: Loss = 0.47168
Epoch 4.88: Loss = 0.438538
Epoch 4.89: Loss = 0.400345
Epoch 4.90: Loss = 0.386124
Epoch 4.91: Loss = 0.357834
Epoch 4.92: Loss = 0.318588
Epoch 4.93: Loss = 0.381943
Epoch 4.94: Loss = 0.420883
Epoch 4.95: Loss = 0.417801
Epoch 4.96: Loss = 0.375519
Epoch 4.97: Loss = 0.475189
Epoch 4.98: Loss = 0.458069
Epoch 4.99: Loss = 0.314865
Epoch 4.100: Loss = 0.492203
Epoch 4.101: Loss = 0.401672
Epoch 4.102: Loss = 0.489563
Epoch 4.103: Loss = 0.337372
Epoch 4.104: Loss = 0.43869
Epoch 4.105: Loss = 0.381821
Epoch 4.106: Loss = 0.475662
Epoch 4.107: Loss = 0.310837
Epoch 4.108: Loss = 0.381226
Epoch 4.109: Loss = 0.421967
Epoch 4.110: Loss = 0.444778
Epoch 4.111: Loss = 0.429672
Epoch 4.112: Loss = 0.396332
Epoch 4.113: Loss = 0.416443
Epoch 4.114: Loss = 0.49826
Epoch 4.115: Loss = 0.423248
Epoch 4.116: Loss = 0.389465
Epoch 4.117: Loss = 0.412994
Epoch 4.118: Loss = 0.37384
Epoch 4.119: Loss = 0.424973
Epoch 4.120: Loss = 0.493317
TRAIN LOSS = 0.413177
TRAIN ACC = 88.1073 % (52867/60000)
Loss = 0.397171
Loss = 0.465622
Loss = 0.517487
Loss = 0.556366
Loss = 0.592682
Loss = 0.42244
Loss = 0.367294
Loss = 0.652817
Loss = 0.569397
Loss = 0.496475
Loss = 0.189804
Loss = 0.268677
Loss = 0.312775
Loss = 0.372299
Loss = 0.188828
Loss = 0.286987
Loss = 0.226028
Loss = 0.0627594
Loss = 0.21344
Loss = 0.548767
TEST LOSS = 0.385406
TEST ACC = 528.67 % (8938/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.449463
Epoch 5.2: Loss = 0.404587
Epoch 5.3: Loss = 0.356277
Epoch 5.4: Loss = 0.369995
Epoch 5.5: Loss = 0.428925
Epoch 5.6: Loss = 0.438812
Epoch 5.7: Loss = 0.453445
Epoch 5.8: Loss = 0.398651
Epoch 5.9: Loss = 0.452026
Epoch 5.10: Loss = 0.360245
Epoch 5.11: Loss = 0.531448
Epoch 5.12: Loss = 0.365189
Epoch 5.13: Loss = 0.397842
Epoch 5.14: Loss = 0.421249
Epoch 5.15: Loss = 0.413757
Epoch 5.16: Loss = 0.358078
Epoch 5.17: Loss = 0.344269
Epoch 5.18: Loss = 0.446579
Epoch 5.19: Loss = 0.393463
Epoch 5.20: Loss = 0.504227
Epoch 5.21: Loss = 0.401642
Epoch 5.22: Loss = 0.371216
Epoch 5.23: Loss = 0.310059
Epoch 5.24: Loss = 0.353394
Epoch 5.25: Loss = 0.355667
Epoch 5.26: Loss = 0.430954
Epoch 5.27: Loss = 0.42572
Epoch 5.28: Loss = 0.343536
Epoch 5.29: Loss = 0.309097
Epoch 5.30: Loss = 0.358307
Epoch 5.31: Loss = 0.438354
Epoch 5.32: Loss = 0.466782
Epoch 5.33: Loss = 0.331757
Epoch 5.34: Loss = 0.368469
Epoch 5.35: Loss = 0.388474
Epoch 5.36: Loss = 0.392151
Epoch 5.37: Loss = 0.290802
Epoch 5.38: Loss = 0.41925
Epoch 5.39: Loss = 0.312363
Epoch 5.40: Loss = 0.523438
Epoch 5.41: Loss = 0.428192
Epoch 5.42: Loss = 0.414291
Epoch 5.43: Loss = 0.376755
Epoch 5.44: Loss = 0.431427
Epoch 5.45: Loss = 0.446136
Epoch 5.46: Loss = 0.362274
Epoch 5.47: Loss = 0.504288
Epoch 5.48: Loss = 0.431168
Epoch 5.49: Loss = 0.476166
Epoch 5.50: Loss = 0.405899
Epoch 5.51: Loss = 0.386948
Epoch 5.52: Loss = 0.40802
Epoch 5.53: Loss = 0.381622
Epoch 5.54: Loss = 0.469406
Epoch 5.55: Loss = 0.401184
Epoch 5.56: Loss = 0.394989
Epoch 5.57: Loss = 0.448761
Epoch 5.58: Loss = 0.379364
Epoch 5.59: Loss = 0.404907
Epoch 5.60: Loss = 0.413162
Epoch 5.61: Loss = 0.441681
Epoch 5.62: Loss = 0.456924
Epoch 5.63: Loss = 0.492218
Epoch 5.64: Loss = 0.37767
Epoch 5.65: Loss = 0.43428
Epoch 5.66: Loss = 0.372269
Epoch 5.67: Loss = 0.384247
Epoch 5.68: Loss = 0.427429
Epoch 5.69: Loss = 0.450882
Epoch 5.70: Loss = 0.515366
Epoch 5.71: Loss = 0.432831
Epoch 5.72: Loss = 0.452225
Epoch 5.73: Loss = 0.546783
Epoch 5.74: Loss = 0.373611
Epoch 5.75: Loss = 0.419403
Epoch 5.76: Loss = 0.438873
Epoch 5.77: Loss = 0.489441
Epoch 5.78: Loss = 0.433594
Epoch 5.79: Loss = 0.454544
Epoch 5.80: Loss = 0.431458
Epoch 5.81: Loss = 0.439865
Epoch 5.82: Loss = 0.286301
Epoch 5.83: Loss = 0.393082
Epoch 5.84: Loss = 0.377686
Epoch 5.85: Loss = 0.494507
Epoch 5.86: Loss = 0.453125
Epoch 5.87: Loss = 0.356857
Epoch 5.88: Loss = 0.391129
Epoch 5.89: Loss = 0.361359
Epoch 5.90: Loss = 0.377731
Epoch 5.91: Loss = 0.384003
Epoch 5.92: Loss = 0.457504
Epoch 5.93: Loss = 0.380356
Epoch 5.94: Loss = 0.321136
Epoch 5.95: Loss = 0.427338
Epoch 5.96: Loss = 0.439194
Epoch 5.97: Loss = 0.439194
Epoch 5.98: Loss = 0.332993
Epoch 5.99: Loss = 0.515121
Epoch 5.100: Loss = 0.416519
Epoch 5.101: Loss = 0.320297
Epoch 5.102: Loss = 0.383301
Epoch 5.103: Loss = 0.479111
Epoch 5.104: Loss = 0.460617
Epoch 5.105: Loss = 0.387222
Epoch 5.106: Loss = 0.361679
Epoch 5.107: Loss = 0.352539
Epoch 5.108: Loss = 0.525513
Epoch 5.109: Loss = 0.462494
Epoch 5.110: Loss = 0.447983
Epoch 5.111: Loss = 0.506042
Epoch 5.112: Loss = 0.332428
Epoch 5.113: Loss = 0.415421
Epoch 5.114: Loss = 0.421555
Epoch 5.115: Loss = 0.369263
Epoch 5.116: Loss = 0.459518
Epoch 5.117: Loss = 0.321838
Epoch 5.118: Loss = 0.492065
Epoch 5.119: Loss = 0.424622
Epoch 5.120: Loss = 0.443924
TRAIN LOSS = 0.412369
TRAIN ACC = 88.652 % (53194/60000)
Loss = 0.385895
Loss = 0.490356
Loss = 0.515198
Loss = 0.564636
Loss = 0.601913
Loss = 0.42868
Loss = 0.365707
Loss = 0.662964
Loss = 0.571579
Loss = 0.479965
Loss = 0.182968
Loss = 0.288162
Loss = 0.33284
Loss = 0.361832
Loss = 0.188965
Loss = 0.275925
Loss = 0.222977
Loss = 0.0626373
Loss = 0.208572
Loss = 0.537979
TEST LOSS = 0.386487
TEST ACC = 531.94 % (8979/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.35376
Epoch 6.2: Loss = 0.545624
Epoch 6.3: Loss = 0.460953
Epoch 6.4: Loss = 0.564362
Epoch 6.5: Loss = 0.463348
Epoch 6.6: Loss = 0.452164
Epoch 6.7: Loss = 0.409485
Epoch 6.8: Loss = 0.399017
Epoch 6.9: Loss = 0.356003
Epoch 6.10: Loss = 0.331985
Epoch 6.11: Loss = 0.310959
Epoch 6.12: Loss = 0.458649
Epoch 6.13: Loss = 0.445282
Epoch 6.14: Loss = 0.490128
Epoch 6.15: Loss = 0.459641
Epoch 6.16: Loss = 0.389923
Epoch 6.17: Loss = 0.400482
Epoch 6.18: Loss = 0.381516
Epoch 6.19: Loss = 0.334671
Epoch 6.20: Loss = 0.409424
Epoch 6.21: Loss = 0.353928
Epoch 6.22: Loss = 0.3517
Epoch 6.23: Loss = 0.297226
Epoch 6.24: Loss = 0.448715
Epoch 6.25: Loss = 0.367722
Epoch 6.26: Loss = 0.430664
Epoch 6.27: Loss = 0.415771
Epoch 6.28: Loss = 0.439819
Epoch 6.29: Loss = 0.41832
Epoch 6.30: Loss = 0.497574
Epoch 6.31: Loss = 0.438644
Epoch 6.32: Loss = 0.495422
Epoch 6.33: Loss = 0.430542
Epoch 6.34: Loss = 0.401276
Epoch 6.35: Loss = 0.377029
Epoch 6.36: Loss = 0.413071
Epoch 6.37: Loss = 0.323883
Epoch 6.38: Loss = 0.324539
Epoch 6.39: Loss = 0.369888
Epoch 6.40: Loss = 0.430984
Epoch 6.41: Loss = 0.524445
Epoch 6.42: Loss = 0.386841
Epoch 6.43: Loss = 0.451904
Epoch 6.44: Loss = 0.48941
Epoch 6.45: Loss = 0.366196
Epoch 6.46: Loss = 0.420685
Epoch 6.47: Loss = 0.404892
Epoch 6.48: Loss = 0.355103
Epoch 6.49: Loss = 0.311264
Epoch 6.50: Loss = 0.440262
Epoch 6.51: Loss = 0.409088
Epoch 6.52: Loss = 0.424194
Epoch 6.53: Loss = 0.340714
Epoch 6.54: Loss = 0.405457
Epoch 6.55: Loss = 0.292709
Epoch 6.56: Loss = 0.341751
Epoch 6.57: Loss = 0.377716
Epoch 6.58: Loss = 0.511093
Epoch 6.59: Loss = 0.438538
Epoch 6.60: Loss = 0.414505
Epoch 6.61: Loss = 0.494003
Epoch 6.62: Loss = 0.413589
Epoch 6.63: Loss = 0.332611
Epoch 6.64: Loss = 0.343338
Epoch 6.65: Loss = 0.387589
Epoch 6.66: Loss = 0.452194
Epoch 6.67: Loss = 0.383743
Epoch 6.68: Loss = 0.367752
Epoch 6.69: Loss = 0.384232
Epoch 6.70: Loss = 0.415634
Epoch 6.71: Loss = 0.592697
Epoch 6.72: Loss = 0.399841
Epoch 6.73: Loss = 0.353531
Epoch 6.74: Loss = 0.458206
Epoch 6.75: Loss = 0.352188
Epoch 6.76: Loss = 0.510544
Epoch 6.77: Loss = 0.406662
Epoch 6.78: Loss = 0.409775
Epoch 6.79: Loss = 0.483734
Epoch 6.80: Loss = 0.364441
Epoch 6.81: Loss = 0.408051
Epoch 6.82: Loss = 0.405945
Epoch 6.83: Loss = 0.388809
Epoch 6.84: Loss = 0.354279
Epoch 6.85: Loss = 0.347214
Epoch 6.86: Loss = 0.374374
Epoch 6.87: Loss = 0.519836
Epoch 6.88: Loss = 0.361664
Epoch 6.89: Loss = 0.345673
Epoch 6.90: Loss = 0.328293
Epoch 6.91: Loss = 0.464218
Epoch 6.92: Loss = 0.36882
Epoch 6.93: Loss = 0.432449
Epoch 6.94: Loss = 0.368729
Epoch 6.95: Loss = 0.434601
Epoch 6.96: Loss = 0.41571
Epoch 6.97: Loss = 0.481964
Epoch 6.98: Loss = 0.340302
Epoch 6.99: Loss = 0.424988
Epoch 6.100: Loss = 0.542969
Epoch 6.101: Loss = 0.45462
Epoch 6.102: Loss = 0.380936
Epoch 6.103: Loss = 0.374741
Epoch 6.104: Loss = 0.319901
Epoch 6.105: Loss = 0.464294
Epoch 6.106: Loss = 0.430054
Epoch 6.107: Loss = 0.357666
Epoch 6.108: Loss = 0.371185
Epoch 6.109: Loss = 0.39325
Epoch 6.110: Loss = 0.578171
Epoch 6.111: Loss = 0.49176
Epoch 6.112: Loss = 0.482925
Epoch 6.113: Loss = 0.410675
Epoch 6.114: Loss = 0.401581
Epoch 6.115: Loss = 0.340561
Epoch 6.116: Loss = 0.397614
Epoch 6.117: Loss = 0.477737
Epoch 6.118: Loss = 0.37883
Epoch 6.119: Loss = 0.437332
Epoch 6.120: Loss = 0.356857
TRAIN LOSS = 0.410568
TRAIN ACC = 88.9648 % (53382/60000)
Loss = 0.382584
Loss = 0.46109
Loss = 0.520233
Loss = 0.550827
Loss = 0.583145
Loss = 0.426453
Loss = 0.358521
Loss = 0.653458
Loss = 0.569351
Loss = 0.482758
Loss = 0.185623
Loss = 0.282501
Loss = 0.316589
Loss = 0.354431
Loss = 0.176651
Loss = 0.281296
Loss = 0.199829
Loss = 0.0543365
Loss = 0.238663
Loss = 0.507858
TEST LOSS = 0.37931
TEST ACC = 533.82 % (8982/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.290283
Epoch 7.2: Loss = 0.349518
Epoch 7.3: Loss = 0.512222
Epoch 7.4: Loss = 0.310333
Epoch 7.5: Loss = 0.382294
Epoch 7.6: Loss = 0.426041
Epoch 7.7: Loss = 0.360352
Epoch 7.8: Loss = 0.425491
Epoch 7.9: Loss = 0.350815
Epoch 7.10: Loss = 0.533127
Epoch 7.11: Loss = 0.391113
Epoch 7.12: Loss = 0.461319
Epoch 7.13: Loss = 0.305893
Epoch 7.14: Loss = 0.457062
Epoch 7.15: Loss = 0.422485
Epoch 7.16: Loss = 0.357361
Epoch 7.17: Loss = 0.429352
Epoch 7.18: Loss = 0.320175
Epoch 7.19: Loss = 0.329971
Epoch 7.20: Loss = 0.438919
Epoch 7.21: Loss = 0.404129
Epoch 7.22: Loss = 0.382294
Epoch 7.23: Loss = 0.504822
Epoch 7.24: Loss = 0.40004
Epoch 7.25: Loss = 0.297226
Epoch 7.26: Loss = 0.454285
Epoch 7.27: Loss = 0.386963
Epoch 7.28: Loss = 0.462051
Epoch 7.29: Loss = 0.392349
Epoch 7.30: Loss = 0.479675
Epoch 7.31: Loss = 0.330902
Epoch 7.32: Loss = 0.320587
Epoch 7.33: Loss = 0.469162
Epoch 7.34: Loss = 0.479462
Epoch 7.35: Loss = 0.341461
Epoch 7.36: Loss = 0.340958
Epoch 7.37: Loss = 0.414063
Epoch 7.38: Loss = 0.318314
Epoch 7.39: Loss = 0.397919
Epoch 7.40: Loss = 0.385254
Epoch 7.41: Loss = 0.360519
Epoch 7.42: Loss = 0.458374
Epoch 7.43: Loss = 0.397888
Epoch 7.44: Loss = 0.504532
Epoch 7.45: Loss = 0.411575
Epoch 7.46: Loss = 0.479492
Epoch 7.47: Loss = 0.41304
Epoch 7.48: Loss = 0.392288
Epoch 7.49: Loss = 0.331177
Epoch 7.50: Loss = 0.426147
Epoch 7.51: Loss = 0.390472
Epoch 7.52: Loss = 0.452164
Epoch 7.53: Loss = 0.473785
Epoch 7.54: Loss = 0.489059
Epoch 7.55: Loss = 0.460358
Epoch 7.56: Loss = 0.425568
Epoch 7.57: Loss = 0.371338
Epoch 7.58: Loss = 0.345551
Epoch 7.59: Loss = 0.449188
Epoch 7.60: Loss = 0.391617
Epoch 7.61: Loss = 0.452393
Epoch 7.62: Loss = 0.369415
Epoch 7.63: Loss = 0.46994
Epoch 7.64: Loss = 0.478363
Epoch 7.65: Loss = 0.376465
Epoch 7.66: Loss = 0.407928
Epoch 7.67: Loss = 0.362732
Epoch 7.68: Loss = 0.410767
Epoch 7.69: Loss = 0.475159
Epoch 7.70: Loss = 0.309235
Epoch 7.71: Loss = 0.42659
Epoch 7.72: Loss = 0.49205
Epoch 7.73: Loss = 0.41394
Epoch 7.74: Loss = 0.380234
Epoch 7.75: Loss = 0.400452
Epoch 7.76: Loss = 0.427658
Epoch 7.77: Loss = 0.445221
Epoch 7.78: Loss = 0.442917
Epoch 7.79: Loss = 0.43779
Epoch 7.80: Loss = 0.355667
Epoch 7.81: Loss = 0.365128
Epoch 7.82: Loss = 0.313385
Epoch 7.83: Loss = 0.447388
Epoch 7.84: Loss = 0.434372
Epoch 7.85: Loss = 0.366852
Epoch 7.86: Loss = 0.345459
Epoch 7.87: Loss = 0.307571
Epoch 7.88: Loss = 0.290848
Epoch 7.89: Loss = 0.404968
Epoch 7.90: Loss = 0.443192
Epoch 7.91: Loss = 0.404572
Epoch 7.92: Loss = 0.515244
Epoch 7.93: Loss = 0.397522
Epoch 7.94: Loss = 0.415405
Epoch 7.95: Loss = 0.313019
Epoch 7.96: Loss = 0.390457
Epoch 7.97: Loss = 0.297073
Epoch 7.98: Loss = 0.305679
Epoch 7.99: Loss = 0.372269
Epoch 7.100: Loss = 0.424927
Epoch 7.101: Loss = 0.417023
Epoch 7.102: Loss = 0.384369
Epoch 7.103: Loss = 0.435593
Epoch 7.104: Loss = 0.291183
Epoch 7.105: Loss = 0.521469
Epoch 7.106: Loss = 0.386703
Epoch 7.107: Loss = 0.411972
Epoch 7.108: Loss = 0.404694
Epoch 7.109: Loss = 0.399673
Epoch 7.110: Loss = 0.39949
Epoch 7.111: Loss = 0.504074
Epoch 7.112: Loss = 0.419235
Epoch 7.113: Loss = 0.478195
Epoch 7.114: Loss = 0.420349
Epoch 7.115: Loss = 0.374802
Epoch 7.116: Loss = 0.357483
Epoch 7.117: Loss = 0.460541
Epoch 7.118: Loss = 0.306244
Epoch 7.119: Loss = 0.454193
Epoch 7.120: Loss = 0.477188
TRAIN LOSS = 0.40361
TRAIN ACC = 89.1937 % (53519/60000)
Loss = 0.385483
Loss = 0.464935
Loss = 0.530258
Loss = 0.559952
Loss = 0.590118
Loss = 0.415466
Loss = 0.362442
Loss = 0.648361
Loss = 0.578354
Loss = 0.51268
Loss = 0.176941
Loss = 0.314743
Loss = 0.30722
Loss = 0.355484
Loss = 0.164215
Loss = 0.25441
Loss = 0.196381
Loss = 0.0555267
Loss = 0.21962
Loss = 0.474152
TEST LOSS = 0.378337
TEST ACC = 535.19 % (8992/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.361679
Epoch 8.2: Loss = 0.381607
Epoch 8.3: Loss = 0.454819
Epoch 8.4: Loss = 0.444321
Epoch 8.5: Loss = 0.387955
Epoch 8.6: Loss = 0.409439
Epoch 8.7: Loss = 0.364075
Epoch 8.8: Loss = 0.366501
Epoch 8.9: Loss = 0.356827
Epoch 8.10: Loss = 0.349808
Epoch 8.11: Loss = 0.282715
Epoch 8.12: Loss = 0.361542
Epoch 8.13: Loss = 0.378967
Epoch 8.14: Loss = 0.540985
Epoch 8.15: Loss = 0.371994
Epoch 8.16: Loss = 0.379349
Epoch 8.17: Loss = 0.365128
Epoch 8.18: Loss = 0.440994
Epoch 8.19: Loss = 0.429306
Epoch 8.20: Loss = 0.393204
Epoch 8.21: Loss = 0.47023
Epoch 8.22: Loss = 0.351944
Epoch 8.23: Loss = 0.417267
Epoch 8.24: Loss = 0.406448
Epoch 8.25: Loss = 0.355026
Epoch 8.26: Loss = 0.352325
Epoch 8.27: Loss = 0.412338
Epoch 8.28: Loss = 0.361298
Epoch 8.29: Loss = 0.37085
Epoch 8.30: Loss = 0.359299
Epoch 8.31: Loss = 0.494751
Epoch 8.32: Loss = 0.46077
Epoch 8.33: Loss = 0.390564
Epoch 8.34: Loss = 0.263596
Epoch 8.35: Loss = 0.37471
Epoch 8.36: Loss = 0.421204
Epoch 8.37: Loss = 0.381897
Epoch 8.38: Loss = 0.432816
Epoch 8.39: Loss = 0.442307
Epoch 8.40: Loss = 0.262131
Epoch 8.41: Loss = 0.38797
Epoch 8.42: Loss = 0.408539
Epoch 8.43: Loss = 0.370529
Epoch 8.44: Loss = 0.391617
Epoch 8.45: Loss = 0.37323
Epoch 8.46: Loss = 0.470245
Epoch 8.47: Loss = 0.385025
Epoch 8.48: Loss = 0.552246
Epoch 8.49: Loss = 0.330872
Epoch 8.50: Loss = 0.395523
Epoch 8.51: Loss = 0.367447
Epoch 8.52: Loss = 0.453873
Epoch 8.53: Loss = 0.44162
Epoch 8.54: Loss = 0.398315
Epoch 8.55: Loss = 0.436096
Epoch 8.56: Loss = 0.40889
Epoch 8.57: Loss = 0.401199
Epoch 8.58: Loss = 0.422867
Epoch 8.59: Loss = 0.327301
Epoch 8.60: Loss = 0.399521
Epoch 8.61: Loss = 0.445786
Epoch 8.62: Loss = 0.311218
Epoch 8.63: Loss = 0.36261
Epoch 8.64: Loss = 0.435684
Epoch 8.65: Loss = 0.358414
Epoch 8.66: Loss = 0.307907
Epoch 8.67: Loss = 0.365768
Epoch 8.68: Loss = 0.374191
Epoch 8.69: Loss = 0.409164
Epoch 8.70: Loss = 0.482864
Epoch 8.71: Loss = 0.367645
Epoch 8.72: Loss = 0.561981
Epoch 8.73: Loss = 0.423141
Epoch 8.74: Loss = 0.432098
Epoch 8.75: Loss = 0.367325
Epoch 8.76: Loss = 0.533188
Epoch 8.77: Loss = 0.395828
Epoch 8.78: Loss = 0.378525
Epoch 8.79: Loss = 0.39978
Epoch 8.80: Loss = 0.408157
Epoch 8.81: Loss = 0.338394
Epoch 8.82: Loss = 0.430344
Epoch 8.83: Loss = 0.449112
Epoch 8.84: Loss = 0.348526
Epoch 8.85: Loss = 0.439697
Epoch 8.86: Loss = 0.361328
Epoch 8.87: Loss = 0.441498
Epoch 8.88: Loss = 0.501022
Epoch 8.89: Loss = 0.421646
Epoch 8.90: Loss = 0.460358
Epoch 8.91: Loss = 0.372696
Epoch 8.92: Loss = 0.367264
Epoch 8.93: Loss = 0.389465
Epoch 8.94: Loss = 0.479019
Epoch 8.95: Loss = 0.416916
Epoch 8.96: Loss = 0.361588
Epoch 8.97: Loss = 0.391449
Epoch 8.98: Loss = 0.403488
Epoch 8.99: Loss = 0.345871
Epoch 8.100: Loss = 0.478592
Epoch 8.101: Loss = 0.434143
Epoch 8.102: Loss = 0.493973
Epoch 8.103: Loss = 0.32103
Epoch 8.104: Loss = 0.369354
Epoch 8.105: Loss = 0.419586
Epoch 8.106: Loss = 0.386688
Epoch 8.107: Loss = 0.437317
Epoch 8.108: Loss = 0.392014
Epoch 8.109: Loss = 0.553024
Epoch 8.110: Loss = 0.476456
Epoch 8.111: Loss = 0.365707
Epoch 8.112: Loss = 0.52211
Epoch 8.113: Loss = 0.333755
Epoch 8.114: Loss = 0.463913
Epoch 8.115: Loss = 0.343842
Epoch 8.116: Loss = 0.377655
Epoch 8.117: Loss = 0.408844
Epoch 8.118: Loss = 0.40947
Epoch 8.119: Loss = 0.307404
Epoch 8.120: Loss = 0.357529
TRAIN LOSS = 0.401825
TRAIN ACC = 89.4592 % (53678/60000)
Loss = 0.388504
Loss = 0.454147
Loss = 0.528931
Loss = 0.577545
Loss = 0.576035
Loss = 0.401642
Loss = 0.379517
Loss = 0.666672
Loss = 0.547852
Loss = 0.503204
Loss = 0.181702
Loss = 0.305984
Loss = 0.312943
Loss = 0.336639
Loss = 0.162613
Loss = 0.275681
Loss = 0.197556
Loss = 0.0542603
Loss = 0.198151
Loss = 0.478058
TEST LOSS = 0.376382
TEST ACC = 536.78 % (9021/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.483032
Epoch 9.2: Loss = 0.634338
Epoch 9.3: Loss = 0.438507
Epoch 9.4: Loss = 0.405991
Epoch 9.5: Loss = 0.407181
Epoch 9.6: Loss = 0.320953
Epoch 9.7: Loss = 0.477051
Epoch 9.8: Loss = 0.292343
Epoch 9.9: Loss = 0.478485
Epoch 9.10: Loss = 0.377884
Epoch 9.11: Loss = 0.425186
Epoch 9.12: Loss = 0.358871
Epoch 9.13: Loss = 0.470947
Epoch 9.14: Loss = 0.385284
Epoch 9.15: Loss = 0.42662
Epoch 9.16: Loss = 0.460922
Epoch 9.17: Loss = 0.475845
Epoch 9.18: Loss = 0.410934
Epoch 9.19: Loss = 0.380692
Epoch 9.20: Loss = 0.38681
Epoch 9.21: Loss = 0.295563
Epoch 9.22: Loss = 0.365936
Epoch 9.23: Loss = 0.418167
Epoch 9.24: Loss = 0.289825
Epoch 9.25: Loss = 0.440735
Epoch 9.26: Loss = 0.375397
Epoch 9.27: Loss = 0.433777
Epoch 9.28: Loss = 0.360245
Epoch 9.29: Loss = 0.383301
Epoch 9.30: Loss = 0.514328
Epoch 9.31: Loss = 0.411407
Epoch 9.32: Loss = 0.431564
Epoch 9.33: Loss = 0.385864
Epoch 9.34: Loss = 0.406479
Epoch 9.35: Loss = 0.36322
Epoch 9.36: Loss = 0.273483
Epoch 9.37: Loss = 0.346344
Epoch 9.38: Loss = 0.457062
Epoch 9.39: Loss = 0.415405
Epoch 9.40: Loss = 0.412537
Epoch 9.41: Loss = 0.356735
Epoch 9.42: Loss = 0.429352
Epoch 9.43: Loss = 0.278793
Epoch 9.44: Loss = 0.497345
Epoch 9.45: Loss = 0.421631
Epoch 9.46: Loss = 0.404556
Epoch 9.47: Loss = 0.390961
Epoch 9.48: Loss = 0.337448
Epoch 9.49: Loss = 0.483551
Epoch 9.50: Loss = 0.331818
Epoch 9.51: Loss = 0.359848
Epoch 9.52: Loss = 0.35463
Epoch 9.53: Loss = 0.474426
Epoch 9.54: Loss = 0.489731
Epoch 9.55: Loss = 0.416031
Epoch 9.56: Loss = 0.372681
Epoch 9.57: Loss = 0.372971
Epoch 9.58: Loss = 0.370209
Epoch 9.59: Loss = 0.384354
Epoch 9.60: Loss = 0.478363
Epoch 9.61: Loss = 0.379974
Epoch 9.62: Loss = 0.295975
Epoch 9.63: Loss = 0.458298
Epoch 9.64: Loss = 0.373596
Epoch 9.65: Loss = 0.341583
Epoch 9.66: Loss = 0.371323
Epoch 9.67: Loss = 0.361969
Epoch 9.68: Loss = 0.312714
Epoch 9.69: Loss = 0.338394
Epoch 9.70: Loss = 0.384064
Epoch 9.71: Loss = 0.346649
Epoch 9.72: Loss = 0.388718
Epoch 9.73: Loss = 0.387024
Epoch 9.74: Loss = 0.397018
Epoch 9.75: Loss = 0.3237
Epoch 9.76: Loss = 0.406799
Epoch 9.77: Loss = 0.351974
Epoch 9.78: Loss = 0.379761
Epoch 9.79: Loss = 0.432526
Epoch 9.80: Loss = 0.334793
Epoch 9.81: Loss = 0.297516
Epoch 9.82: Loss = 0.329483
Epoch 9.83: Loss = 0.385254
Epoch 9.84: Loss = 0.339844
Epoch 9.85: Loss = 0.35025
Epoch 9.86: Loss = 0.349411
Epoch 9.87: Loss = 0.354385
Epoch 9.88: Loss = 0.43985
Epoch 9.89: Loss = 0.405548
Epoch 9.90: Loss = 0.322296
Epoch 9.91: Loss = 0.341171
Epoch 9.92: Loss = 0.500015
Epoch 9.93: Loss = 0.35376
Epoch 9.94: Loss = 0.429108
Epoch 9.95: Loss = 0.44368
Epoch 9.96: Loss = 0.409439
Epoch 9.97: Loss = 0.337189
Epoch 9.98: Loss = 0.368378
Epoch 9.99: Loss = 0.466415
Epoch 9.100: Loss = 0.429611
Epoch 9.101: Loss = 0.373291
Epoch 9.102: Loss = 0.373444
Epoch 9.103: Loss = 0.519989
Epoch 9.104: Loss = 0.482132
Epoch 9.105: Loss = 0.342957
Epoch 9.106: Loss = 0.512634
Epoch 9.107: Loss = 0.397797
Epoch 9.108: Loss = 0.3582
Epoch 9.109: Loss = 0.373276
Epoch 9.110: Loss = 0.649643
Epoch 9.111: Loss = 0.378769
Epoch 9.112: Loss = 0.464096
Epoch 9.113: Loss = 0.33577
Epoch 9.114: Loss = 0.472473
Epoch 9.115: Loss = 0.456024
Epoch 9.116: Loss = 0.470154
Epoch 9.117: Loss = 0.316956
Epoch 9.118: Loss = 0.446167
Epoch 9.119: Loss = 0.459686
Epoch 9.120: Loss = 0.510849
TRAIN LOSS = 0.399979
TRAIN ACC = 89.7995 % (53882/60000)
Loss = 0.382889
Loss = 0.471359
Loss = 0.522354
Loss = 0.570847
Loss = 0.56337
Loss = 0.405533
Loss = 0.366608
Loss = 0.689468
Loss = 0.539917
Loss = 0.510849
Loss = 0.176102
Loss = 0.325897
Loss = 0.303329
Loss = 0.356888
Loss = 0.165024
Loss = 0.30426
Loss = 0.224411
Loss = 0.0615234
Loss = 0.207199
Loss = 0.507507
TEST LOSS = 0.382767
TEST ACC = 538.818 % (9022/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.461563
Epoch 10.2: Loss = 0.329971
Epoch 10.3: Loss = 0.349213
Epoch 10.4: Loss = 0.505905
Epoch 10.5: Loss = 0.296494
Epoch 10.6: Loss = 0.339325
Epoch 10.7: Loss = 0.329712
Epoch 10.8: Loss = 0.325928
Epoch 10.9: Loss = 0.389694
Epoch 10.10: Loss = 0.422302
Epoch 10.11: Loss = 0.387512
Epoch 10.12: Loss = 0.44754
Epoch 10.13: Loss = 0.417648
Epoch 10.14: Loss = 0.202591
Epoch 10.15: Loss = 0.387817
Epoch 10.16: Loss = 0.356812
Epoch 10.17: Loss = 0.293671
Epoch 10.18: Loss = 0.434402
Epoch 10.19: Loss = 0.39798
Epoch 10.20: Loss = 0.410934
Epoch 10.21: Loss = 0.423447
Epoch 10.22: Loss = 0.481461
Epoch 10.23: Loss = 0.301025
Epoch 10.24: Loss = 0.41655
Epoch 10.25: Loss = 0.433182
Epoch 10.26: Loss = 0.315033
Epoch 10.27: Loss = 0.524261
Epoch 10.28: Loss = 0.332336
Epoch 10.29: Loss = 0.444809
Epoch 10.30: Loss = 0.552017
Epoch 10.31: Loss = 0.44162
Epoch 10.32: Loss = 0.45343
Epoch 10.33: Loss = 0.333954
Epoch 10.34: Loss = 0.428772
Epoch 10.35: Loss = 0.35614
Epoch 10.36: Loss = 0.235489
Epoch 10.37: Loss = 0.414993
Epoch 10.38: Loss = 0.443878
Epoch 10.39: Loss = 0.349136
Epoch 10.40: Loss = 0.327896
Epoch 10.41: Loss = 0.451431
Epoch 10.42: Loss = 0.439011
Epoch 10.43: Loss = 0.477921
Epoch 10.44: Loss = 0.374054
Epoch 10.45: Loss = 0.445313
Epoch 10.46: Loss = 0.431625
Epoch 10.47: Loss = 0.391388
Epoch 10.48: Loss = 0.458389
Epoch 10.49: Loss = 0.447891
Epoch 10.50: Loss = 0.421402
Epoch 10.51: Loss = 0.327515
Epoch 10.52: Loss = 0.359314
Epoch 10.53: Loss = 0.430573
Epoch 10.54: Loss = 0.36937
Epoch 10.55: Loss = 0.360718
Epoch 10.56: Loss = 0.324417
Epoch 10.57: Loss = 0.516357
Epoch 10.58: Loss = 0.432739
Epoch 10.59: Loss = 0.447403
Epoch 10.60: Loss = 0.318069
Epoch 10.61: Loss = 0.459167
Epoch 10.62: Loss = 0.391602
Epoch 10.63: Loss = 0.359863
Epoch 10.64: Loss = 0.354034
Epoch 10.65: Loss = 0.352417
Epoch 10.66: Loss = 0.382782
Epoch 10.67: Loss = 0.34317
Epoch 10.68: Loss = 0.380127
Epoch 10.69: Loss = 0.416748
Epoch 10.70: Loss = 0.317871
Epoch 10.71: Loss = 0.471359
Epoch 10.72: Loss = 0.396637
Epoch 10.73: Loss = 0.435257
Epoch 10.74: Loss = 0.408386
Epoch 10.75: Loss = 0.348297
Epoch 10.76: Loss = 0.298599
Epoch 10.77: Loss = 0.478622
Epoch 10.78: Loss = 0.320969
Epoch 10.79: Loss = 0.580963
Epoch 10.80: Loss = 0.319717
Epoch 10.81: Loss = 0.521713
Epoch 10.82: Loss = 0.35585
Epoch 10.83: Loss = 0.449341
Epoch 10.84: Loss = 0.40477
Epoch 10.85: Loss = 0.369888
Epoch 10.86: Loss = 0.410599
Epoch 10.87: Loss = 0.338531
Epoch 10.88: Loss = 0.372894
Epoch 10.89: Loss = 0.369263
Epoch 10.90: Loss = 0.49057
Epoch 10.91: Loss = 0.502777
Epoch 10.92: Loss = 0.449677
Epoch 10.93: Loss = 0.287109
Epoch 10.94: Loss = 0.41124
Epoch 10.95: Loss = 0.492508
Epoch 10.96: Loss = 0.423645
Epoch 10.97: Loss = 0.436508
Epoch 10.98: Loss = 0.352051
Epoch 10.99: Loss = 0.356644
Epoch 10.100: Loss = 0.382996
Epoch 10.101: Loss = 0.344254
Epoch 10.102: Loss = 0.416016
Epoch 10.103: Loss = 0.533829
Epoch 10.104: Loss = 0.372971
Epoch 10.105: Loss = 0.541718
Epoch 10.106: Loss = 0.347961
Epoch 10.107: Loss = 0.443497
Epoch 10.108: Loss = 0.497711
Epoch 10.109: Loss = 0.341263
Epoch 10.110: Loss = 0.424408
Epoch 10.111: Loss = 0.444901
Epoch 10.112: Loss = 0.461136
Epoch 10.113: Loss = 0.47052
Epoch 10.114: Loss = 0.4422
Epoch 10.115: Loss = 0.306763
Epoch 10.116: Loss = 0.565643
Epoch 10.117: Loss = 0.536179
Epoch 10.118: Loss = 0.460602
Epoch 10.119: Loss = 0.45369
Epoch 10.120: Loss = 0.376801
TRAIN LOSS = 0.404144
TRAIN ACC = 90.0177 % (54013/60000)
Loss = 0.379471
Loss = 0.467682
Loss = 0.524384
Loss = 0.572052
Loss = 0.567169
Loss = 0.403046
Loss = 0.354935
Loss = 0.662354
Loss = 0.546555
Loss = 0.503159
Loss = 0.182098
Loss = 0.314301
Loss = 0.284256
Loss = 0.353531
Loss = 0.165253
Loss = 0.266846
Loss = 0.223312
Loss = 0.0593262
Loss = 0.192917
Loss = 0.522812
TEST LOSS = 0.377273
TEST ACC = 540.129 % (9038/10000)

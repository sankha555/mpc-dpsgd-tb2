Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.36028
Epoch 1.2: Loss = 2.33185
Epoch 1.3: Loss = 2.31581
Epoch 1.4: Loss = 2.25652
Epoch 1.5: Loss = 2.23338
Epoch 1.6: Loss = 2.21494
Epoch 1.7: Loss = 2.19908
Epoch 1.8: Loss = 2.18556
Epoch 1.9: Loss = 2.13022
Epoch 1.10: Loss = 2.12392
Epoch 1.11: Loss = 2.09268
Epoch 1.12: Loss = 2.08827
Epoch 1.13: Loss = 2.02858
Epoch 1.14: Loss = 2.01822
Epoch 1.15: Loss = 1.98267
Epoch 1.16: Loss = 1.93781
Epoch 1.17: Loss = 1.93904
Epoch 1.18: Loss = 1.90302
Epoch 1.19: Loss = 1.86917
Epoch 1.20: Loss = 1.83923
Epoch 1.21: Loss = 1.84888
Epoch 1.22: Loss = 1.80251
Epoch 1.23: Loss = 1.76189
Epoch 1.24: Loss = 1.72495
Epoch 1.25: Loss = 1.67595
Epoch 1.26: Loss = 1.69659
Epoch 1.27: Loss = 1.65923
Epoch 1.28: Loss = 1.65239
Epoch 1.29: Loss = 1.59492
Epoch 1.30: Loss = 1.53882
Epoch 1.31: Loss = 1.50226
Epoch 1.32: Loss = 1.50024
Epoch 1.33: Loss = 1.43851
Epoch 1.34: Loss = 1.4016
Epoch 1.35: Loss = 1.44037
Epoch 1.36: Loss = 1.32069
Epoch 1.37: Loss = 1.40001
Epoch 1.38: Loss = 1.27965
Epoch 1.39: Loss = 1.29857
Epoch 1.40: Loss = 1.31732
Epoch 1.41: Loss = 1.23364
Epoch 1.42: Loss = 1.27571
Epoch 1.43: Loss = 1.20491
Epoch 1.44: Loss = 1.18987
Epoch 1.45: Loss = 1.17566
Epoch 1.46: Loss = 1.17984
Epoch 1.47: Loss = 1.08025
Epoch 1.48: Loss = 1.086
Epoch 1.49: Loss = 1.11748
Epoch 1.50: Loss = 1.08896
Epoch 1.51: Loss = 1.07805
Epoch 1.52: Loss = 0.984543
Epoch 1.53: Loss = 1.11427
Epoch 1.54: Loss = 0.984848
Epoch 1.55: Loss = 0.943893
Epoch 1.56: Loss = 0.986282
Epoch 1.57: Loss = 0.961044
Epoch 1.58: Loss = 0.927292
Epoch 1.59: Loss = 0.894897
Epoch 1.60: Loss = 0.912476
Epoch 1.61: Loss = 0.89679
Epoch 1.62: Loss = 0.940964
Epoch 1.63: Loss = 0.905182
Epoch 1.64: Loss = 0.871506
Epoch 1.65: Loss = 0.850937
Epoch 1.66: Loss = 0.825577
Epoch 1.67: Loss = 0.862228
Epoch 1.68: Loss = 0.812531
Epoch 1.69: Loss = 0.757721
Epoch 1.70: Loss = 0.775436
Epoch 1.71: Loss = 0.779114
Epoch 1.72: Loss = 0.797333
Epoch 1.73: Loss = 0.810684
Epoch 1.74: Loss = 0.780426
Epoch 1.75: Loss = 0.753174
Epoch 1.76: Loss = 0.755005
Epoch 1.77: Loss = 0.722488
Epoch 1.78: Loss = 0.735321
Epoch 1.79: Loss = 0.753082
Epoch 1.80: Loss = 0.758331
Epoch 1.81: Loss = 0.71608
Epoch 1.82: Loss = 0.718979
Epoch 1.83: Loss = 0.691376
Epoch 1.84: Loss = 0.725952
Epoch 1.85: Loss = 0.698151
Epoch 1.86: Loss = 0.639877
Epoch 1.87: Loss = 0.711823
Epoch 1.88: Loss = 0.658112
Epoch 1.89: Loss = 0.620087
Epoch 1.90: Loss = 0.589249
Epoch 1.91: Loss = 0.635147
Epoch 1.92: Loss = 0.691666
Epoch 1.93: Loss = 0.702072
Epoch 1.94: Loss = 0.665833
Epoch 1.95: Loss = 0.609039
Epoch 1.96: Loss = 0.600235
Epoch 1.97: Loss = 0.615036
Epoch 1.98: Loss = 0.649292
Epoch 1.99: Loss = 0.618668
Epoch 1.100: Loss = 0.638794
Epoch 1.101: Loss = 0.569122
Epoch 1.102: Loss = 0.665665
Epoch 1.103: Loss = 0.666138
Epoch 1.104: Loss = 0.571274
Epoch 1.105: Loss = 0.617737
Epoch 1.106: Loss = 0.601715
Epoch 1.107: Loss = 0.583557
Epoch 1.108: Loss = 0.633209
Epoch 1.109: Loss = 0.635468
Epoch 1.110: Loss = 0.636139
Epoch 1.111: Loss = 0.507431
Epoch 1.112: Loss = 0.55896
Epoch 1.113: Loss = 0.542435
Epoch 1.114: Loss = 0.599228
Epoch 1.115: Loss = 0.56218
Epoch 1.116: Loss = 0.538345
Epoch 1.117: Loss = 0.533279
Epoch 1.118: Loss = 0.607559
Epoch 1.119: Loss = 0.484161
Epoch 1.120: Loss = 0.602432
TRAIN LOSS = 1.12819
TRAIN ACC = 68.8095 % (41287/60000)
Loss = 0.559448
Loss = 0.587265
Loss = 0.705261
Loss = 0.654312
Loss = 0.696396
Loss = 0.598938
Loss = 0.554642
Loss = 0.707275
Loss = 0.680801
Loss = 0.609985
Loss = 0.333191
Loss = 0.456848
Loss = 0.354294
Loss = 0.541748
Loss = 0.396881
Loss = 0.39682
Loss = 0.373993
Loss = 0.21904
Loss = 0.396103
Loss = 0.633438
TEST LOSS = 0.522834
TEST ACC = 412.869 % (8447/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.585388
Epoch 2.2: Loss = 0.512604
Epoch 2.3: Loss = 0.475632
Epoch 2.4: Loss = 0.516922
Epoch 2.5: Loss = 0.595749
Epoch 2.6: Loss = 0.574295
Epoch 2.7: Loss = 0.512451
Epoch 2.8: Loss = 0.495819
Epoch 2.9: Loss = 0.570984
Epoch 2.10: Loss = 0.464615
Epoch 2.11: Loss = 0.530777
Epoch 2.12: Loss = 0.618652
Epoch 2.13: Loss = 0.528992
Epoch 2.14: Loss = 0.532791
Epoch 2.15: Loss = 0.55545
Epoch 2.16: Loss = 0.504852
Epoch 2.17: Loss = 0.48201
Epoch 2.18: Loss = 0.50119
Epoch 2.19: Loss = 0.461975
Epoch 2.20: Loss = 0.544601
Epoch 2.21: Loss = 0.486298
Epoch 2.22: Loss = 0.552689
Epoch 2.23: Loss = 0.506546
Epoch 2.24: Loss = 0.507019
Epoch 2.25: Loss = 0.489471
Epoch 2.26: Loss = 0.470551
Epoch 2.27: Loss = 0.525513
Epoch 2.28: Loss = 0.598801
Epoch 2.29: Loss = 0.510101
Epoch 2.30: Loss = 0.432159
Epoch 2.31: Loss = 0.5112
Epoch 2.32: Loss = 0.599243
Epoch 2.33: Loss = 0.475433
Epoch 2.34: Loss = 0.491669
Epoch 2.35: Loss = 0.485779
Epoch 2.36: Loss = 0.507416
Epoch 2.37: Loss = 0.464661
Epoch 2.38: Loss = 0.463699
Epoch 2.39: Loss = 0.461823
Epoch 2.40: Loss = 0.410187
Epoch 2.41: Loss = 0.488907
Epoch 2.42: Loss = 0.430313
Epoch 2.43: Loss = 0.474884
Epoch 2.44: Loss = 0.475555
Epoch 2.45: Loss = 0.500015
Epoch 2.46: Loss = 0.537872
Epoch 2.47: Loss = 0.460205
Epoch 2.48: Loss = 0.514343
Epoch 2.49: Loss = 0.468613
Epoch 2.50: Loss = 0.435181
Epoch 2.51: Loss = 0.466263
Epoch 2.52: Loss = 0.458603
Epoch 2.53: Loss = 0.401291
Epoch 2.54: Loss = 0.429199
Epoch 2.55: Loss = 0.451828
Epoch 2.56: Loss = 0.465195
Epoch 2.57: Loss = 0.445984
Epoch 2.58: Loss = 0.43486
Epoch 2.59: Loss = 0.528427
Epoch 2.60: Loss = 0.469421
Epoch 2.61: Loss = 0.489792
Epoch 2.62: Loss = 0.460815
Epoch 2.63: Loss = 0.392792
Epoch 2.64: Loss = 0.346329
Epoch 2.65: Loss = 0.420151
Epoch 2.66: Loss = 0.445908
Epoch 2.67: Loss = 0.455704
Epoch 2.68: Loss = 0.501251
Epoch 2.69: Loss = 0.495956
Epoch 2.70: Loss = 0.405411
Epoch 2.71: Loss = 0.38324
Epoch 2.72: Loss = 0.404846
Epoch 2.73: Loss = 0.397369
Epoch 2.74: Loss = 0.492096
Epoch 2.75: Loss = 0.496841
Epoch 2.76: Loss = 0.401886
Epoch 2.77: Loss = 0.428741
Epoch 2.78: Loss = 0.428864
Epoch 2.79: Loss = 0.450928
Epoch 2.80: Loss = 0.384262
Epoch 2.81: Loss = 0.527008
Epoch 2.82: Loss = 0.458542
Epoch 2.83: Loss = 0.486252
Epoch 2.84: Loss = 0.451965
Epoch 2.85: Loss = 0.432693
Epoch 2.86: Loss = 0.542923
Epoch 2.87: Loss = 0.372086
Epoch 2.88: Loss = 0.374741
Epoch 2.89: Loss = 0.441193
Epoch 2.90: Loss = 0.364288
Epoch 2.91: Loss = 0.442566
Epoch 2.92: Loss = 0.422104
Epoch 2.93: Loss = 0.415497
Epoch 2.94: Loss = 0.383179
Epoch 2.95: Loss = 0.385651
Epoch 2.96: Loss = 0.463821
Epoch 2.97: Loss = 0.439117
Epoch 2.98: Loss = 0.407303
Epoch 2.99: Loss = 0.417511
Epoch 2.100: Loss = 0.451553
Epoch 2.101: Loss = 0.368881
Epoch 2.102: Loss = 0.424515
Epoch 2.103: Loss = 0.41301
Epoch 2.104: Loss = 0.425934
Epoch 2.105: Loss = 0.403793
Epoch 2.106: Loss = 0.415436
Epoch 2.107: Loss = 0.49205
Epoch 2.108: Loss = 0.441483
Epoch 2.109: Loss = 0.4254
Epoch 2.110: Loss = 0.441742
Epoch 2.111: Loss = 0.474335
Epoch 2.112: Loss = 0.437195
Epoch 2.113: Loss = 0.435272
Epoch 2.114: Loss = 0.37204
Epoch 2.115: Loss = 0.467819
Epoch 2.116: Loss = 0.511505
Epoch 2.117: Loss = 0.380615
Epoch 2.118: Loss = 0.406387
Epoch 2.119: Loss = 0.376862
Epoch 2.120: Loss = 0.433334
TRAIN LOSS = 0.464691
TRAIN ACC = 85.9177 % (51553/60000)
Loss = 0.415176
Loss = 0.478821
Loss = 0.552277
Loss = 0.544052
Loss = 0.588989
Loss = 0.44812
Loss = 0.407349
Loss = 0.601059
Loss = 0.558624
Loss = 0.49614
Loss = 0.225845
Loss = 0.326416
Loss = 0.317169
Loss = 0.41626
Loss = 0.241806
Loss = 0.318283
Loss = 0.26619
Loss = 0.103821
Loss = 0.267197
Loss = 0.531036
TEST LOSS = 0.405231
TEST ACC = 515.529 % (8784/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.365311
Epoch 3.2: Loss = 0.343414
Epoch 3.3: Loss = 0.437576
Epoch 3.4: Loss = 0.424255
Epoch 3.5: Loss = 0.46936
Epoch 3.6: Loss = 0.427689
Epoch 3.7: Loss = 0.414429
Epoch 3.8: Loss = 0.430283
Epoch 3.9: Loss = 0.451202
Epoch 3.10: Loss = 0.389908
Epoch 3.11: Loss = 0.466415
Epoch 3.12: Loss = 0.381226
Epoch 3.13: Loss = 0.40976
Epoch 3.14: Loss = 0.418381
Epoch 3.15: Loss = 0.40448
Epoch 3.16: Loss = 0.398178
Epoch 3.17: Loss = 0.388824
Epoch 3.18: Loss = 0.441925
Epoch 3.19: Loss = 0.392609
Epoch 3.20: Loss = 0.442108
Epoch 3.21: Loss = 0.45488
Epoch 3.22: Loss = 0.472458
Epoch 3.23: Loss = 0.371536
Epoch 3.24: Loss = 0.427917
Epoch 3.25: Loss = 0.430481
Epoch 3.26: Loss = 0.382523
Epoch 3.27: Loss = 0.409882
Epoch 3.28: Loss = 0.34668
Epoch 3.29: Loss = 0.453659
Epoch 3.30: Loss = 0.386902
Epoch 3.31: Loss = 0.482849
Epoch 3.32: Loss = 0.450821
Epoch 3.33: Loss = 0.44278
Epoch 3.34: Loss = 0.375641
Epoch 3.35: Loss = 0.300781
Epoch 3.36: Loss = 0.383591
Epoch 3.37: Loss = 0.339066
Epoch 3.38: Loss = 0.46843
Epoch 3.39: Loss = 0.417023
Epoch 3.40: Loss = 0.392899
Epoch 3.41: Loss = 0.448456
Epoch 3.42: Loss = 0.42482
Epoch 3.43: Loss = 0.418579
Epoch 3.44: Loss = 0.44342
Epoch 3.45: Loss = 0.448532
Epoch 3.46: Loss = 0.395737
Epoch 3.47: Loss = 0.345352
Epoch 3.48: Loss = 0.466934
Epoch 3.49: Loss = 0.43309
Epoch 3.50: Loss = 0.351746
Epoch 3.51: Loss = 0.461044
Epoch 3.52: Loss = 0.342438
Epoch 3.53: Loss = 0.45575
Epoch 3.54: Loss = 0.449875
Epoch 3.55: Loss = 0.433273
Epoch 3.56: Loss = 0.442963
Epoch 3.57: Loss = 0.350662
Epoch 3.58: Loss = 0.396606
Epoch 3.59: Loss = 0.430679
Epoch 3.60: Loss = 0.442749
Epoch 3.61: Loss = 0.461731
Epoch 3.62: Loss = 0.398834
Epoch 3.63: Loss = 0.381195
Epoch 3.64: Loss = 0.449646
Epoch 3.65: Loss = 0.403091
Epoch 3.66: Loss = 0.398483
Epoch 3.67: Loss = 0.378769
Epoch 3.68: Loss = 0.487015
Epoch 3.69: Loss = 0.46463
Epoch 3.70: Loss = 0.423019
Epoch 3.71: Loss = 0.347702
Epoch 3.72: Loss = 0.326233
Epoch 3.73: Loss = 0.418243
Epoch 3.74: Loss = 0.35968
Epoch 3.75: Loss = 0.400894
Epoch 3.76: Loss = 0.433868
Epoch 3.77: Loss = 0.382858
Epoch 3.78: Loss = 0.384354
Epoch 3.79: Loss = 0.416626
Epoch 3.80: Loss = 0.355133
Epoch 3.81: Loss = 0.359879
Epoch 3.82: Loss = 0.440994
Epoch 3.83: Loss = 0.438416
Epoch 3.84: Loss = 0.327942
Epoch 3.85: Loss = 0.335434
Epoch 3.86: Loss = 0.31636
Epoch 3.87: Loss = 0.453033
Epoch 3.88: Loss = 0.441833
Epoch 3.89: Loss = 0.374023
Epoch 3.90: Loss = 0.447845
Epoch 3.91: Loss = 0.355255
Epoch 3.92: Loss = 0.392746
Epoch 3.93: Loss = 0.420517
Epoch 3.94: Loss = 0.382172
Epoch 3.95: Loss = 0.449417
Epoch 3.96: Loss = 0.458145
Epoch 3.97: Loss = 0.412491
Epoch 3.98: Loss = 0.420105
Epoch 3.99: Loss = 0.373077
Epoch 3.100: Loss = 0.446457
Epoch 3.101: Loss = 0.402481
Epoch 3.102: Loss = 0.417313
Epoch 3.103: Loss = 0.426132
Epoch 3.104: Loss = 0.353119
Epoch 3.105: Loss = 0.33049
Epoch 3.106: Loss = 0.367462
Epoch 3.107: Loss = 0.385529
Epoch 3.108: Loss = 0.364029
Epoch 3.109: Loss = 0.386597
Epoch 3.110: Loss = 0.365616
Epoch 3.111: Loss = 0.453049
Epoch 3.112: Loss = 0.442963
Epoch 3.113: Loss = 0.426178
Epoch 3.114: Loss = 0.345413
Epoch 3.115: Loss = 0.510193
Epoch 3.116: Loss = 0.370682
Epoch 3.117: Loss = 0.474823
Epoch 3.118: Loss = 0.389709
Epoch 3.119: Loss = 0.463043
Epoch 3.120: Loss = 0.329407
TRAIN LOSS = 0.408279
TRAIN ACC = 87.886 % (52734/60000)
Loss = 0.390976
Loss = 0.455643
Loss = 0.527267
Loss = 0.535599
Loss = 0.562469
Loss = 0.436539
Loss = 0.36467
Loss = 0.603012
Loss = 0.535141
Loss = 0.465057
Loss = 0.20639
Loss = 0.300583
Loss = 0.317703
Loss = 0.386673
Loss = 0.214203
Loss = 0.2939
Loss = 0.229034
Loss = 0.0735168
Loss = 0.250626
Loss = 0.528931
TEST LOSS = 0.383897
TEST ACC = 527.339 % (8891/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.324036
Epoch 4.2: Loss = 0.30722
Epoch 4.3: Loss = 0.367203
Epoch 4.4: Loss = 0.348892
Epoch 4.5: Loss = 0.384644
Epoch 4.6: Loss = 0.429489
Epoch 4.7: Loss = 0.373642
Epoch 4.8: Loss = 0.400009
Epoch 4.9: Loss = 0.409103
Epoch 4.10: Loss = 0.47789
Epoch 4.11: Loss = 0.444412
Epoch 4.12: Loss = 0.390808
Epoch 4.13: Loss = 0.373169
Epoch 4.14: Loss = 0.456726
Epoch 4.15: Loss = 0.408051
Epoch 4.16: Loss = 0.348282
Epoch 4.17: Loss = 0.494553
Epoch 4.18: Loss = 0.386429
Epoch 4.19: Loss = 0.361359
Epoch 4.20: Loss = 0.336304
Epoch 4.21: Loss = 0.359024
Epoch 4.22: Loss = 0.394791
Epoch 4.23: Loss = 0.337433
Epoch 4.24: Loss = 0.387268
Epoch 4.25: Loss = 0.395935
Epoch 4.26: Loss = 0.331757
Epoch 4.27: Loss = 0.441254
Epoch 4.28: Loss = 0.353729
Epoch 4.29: Loss = 0.404343
Epoch 4.30: Loss = 0.362701
Epoch 4.31: Loss = 0.414688
Epoch 4.32: Loss = 0.440659
Epoch 4.33: Loss = 0.390213
Epoch 4.34: Loss = 0.383148
Epoch 4.35: Loss = 0.376938
Epoch 4.36: Loss = 0.460724
Epoch 4.37: Loss = 0.327499
Epoch 4.38: Loss = 0.410309
Epoch 4.39: Loss = 0.38501
Epoch 4.40: Loss = 0.359161
Epoch 4.41: Loss = 0.322037
Epoch 4.42: Loss = 0.351242
Epoch 4.43: Loss = 0.444626
Epoch 4.44: Loss = 0.405228
Epoch 4.45: Loss = 0.40976
Epoch 4.46: Loss = 0.43045
Epoch 4.47: Loss = 0.473358
Epoch 4.48: Loss = 0.415695
Epoch 4.49: Loss = 0.413635
Epoch 4.50: Loss = 0.453735
Epoch 4.51: Loss = 0.450851
Epoch 4.52: Loss = 0.422211
Epoch 4.53: Loss = 0.394211
Epoch 4.54: Loss = 0.438049
Epoch 4.55: Loss = 0.341202
Epoch 4.56: Loss = 0.528229
Epoch 4.57: Loss = 0.40744
Epoch 4.58: Loss = 0.375641
Epoch 4.59: Loss = 0.376968
Epoch 4.60: Loss = 0.423889
Epoch 4.61: Loss = 0.386642
Epoch 4.62: Loss = 0.321915
Epoch 4.63: Loss = 0.368088
Epoch 4.64: Loss = 0.312912
Epoch 4.65: Loss = 0.434311
Epoch 4.66: Loss = 0.476212
Epoch 4.67: Loss = 0.387924
Epoch 4.68: Loss = 0.437836
Epoch 4.69: Loss = 0.331268
Epoch 4.70: Loss = 0.270538
Epoch 4.71: Loss = 0.358551
Epoch 4.72: Loss = 0.383118
Epoch 4.73: Loss = 0.494965
Epoch 4.74: Loss = 0.313675
Epoch 4.75: Loss = 0.362747
Epoch 4.76: Loss = 0.388351
Epoch 4.77: Loss = 0.421097
Epoch 4.78: Loss = 0.486023
Epoch 4.79: Loss = 0.387817
Epoch 4.80: Loss = 0.426941
Epoch 4.81: Loss = 0.415054
Epoch 4.82: Loss = 0.44928
Epoch 4.83: Loss = 0.399017
Epoch 4.84: Loss = 0.391174
Epoch 4.85: Loss = 0.344879
Epoch 4.86: Loss = 0.387589
Epoch 4.87: Loss = 0.287231
Epoch 4.88: Loss = 0.378647
Epoch 4.89: Loss = 0.407425
Epoch 4.90: Loss = 0.372833
Epoch 4.91: Loss = 0.365234
Epoch 4.92: Loss = 0.479095
Epoch 4.93: Loss = 0.353851
Epoch 4.94: Loss = 0.368835
Epoch 4.95: Loss = 0.446777
Epoch 4.96: Loss = 0.41629
Epoch 4.97: Loss = 0.313004
Epoch 4.98: Loss = 0.42598
Epoch 4.99: Loss = 0.366287
Epoch 4.100: Loss = 0.372177
Epoch 4.101: Loss = 0.3414
Epoch 4.102: Loss = 0.427185
Epoch 4.103: Loss = 0.424423
Epoch 4.104: Loss = 0.358017
Epoch 4.105: Loss = 0.4487
Epoch 4.106: Loss = 0.412338
Epoch 4.107: Loss = 0.384918
Epoch 4.108: Loss = 0.335358
Epoch 4.109: Loss = 0.410522
Epoch 4.110: Loss = 0.388428
Epoch 4.111: Loss = 0.337158
Epoch 4.112: Loss = 0.337479
Epoch 4.113: Loss = 0.393097
Epoch 4.114: Loss = 0.427048
Epoch 4.115: Loss = 0.383438
Epoch 4.116: Loss = 0.468918
Epoch 4.117: Loss = 0.413422
Epoch 4.118: Loss = 0.311752
Epoch 4.119: Loss = 0.309814
Epoch 4.120: Loss = 0.348969
TRAIN LOSS = 0.391418
TRAIN ACC = 88.8184 % (53293/60000)
Loss = 0.372177
Loss = 0.460083
Loss = 0.504456
Loss = 0.536072
Loss = 0.55748
Loss = 0.416916
Loss = 0.347855
Loss = 0.595825
Loss = 0.527847
Loss = 0.449692
Loss = 0.192734
Loss = 0.299881
Loss = 0.302963
Loss = 0.369339
Loss = 0.193512
Loss = 0.293808
Loss = 0.220352
Loss = 0.0675201
Loss = 0.235153
Loss = 0.533997
TEST LOSS = 0.373883
TEST ACC = 532.928 % (8943/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.402985
Epoch 5.2: Loss = 0.452011
Epoch 5.3: Loss = 0.391388
Epoch 5.4: Loss = 0.247391
Epoch 5.5: Loss = 0.412262
Epoch 5.6: Loss = 0.394745
Epoch 5.7: Loss = 0.415955
Epoch 5.8: Loss = 0.295181
Epoch 5.9: Loss = 0.319443
Epoch 5.10: Loss = 0.301392
Epoch 5.11: Loss = 0.309402
Epoch 5.12: Loss = 0.392151
Epoch 5.13: Loss = 0.404434
Epoch 5.14: Loss = 0.37233
Epoch 5.15: Loss = 0.3125
Epoch 5.16: Loss = 0.406143
Epoch 5.17: Loss = 0.490555
Epoch 5.18: Loss = 0.361298
Epoch 5.19: Loss = 0.433624
Epoch 5.20: Loss = 0.350372
Epoch 5.21: Loss = 0.343628
Epoch 5.22: Loss = 0.333252
Epoch 5.23: Loss = 0.374573
Epoch 5.24: Loss = 0.382858
Epoch 5.25: Loss = 0.334122
Epoch 5.26: Loss = 0.327759
Epoch 5.27: Loss = 0.385681
Epoch 5.28: Loss = 0.38176
Epoch 5.29: Loss = 0.327362
Epoch 5.30: Loss = 0.447037
Epoch 5.31: Loss = 0.431717
Epoch 5.32: Loss = 0.361191
Epoch 5.33: Loss = 0.411087
Epoch 5.34: Loss = 0.412079
Epoch 5.35: Loss = 0.318512
Epoch 5.36: Loss = 0.424484
Epoch 5.37: Loss = 0.39801
Epoch 5.38: Loss = 0.435669
Epoch 5.39: Loss = 0.404724
Epoch 5.40: Loss = 0.412064
Epoch 5.41: Loss = 0.369476
Epoch 5.42: Loss = 0.350571
Epoch 5.43: Loss = 0.357208
Epoch 5.44: Loss = 0.465256
Epoch 5.45: Loss = 0.464767
Epoch 5.46: Loss = 0.499191
Epoch 5.47: Loss = 0.477005
Epoch 5.48: Loss = 0.449875
Epoch 5.49: Loss = 0.326462
Epoch 5.50: Loss = 0.327972
Epoch 5.51: Loss = 0.346359
Epoch 5.52: Loss = 0.480743
Epoch 5.53: Loss = 0.485504
Epoch 5.54: Loss = 0.241074
Epoch 5.55: Loss = 0.347809
Epoch 5.56: Loss = 0.412521
Epoch 5.57: Loss = 0.402771
Epoch 5.58: Loss = 0.341431
Epoch 5.59: Loss = 0.361526
Epoch 5.60: Loss = 0.395447
Epoch 5.61: Loss = 0.46994
Epoch 5.62: Loss = 0.350189
Epoch 5.63: Loss = 0.386398
Epoch 5.64: Loss = 0.376846
Epoch 5.65: Loss = 0.375656
Epoch 5.66: Loss = 0.343246
Epoch 5.67: Loss = 0.378952
Epoch 5.68: Loss = 0.36055
Epoch 5.69: Loss = 0.492828
Epoch 5.70: Loss = 0.345993
Epoch 5.71: Loss = 0.422546
Epoch 5.72: Loss = 0.440643
Epoch 5.73: Loss = 0.364563
Epoch 5.74: Loss = 0.318024
Epoch 5.75: Loss = 0.460632
Epoch 5.76: Loss = 0.354721
Epoch 5.77: Loss = 0.514069
Epoch 5.78: Loss = 0.373459
Epoch 5.79: Loss = 0.401779
Epoch 5.80: Loss = 0.351761
Epoch 5.81: Loss = 0.387375
Epoch 5.82: Loss = 0.418152
Epoch 5.83: Loss = 0.367249
Epoch 5.84: Loss = 0.386444
Epoch 5.85: Loss = 0.379227
Epoch 5.86: Loss = 0.365417
Epoch 5.87: Loss = 0.336243
Epoch 5.88: Loss = 0.367401
Epoch 5.89: Loss = 0.425797
Epoch 5.90: Loss = 0.45517
Epoch 5.91: Loss = 0.422104
Epoch 5.92: Loss = 0.290344
Epoch 5.93: Loss = 0.469269
Epoch 5.94: Loss = 0.340988
Epoch 5.95: Loss = 0.343445
Epoch 5.96: Loss = 0.352402
Epoch 5.97: Loss = 0.390533
Epoch 5.98: Loss = 0.443115
Epoch 5.99: Loss = 0.327789
Epoch 5.100: Loss = 0.41214
Epoch 5.101: Loss = 0.314636
Epoch 5.102: Loss = 0.331009
Epoch 5.103: Loss = 0.368469
Epoch 5.104: Loss = 0.366135
Epoch 5.105: Loss = 0.310516
Epoch 5.106: Loss = 0.382385
Epoch 5.107: Loss = 0.421158
Epoch 5.108: Loss = 0.574127
Epoch 5.109: Loss = 0.34787
Epoch 5.110: Loss = 0.295502
Epoch 5.111: Loss = 0.30835
Epoch 5.112: Loss = 0.442245
Epoch 5.113: Loss = 0.329514
Epoch 5.114: Loss = 0.353592
Epoch 5.115: Loss = 0.399536
Epoch 5.116: Loss = 0.417923
Epoch 5.117: Loss = 0.425476
Epoch 5.118: Loss = 0.38945
Epoch 5.119: Loss = 0.341431
Epoch 5.120: Loss = 0.300995
TRAIN LOSS = 0.383331
TRAIN ACC = 89.3158 % (53592/60000)
Loss = 0.351685
Loss = 0.456024
Loss = 0.497726
Loss = 0.521545
Loss = 0.543839
Loss = 0.408615
Loss = 0.327484
Loss = 0.597336
Loss = 0.521561
Loss = 0.438324
Loss = 0.185104
Loss = 0.301544
Loss = 0.314133
Loss = 0.354462
Loss = 0.177582
Loss = 0.282715
Loss = 0.203751
Loss = 0.0505371
Loss = 0.246841
Loss = 0.521317
TEST LOSS = 0.365106
TEST ACC = 535.919 % (9021/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.400421
Epoch 6.2: Loss = 0.382462
Epoch 6.3: Loss = 0.367508
Epoch 6.4: Loss = 0.438324
Epoch 6.5: Loss = 0.425369
Epoch 6.6: Loss = 0.389816
Epoch 6.7: Loss = 0.300018
Epoch 6.8: Loss = 0.441818
Epoch 6.9: Loss = 0.331375
Epoch 6.10: Loss = 0.415527
Epoch 6.11: Loss = 0.484512
Epoch 6.12: Loss = 0.451096
Epoch 6.13: Loss = 0.381226
Epoch 6.14: Loss = 0.489319
Epoch 6.15: Loss = 0.399124
Epoch 6.16: Loss = 0.516174
Epoch 6.17: Loss = 0.368805
Epoch 6.18: Loss = 0.390427
Epoch 6.19: Loss = 0.434418
Epoch 6.20: Loss = 0.311722
Epoch 6.21: Loss = 0.42981
Epoch 6.22: Loss = 0.42189
Epoch 6.23: Loss = 0.341034
Epoch 6.24: Loss = 0.411789
Epoch 6.25: Loss = 0.5242
Epoch 6.26: Loss = 0.345978
Epoch 6.27: Loss = 0.40094
Epoch 6.28: Loss = 0.299301
Epoch 6.29: Loss = 0.391357
Epoch 6.30: Loss = 0.326416
Epoch 6.31: Loss = 0.381485
Epoch 6.32: Loss = 0.428848
Epoch 6.33: Loss = 0.363129
Epoch 6.34: Loss = 0.398712
Epoch 6.35: Loss = 0.373413
Epoch 6.36: Loss = 0.345352
Epoch 6.37: Loss = 0.449219
Epoch 6.38: Loss = 0.367004
Epoch 6.39: Loss = 0.39653
Epoch 6.40: Loss = 0.482513
Epoch 6.41: Loss = 0.397369
Epoch 6.42: Loss = 0.355057
Epoch 6.43: Loss = 0.31105
Epoch 6.44: Loss = 0.355835
Epoch 6.45: Loss = 0.265442
Epoch 6.46: Loss = 0.333435
Epoch 6.47: Loss = 0.347733
Epoch 6.48: Loss = 0.388336
Epoch 6.49: Loss = 0.314484
Epoch 6.50: Loss = 0.283875
Epoch 6.51: Loss = 0.421005
Epoch 6.52: Loss = 0.346939
Epoch 6.53: Loss = 0.27475
Epoch 6.54: Loss = 0.352509
Epoch 6.55: Loss = 0.328766
Epoch 6.56: Loss = 0.408722
Epoch 6.57: Loss = 0.371231
Epoch 6.58: Loss = 0.365891
Epoch 6.59: Loss = 0.309601
Epoch 6.60: Loss = 0.369812
Epoch 6.61: Loss = 0.382462
Epoch 6.62: Loss = 0.380325
Epoch 6.63: Loss = 0.360535
Epoch 6.64: Loss = 0.285934
Epoch 6.65: Loss = 0.39888
Epoch 6.66: Loss = 0.3638
Epoch 6.67: Loss = 0.297943
Epoch 6.68: Loss = 0.337494
Epoch 6.69: Loss = 0.37648
Epoch 6.70: Loss = 0.422165
Epoch 6.71: Loss = 0.323425
Epoch 6.72: Loss = 0.420273
Epoch 6.73: Loss = 0.363815
Epoch 6.74: Loss = 0.384476
Epoch 6.75: Loss = 0.517578
Epoch 6.76: Loss = 0.429504
Epoch 6.77: Loss = 0.433243
Epoch 6.78: Loss = 0.398575
Epoch 6.79: Loss = 0.310974
Epoch 6.80: Loss = 0.337555
Epoch 6.81: Loss = 0.448395
Epoch 6.82: Loss = 0.30719
Epoch 6.83: Loss = 0.420792
Epoch 6.84: Loss = 0.396545
Epoch 6.85: Loss = 0.28093
Epoch 6.86: Loss = 0.361465
Epoch 6.87: Loss = 0.343079
Epoch 6.88: Loss = 0.412918
Epoch 6.89: Loss = 0.369568
Epoch 6.90: Loss = 0.359497
Epoch 6.91: Loss = 0.389771
Epoch 6.92: Loss = 0.376602
Epoch 6.93: Loss = 0.370697
Epoch 6.94: Loss = 0.384964
Epoch 6.95: Loss = 0.384811
Epoch 6.96: Loss = 0.302246
Epoch 6.97: Loss = 0.301254
Epoch 6.98: Loss = 0.424026
Epoch 6.99: Loss = 0.327377
Epoch 6.100: Loss = 0.379303
Epoch 6.101: Loss = 0.435318
Epoch 6.102: Loss = 0.485336
Epoch 6.103: Loss = 0.414719
Epoch 6.104: Loss = 0.348282
Epoch 6.105: Loss = 0.37822
Epoch 6.106: Loss = 0.410172
Epoch 6.107: Loss = 0.391479
Epoch 6.108: Loss = 0.383713
Epoch 6.109: Loss = 0.342056
Epoch 6.110: Loss = 0.31691
Epoch 6.111: Loss = 0.346497
Epoch 6.112: Loss = 0.319275
Epoch 6.113: Loss = 0.361008
Epoch 6.114: Loss = 0.310699
Epoch 6.115: Loss = 0.394104
Epoch 6.116: Loss = 0.503174
Epoch 6.117: Loss = 0.305359
Epoch 6.118: Loss = 0.299896
Epoch 6.119: Loss = 0.333191
Epoch 6.120: Loss = 0.379852
TRAIN LOSS = 0.376892
TRAIN ACC = 89.7827 % (53872/60000)
Loss = 0.35144
Loss = 0.447586
Loss = 0.499557
Loss = 0.524857
Loss = 0.534576
Loss = 0.390778
Loss = 0.323166
Loss = 0.603363
Loss = 0.531418
Loss = 0.461212
Loss = 0.183029
Loss = 0.29393
Loss = 0.352692
Loss = 0.360123
Loss = 0.167145
Loss = 0.28392
Loss = 0.198639
Loss = 0.0489502
Loss = 0.264008
Loss = 0.513855
TEST LOSS = 0.366712
TEST ACC = 538.719 % (9054/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.346359
Epoch 7.2: Loss = 0.36171
Epoch 7.3: Loss = 0.371094
Epoch 7.4: Loss = 0.384232
Epoch 7.5: Loss = 0.318466
Epoch 7.6: Loss = 0.349121
Epoch 7.7: Loss = 0.247131
Epoch 7.8: Loss = 0.333435
Epoch 7.9: Loss = 0.394592
Epoch 7.10: Loss = 0.34906
Epoch 7.11: Loss = 0.365784
Epoch 7.12: Loss = 0.493835
Epoch 7.13: Loss = 0.382355
Epoch 7.14: Loss = 0.359802
Epoch 7.15: Loss = 0.39238
Epoch 7.16: Loss = 0.459244
Epoch 7.17: Loss = 0.369827
Epoch 7.18: Loss = 0.421295
Epoch 7.19: Loss = 0.352768
Epoch 7.20: Loss = 0.35585
Epoch 7.21: Loss = 0.371338
Epoch 7.22: Loss = 0.373917
Epoch 7.23: Loss = 0.428787
Epoch 7.24: Loss = 0.338211
Epoch 7.25: Loss = 0.387817
Epoch 7.26: Loss = 0.385727
Epoch 7.27: Loss = 0.459793
Epoch 7.28: Loss = 0.401855
Epoch 7.29: Loss = 0.343155
Epoch 7.30: Loss = 0.412659
Epoch 7.31: Loss = 0.509171
Epoch 7.32: Loss = 0.363815
Epoch 7.33: Loss = 0.354309
Epoch 7.34: Loss = 0.359421
Epoch 7.35: Loss = 0.314362
Epoch 7.36: Loss = 0.285522
Epoch 7.37: Loss = 0.408722
Epoch 7.38: Loss = 0.391373
Epoch 7.39: Loss = 0.3311
Epoch 7.40: Loss = 0.433105
Epoch 7.41: Loss = 0.38858
Epoch 7.42: Loss = 0.380814
Epoch 7.43: Loss = 0.345734
Epoch 7.44: Loss = 0.495529
Epoch 7.45: Loss = 0.3405
Epoch 7.46: Loss = 0.372437
Epoch 7.47: Loss = 0.407333
Epoch 7.48: Loss = 0.369141
Epoch 7.49: Loss = 0.544571
Epoch 7.50: Loss = 0.440659
Epoch 7.51: Loss = 0.441025
Epoch 7.52: Loss = 0.390106
Epoch 7.53: Loss = 0.409195
Epoch 7.54: Loss = 0.305878
Epoch 7.55: Loss = 0.508499
Epoch 7.56: Loss = 0.281174
Epoch 7.57: Loss = 0.34639
Epoch 7.58: Loss = 0.393967
Epoch 7.59: Loss = 0.350662
Epoch 7.60: Loss = 0.37471
Epoch 7.61: Loss = 0.408264
Epoch 7.62: Loss = 0.293228
Epoch 7.63: Loss = 0.461441
Epoch 7.64: Loss = 0.245468
Epoch 7.65: Loss = 0.432312
Epoch 7.66: Loss = 0.317444
Epoch 7.67: Loss = 0.469315
Epoch 7.68: Loss = 0.387619
Epoch 7.69: Loss = 0.291595
Epoch 7.70: Loss = 0.228638
Epoch 7.71: Loss = 0.370941
Epoch 7.72: Loss = 0.304916
Epoch 7.73: Loss = 0.410721
Epoch 7.74: Loss = 0.412537
Epoch 7.75: Loss = 0.216385
Epoch 7.76: Loss = 0.382324
Epoch 7.77: Loss = 0.323532
Epoch 7.78: Loss = 0.324753
Epoch 7.79: Loss = 0.355545
Epoch 7.80: Loss = 0.45314
Epoch 7.81: Loss = 0.395264
Epoch 7.82: Loss = 0.465881
Epoch 7.83: Loss = 0.340591
Epoch 7.84: Loss = 0.452377
Epoch 7.85: Loss = 0.43808
Epoch 7.86: Loss = 0.368042
Epoch 7.87: Loss = 0.328262
Epoch 7.88: Loss = 0.344345
Epoch 7.89: Loss = 0.301376
Epoch 7.90: Loss = 0.366287
Epoch 7.91: Loss = 0.229172
Epoch 7.92: Loss = 0.362686
Epoch 7.93: Loss = 0.434937
Epoch 7.94: Loss = 0.408081
Epoch 7.95: Loss = 0.291321
Epoch 7.96: Loss = 0.377518
Epoch 7.97: Loss = 0.347305
Epoch 7.98: Loss = 0.366684
Epoch 7.99: Loss = 0.439804
Epoch 7.100: Loss = 0.345413
Epoch 7.101: Loss = 0.401993
Epoch 7.102: Loss = 0.380768
Epoch 7.103: Loss = 0.430099
Epoch 7.104: Loss = 0.416077
Epoch 7.105: Loss = 0.349121
Epoch 7.106: Loss = 0.324203
Epoch 7.107: Loss = 0.35965
Epoch 7.108: Loss = 0.32579
Epoch 7.109: Loss = 0.386597
Epoch 7.110: Loss = 0.442764
Epoch 7.111: Loss = 0.4151
Epoch 7.112: Loss = 0.3022
Epoch 7.113: Loss = 0.365814
Epoch 7.114: Loss = 0.428009
Epoch 7.115: Loss = 0.366409
Epoch 7.116: Loss = 0.376678
Epoch 7.117: Loss = 0.32077
Epoch 7.118: Loss = 0.401138
Epoch 7.119: Loss = 0.354431
Epoch 7.120: Loss = 0.305054
TRAIN LOSS = 0.37413
TRAIN ACC = 90.0726 % (54046/60000)
Loss = 0.358734
Loss = 0.441025
Loss = 0.498947
Loss = 0.526886
Loss = 0.538193
Loss = 0.381729
Loss = 0.312576
Loss = 0.615326
Loss = 0.545456
Loss = 0.464432
Loss = 0.16658
Loss = 0.303925
Loss = 0.339569
Loss = 0.358047
Loss = 0.172211
Loss = 0.261261
Loss = 0.198044
Loss = 0.0524597
Loss = 0.25351
Loss = 0.500885
TEST LOSS = 0.36449
TEST ACC = 540.459 % (9040/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.388916
Epoch 8.2: Loss = 0.340973
Epoch 8.3: Loss = 0.463028
Epoch 8.4: Loss = 0.330093
Epoch 8.5: Loss = 0.339539
Epoch 8.6: Loss = 0.453568
Epoch 8.7: Loss = 0.334351
Epoch 8.8: Loss = 0.454926
Epoch 8.9: Loss = 0.495468
Epoch 8.10: Loss = 0.42099
Epoch 8.11: Loss = 0.362106
Epoch 8.12: Loss = 0.442825
Epoch 8.13: Loss = 0.396042
Epoch 8.14: Loss = 0.288452
Epoch 8.15: Loss = 0.311844
Epoch 8.16: Loss = 0.493256
Epoch 8.17: Loss = 0.214279
Epoch 8.18: Loss = 0.356857
Epoch 8.19: Loss = 0.287094
Epoch 8.20: Loss = 0.452011
Epoch 8.21: Loss = 0.411209
Epoch 8.22: Loss = 0.363663
Epoch 8.23: Loss = 0.416199
Epoch 8.24: Loss = 0.405014
Epoch 8.25: Loss = 0.349167
Epoch 8.26: Loss = 0.275818
Epoch 8.27: Loss = 0.460602
Epoch 8.28: Loss = 0.443222
Epoch 8.29: Loss = 0.475128
Epoch 8.30: Loss = 0.539337
Epoch 8.31: Loss = 0.376053
Epoch 8.32: Loss = 0.336121
Epoch 8.33: Loss = 0.391312
Epoch 8.34: Loss = 0.403687
Epoch 8.35: Loss = 0.282333
Epoch 8.36: Loss = 0.355957
Epoch 8.37: Loss = 0.271103
Epoch 8.38: Loss = 0.360611
Epoch 8.39: Loss = 0.295898
Epoch 8.40: Loss = 0.44928
Epoch 8.41: Loss = 0.457718
Epoch 8.42: Loss = 0.436768
Epoch 8.43: Loss = 0.223145
Epoch 8.44: Loss = 0.391556
Epoch 8.45: Loss = 0.277115
Epoch 8.46: Loss = 0.342957
Epoch 8.47: Loss = 0.384552
Epoch 8.48: Loss = 0.297119
Epoch 8.49: Loss = 0.496033
Epoch 8.50: Loss = 0.440933
Epoch 8.51: Loss = 0.35759
Epoch 8.52: Loss = 0.438004
Epoch 8.53: Loss = 0.32782
Epoch 8.54: Loss = 0.364136
Epoch 8.55: Loss = 0.350418
Epoch 8.56: Loss = 0.313126
Epoch 8.57: Loss = 0.392365
Epoch 8.58: Loss = 0.348663
Epoch 8.59: Loss = 0.330475
Epoch 8.60: Loss = 0.229706
Epoch 8.61: Loss = 0.328506
Epoch 8.62: Loss = 0.36615
Epoch 8.63: Loss = 0.343079
Epoch 8.64: Loss = 0.308716
Epoch 8.65: Loss = 0.341385
Epoch 8.66: Loss = 0.305603
Epoch 8.67: Loss = 0.34668
Epoch 8.68: Loss = 0.370636
Epoch 8.69: Loss = 0.337662
Epoch 8.70: Loss = 0.349625
Epoch 8.71: Loss = 0.371414
Epoch 8.72: Loss = 0.306396
Epoch 8.73: Loss = 0.365799
Epoch 8.74: Loss = 0.538422
Epoch 8.75: Loss = 0.350922
Epoch 8.76: Loss = 0.352051
Epoch 8.77: Loss = 0.383606
Epoch 8.78: Loss = 0.437546
Epoch 8.79: Loss = 0.371735
Epoch 8.80: Loss = 0.350006
Epoch 8.81: Loss = 0.404953
Epoch 8.82: Loss = 0.367874
Epoch 8.83: Loss = 0.382645
Epoch 8.84: Loss = 0.33493
Epoch 8.85: Loss = 0.321487
Epoch 8.86: Loss = 0.299789
Epoch 8.87: Loss = 0.487656
Epoch 8.88: Loss = 0.349991
Epoch 8.89: Loss = 0.343307
Epoch 8.90: Loss = 0.413025
Epoch 8.91: Loss = 0.533661
Epoch 8.92: Loss = 0.417404
Epoch 8.93: Loss = 0.297379
Epoch 8.94: Loss = 0.44043
Epoch 8.95: Loss = 0.342804
Epoch 8.96: Loss = 0.365295
Epoch 8.97: Loss = 0.408493
Epoch 8.98: Loss = 0.370651
Epoch 8.99: Loss = 0.409988
Epoch 8.100: Loss = 0.437073
Epoch 8.101: Loss = 0.354034
Epoch 8.102: Loss = 0.376587
Epoch 8.103: Loss = 0.38913
Epoch 8.104: Loss = 0.289902
Epoch 8.105: Loss = 0.321625
Epoch 8.106: Loss = 0.376282
Epoch 8.107: Loss = 0.355637
Epoch 8.108: Loss = 0.346024
Epoch 8.109: Loss = 0.324326
Epoch 8.110: Loss = 0.366547
Epoch 8.111: Loss = 0.270432
Epoch 8.112: Loss = 0.401138
Epoch 8.113: Loss = 0.305466
Epoch 8.114: Loss = 0.480988
Epoch 8.115: Loss = 0.338562
Epoch 8.116: Loss = 0.377502
Epoch 8.117: Loss = 0.367065
Epoch 8.118: Loss = 0.421265
Epoch 8.119: Loss = 0.242737
Epoch 8.120: Loss = 0.318008
TRAIN LOSS = 0.37056
TRAIN ACC = 90.3259 % (54198/60000)
Loss = 0.338272
Loss = 0.438812
Loss = 0.520142
Loss = 0.519272
Loss = 0.527725
Loss = 0.3685
Loss = 0.315826
Loss = 0.612244
Loss = 0.526672
Loss = 0.459396
Loss = 0.170059
Loss = 0.306503
Loss = 0.336044
Loss = 0.35228
Loss = 0.152222
Loss = 0.27594
Loss = 0.205536
Loss = 0.0505524
Loss = 0.264877
Loss = 0.470184
TEST LOSS = 0.360553
TEST ACC = 541.978 % (9081/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.396729
Epoch 9.2: Loss = 0.367081
Epoch 9.3: Loss = 0.348373
Epoch 9.4: Loss = 0.480865
Epoch 9.5: Loss = 0.300705
Epoch 9.6: Loss = 0.345428
Epoch 9.7: Loss = 0.433105
Epoch 9.8: Loss = 0.277481
Epoch 9.9: Loss = 0.350067
Epoch 9.10: Loss = 0.341507
Epoch 9.11: Loss = 0.354919
Epoch 9.12: Loss = 0.404526
Epoch 9.13: Loss = 0.34906
Epoch 9.14: Loss = 0.312943
Epoch 9.15: Loss = 0.460785
Epoch 9.16: Loss = 0.317245
Epoch 9.17: Loss = 0.357651
Epoch 9.18: Loss = 0.370422
Epoch 9.19: Loss = 0.293106
Epoch 9.20: Loss = 0.329926
Epoch 9.21: Loss = 0.500565
Epoch 9.22: Loss = 0.465073
Epoch 9.23: Loss = 0.433853
Epoch 9.24: Loss = 0.511185
Epoch 9.25: Loss = 0.354675
Epoch 9.26: Loss = 0.284912
Epoch 9.27: Loss = 0.295441
Epoch 9.28: Loss = 0.370285
Epoch 9.29: Loss = 0.3461
Epoch 9.30: Loss = 0.348145
Epoch 9.31: Loss = 0.300018
Epoch 9.32: Loss = 0.30011
Epoch 9.33: Loss = 0.271088
Epoch 9.34: Loss = 0.382599
Epoch 9.35: Loss = 0.393372
Epoch 9.36: Loss = 0.376389
Epoch 9.37: Loss = 0.364273
Epoch 9.38: Loss = 0.455719
Epoch 9.39: Loss = 0.315506
Epoch 9.40: Loss = 0.346237
Epoch 9.41: Loss = 0.455841
Epoch 9.42: Loss = 0.483292
Epoch 9.43: Loss = 0.377106
Epoch 9.44: Loss = 0.259933
Epoch 9.45: Loss = 0.452286
Epoch 9.46: Loss = 0.378998
Epoch 9.47: Loss = 0.287354
Epoch 9.48: Loss = 0.57872
Epoch 9.49: Loss = 0.339935
Epoch 9.50: Loss = 0.337631
Epoch 9.51: Loss = 0.285339
Epoch 9.52: Loss = 0.370483
Epoch 9.53: Loss = 0.468735
Epoch 9.54: Loss = 0.310425
Epoch 9.55: Loss = 0.374847
Epoch 9.56: Loss = 0.324951
Epoch 9.57: Loss = 0.292191
Epoch 9.58: Loss = 0.465637
Epoch 9.59: Loss = 0.406219
Epoch 9.60: Loss = 0.440155
Epoch 9.61: Loss = 0.308472
Epoch 9.62: Loss = 0.3647
Epoch 9.63: Loss = 0.364639
Epoch 9.64: Loss = 0.350983
Epoch 9.65: Loss = 0.241302
Epoch 9.66: Loss = 0.300507
Epoch 9.67: Loss = 0.346497
Epoch 9.68: Loss = 0.342224
Epoch 9.69: Loss = 0.396088
Epoch 9.70: Loss = 0.337921
Epoch 9.71: Loss = 0.453766
Epoch 9.72: Loss = 0.353333
Epoch 9.73: Loss = 0.32399
Epoch 9.74: Loss = 0.383972
Epoch 9.75: Loss = 0.361969
Epoch 9.76: Loss = 0.365021
Epoch 9.77: Loss = 0.438156
Epoch 9.78: Loss = 0.348526
Epoch 9.79: Loss = 0.337997
Epoch 9.80: Loss = 0.28569
Epoch 9.81: Loss = 0.333771
Epoch 9.82: Loss = 0.382599
Epoch 9.83: Loss = 0.377853
Epoch 9.84: Loss = 0.384079
Epoch 9.85: Loss = 0.335861
Epoch 9.86: Loss = 0.333603
Epoch 9.87: Loss = 0.422592
Epoch 9.88: Loss = 0.401077
Epoch 9.89: Loss = 0.359406
Epoch 9.90: Loss = 0.45462
Epoch 9.91: Loss = 0.337875
Epoch 9.92: Loss = 0.493439
Epoch 9.93: Loss = 0.409836
Epoch 9.94: Loss = 0.537048
Epoch 9.95: Loss = 0.448212
Epoch 9.96: Loss = 0.373718
Epoch 9.97: Loss = 0.411606
Epoch 9.98: Loss = 0.343292
Epoch 9.99: Loss = 0.360138
Epoch 9.100: Loss = 0.323425
Epoch 9.101: Loss = 0.476044
Epoch 9.102: Loss = 0.426544
Epoch 9.103: Loss = 0.302841
Epoch 9.104: Loss = 0.409531
Epoch 9.105: Loss = 0.417816
Epoch 9.106: Loss = 0.411118
Epoch 9.107: Loss = 0.352478
Epoch 9.108: Loss = 0.253372
Epoch 9.109: Loss = 0.324799
Epoch 9.110: Loss = 0.359222
Epoch 9.111: Loss = 0.258057
Epoch 9.112: Loss = 0.385818
Epoch 9.113: Loss = 0.331467
Epoch 9.114: Loss = 0.395233
Epoch 9.115: Loss = 0.25386
Epoch 9.116: Loss = 0.424438
Epoch 9.117: Loss = 0.407028
Epoch 9.118: Loss = 0.408432
Epoch 9.119: Loss = 0.429398
Epoch 9.120: Loss = 0.310654
TRAIN LOSS = 0.370544
TRAIN ACC = 90.5014 % (54303/60000)
Loss = 0.316498
Loss = 0.420425
Loss = 0.494583
Loss = 0.509399
Loss = 0.505859
Loss = 0.385239
Loss = 0.308105
Loss = 0.600708
Loss = 0.520493
Loss = 0.45343
Loss = 0.163269
Loss = 0.284012
Loss = 0.337082
Loss = 0.34996
Loss = 0.150375
Loss = 0.27359
Loss = 0.189667
Loss = 0.0490265
Loss = 0.25206
Loss = 0.468597
TEST LOSS = 0.351619
TEST ACC = 543.03 % (9089/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.444214
Epoch 10.2: Loss = 0.303757
Epoch 10.3: Loss = 0.475052
Epoch 10.4: Loss = 0.425171
Epoch 10.5: Loss = 0.348846
Epoch 10.6: Loss = 0.409561
Epoch 10.7: Loss = 0.3526
Epoch 10.8: Loss = 0.256927
Epoch 10.9: Loss = 0.350388
Epoch 10.10: Loss = 0.406952
Epoch 10.11: Loss = 0.367584
Epoch 10.12: Loss = 0.478241
Epoch 10.13: Loss = 0.419754
Epoch 10.14: Loss = 0.461075
Epoch 10.15: Loss = 0.319168
Epoch 10.16: Loss = 0.435089
Epoch 10.17: Loss = 0.322449
Epoch 10.18: Loss = 0.259262
Epoch 10.19: Loss = 0.300217
Epoch 10.20: Loss = 0.226532
Epoch 10.21: Loss = 0.36293
Epoch 10.22: Loss = 0.356949
Epoch 10.23: Loss = 0.42807
Epoch 10.24: Loss = 0.305923
Epoch 10.25: Loss = 0.331497
Epoch 10.26: Loss = 0.389862
Epoch 10.27: Loss = 0.21579
Epoch 10.28: Loss = 0.303772
Epoch 10.29: Loss = 0.461578
Epoch 10.30: Loss = 0.409119
Epoch 10.31: Loss = 0.383713
Epoch 10.32: Loss = 0.413269
Epoch 10.33: Loss = 0.422195
Epoch 10.34: Loss = 0.503036
Epoch 10.35: Loss = 0.412918
Epoch 10.36: Loss = 0.496078
Epoch 10.37: Loss = 0.459229
Epoch 10.38: Loss = 0.352676
Epoch 10.39: Loss = 0.410339
Epoch 10.40: Loss = 0.449966
Epoch 10.41: Loss = 0.317169
Epoch 10.42: Loss = 0.338455
Epoch 10.43: Loss = 0.289597
Epoch 10.44: Loss = 0.422089
Epoch 10.45: Loss = 0.435028
Epoch 10.46: Loss = 0.386169
Epoch 10.47: Loss = 0.354446
Epoch 10.48: Loss = 0.371216
Epoch 10.49: Loss = 0.421371
Epoch 10.50: Loss = 0.352707
Epoch 10.51: Loss = 0.273224
Epoch 10.52: Loss = 0.265793
Epoch 10.53: Loss = 0.332031
Epoch 10.54: Loss = 0.333313
Epoch 10.55: Loss = 0.382629
Epoch 10.56: Loss = 0.323273
Epoch 10.57: Loss = 0.317001
Epoch 10.58: Loss = 0.251633
Epoch 10.59: Loss = 0.421829
Epoch 10.60: Loss = 0.361633
Epoch 10.61: Loss = 0.320389
Epoch 10.62: Loss = 0.351532
Epoch 10.63: Loss = 0.315552
Epoch 10.64: Loss = 0.301193
Epoch 10.65: Loss = 0.425674
Epoch 10.66: Loss = 0.402405
Epoch 10.67: Loss = 0.390961
Epoch 10.68: Loss = 0.384338
Epoch 10.69: Loss = 0.332642
Epoch 10.70: Loss = 0.31665
Epoch 10.71: Loss = 0.351288
Epoch 10.72: Loss = 0.355042
Epoch 10.73: Loss = 0.358353
Epoch 10.74: Loss = 0.352264
Epoch 10.75: Loss = 0.341537
Epoch 10.76: Loss = 0.371002
Epoch 10.77: Loss = 0.387909
Epoch 10.78: Loss = 0.394119
Epoch 10.79: Loss = 0.467667
Epoch 10.80: Loss = 0.314133
Epoch 10.81: Loss = 0.427765
Epoch 10.82: Loss = 0.390411
Epoch 10.83: Loss = 0.357285
Epoch 10.84: Loss = 0.353241
Epoch 10.85: Loss = 0.318207
Epoch 10.86: Loss = 0.446487
Epoch 10.87: Loss = 0.356369
Epoch 10.88: Loss = 0.445068
Epoch 10.89: Loss = 0.325455
Epoch 10.90: Loss = 0.315201
Epoch 10.91: Loss = 0.379913
Epoch 10.92: Loss = 0.320633
Epoch 10.93: Loss = 0.404068
Epoch 10.94: Loss = 0.330414
Epoch 10.95: Loss = 0.402237
Epoch 10.96: Loss = 0.343216
Epoch 10.97: Loss = 0.300278
Epoch 10.98: Loss = 0.349182
Epoch 10.99: Loss = 0.426346
Epoch 10.100: Loss = 0.265472
Epoch 10.101: Loss = 0.268509
Epoch 10.102: Loss = 0.348358
Epoch 10.103: Loss = 0.379669
Epoch 10.104: Loss = 0.377609
Epoch 10.105: Loss = 0.50975
Epoch 10.106: Loss = 0.320114
Epoch 10.107: Loss = 0.36731
Epoch 10.108: Loss = 0.363159
Epoch 10.109: Loss = 0.319748
Epoch 10.110: Loss = 0.354843
Epoch 10.111: Loss = 0.431778
Epoch 10.112: Loss = 0.369034
Epoch 10.113: Loss = 0.313889
Epoch 10.114: Loss = 0.334625
Epoch 10.115: Loss = 0.443619
Epoch 10.116: Loss = 0.409927
Epoch 10.117: Loss = 0.413956
Epoch 10.118: Loss = 0.414749
Epoch 10.119: Loss = 0.346146
Epoch 10.120: Loss = 0.368942
TRAIN LOSS = 0.367752
TRAIN ACC = 90.7349 % (54443/60000)
Loss = 0.319839
Loss = 0.434647
Loss = 0.492386
Loss = 0.530396
Loss = 0.506424
Loss = 0.374161
Loss = 0.314987
Loss = 0.614029
Loss = 0.529373
Loss = 0.461685
Loss = 0.163651
Loss = 0.282547
Loss = 0.340393
Loss = 0.343201
Loss = 0.149109
Loss = 0.283813
Loss = 0.198334
Loss = 0.0464783
Loss = 0.259659
Loss = 0.454788
TEST LOSS = 0.354995
TEST ACC = 544.429 % (9118/10000)

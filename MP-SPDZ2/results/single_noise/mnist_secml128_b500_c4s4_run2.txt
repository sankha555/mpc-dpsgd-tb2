Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.2813
Epoch 1.2: Loss = 2.2368
Epoch 1.3: Loss = 2.25449
Epoch 1.4: Loss = 2.20782
Epoch 1.5: Loss = 2.19022
Epoch 1.6: Loss = 2.14468
Epoch 1.7: Loss = 2.12627
Epoch 1.8: Loss = 2.10429
Epoch 1.9: Loss = 2.08672
Epoch 1.10: Loss = 2.05363
Epoch 1.11: Loss = 2.03223
Epoch 1.12: Loss = 2.0192
Epoch 1.13: Loss = 2.00862
Epoch 1.14: Loss = 1.96323
Epoch 1.15: Loss = 1.95634
Epoch 1.16: Loss = 1.92905
Epoch 1.17: Loss = 1.9035
Epoch 1.18: Loss = 1.84595
Epoch 1.19: Loss = 1.83194
Epoch 1.20: Loss = 1.78877
Epoch 1.21: Loss = 1.78738
Epoch 1.22: Loss = 1.76003
Epoch 1.23: Loss = 1.69206
Epoch 1.24: Loss = 1.70354
Epoch 1.25: Loss = 1.66006
Epoch 1.26: Loss = 1.67223
Epoch 1.27: Loss = 1.60619
Epoch 1.28: Loss = 1.62386
Epoch 1.29: Loss = 1.56743
Epoch 1.30: Loss = 1.55132
Epoch 1.31: Loss = 1.58148
Epoch 1.32: Loss = 1.51857
Epoch 1.33: Loss = 1.47108
Epoch 1.34: Loss = 1.50146
Epoch 1.35: Loss = 1.4366
Epoch 1.36: Loss = 1.44446
Epoch 1.37: Loss = 1.4102
Epoch 1.38: Loss = 1.38336
Epoch 1.39: Loss = 1.30794
Epoch 1.40: Loss = 1.3461
Epoch 1.41: Loss = 1.29483
Epoch 1.42: Loss = 1.24489
Epoch 1.43: Loss = 1.23643
Epoch 1.44: Loss = 1.22693
Epoch 1.45: Loss = 1.18486
Epoch 1.46: Loss = 1.25732
Epoch 1.47: Loss = 1.1777
Epoch 1.48: Loss = 1.11604
Epoch 1.49: Loss = 1.16154
Epoch 1.50: Loss = 1.10445
Epoch 1.51: Loss = 1.12442
Epoch 1.52: Loss = 1.10007
Epoch 1.53: Loss = 1.08829
Epoch 1.54: Loss = 1.08832
Epoch 1.55: Loss = 1.05702
Epoch 1.56: Loss = 1.04848
Epoch 1.57: Loss = 1.07216
Epoch 1.58: Loss = 1.02431
Epoch 1.59: Loss = 1.01926
Epoch 1.60: Loss = 0.952438
Epoch 1.61: Loss = 0.963196
Epoch 1.62: Loss = 0.9431
Epoch 1.63: Loss = 0.962219
Epoch 1.64: Loss = 0.940765
Epoch 1.65: Loss = 0.862198
Epoch 1.66: Loss = 0.895004
Epoch 1.67: Loss = 0.819992
Epoch 1.68: Loss = 0.895828
Epoch 1.69: Loss = 0.875656
Epoch 1.70: Loss = 0.952011
Epoch 1.71: Loss = 0.830719
Epoch 1.72: Loss = 0.81572
Epoch 1.73: Loss = 0.743561
Epoch 1.74: Loss = 0.933334
Epoch 1.75: Loss = 0.833588
Epoch 1.76: Loss = 0.809937
Epoch 1.77: Loss = 0.866074
Epoch 1.78: Loss = 0.830597
Epoch 1.79: Loss = 0.794189
Epoch 1.80: Loss = 0.868713
Epoch 1.81: Loss = 0.816452
Epoch 1.82: Loss = 0.756058
Epoch 1.83: Loss = 0.744308
Epoch 1.84: Loss = 0.753967
Epoch 1.85: Loss = 0.686432
Epoch 1.86: Loss = 0.773819
Epoch 1.87: Loss = 0.708801
Epoch 1.88: Loss = 0.732376
Epoch 1.89: Loss = 0.76825
Epoch 1.90: Loss = 0.742569
Epoch 1.91: Loss = 0.679565
Epoch 1.92: Loss = 0.776474
Epoch 1.93: Loss = 0.64772
Epoch 1.94: Loss = 0.694626
Epoch 1.95: Loss = 0.781891
Epoch 1.96: Loss = 0.689758
Epoch 1.97: Loss = 0.676346
Epoch 1.98: Loss = 0.724243
Epoch 1.99: Loss = 0.716339
Epoch 1.100: Loss = 0.614426
Epoch 1.101: Loss = 0.676758
Epoch 1.102: Loss = 0.731949
Epoch 1.103: Loss = 0.648926
Epoch 1.104: Loss = 0.711517
Epoch 1.105: Loss = 0.631927
Epoch 1.106: Loss = 0.651764
Epoch 1.107: Loss = 0.608307
Epoch 1.108: Loss = 0.654236
Epoch 1.109: Loss = 0.693909
Epoch 1.110: Loss = 0.6091
Epoch 1.111: Loss = 0.63623
Epoch 1.112: Loss = 0.654495
Epoch 1.113: Loss = 0.58168
Epoch 1.114: Loss = 0.706787
Epoch 1.115: Loss = 0.64444
Epoch 1.116: Loss = 0.674973
Epoch 1.117: Loss = 0.604706
Epoch 1.118: Loss = 0.608887
Epoch 1.119: Loss = 0.749451
Epoch 1.120: Loss = 0.593109
TRAIN LOSS = 1.16306
TRAIN ACC = 68.3228 % (40996/60000)
Loss = 0.62825
Loss = 0.642502
Loss = 0.748581
Loss = 0.713455
Loss = 0.756622
Loss = 0.648743
Loss = 0.599274
Loss = 0.780975
Loss = 0.732574
Loss = 0.6698
Loss = 0.366684
Loss = 0.551117
Loss = 0.452896
Loss = 0.570816
Loss = 0.448456
Loss = 0.471008
Loss = 0.4272
Loss = 0.264633
Loss = 0.443634
Loss = 0.729034
TEST LOSS = 0.582313
TEST ACC = 409.959 % (8235/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.644531
Epoch 2.2: Loss = 0.589676
Epoch 2.3: Loss = 0.597336
Epoch 2.4: Loss = 0.596222
Epoch 2.5: Loss = 0.568237
Epoch 2.6: Loss = 0.650452
Epoch 2.7: Loss = 0.605423
Epoch 2.8: Loss = 0.620041
Epoch 2.9: Loss = 0.513916
Epoch 2.10: Loss = 0.575226
Epoch 2.11: Loss = 0.571732
Epoch 2.12: Loss = 0.571243
Epoch 2.13: Loss = 0.585266
Epoch 2.14: Loss = 0.583679
Epoch 2.15: Loss = 0.581879
Epoch 2.16: Loss = 0.529129
Epoch 2.17: Loss = 0.584518
Epoch 2.18: Loss = 0.582733
Epoch 2.19: Loss = 0.59845
Epoch 2.20: Loss = 0.634399
Epoch 2.21: Loss = 0.446732
Epoch 2.22: Loss = 0.519562
Epoch 2.23: Loss = 0.545044
Epoch 2.24: Loss = 0.561874
Epoch 2.25: Loss = 0.540039
Epoch 2.26: Loss = 0.56427
Epoch 2.27: Loss = 0.507751
Epoch 2.28: Loss = 0.538315
Epoch 2.29: Loss = 0.556839
Epoch 2.30: Loss = 0.555893
Epoch 2.31: Loss = 0.504425
Epoch 2.32: Loss = 0.604782
Epoch 2.33: Loss = 0.516815
Epoch 2.34: Loss = 0.506332
Epoch 2.35: Loss = 0.502762
Epoch 2.36: Loss = 0.538086
Epoch 2.37: Loss = 0.558548
Epoch 2.38: Loss = 0.534439
Epoch 2.39: Loss = 0.514099
Epoch 2.40: Loss = 0.535263
Epoch 2.41: Loss = 0.559219
Epoch 2.42: Loss = 0.516312
Epoch 2.43: Loss = 0.596771
Epoch 2.44: Loss = 0.517349
Epoch 2.45: Loss = 0.498611
Epoch 2.46: Loss = 0.552673
Epoch 2.47: Loss = 0.546432
Epoch 2.48: Loss = 0.585602
Epoch 2.49: Loss = 0.469894
Epoch 2.50: Loss = 0.51207
Epoch 2.51: Loss = 0.533112
Epoch 2.52: Loss = 0.393143
Epoch 2.53: Loss = 0.560532
Epoch 2.54: Loss = 0.57428
Epoch 2.55: Loss = 0.510712
Epoch 2.56: Loss = 0.54425
Epoch 2.57: Loss = 0.582687
Epoch 2.58: Loss = 0.530655
Epoch 2.59: Loss = 0.49501
Epoch 2.60: Loss = 0.5186
Epoch 2.61: Loss = 0.461426
Epoch 2.62: Loss = 0.519073
Epoch 2.63: Loss = 0.622986
Epoch 2.64: Loss = 0.501129
Epoch 2.65: Loss = 0.524902
Epoch 2.66: Loss = 0.578766
Epoch 2.67: Loss = 0.540665
Epoch 2.68: Loss = 0.501068
Epoch 2.69: Loss = 0.489105
Epoch 2.70: Loss = 0.556412
Epoch 2.71: Loss = 0.571548
Epoch 2.72: Loss = 0.459427
Epoch 2.73: Loss = 0.529724
Epoch 2.74: Loss = 0.488403
Epoch 2.75: Loss = 0.598007
Epoch 2.76: Loss = 0.531967
Epoch 2.77: Loss = 0.510727
Epoch 2.78: Loss = 0.520187
Epoch 2.79: Loss = 0.387421
Epoch 2.80: Loss = 0.499344
Epoch 2.81: Loss = 0.527145
Epoch 2.82: Loss = 0.519485
Epoch 2.83: Loss = 0.546417
Epoch 2.84: Loss = 0.655655
Epoch 2.85: Loss = 0.519089
Epoch 2.86: Loss = 0.495682
Epoch 2.87: Loss = 0.460556
Epoch 2.88: Loss = 0.550949
Epoch 2.89: Loss = 0.510742
Epoch 2.90: Loss = 0.635468
Epoch 2.91: Loss = 0.471481
Epoch 2.92: Loss = 0.518524
Epoch 2.93: Loss = 0.463638
Epoch 2.94: Loss = 0.506882
Epoch 2.95: Loss = 0.546158
Epoch 2.96: Loss = 0.470062
Epoch 2.97: Loss = 0.426819
Epoch 2.98: Loss = 0.545044
Epoch 2.99: Loss = 0.544556
Epoch 2.100: Loss = 0.527786
Epoch 2.101: Loss = 0.497971
Epoch 2.102: Loss = 0.50795
Epoch 2.103: Loss = 0.494888
Epoch 2.104: Loss = 0.482056
Epoch 2.105: Loss = 0.523331
Epoch 2.106: Loss = 0.439606
Epoch 2.107: Loss = 0.481674
Epoch 2.108: Loss = 0.537369
Epoch 2.109: Loss = 0.521622
Epoch 2.110: Loss = 0.513748
Epoch 2.111: Loss = 0.413284
Epoch 2.112: Loss = 0.416611
Epoch 2.113: Loss = 0.52478
Epoch 2.114: Loss = 0.517029
Epoch 2.115: Loss = 0.525284
Epoch 2.116: Loss = 0.409302
Epoch 2.117: Loss = 0.510406
Epoch 2.118: Loss = 0.45607
Epoch 2.119: Loss = 0.539886
Epoch 2.120: Loss = 0.45134
TRAIN LOSS = 0.530273
TRAIN ACC = 83.8135 % (50291/60000)
Loss = 0.448288
Loss = 0.531448
Loss = 0.606003
Loss = 0.589935
Loss = 0.664108
Loss = 0.515213
Loss = 0.448273
Loss = 0.686768
Loss = 0.620728
Loss = 0.546249
Loss = 0.247208
Loss = 0.417175
Loss = 0.378189
Loss = 0.464249
Loss = 0.287354
Loss = 0.364883
Loss = 0.325226
Loss = 0.126587
Loss = 0.303345
Loss = 0.637558
TEST LOSS = 0.460439
TEST ACC = 502.91 % (8588/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.443451
Epoch 3.2: Loss = 0.403046
Epoch 3.3: Loss = 0.488617
Epoch 3.4: Loss = 0.490829
Epoch 3.5: Loss = 0.424759
Epoch 3.6: Loss = 0.497086
Epoch 3.7: Loss = 0.470291
Epoch 3.8: Loss = 0.559723
Epoch 3.9: Loss = 0.515778
Epoch 3.10: Loss = 0.518555
Epoch 3.11: Loss = 0.456573
Epoch 3.12: Loss = 0.464096
Epoch 3.13: Loss = 0.414383
Epoch 3.14: Loss = 0.496826
Epoch 3.15: Loss = 0.515854
Epoch 3.16: Loss = 0.493118
Epoch 3.17: Loss = 0.49382
Epoch 3.18: Loss = 0.403717
Epoch 3.19: Loss = 0.382233
Epoch 3.20: Loss = 0.470428
Epoch 3.21: Loss = 0.448776
Epoch 3.22: Loss = 0.441116
Epoch 3.23: Loss = 0.478516
Epoch 3.24: Loss = 0.513168
Epoch 3.25: Loss = 0.509399
Epoch 3.26: Loss = 0.512756
Epoch 3.27: Loss = 0.543213
Epoch 3.28: Loss = 0.444641
Epoch 3.29: Loss = 0.465927
Epoch 3.30: Loss = 0.510376
Epoch 3.31: Loss = 0.477051
Epoch 3.32: Loss = 0.422073
Epoch 3.33: Loss = 0.511627
Epoch 3.34: Loss = 0.35704
Epoch 3.35: Loss = 0.552277
Epoch 3.36: Loss = 0.387817
Epoch 3.37: Loss = 0.43399
Epoch 3.38: Loss = 0.384171
Epoch 3.39: Loss = 0.408524
Epoch 3.40: Loss = 0.534836
Epoch 3.41: Loss = 0.451416
Epoch 3.42: Loss = 0.374344
Epoch 3.43: Loss = 0.433884
Epoch 3.44: Loss = 0.508591
Epoch 3.45: Loss = 0.456451
Epoch 3.46: Loss = 0.399261
Epoch 3.47: Loss = 0.453629
Epoch 3.48: Loss = 0.468781
Epoch 3.49: Loss = 0.437637
Epoch 3.50: Loss = 0.572281
Epoch 3.51: Loss = 0.487549
Epoch 3.52: Loss = 0.489883
Epoch 3.53: Loss = 0.418655
Epoch 3.54: Loss = 0.51268
Epoch 3.55: Loss = 0.507889
Epoch 3.56: Loss = 0.473022
Epoch 3.57: Loss = 0.497696
Epoch 3.58: Loss = 0.467377
Epoch 3.59: Loss = 0.466965
Epoch 3.60: Loss = 0.510422
Epoch 3.61: Loss = 0.446548
Epoch 3.62: Loss = 0.371292
Epoch 3.63: Loss = 0.442307
Epoch 3.64: Loss = 0.509338
Epoch 3.65: Loss = 0.442017
Epoch 3.66: Loss = 0.410706
Epoch 3.67: Loss = 0.400757
Epoch 3.68: Loss = 0.543884
Epoch 3.69: Loss = 0.464355
Epoch 3.70: Loss = 0.464767
Epoch 3.71: Loss = 0.541061
Epoch 3.72: Loss = 0.430344
Epoch 3.73: Loss = 0.515945
Epoch 3.74: Loss = 0.498199
Epoch 3.75: Loss = 0.412628
Epoch 3.76: Loss = 0.461792
Epoch 3.77: Loss = 0.431793
Epoch 3.78: Loss = 0.482147
Epoch 3.79: Loss = 0.47348
Epoch 3.80: Loss = 0.503342
Epoch 3.81: Loss = 0.454605
Epoch 3.82: Loss = 0.528961
Epoch 3.83: Loss = 0.546341
Epoch 3.84: Loss = 0.417755
Epoch 3.85: Loss = 0.442871
Epoch 3.86: Loss = 0.542694
Epoch 3.87: Loss = 0.45256
Epoch 3.88: Loss = 0.402023
Epoch 3.89: Loss = 0.412521
Epoch 3.90: Loss = 0.446381
Epoch 3.91: Loss = 0.535202
Epoch 3.92: Loss = 0.39653
Epoch 3.93: Loss = 0.587067
Epoch 3.94: Loss = 0.473068
Epoch 3.95: Loss = 0.500366
Epoch 3.96: Loss = 0.393555
Epoch 3.97: Loss = 0.391006
Epoch 3.98: Loss = 0.412064
Epoch 3.99: Loss = 0.510971
Epoch 3.100: Loss = 0.409973
Epoch 3.101: Loss = 0.419006
Epoch 3.102: Loss = 0.413071
Epoch 3.103: Loss = 0.408768
Epoch 3.104: Loss = 0.546204
Epoch 3.105: Loss = 0.446854
Epoch 3.106: Loss = 0.478973
Epoch 3.107: Loss = 0.393723
Epoch 3.108: Loss = 0.548325
Epoch 3.109: Loss = 0.451096
Epoch 3.110: Loss = 0.482941
Epoch 3.111: Loss = 0.46698
Epoch 3.112: Loss = 0.423386
Epoch 3.113: Loss = 0.39241
Epoch 3.114: Loss = 0.444366
Epoch 3.115: Loss = 0.491638
Epoch 3.116: Loss = 0.462936
Epoch 3.117: Loss = 0.454514
Epoch 3.118: Loss = 0.43425
Epoch 3.119: Loss = 0.447678
Epoch 3.120: Loss = 0.433548
TRAIN LOSS = 0.463928
TRAIN ACC = 86.2549 % (51755/60000)
Loss = 0.379517
Loss = 0.522018
Loss = 0.593994
Loss = 0.567719
Loss = 0.668823
Loss = 0.453186
Loss = 0.410004
Loss = 0.672043
Loss = 0.581863
Loss = 0.516953
Loss = 0.240479
Loss = 0.342743
Loss = 0.399261
Loss = 0.393982
Loss = 0.238861
Loss = 0.339676
Loss = 0.268921
Loss = 0.106506
Loss = 0.259979
Loss = 0.61644
TEST LOSS = 0.428648
TEST ACC = 517.549 % (8774/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.429398
Epoch 4.2: Loss = 0.412674
Epoch 4.3: Loss = 0.403412
Epoch 4.4: Loss = 0.428894
Epoch 4.5: Loss = 0.442871
Epoch 4.6: Loss = 0.53685
Epoch 4.7: Loss = 0.566299
Epoch 4.8: Loss = 0.380432
Epoch 4.9: Loss = 0.448044
Epoch 4.10: Loss = 0.39241
Epoch 4.11: Loss = 0.464111
Epoch 4.12: Loss = 0.488052
Epoch 4.13: Loss = 0.404816
Epoch 4.14: Loss = 0.420639
Epoch 4.15: Loss = 0.4207
Epoch 4.16: Loss = 0.369049
Epoch 4.17: Loss = 0.428497
Epoch 4.18: Loss = 0.511978
Epoch 4.19: Loss = 0.489471
Epoch 4.20: Loss = 0.54483
Epoch 4.21: Loss = 0.487564
Epoch 4.22: Loss = 0.423386
Epoch 4.23: Loss = 0.404922
Epoch 4.24: Loss = 0.463898
Epoch 4.25: Loss = 0.442459
Epoch 4.26: Loss = 0.452179
Epoch 4.27: Loss = 0.48967
Epoch 4.28: Loss = 0.409393
Epoch 4.29: Loss = 0.464096
Epoch 4.30: Loss = 0.496933
Epoch 4.31: Loss = 0.412582
Epoch 4.32: Loss = 0.459473
Epoch 4.33: Loss = 0.430206
Epoch 4.34: Loss = 0.44368
Epoch 4.35: Loss = 0.462128
Epoch 4.36: Loss = 0.450439
Epoch 4.37: Loss = 0.391907
Epoch 4.38: Loss = 0.343674
Epoch 4.39: Loss = 0.46199
Epoch 4.40: Loss = 0.536606
Epoch 4.41: Loss = 0.550537
Epoch 4.42: Loss = 0.465668
Epoch 4.43: Loss = 0.434433
Epoch 4.44: Loss = 0.42453
Epoch 4.45: Loss = 0.476501
Epoch 4.46: Loss = 0.389099
Epoch 4.47: Loss = 0.504227
Epoch 4.48: Loss = 0.424637
Epoch 4.49: Loss = 0.400528
Epoch 4.50: Loss = 0.417282
Epoch 4.51: Loss = 0.416763
Epoch 4.52: Loss = 0.402679
Epoch 4.53: Loss = 0.534988
Epoch 4.54: Loss = 0.365021
Epoch 4.55: Loss = 0.512268
Epoch 4.56: Loss = 0.398224
Epoch 4.57: Loss = 0.397446
Epoch 4.58: Loss = 0.455643
Epoch 4.59: Loss = 0.395645
Epoch 4.60: Loss = 0.544037
Epoch 4.61: Loss = 0.551224
Epoch 4.62: Loss = 0.44075
Epoch 4.63: Loss = 0.413132
Epoch 4.64: Loss = 0.502777
Epoch 4.65: Loss = 0.506683
Epoch 4.66: Loss = 0.460403
Epoch 4.67: Loss = 0.44986
Epoch 4.68: Loss = 0.472305
Epoch 4.69: Loss = 0.435028
Epoch 4.70: Loss = 0.37291
Epoch 4.71: Loss = 0.481155
Epoch 4.72: Loss = 0.5457
Epoch 4.73: Loss = 0.465424
Epoch 4.74: Loss = 0.389587
Epoch 4.75: Loss = 0.353333
Epoch 4.76: Loss = 0.490768
Epoch 4.77: Loss = 0.389114
Epoch 4.78: Loss = 0.542206
Epoch 4.79: Loss = 0.551483
Epoch 4.80: Loss = 0.41748
Epoch 4.81: Loss = 0.376114
Epoch 4.82: Loss = 0.462311
Epoch 4.83: Loss = 0.428986
Epoch 4.84: Loss = 0.381882
Epoch 4.85: Loss = 0.395798
Epoch 4.86: Loss = 0.545456
Epoch 4.87: Loss = 0.483337
Epoch 4.88: Loss = 0.419525
Epoch 4.89: Loss = 0.384903
Epoch 4.90: Loss = 0.429504
Epoch 4.91: Loss = 0.414063
Epoch 4.92: Loss = 0.44249
Epoch 4.93: Loss = 0.40358
Epoch 4.94: Loss = 0.483765
Epoch 4.95: Loss = 0.458313
Epoch 4.96: Loss = 0.45488
Epoch 4.97: Loss = 0.460907
Epoch 4.98: Loss = 0.425964
Epoch 4.99: Loss = 0.398621
Epoch 4.100: Loss = 0.370605
Epoch 4.101: Loss = 0.566681
Epoch 4.102: Loss = 0.371613
Epoch 4.103: Loss = 0.404404
Epoch 4.104: Loss = 0.408981
Epoch 4.105: Loss = 0.459274
Epoch 4.106: Loss = 0.537766
Epoch 4.107: Loss = 0.462326
Epoch 4.108: Loss = 0.432953
Epoch 4.109: Loss = 0.474396
Epoch 4.110: Loss = 0.324707
Epoch 4.111: Loss = 0.60405
Epoch 4.112: Loss = 0.462769
Epoch 4.113: Loss = 0.469315
Epoch 4.114: Loss = 0.402237
Epoch 4.115: Loss = 0.377823
Epoch 4.116: Loss = 0.445313
Epoch 4.117: Loss = 0.409424
Epoch 4.118: Loss = 0.461014
Epoch 4.119: Loss = 0.430847
Epoch 4.120: Loss = 0.462112
TRAIN LOSS = 0.447006
TRAIN ACC = 87.4466 % (52471/60000)
Loss = 0.362991
Loss = 0.508163
Loss = 0.589264
Loss = 0.564911
Loss = 0.650177
Loss = 0.440781
Loss = 0.406708
Loss = 0.670273
Loss = 0.586487
Loss = 0.493896
Loss = 0.213318
Loss = 0.309372
Loss = 0.397217
Loss = 0.402145
Loss = 0.222778
Loss = 0.364609
Loss = 0.256073
Loss = 0.085968
Loss = 0.278488
Loss = 0.590836
TEST LOSS = 0.419723
TEST ACC = 524.709 % (8845/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.424561
Epoch 5.2: Loss = 0.524826
Epoch 5.3: Loss = 0.401337
Epoch 5.4: Loss = 0.436462
Epoch 5.5: Loss = 0.44046
Epoch 5.6: Loss = 0.434509
Epoch 5.7: Loss = 0.415009
Epoch 5.8: Loss = 0.4767
Epoch 5.9: Loss = 0.436264
Epoch 5.10: Loss = 0.509537
Epoch 5.11: Loss = 0.390076
Epoch 5.12: Loss = 0.360733
Epoch 5.13: Loss = 0.479034
Epoch 5.14: Loss = 0.534195
Epoch 5.15: Loss = 0.354721
Epoch 5.16: Loss = 0.454742
Epoch 5.17: Loss = 0.423279
Epoch 5.18: Loss = 0.469101
Epoch 5.19: Loss = 0.425568
Epoch 5.20: Loss = 0.36731
Epoch 5.21: Loss = 0.426193
Epoch 5.22: Loss = 0.421631
Epoch 5.23: Loss = 0.447311
Epoch 5.24: Loss = 0.528473
Epoch 5.25: Loss = 0.397598
Epoch 5.26: Loss = 0.401535
Epoch 5.27: Loss = 0.462875
Epoch 5.28: Loss = 0.463516
Epoch 5.29: Loss = 0.369873
Epoch 5.30: Loss = 0.354385
Epoch 5.31: Loss = 0.44931
Epoch 5.32: Loss = 0.414688
Epoch 5.33: Loss = 0.366516
Epoch 5.34: Loss = 0.347061
Epoch 5.35: Loss = 0.387833
Epoch 5.36: Loss = 0.458969
Epoch 5.37: Loss = 0.437088
Epoch 5.38: Loss = 0.481384
Epoch 5.39: Loss = 0.507446
Epoch 5.40: Loss = 0.596085
Epoch 5.41: Loss = 0.358688
Epoch 5.42: Loss = 0.422516
Epoch 5.43: Loss = 0.367493
Epoch 5.44: Loss = 0.417145
Epoch 5.45: Loss = 0.416641
Epoch 5.46: Loss = 0.404907
Epoch 5.47: Loss = 0.547119
Epoch 5.48: Loss = 0.490677
Epoch 5.49: Loss = 0.565552
Epoch 5.50: Loss = 0.416672
Epoch 5.51: Loss = 0.350876
Epoch 5.52: Loss = 0.51265
Epoch 5.53: Loss = 0.486832
Epoch 5.54: Loss = 0.343185
Epoch 5.55: Loss = 0.371689
Epoch 5.56: Loss = 0.441666
Epoch 5.57: Loss = 0.477249
Epoch 5.58: Loss = 0.492493
Epoch 5.59: Loss = 0.42453
Epoch 5.60: Loss = 0.504333
Epoch 5.61: Loss = 0.521866
Epoch 5.62: Loss = 0.498825
Epoch 5.63: Loss = 0.386459
Epoch 5.64: Loss = 0.419189
Epoch 5.65: Loss = 0.444992
Epoch 5.66: Loss = 0.448669
Epoch 5.67: Loss = 0.483826
Epoch 5.68: Loss = 0.547089
Epoch 5.69: Loss = 0.466049
Epoch 5.70: Loss = 0.479034
Epoch 5.71: Loss = 0.404831
Epoch 5.72: Loss = 0.493927
Epoch 5.73: Loss = 0.502808
Epoch 5.74: Loss = 0.536118
Epoch 5.75: Loss = 0.407394
Epoch 5.76: Loss = 0.376129
Epoch 5.77: Loss = 0.368683
Epoch 5.78: Loss = 0.449554
Epoch 5.79: Loss = 0.397247
Epoch 5.80: Loss = 0.57132
Epoch 5.81: Loss = 0.468674
Epoch 5.82: Loss = 0.398468
Epoch 5.83: Loss = 0.458389
Epoch 5.84: Loss = 0.373016
Epoch 5.85: Loss = 0.376526
Epoch 5.86: Loss = 0.510483
Epoch 5.87: Loss = 0.468964
Epoch 5.88: Loss = 0.502533
Epoch 5.89: Loss = 0.439728
Epoch 5.90: Loss = 0.629211
Epoch 5.91: Loss = 0.4814
Epoch 5.92: Loss = 0.414413
Epoch 5.93: Loss = 0.410919
Epoch 5.94: Loss = 0.475113
Epoch 5.95: Loss = 0.422821
Epoch 5.96: Loss = 0.390198
Epoch 5.97: Loss = 0.513077
Epoch 5.98: Loss = 0.425812
Epoch 5.99: Loss = 0.49115
Epoch 5.100: Loss = 0.496185
Epoch 5.101: Loss = 0.363678
Epoch 5.102: Loss = 0.362961
Epoch 5.103: Loss = 0.423553
Epoch 5.104: Loss = 0.456345
Epoch 5.105: Loss = 0.571579
Epoch 5.106: Loss = 0.379074
Epoch 5.107: Loss = 0.544144
Epoch 5.108: Loss = 0.484131
Epoch 5.109: Loss = 0.570755
Epoch 5.110: Loss = 0.423431
Epoch 5.111: Loss = 0.406082
Epoch 5.112: Loss = 0.379578
Epoch 5.113: Loss = 0.35881
Epoch 5.114: Loss = 0.519043
Epoch 5.115: Loss = 0.407776
Epoch 5.116: Loss = 0.412979
Epoch 5.117: Loss = 0.423203
Epoch 5.118: Loss = 0.476379
Epoch 5.119: Loss = 0.423996
Epoch 5.120: Loss = 0.423248
TRAIN LOSS = 0.445435
TRAIN ACC = 87.8799 % (52731/60000)
Loss = 0.359314
Loss = 0.495972
Loss = 0.591476
Loss = 0.568985
Loss = 0.638077
Loss = 0.422241
Loss = 0.404205
Loss = 0.68544
Loss = 0.598907
Loss = 0.508835
Loss = 0.249313
Loss = 0.284958
Loss = 0.436325
Loss = 0.41304
Loss = 0.214752
Loss = 0.362106
Loss = 0.254639
Loss = 0.0855713
Loss = 0.283722
Loss = 0.646194
TEST LOSS = 0.425204
TEST ACC = 527.309 % (8869/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.478943
Epoch 6.2: Loss = 0.477661
Epoch 6.3: Loss = 0.475143
Epoch 6.4: Loss = 0.380173
Epoch 6.5: Loss = 0.410736
Epoch 6.6: Loss = 0.402512
Epoch 6.7: Loss = 0.474396
Epoch 6.8: Loss = 0.344086
Epoch 6.9: Loss = 0.347183
Epoch 6.10: Loss = 0.55069
Epoch 6.11: Loss = 0.58548
Epoch 6.12: Loss = 0.346939
Epoch 6.13: Loss = 0.456741
Epoch 6.14: Loss = 0.483795
Epoch 6.15: Loss = 0.405685
Epoch 6.16: Loss = 0.412445
Epoch 6.17: Loss = 0.455948
Epoch 6.18: Loss = 0.468369
Epoch 6.19: Loss = 0.378265
Epoch 6.20: Loss = 0.485001
Epoch 6.21: Loss = 0.526154
Epoch 6.22: Loss = 0.448624
Epoch 6.23: Loss = 0.485809
Epoch 6.24: Loss = 0.446228
Epoch 6.25: Loss = 0.495361
Epoch 6.26: Loss = 0.422348
Epoch 6.27: Loss = 0.420029
Epoch 6.28: Loss = 0.480255
Epoch 6.29: Loss = 0.536224
Epoch 6.30: Loss = 0.477982
Epoch 6.31: Loss = 0.457199
Epoch 6.32: Loss = 0.35582
Epoch 6.33: Loss = 0.56105
Epoch 6.34: Loss = 0.466873
Epoch 6.35: Loss = 0.467758
Epoch 6.36: Loss = 0.474426
Epoch 6.37: Loss = 0.468323
Epoch 6.38: Loss = 0.500015
Epoch 6.39: Loss = 0.400238
Epoch 6.40: Loss = 0.449432
Epoch 6.41: Loss = 0.475784
Epoch 6.42: Loss = 0.470032
Epoch 6.43: Loss = 0.540588
Epoch 6.44: Loss = 0.605652
Epoch 6.45: Loss = 0.46106
Epoch 6.46: Loss = 0.490631
Epoch 6.47: Loss = 0.439087
Epoch 6.48: Loss = 0.418015
Epoch 6.49: Loss = 0.45903
Epoch 6.50: Loss = 0.519714
Epoch 6.51: Loss = 0.497604
Epoch 6.52: Loss = 0.429367
Epoch 6.53: Loss = 0.389801
Epoch 6.54: Loss = 0.442368
Epoch 6.55: Loss = 0.432327
Epoch 6.56: Loss = 0.385651
Epoch 6.57: Loss = 0.380829
Epoch 6.58: Loss = 0.413132
Epoch 6.59: Loss = 0.394104
Epoch 6.60: Loss = 0.517014
Epoch 6.61: Loss = 0.369324
Epoch 6.62: Loss = 0.399002
Epoch 6.63: Loss = 0.335556
Epoch 6.64: Loss = 0.559723
Epoch 6.65: Loss = 0.479706
Epoch 6.66: Loss = 0.478302
Epoch 6.67: Loss = 0.347778
Epoch 6.68: Loss = 0.495163
Epoch 6.69: Loss = 0.441437
Epoch 6.70: Loss = 0.591431
Epoch 6.71: Loss = 0.45076
Epoch 6.72: Loss = 0.477081
Epoch 6.73: Loss = 0.49884
Epoch 6.74: Loss = 0.386215
Epoch 6.75: Loss = 0.603821
Epoch 6.76: Loss = 0.374222
Epoch 6.77: Loss = 0.499863
Epoch 6.78: Loss = 0.468063
Epoch 6.79: Loss = 0.336761
Epoch 6.80: Loss = 0.416412
Epoch 6.81: Loss = 0.497025
Epoch 6.82: Loss = 0.431717
Epoch 6.83: Loss = 0.496338
Epoch 6.84: Loss = 0.525955
Epoch 6.85: Loss = 0.524231
Epoch 6.86: Loss = 0.463074
Epoch 6.87: Loss = 0.451263
Epoch 6.88: Loss = 0.343674
Epoch 6.89: Loss = 0.426544
Epoch 6.90: Loss = 0.437683
Epoch 6.91: Loss = 0.454712
Epoch 6.92: Loss = 0.388321
Epoch 6.93: Loss = 0.472733
Epoch 6.94: Loss = 0.417938
Epoch 6.95: Loss = 0.483521
Epoch 6.96: Loss = 0.356415
Epoch 6.97: Loss = 0.46489
Epoch 6.98: Loss = 0.509674
Epoch 6.99: Loss = 0.590149
Epoch 6.100: Loss = 0.440964
Epoch 6.101: Loss = 0.488419
Epoch 6.102: Loss = 0.507599
Epoch 6.103: Loss = 0.485153
Epoch 6.104: Loss = 0.517395
Epoch 6.105: Loss = 0.643356
Epoch 6.106: Loss = 0.513977
Epoch 6.107: Loss = 0.499969
Epoch 6.108: Loss = 0.539902
Epoch 6.109: Loss = 0.449799
Epoch 6.110: Loss = 0.45433
Epoch 6.111: Loss = 0.437988
Epoch 6.112: Loss = 0.537247
Epoch 6.113: Loss = 0.425827
Epoch 6.114: Loss = 0.520599
Epoch 6.115: Loss = 0.383728
Epoch 6.116: Loss = 0.403107
Epoch 6.117: Loss = 0.427307
Epoch 6.118: Loss = 0.529297
Epoch 6.119: Loss = 0.412064
Epoch 6.120: Loss = 0.463913
TRAIN LOSS = 0.459579
TRAIN ACC = 88.0127 % (52810/60000)
Loss = 0.361496
Loss = 0.541412
Loss = 0.618515
Loss = 0.572983
Loss = 0.648346
Loss = 0.457764
Loss = 0.390182
Loss = 0.697388
Loss = 0.648041
Loss = 0.514893
Loss = 0.238785
Loss = 0.317825
Loss = 0.424133
Loss = 0.42421
Loss = 0.195938
Loss = 0.373444
Loss = 0.236481
Loss = 0.0823822
Loss = 0.281036
Loss = 0.671738
TEST LOSS = 0.434849
TEST ACC = 528.099 % (8892/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.338837
Epoch 7.2: Loss = 0.541656
Epoch 7.3: Loss = 0.460526
Epoch 7.4: Loss = 0.301865
Epoch 7.5: Loss = 0.423447
Epoch 7.6: Loss = 0.505173
Epoch 7.7: Loss = 0.465759
Epoch 7.8: Loss = 0.476822
Epoch 7.9: Loss = 0.439377
Epoch 7.10: Loss = 0.516464
Epoch 7.11: Loss = 0.432648
Epoch 7.12: Loss = 0.417038
Epoch 7.13: Loss = 0.47081
Epoch 7.14: Loss = 0.328583
Epoch 7.15: Loss = 0.314102
Epoch 7.16: Loss = 0.521484
Epoch 7.17: Loss = 0.458374
Epoch 7.18: Loss = 0.423645
Epoch 7.19: Loss = 0.462021
Epoch 7.20: Loss = 0.535431
Epoch 7.21: Loss = 0.478592
Epoch 7.22: Loss = 0.419662
Epoch 7.23: Loss = 0.48584
Epoch 7.24: Loss = 0.538956
Epoch 7.25: Loss = 0.499725
Epoch 7.26: Loss = 0.482239
Epoch 7.27: Loss = 0.441147
Epoch 7.28: Loss = 0.560013
Epoch 7.29: Loss = 0.429749
Epoch 7.30: Loss = 0.415268
Epoch 7.31: Loss = 0.482407
Epoch 7.32: Loss = 0.511902
Epoch 7.33: Loss = 0.509308
Epoch 7.34: Loss = 0.522903
Epoch 7.35: Loss = 0.530365
Epoch 7.36: Loss = 0.545074
Epoch 7.37: Loss = 0.472885
Epoch 7.38: Loss = 0.418442
Epoch 7.39: Loss = 0.50885
Epoch 7.40: Loss = 0.38266
Epoch 7.41: Loss = 0.333359
Epoch 7.42: Loss = 0.489349
Epoch 7.43: Loss = 0.446732
Epoch 7.44: Loss = 0.471863
Epoch 7.45: Loss = 0.442703
Epoch 7.46: Loss = 0.542542
Epoch 7.47: Loss = 0.425705
Epoch 7.48: Loss = 0.356567
Epoch 7.49: Loss = 0.470688
Epoch 7.50: Loss = 0.398117
Epoch 7.51: Loss = 0.594162
Epoch 7.52: Loss = 0.513123
Epoch 7.53: Loss = 0.494446
Epoch 7.54: Loss = 0.430222
Epoch 7.55: Loss = 0.476257
Epoch 7.56: Loss = 0.42688
Epoch 7.57: Loss = 0.473419
Epoch 7.58: Loss = 0.478134
Epoch 7.59: Loss = 0.486588
Epoch 7.60: Loss = 0.571777
Epoch 7.61: Loss = 0.382767
Epoch 7.62: Loss = 0.431229
Epoch 7.63: Loss = 0.352386
Epoch 7.64: Loss = 0.45047
Epoch 7.65: Loss = 0.529236
Epoch 7.66: Loss = 0.471344
Epoch 7.67: Loss = 0.44693
Epoch 7.68: Loss = 0.359711
Epoch 7.69: Loss = 0.54509
Epoch 7.70: Loss = 0.365845
Epoch 7.71: Loss = 0.394318
Epoch 7.72: Loss = 0.505341
Epoch 7.73: Loss = 0.459427
Epoch 7.74: Loss = 0.487335
Epoch 7.75: Loss = 0.458038
Epoch 7.76: Loss = 0.430328
Epoch 7.77: Loss = 0.552933
Epoch 7.78: Loss = 0.524017
Epoch 7.79: Loss = 0.502289
Epoch 7.80: Loss = 0.517776
Epoch 7.81: Loss = 0.426453
Epoch 7.82: Loss = 0.626007
Epoch 7.83: Loss = 0.441086
Epoch 7.84: Loss = 0.504456
Epoch 7.85: Loss = 0.366638
Epoch 7.86: Loss = 0.490326
Epoch 7.87: Loss = 0.509811
Epoch 7.88: Loss = 0.379318
Epoch 7.89: Loss = 0.558655
Epoch 7.90: Loss = 0.504166
Epoch 7.91: Loss = 0.452713
Epoch 7.92: Loss = 0.581284
Epoch 7.93: Loss = 0.413345
Epoch 7.94: Loss = 0.434555
Epoch 7.95: Loss = 0.405167
Epoch 7.96: Loss = 0.460785
Epoch 7.97: Loss = 0.501984
Epoch 7.98: Loss = 0.453888
Epoch 7.99: Loss = 0.577194
Epoch 7.100: Loss = 0.413391
Epoch 7.101: Loss = 0.322968
Epoch 7.102: Loss = 0.52916
Epoch 7.103: Loss = 0.511368
Epoch 7.104: Loss = 0.391266
Epoch 7.105: Loss = 0.479401
Epoch 7.106: Loss = 0.437393
Epoch 7.107: Loss = 0.58075
Epoch 7.108: Loss = 0.430145
Epoch 7.109: Loss = 0.456009
Epoch 7.110: Loss = 0.46727
Epoch 7.111: Loss = 0.417572
Epoch 7.112: Loss = 0.454697
Epoch 7.113: Loss = 0.46434
Epoch 7.114: Loss = 0.409042
Epoch 7.115: Loss = 0.440292
Epoch 7.116: Loss = 0.420898
Epoch 7.117: Loss = 0.52507
Epoch 7.118: Loss = 0.517822
Epoch 7.119: Loss = 0.448593
Epoch 7.120: Loss = 0.429245
TRAIN LOSS = 0.463272
TRAIN ACC = 88.2675 % (52963/60000)
Loss = 0.385803
Loss = 0.544724
Loss = 0.645721
Loss = 0.593048
Loss = 0.690765
Loss = 0.470795
Loss = 0.430008
Loss = 0.736511
Loss = 0.66011
Loss = 0.482651
Loss = 0.232773
Loss = 0.320068
Loss = 0.45636
Loss = 0.416565
Loss = 0.176315
Loss = 0.343796
Loss = 0.22023
Loss = 0.0741272
Loss = 0.287857
Loss = 0.675156
TEST LOSS = 0.442169
TEST ACC = 529.63 % (8906/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.355072
Epoch 8.2: Loss = 0.505707
Epoch 8.3: Loss = 0.418442
Epoch 8.4: Loss = 0.396194
Epoch 8.5: Loss = 0.492371
Epoch 8.6: Loss = 0.389008
Epoch 8.7: Loss = 0.387253
Epoch 8.8: Loss = 0.650055
Epoch 8.9: Loss = 0.46994
Epoch 8.10: Loss = 0.477371
Epoch 8.11: Loss = 0.460754
Epoch 8.12: Loss = 0.447632
Epoch 8.13: Loss = 0.504959
Epoch 8.14: Loss = 0.607651
Epoch 8.15: Loss = 0.639236
Epoch 8.16: Loss = 0.473267
Epoch 8.17: Loss = 0.425308
Epoch 8.18: Loss = 0.474686
Epoch 8.19: Loss = 0.567276
Epoch 8.20: Loss = 0.429642
Epoch 8.21: Loss = 0.508102
Epoch 8.22: Loss = 0.459152
Epoch 8.23: Loss = 0.382065
Epoch 8.24: Loss = 0.544357
Epoch 8.25: Loss = 0.437271
Epoch 8.26: Loss = 0.380051
Epoch 8.27: Loss = 0.601212
Epoch 8.28: Loss = 0.558914
Epoch 8.29: Loss = 0.462494
Epoch 8.30: Loss = 0.480881
Epoch 8.31: Loss = 0.585175
Epoch 8.32: Loss = 0.410767
Epoch 8.33: Loss = 0.380402
Epoch 8.34: Loss = 0.506485
Epoch 8.35: Loss = 0.444016
Epoch 8.36: Loss = 0.490494
Epoch 8.37: Loss = 0.584213
Epoch 8.38: Loss = 0.35257
Epoch 8.39: Loss = 0.556229
Epoch 8.40: Loss = 0.471176
Epoch 8.41: Loss = 0.409973
Epoch 8.42: Loss = 0.422546
Epoch 8.43: Loss = 0.448181
Epoch 8.44: Loss = 0.367416
Epoch 8.45: Loss = 0.532013
Epoch 8.46: Loss = 0.465881
Epoch 8.47: Loss = 0.569168
Epoch 8.48: Loss = 0.458633
Epoch 8.49: Loss = 0.444244
Epoch 8.50: Loss = 0.401398
Epoch 8.51: Loss = 0.488876
Epoch 8.52: Loss = 0.449219
Epoch 8.53: Loss = 0.441086
Epoch 8.54: Loss = 0.419388
Epoch 8.55: Loss = 0.424026
Epoch 8.56: Loss = 0.401077
Epoch 8.57: Loss = 0.391068
Epoch 8.58: Loss = 0.378326
Epoch 8.59: Loss = 0.364548
Epoch 8.60: Loss = 0.478439
Epoch 8.61: Loss = 0.461411
Epoch 8.62: Loss = 0.407776
Epoch 8.63: Loss = 0.413391
Epoch 8.64: Loss = 0.462234
Epoch 8.65: Loss = 0.388657
Epoch 8.66: Loss = 0.399979
Epoch 8.67: Loss = 0.575241
Epoch 8.68: Loss = 0.51442
Epoch 8.69: Loss = 0.417496
Epoch 8.70: Loss = 0.428406
Epoch 8.71: Loss = 0.373154
Epoch 8.72: Loss = 0.513611
Epoch 8.73: Loss = 0.511322
Epoch 8.74: Loss = 0.454208
Epoch 8.75: Loss = 0.479416
Epoch 8.76: Loss = 0.429443
Epoch 8.77: Loss = 0.499603
Epoch 8.78: Loss = 0.481354
Epoch 8.79: Loss = 0.390732
Epoch 8.80: Loss = 0.395172
Epoch 8.81: Loss = 0.470978
Epoch 8.82: Loss = 0.439651
Epoch 8.83: Loss = 0.622604
Epoch 8.84: Loss = 0.389191
Epoch 8.85: Loss = 0.409409
Epoch 8.86: Loss = 0.508118
Epoch 8.87: Loss = 0.461655
Epoch 8.88: Loss = 0.504852
Epoch 8.89: Loss = 0.470505
Epoch 8.90: Loss = 0.419617
Epoch 8.91: Loss = 0.440048
Epoch 8.92: Loss = 0.498291
Epoch 8.93: Loss = 0.520477
Epoch 8.94: Loss = 0.50798
Epoch 8.95: Loss = 0.404083
Epoch 8.96: Loss = 0.440323
Epoch 8.97: Loss = 0.467941
Epoch 8.98: Loss = 0.417023
Epoch 8.99: Loss = 0.522018
Epoch 8.100: Loss = 0.475327
Epoch 8.101: Loss = 0.521271
Epoch 8.102: Loss = 0.471146
Epoch 8.103: Loss = 0.403137
Epoch 8.104: Loss = 0.361282
Epoch 8.105: Loss = 0.450119
Epoch 8.106: Loss = 0.411331
Epoch 8.107: Loss = 0.529144
Epoch 8.108: Loss = 0.490417
Epoch 8.109: Loss = 0.338211
Epoch 8.110: Loss = 0.577759
Epoch 8.111: Loss = 0.418777
Epoch 8.112: Loss = 0.480606
Epoch 8.113: Loss = 0.496017
Epoch 8.114: Loss = 0.467072
Epoch 8.115: Loss = 0.395447
Epoch 8.116: Loss = 0.39122
Epoch 8.117: Loss = 0.394028
Epoch 8.118: Loss = 0.509705
Epoch 8.119: Loss = 0.494003
Epoch 8.120: Loss = 0.392151
TRAIN LOSS = 0.460861
TRAIN ACC = 88.5239 % (53117/60000)
Loss = 0.35437
Loss = 0.546921
Loss = 0.633759
Loss = 0.567276
Loss = 0.681732
Loss = 0.45134
Loss = 0.423599
Loss = 0.725708
Loss = 0.629395
Loss = 0.46344
Loss = 0.242691
Loss = 0.312241
Loss = 0.44635
Loss = 0.405853
Loss = 0.179993
Loss = 0.32663
Loss = 0.224976
Loss = 0.055191
Loss = 0.271271
Loss = 0.665512
TEST LOSS = 0.430412
TEST ACC = 531.169 % (8940/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.513718
Epoch 9.2: Loss = 0.415314
Epoch 9.3: Loss = 0.482361
Epoch 9.4: Loss = 0.351898
Epoch 9.5: Loss = 0.379593
Epoch 9.6: Loss = 0.452179
Epoch 9.7: Loss = 0.419922
Epoch 9.8: Loss = 0.531677
Epoch 9.9: Loss = 0.350235
Epoch 9.10: Loss = 0.406021
Epoch 9.11: Loss = 0.543396
Epoch 9.12: Loss = 0.542908
Epoch 9.13: Loss = 0.467392
Epoch 9.14: Loss = 0.545593
Epoch 9.15: Loss = 0.438828
Epoch 9.16: Loss = 0.440033
Epoch 9.17: Loss = 0.561096
Epoch 9.18: Loss = 0.437897
Epoch 9.19: Loss = 0.414154
Epoch 9.20: Loss = 0.472015
Epoch 9.21: Loss = 0.557404
Epoch 9.22: Loss = 0.464981
Epoch 9.23: Loss = 0.512543
Epoch 9.24: Loss = 0.356323
Epoch 9.25: Loss = 0.522278
Epoch 9.26: Loss = 0.511765
Epoch 9.27: Loss = 0.395584
Epoch 9.28: Loss = 0.381332
Epoch 9.29: Loss = 0.394852
Epoch 9.30: Loss = 0.463287
Epoch 9.31: Loss = 0.555038
Epoch 9.32: Loss = 0.453629
Epoch 9.33: Loss = 0.461243
Epoch 9.34: Loss = 0.500946
Epoch 9.35: Loss = 0.489594
Epoch 9.36: Loss = 0.492203
Epoch 9.37: Loss = 0.384003
Epoch 9.38: Loss = 0.441757
Epoch 9.39: Loss = 0.385117
Epoch 9.40: Loss = 0.437531
Epoch 9.41: Loss = 0.419876
Epoch 9.42: Loss = 0.489624
Epoch 9.43: Loss = 0.518326
Epoch 9.44: Loss = 0.389969
Epoch 9.45: Loss = 0.660553
Epoch 9.46: Loss = 0.445374
Epoch 9.47: Loss = 0.572174
Epoch 9.48: Loss = 0.432571
Epoch 9.49: Loss = 0.415573
Epoch 9.50: Loss = 0.460388
Epoch 9.51: Loss = 0.509933
Epoch 9.52: Loss = 0.464966
Epoch 9.53: Loss = 0.390701
Epoch 9.54: Loss = 0.524551
Epoch 9.55: Loss = 0.436386
Epoch 9.56: Loss = 0.498993
Epoch 9.57: Loss = 0.404205
Epoch 9.58: Loss = 0.457657
Epoch 9.59: Loss = 0.690506
Epoch 9.60: Loss = 0.547073
Epoch 9.61: Loss = 0.458191
Epoch 9.62: Loss = 0.370911
Epoch 9.63: Loss = 0.484711
Epoch 9.64: Loss = 0.448669
Epoch 9.65: Loss = 0.54335
Epoch 9.66: Loss = 0.476868
Epoch 9.67: Loss = 0.462616
Epoch 9.68: Loss = 0.390335
Epoch 9.69: Loss = 0.480392
Epoch 9.70: Loss = 0.400833
Epoch 9.71: Loss = 0.461227
Epoch 9.72: Loss = 0.479919
Epoch 9.73: Loss = 0.46669
Epoch 9.74: Loss = 0.602371
Epoch 9.75: Loss = 0.429886
Epoch 9.76: Loss = 0.416992
Epoch 9.77: Loss = 0.410156
Epoch 9.78: Loss = 0.512054
Epoch 9.79: Loss = 0.548111
Epoch 9.80: Loss = 0.536087
Epoch 9.81: Loss = 0.432968
Epoch 9.82: Loss = 0.48526
Epoch 9.83: Loss = 0.449524
Epoch 9.84: Loss = 0.470093
Epoch 9.85: Loss = 0.432602
Epoch 9.86: Loss = 0.415375
Epoch 9.87: Loss = 0.545456
Epoch 9.88: Loss = 0.405228
Epoch 9.89: Loss = 0.514557
Epoch 9.90: Loss = 0.352158
Epoch 9.91: Loss = 0.501984
Epoch 9.92: Loss = 0.424301
Epoch 9.93: Loss = 0.406143
Epoch 9.94: Loss = 0.496262
Epoch 9.95: Loss = 0.499512
Epoch 9.96: Loss = 0.537827
Epoch 9.97: Loss = 0.39859
Epoch 9.98: Loss = 0.500336
Epoch 9.99: Loss = 0.449753
Epoch 9.100: Loss = 0.583847
Epoch 9.101: Loss = 0.44339
Epoch 9.102: Loss = 0.575394
Epoch 9.103: Loss = 0.395462
Epoch 9.104: Loss = 0.438416
Epoch 9.105: Loss = 0.415009
Epoch 9.106: Loss = 0.436035
Epoch 9.107: Loss = 0.527817
Epoch 9.108: Loss = 0.44516
Epoch 9.109: Loss = 0.490265
Epoch 9.110: Loss = 0.664063
Epoch 9.111: Loss = 0.456116
Epoch 9.112: Loss = 0.435822
Epoch 9.113: Loss = 0.551224
Epoch 9.114: Loss = 0.391251
Epoch 9.115: Loss = 0.48468
Epoch 9.116: Loss = 0.444473
Epoch 9.117: Loss = 0.536331
Epoch 9.118: Loss = 0.352707
Epoch 9.119: Loss = 0.516144
Epoch 9.120: Loss = 0.458069
TRAIN LOSS = 0.468307
TRAIN ACC = 88.6719 % (53206/60000)
Loss = 0.408493
Loss = 0.587067
Loss = 0.719986
Loss = 0.60994
Loss = 0.686569
Loss = 0.477554
Loss = 0.433609
Loss = 0.747345
Loss = 0.663757
Loss = 0.471237
Loss = 0.251434
Loss = 0.385635
Loss = 0.464645
Loss = 0.427048
Loss = 0.209763
Loss = 0.342697
Loss = 0.286942
Loss = 0.0601807
Loss = 0.262329
Loss = 0.670349
TEST LOSS = 0.458329
TEST ACC = 532.059 % (8891/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.491592
Epoch 10.2: Loss = 0.400955
Epoch 10.3: Loss = 0.361633
Epoch 10.4: Loss = 0.467606
Epoch 10.5: Loss = 0.576508
Epoch 10.6: Loss = 0.516937
Epoch 10.7: Loss = 0.569519
Epoch 10.8: Loss = 0.451401
Epoch 10.9: Loss = 0.498917
Epoch 10.10: Loss = 0.445175
Epoch 10.11: Loss = 0.486984
Epoch 10.12: Loss = 0.335724
Epoch 10.13: Loss = 0.367874
Epoch 10.14: Loss = 0.389618
Epoch 10.15: Loss = 0.360901
Epoch 10.16: Loss = 0.612381
Epoch 10.17: Loss = 0.452576
Epoch 10.18: Loss = 0.5065
Epoch 10.19: Loss = 0.38765
Epoch 10.20: Loss = 0.349869
Epoch 10.21: Loss = 0.471252
Epoch 10.22: Loss = 0.495392
Epoch 10.23: Loss = 0.487045
Epoch 10.24: Loss = 0.533264
Epoch 10.25: Loss = 0.515137
Epoch 10.26: Loss = 0.547012
Epoch 10.27: Loss = 0.463745
Epoch 10.28: Loss = 0.452011
Epoch 10.29: Loss = 0.361038
Epoch 10.30: Loss = 0.465729
Epoch 10.31: Loss = 0.41156
Epoch 10.32: Loss = 0.415176
Epoch 10.33: Loss = 0.606735
Epoch 10.34: Loss = 0.444
Epoch 10.35: Loss = 0.45874
Epoch 10.36: Loss = 0.463715
Epoch 10.37: Loss = 0.535797
Epoch 10.38: Loss = 0.460129
Epoch 10.39: Loss = 0.463791
Epoch 10.40: Loss = 0.42334
Epoch 10.41: Loss = 0.381424
Epoch 10.42: Loss = 0.451294
Epoch 10.43: Loss = 0.642975
Epoch 10.44: Loss = 0.444382
Epoch 10.45: Loss = 0.486679
Epoch 10.46: Loss = 0.457962
Epoch 10.47: Loss = 0.57869
Epoch 10.48: Loss = 0.627411
Epoch 10.49: Loss = 0.480179
Epoch 10.50: Loss = 0.473618
Epoch 10.51: Loss = 0.520676
Epoch 10.52: Loss = 0.340286
Epoch 10.53: Loss = 0.435242
Epoch 10.54: Loss = 0.405991
Epoch 10.55: Loss = 0.595566
Epoch 10.56: Loss = 0.599823
Epoch 10.57: Loss = 0.355591
Epoch 10.58: Loss = 0.51886
Epoch 10.59: Loss = 0.472717
Epoch 10.60: Loss = 0.655518
Epoch 10.61: Loss = 0.45665
Epoch 10.62: Loss = 0.502502
Epoch 10.63: Loss = 0.583862
Epoch 10.64: Loss = 0.461456
Epoch 10.65: Loss = 0.422516
Epoch 10.66: Loss = 0.392319
Epoch 10.67: Loss = 0.443054
Epoch 10.68: Loss = 0.460342
Epoch 10.69: Loss = 0.562119
Epoch 10.70: Loss = 0.356247
Epoch 10.71: Loss = 0.489365
Epoch 10.72: Loss = 0.425583
Epoch 10.73: Loss = 0.519836
Epoch 10.74: Loss = 0.543137
Epoch 10.75: Loss = 0.43988
Epoch 10.76: Loss = 0.438126
Epoch 10.77: Loss = 0.386826
Epoch 10.78: Loss = 0.620895
Epoch 10.79: Loss = 0.533844
Epoch 10.80: Loss = 0.36644
Epoch 10.81: Loss = 0.509354
Epoch 10.82: Loss = 0.639633
Epoch 10.83: Loss = 0.50499
Epoch 10.84: Loss = 0.336731
Epoch 10.85: Loss = 0.492401
Epoch 10.86: Loss = 0.497787
Epoch 10.87: Loss = 0.372803
Epoch 10.88: Loss = 0.409042
Epoch 10.89: Loss = 0.494385
Epoch 10.90: Loss = 0.453751
Epoch 10.91: Loss = 0.513123
Epoch 10.92: Loss = 0.57785
Epoch 10.93: Loss = 0.407394
Epoch 10.94: Loss = 0.444382
Epoch 10.95: Loss = 0.484314
Epoch 10.96: Loss = 0.465027
Epoch 10.97: Loss = 0.660095
Epoch 10.98: Loss = 0.4646
Epoch 10.99: Loss = 0.516266
Epoch 10.100: Loss = 0.57637
Epoch 10.101: Loss = 0.466431
Epoch 10.102: Loss = 0.46965
Epoch 10.103: Loss = 0.49704
Epoch 10.104: Loss = 0.426544
Epoch 10.105: Loss = 0.40741
Epoch 10.106: Loss = 0.545547
Epoch 10.107: Loss = 0.451767
Epoch 10.108: Loss = 0.470474
Epoch 10.109: Loss = 0.534851
Epoch 10.110: Loss = 0.464569
Epoch 10.111: Loss = 0.528519
Epoch 10.112: Loss = 0.488403
Epoch 10.113: Loss = 0.413681
Epoch 10.114: Loss = 0.60083
Epoch 10.115: Loss = 0.5271
Epoch 10.116: Loss = 0.361862
Epoch 10.117: Loss = 0.52504
Epoch 10.118: Loss = 0.593475
Epoch 10.119: Loss = 0.439728
Epoch 10.120: Loss = 0.477325
TRAIN LOSS = 0.47789
TRAIN ACC = 88.5986 % (53161/60000)
Loss = 0.3992
Loss = 0.587814
Loss = 0.67952
Loss = 0.620834
Loss = 0.69574
Loss = 0.469299
Loss = 0.431824
Loss = 0.755371
Loss = 0.675217
Loss = 0.502365
Loss = 0.230362
Loss = 0.339127
Loss = 0.510956
Loss = 0.456741
Loss = 0.178619
Loss = 0.410004
Loss = 0.287109
Loss = 0.0593872
Loss = 0.26828
Loss = 0.704117
TEST LOSS = 0.463094
TEST ACC = 531.609 % (8905/10000)

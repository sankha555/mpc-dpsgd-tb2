Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.32382
Epoch 1.2: Loss = 2.28159
Epoch 1.3: Loss = 2.20555
Epoch 1.4: Loss = 2.17685
Epoch 1.5: Loss = 2.12083
Epoch 1.6: Loss = 2.06067
Epoch 1.7: Loss = 2.02832
Epoch 1.8: Loss = 2.01627
Epoch 1.9: Loss = 1.94278
Epoch 1.10: Loss = 1.9059
Epoch 1.11: Loss = 1.84842
Epoch 1.12: Loss = 1.82788
Epoch 1.13: Loss = 1.77641
Epoch 1.14: Loss = 1.756
Epoch 1.15: Loss = 1.72028
Epoch 1.16: Loss = 1.63832
Epoch 1.17: Loss = 1.62103
Epoch 1.18: Loss = 1.62383
Epoch 1.19: Loss = 1.56798
Epoch 1.20: Loss = 1.53606
Epoch 1.21: Loss = 1.55713
Epoch 1.22: Loss = 1.43864
Epoch 1.23: Loss = 1.47971
Epoch 1.24: Loss = 1.38141
Epoch 1.25: Loss = 1.41521
Epoch 1.26: Loss = 1.37468
Epoch 1.27: Loss = 1.31123
Epoch 1.28: Loss = 1.33415
Epoch 1.29: Loss = 1.30974
Epoch 1.30: Loss = 1.34113
Epoch 1.31: Loss = 1.24695
Epoch 1.32: Loss = 1.23929
Epoch 1.33: Loss = 1.26146
Epoch 1.34: Loss = 1.16905
Epoch 1.35: Loss = 1.1869
Epoch 1.36: Loss = 1.17847
Epoch 1.37: Loss = 1.15817
Epoch 1.38: Loss = 1.17987
Epoch 1.39: Loss = 1.12204
Epoch 1.40: Loss = 1.09068
Epoch 1.41: Loss = 1.18706
Epoch 1.42: Loss = 1.03738
Epoch 1.43: Loss = 1.14215
Epoch 1.44: Loss = 1.02112
Epoch 1.45: Loss = 1.09987
Epoch 1.46: Loss = 1.09145
Epoch 1.47: Loss = 1.01871
Epoch 1.48: Loss = 1.07512
Epoch 1.49: Loss = 0.975815
Epoch 1.50: Loss = 0.979477
Epoch 1.51: Loss = 0.946579
Epoch 1.52: Loss = 0.987457
Epoch 1.53: Loss = 0.954727
Epoch 1.54: Loss = 0.962845
Epoch 1.55: Loss = 0.937454
Epoch 1.56: Loss = 0.999573
Epoch 1.57: Loss = 0.97052
Epoch 1.58: Loss = 0.973801
Epoch 1.59: Loss = 0.921768
Epoch 1.60: Loss = 0.932404
Epoch 1.61: Loss = 0.90361
Epoch 1.62: Loss = 0.856613
Epoch 1.63: Loss = 0.863983
Epoch 1.64: Loss = 0.898224
Epoch 1.65: Loss = 0.868378
Epoch 1.66: Loss = 0.884201
Epoch 1.67: Loss = 0.893356
Epoch 1.68: Loss = 0.813736
Epoch 1.69: Loss = 0.967545
Epoch 1.70: Loss = 0.863708
Epoch 1.71: Loss = 0.880112
Epoch 1.72: Loss = 0.888168
Epoch 1.73: Loss = 0.872818
Epoch 1.74: Loss = 0.897095
Epoch 1.75: Loss = 0.859802
Epoch 1.76: Loss = 0.798126
Epoch 1.77: Loss = 0.820862
Epoch 1.78: Loss = 0.888275
Epoch 1.79: Loss = 0.890396
Epoch 1.80: Loss = 0.819855
Epoch 1.81: Loss = 0.811096
Epoch 1.82: Loss = 0.745972
Epoch 1.83: Loss = 0.810745
Epoch 1.84: Loss = 0.767578
Epoch 1.85: Loss = 0.830902
Epoch 1.86: Loss = 0.861115
Epoch 1.87: Loss = 0.825058
Epoch 1.88: Loss = 0.834534
Epoch 1.89: Loss = 0.786758
Epoch 1.90: Loss = 0.841354
Epoch 1.91: Loss = 0.790146
Epoch 1.92: Loss = 0.817337
Epoch 1.93: Loss = 0.808929
Epoch 1.94: Loss = 0.869385
Epoch 1.95: Loss = 0.741928
Epoch 1.96: Loss = 0.778915
Epoch 1.97: Loss = 0.79747
Epoch 1.98: Loss = 0.787659
Epoch 1.99: Loss = 0.740967
Epoch 1.100: Loss = 0.794174
Epoch 1.101: Loss = 0.803757
Epoch 1.102: Loss = 0.824936
Epoch 1.103: Loss = 0.796051
Epoch 1.104: Loss = 0.738449
Epoch 1.105: Loss = 0.705917
Epoch 1.106: Loss = 0.728607
Epoch 1.107: Loss = 0.730515
Epoch 1.108: Loss = 0.727676
Epoch 1.109: Loss = 0.766327
Epoch 1.110: Loss = 0.798294
Epoch 1.111: Loss = 0.746567
Epoch 1.112: Loss = 0.780396
Epoch 1.113: Loss = 0.780853
Epoch 1.114: Loss = 0.737946
Epoch 1.115: Loss = 0.812958
Epoch 1.116: Loss = 0.869049
Epoch 1.117: Loss = 0.685654
Epoch 1.118: Loss = 0.702484
Epoch 1.119: Loss = 0.708145
Epoch 1.120: Loss = 0.779831
TRAIN LOSS = 1.10556
TRAIN ACC = 63.3224 % (37995/60000)
Loss = 0.680573
Loss = 0.779251
Loss = 0.786346
Loss = 0.686996
Loss = 0.675629
Loss = 0.843094
Loss = 0.868958
Loss = 0.817383
Loss = 0.738251
Loss = 0.698975
Loss = 0.812149
Loss = 0.758347
Loss = 0.767639
Loss = 0.764404
Loss = 0.726959
Loss = 0.791458
Loss = 0.718781
Loss = 0.763046
Loss = 0.808441
Loss = 0.759018
TEST LOSS = 0.762285
TEST ACC = 379.95 % (7292/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.758942
Epoch 2.2: Loss = 0.713181
Epoch 2.3: Loss = 0.74942
Epoch 2.4: Loss = 0.727661
Epoch 2.5: Loss = 0.769333
Epoch 2.6: Loss = 0.695465
Epoch 2.7: Loss = 0.71312
Epoch 2.8: Loss = 0.724396
Epoch 2.9: Loss = 0.675095
Epoch 2.10: Loss = 0.708694
Epoch 2.11: Loss = 0.679581
Epoch 2.12: Loss = 0.732849
Epoch 2.13: Loss = 0.713974
Epoch 2.14: Loss = 0.744858
Epoch 2.15: Loss = 0.791824
Epoch 2.16: Loss = 0.706253
Epoch 2.17: Loss = 0.696579
Epoch 2.18: Loss = 0.762054
Epoch 2.19: Loss = 0.75029
Epoch 2.20: Loss = 0.734421
Epoch 2.21: Loss = 0.717392
Epoch 2.22: Loss = 0.691681
Epoch 2.23: Loss = 0.850739
Epoch 2.24: Loss = 0.737061
Epoch 2.25: Loss = 0.716568
Epoch 2.26: Loss = 0.758301
Epoch 2.27: Loss = 0.713318
Epoch 2.28: Loss = 0.704575
Epoch 2.29: Loss = 0.724304
Epoch 2.30: Loss = 0.698715
Epoch 2.31: Loss = 0.653458
Epoch 2.32: Loss = 0.718384
Epoch 2.33: Loss = 0.697479
Epoch 2.34: Loss = 0.774048
Epoch 2.35: Loss = 0.717346
Epoch 2.36: Loss = 0.725052
Epoch 2.37: Loss = 0.686127
Epoch 2.38: Loss = 0.763397
Epoch 2.39: Loss = 0.702362
Epoch 2.40: Loss = 0.763351
Epoch 2.41: Loss = 0.800018
Epoch 2.42: Loss = 0.645538
Epoch 2.43: Loss = 0.732361
Epoch 2.44: Loss = 0.686188
Epoch 2.45: Loss = 0.64473
Epoch 2.46: Loss = 0.764832
Epoch 2.47: Loss = 0.727051
Epoch 2.48: Loss = 0.704697
Epoch 2.49: Loss = 0.775742
Epoch 2.50: Loss = 0.693161
Epoch 2.51: Loss = 0.706848
Epoch 2.52: Loss = 0.646515
Epoch 2.53: Loss = 0.679581
Epoch 2.54: Loss = 0.759399
Epoch 2.55: Loss = 0.638916
Epoch 2.56: Loss = 0.688034
Epoch 2.57: Loss = 0.671204
Epoch 2.58: Loss = 0.664383
Epoch 2.59: Loss = 0.792313
Epoch 2.60: Loss = 0.618683
Epoch 2.61: Loss = 0.693542
Epoch 2.62: Loss = 0.800858
Epoch 2.63: Loss = 0.690659
Epoch 2.64: Loss = 0.582413
Epoch 2.65: Loss = 0.752029
Epoch 2.66: Loss = 0.743103
Epoch 2.67: Loss = 0.693954
Epoch 2.68: Loss = 0.602219
Epoch 2.69: Loss = 0.740494
Epoch 2.70: Loss = 0.736862
Epoch 2.71: Loss = 0.60524
Epoch 2.72: Loss = 0.722992
Epoch 2.73: Loss = 0.817474
Epoch 2.74: Loss = 0.712479
Epoch 2.75: Loss = 0.634506
Epoch 2.76: Loss = 0.756744
Epoch 2.77: Loss = 0.696533
Epoch 2.78: Loss = 0.734177
Epoch 2.79: Loss = 0.739716
Epoch 2.80: Loss = 0.767487
Epoch 2.81: Loss = 0.730362
Epoch 2.82: Loss = 0.650421
Epoch 2.83: Loss = 0.751724
Epoch 2.84: Loss = 0.657333
Epoch 2.85: Loss = 0.778534
Epoch 2.86: Loss = 0.694992
Epoch 2.87: Loss = 0.656769
Epoch 2.88: Loss = 0.650894
Epoch 2.89: Loss = 0.651794
Epoch 2.90: Loss = 0.763474
Epoch 2.91: Loss = 0.690384
Epoch 2.92: Loss = 0.660095
Epoch 2.93: Loss = 0.6866
Epoch 2.94: Loss = 0.677582
Epoch 2.95: Loss = 0.599335
Epoch 2.96: Loss = 0.668518
Epoch 2.97: Loss = 0.67807
Epoch 2.98: Loss = 0.676239
Epoch 2.99: Loss = 0.686798
Epoch 2.100: Loss = 0.650604
Epoch 2.101: Loss = 0.727722
Epoch 2.102: Loss = 0.631943
Epoch 2.103: Loss = 0.616165
Epoch 2.104: Loss = 0.629044
Epoch 2.105: Loss = 0.698761
Epoch 2.106: Loss = 0.671387
Epoch 2.107: Loss = 0.718506
Epoch 2.108: Loss = 0.719681
Epoch 2.109: Loss = 0.660477
Epoch 2.110: Loss = 0.709061
Epoch 2.111: Loss = 0.623932
Epoch 2.112: Loss = 0.678238
Epoch 2.113: Loss = 0.606918
Epoch 2.114: Loss = 0.763275
Epoch 2.115: Loss = 0.705948
Epoch 2.116: Loss = 0.599487
Epoch 2.117: Loss = 0.808182
Epoch 2.118: Loss = 0.646606
Epoch 2.119: Loss = 0.704575
Epoch 2.120: Loss = 0.712311
TRAIN LOSS = 0.705154
TRAIN ACC = 75.8606 % (45518/60000)
Loss = 0.60788
Loss = 0.714615
Loss = 0.688095
Loss = 0.595306
Loss = 0.612869
Loss = 0.78891
Loss = 0.834686
Loss = 0.780884
Loss = 0.678772
Loss = 0.627258
Loss = 0.787964
Loss = 0.73407
Loss = 0.722137
Loss = 0.700439
Loss = 0.671158
Loss = 0.724976
Loss = 0.639908
Loss = 0.728912
Loss = 0.774277
Loss = 0.696426
TEST LOSS = 0.705477
TEST ACC = 455.179 % (7611/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.691895
Epoch 3.2: Loss = 0.633331
Epoch 3.3: Loss = 0.636841
Epoch 3.4: Loss = 0.646622
Epoch 3.5: Loss = 0.70752
Epoch 3.6: Loss = 0.625183
Epoch 3.7: Loss = 0.659363
Epoch 3.8: Loss = 0.644531
Epoch 3.9: Loss = 0.733749
Epoch 3.10: Loss = 0.72171
Epoch 3.11: Loss = 0.679871
Epoch 3.12: Loss = 0.610352
Epoch 3.13: Loss = 0.625015
Epoch 3.14: Loss = 0.646011
Epoch 3.15: Loss = 0.610062
Epoch 3.16: Loss = 0.641739
Epoch 3.17: Loss = 0.75441
Epoch 3.18: Loss = 0.75235
Epoch 3.19: Loss = 0.739227
Epoch 3.20: Loss = 0.610809
Epoch 3.21: Loss = 0.642883
Epoch 3.22: Loss = 0.65947
Epoch 3.23: Loss = 0.775467
Epoch 3.24: Loss = 0.713821
Epoch 3.25: Loss = 0.671219
Epoch 3.26: Loss = 0.703629
Epoch 3.27: Loss = 0.453094
Epoch 3.28: Loss = 0.670929
Epoch 3.29: Loss = 0.717575
Epoch 3.30: Loss = 0.662354
Epoch 3.31: Loss = 0.621719
Epoch 3.32: Loss = 0.724411
Epoch 3.33: Loss = 0.668808
Epoch 3.34: Loss = 0.580093
Epoch 3.35: Loss = 0.720078
Epoch 3.36: Loss = 0.763214
Epoch 3.37: Loss = 0.682739
Epoch 3.38: Loss = 0.648712
Epoch 3.39: Loss = 0.709213
Epoch 3.40: Loss = 0.695801
Epoch 3.41: Loss = 0.780228
Epoch 3.42: Loss = 0.607513
Epoch 3.43: Loss = 0.738586
Epoch 3.44: Loss = 0.646606
Epoch 3.45: Loss = 0.646835
Epoch 3.46: Loss = 0.745255
Epoch 3.47: Loss = 0.611023
Epoch 3.48: Loss = 0.679352
Epoch 3.49: Loss = 0.644226
Epoch 3.50: Loss = 0.776733
Epoch 3.51: Loss = 0.656891
Epoch 3.52: Loss = 0.674286
Epoch 3.53: Loss = 0.748322
Epoch 3.54: Loss = 0.678833
Epoch 3.55: Loss = 0.666153
Epoch 3.56: Loss = 0.703278
Epoch 3.57: Loss = 0.688797
Epoch 3.58: Loss = 0.673325
Epoch 3.59: Loss = 0.675644
Epoch 3.60: Loss = 0.612457
Epoch 3.61: Loss = 0.724701
Epoch 3.62: Loss = 0.61232
Epoch 3.63: Loss = 0.613434
Epoch 3.64: Loss = 0.694122
Epoch 3.65: Loss = 0.709229
Epoch 3.66: Loss = 0.716797
Epoch 3.67: Loss = 0.62558
Epoch 3.68: Loss = 0.713257
Epoch 3.69: Loss = 0.670746
Epoch 3.70: Loss = 0.603729
Epoch 3.71: Loss = 0.620193
Epoch 3.72: Loss = 0.708099
Epoch 3.73: Loss = 0.667725
Epoch 3.74: Loss = 0.615158
Epoch 3.75: Loss = 0.69104
Epoch 3.76: Loss = 0.697433
Epoch 3.77: Loss = 0.663864
Epoch 3.78: Loss = 0.743164
Epoch 3.79: Loss = 0.516937
Epoch 3.80: Loss = 0.644424
Epoch 3.81: Loss = 0.666367
Epoch 3.82: Loss = 0.636154
Epoch 3.83: Loss = 0.627563
Epoch 3.84: Loss = 0.661789
Epoch 3.85: Loss = 0.738907
Epoch 3.86: Loss = 0.576416
Epoch 3.87: Loss = 0.709732
Epoch 3.88: Loss = 0.706848
Epoch 3.89: Loss = 0.524475
Epoch 3.90: Loss = 0.65213
Epoch 3.91: Loss = 0.665573
Epoch 3.92: Loss = 0.748474
Epoch 3.93: Loss = 0.815857
Epoch 3.94: Loss = 0.678665
Epoch 3.95: Loss = 0.617569
Epoch 3.96: Loss = 0.738098
Epoch 3.97: Loss = 0.726807
Epoch 3.98: Loss = 0.550827
Epoch 3.99: Loss = 0.62793
Epoch 3.100: Loss = 0.720261
Epoch 3.101: Loss = 0.800781
Epoch 3.102: Loss = 0.646851
Epoch 3.103: Loss = 0.678238
Epoch 3.104: Loss = 0.681763
Epoch 3.105: Loss = 0.66037
Epoch 3.106: Loss = 0.675705
Epoch 3.107: Loss = 0.585526
Epoch 3.108: Loss = 0.606293
Epoch 3.109: Loss = 0.796921
Epoch 3.110: Loss = 0.75238
Epoch 3.111: Loss = 0.589111
Epoch 3.112: Loss = 0.719406
Epoch 3.113: Loss = 0.682053
Epoch 3.114: Loss = 0.714752
Epoch 3.115: Loss = 0.706421
Epoch 3.116: Loss = 0.820709
Epoch 3.117: Loss = 0.701462
Epoch 3.118: Loss = 0.619583
Epoch 3.119: Loss = 0.607849
Epoch 3.120: Loss = 0.646988
TRAIN LOSS = 0.673737
TRAIN ACC = 78.1021 % (46863/60000)
Loss = 0.601456
Loss = 0.697571
Loss = 0.660721
Loss = 0.598053
Loss = 0.594574
Loss = 0.773132
Loss = 0.8591
Loss = 0.764069
Loss = 0.676514
Loss = 0.608948
Loss = 0.784409
Loss = 0.747574
Loss = 0.697174
Loss = 0.721191
Loss = 0.661423
Loss = 0.698547
Loss = 0.661148
Loss = 0.709747
Loss = 0.762344
Loss = 0.6931
TEST LOSS = 0.69854
TEST ACC = 468.629 % (7800/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.601563
Epoch 4.2: Loss = 0.601105
Epoch 4.3: Loss = 0.590683
Epoch 4.4: Loss = 0.689148
Epoch 4.5: Loss = 0.615067
Epoch 4.6: Loss = 0.678467
Epoch 4.7: Loss = 0.661453
Epoch 4.8: Loss = 0.717911
Epoch 4.9: Loss = 0.653473
Epoch 4.10: Loss = 0.674438
Epoch 4.11: Loss = 0.623764
Epoch 4.12: Loss = 0.597153
Epoch 4.13: Loss = 0.699265
Epoch 4.14: Loss = 0.603836
Epoch 4.15: Loss = 0.696045
Epoch 4.16: Loss = 0.646072
Epoch 4.17: Loss = 0.652084
Epoch 4.18: Loss = 0.735001
Epoch 4.19: Loss = 0.677109
Epoch 4.20: Loss = 0.715134
Epoch 4.21: Loss = 0.669174
Epoch 4.22: Loss = 0.643036
Epoch 4.23: Loss = 0.738251
Epoch 4.24: Loss = 0.663788
Epoch 4.25: Loss = 0.718826
Epoch 4.26: Loss = 0.809647
Epoch 4.27: Loss = 0.703735
Epoch 4.28: Loss = 0.519089
Epoch 4.29: Loss = 0.732269
Epoch 4.30: Loss = 0.665482
Epoch 4.31: Loss = 0.603195
Epoch 4.32: Loss = 0.5867
Epoch 4.33: Loss = 0.752014
Epoch 4.34: Loss = 0.889633
Epoch 4.35: Loss = 0.604401
Epoch 4.36: Loss = 0.650467
Epoch 4.37: Loss = 0.657425
Epoch 4.38: Loss = 0.709412
Epoch 4.39: Loss = 0.65448
Epoch 4.40: Loss = 0.701797
Epoch 4.41: Loss = 0.609665
Epoch 4.42: Loss = 0.647842
Epoch 4.43: Loss = 0.738708
Epoch 4.44: Loss = 0.689575
Epoch 4.45: Loss = 0.713715
Epoch 4.46: Loss = 0.603577
Epoch 4.47: Loss = 0.737045
Epoch 4.48: Loss = 0.645309
Epoch 4.49: Loss = 0.660324
Epoch 4.50: Loss = 0.630768
Epoch 4.51: Loss = 0.651382
Epoch 4.52: Loss = 0.655426
Epoch 4.53: Loss = 0.645645
Epoch 4.54: Loss = 0.72403
Epoch 4.55: Loss = 0.654877
Epoch 4.56: Loss = 0.595444
Epoch 4.57: Loss = 0.637802
Epoch 4.58: Loss = 0.688583
Epoch 4.59: Loss = 0.703384
Epoch 4.60: Loss = 0.670944
Epoch 4.61: Loss = 0.737625
Epoch 4.62: Loss = 0.622559
Epoch 4.63: Loss = 0.703217
Epoch 4.64: Loss = 0.567459
Epoch 4.65: Loss = 0.509232
Epoch 4.66: Loss = 0.757507
Epoch 4.67: Loss = 0.667557
Epoch 4.68: Loss = 0.714203
Epoch 4.69: Loss = 0.63028
Epoch 4.70: Loss = 0.576691
Epoch 4.71: Loss = 0.655807
Epoch 4.72: Loss = 0.688751
Epoch 4.73: Loss = 0.708664
Epoch 4.74: Loss = 0.64978
Epoch 4.75: Loss = 0.732697
Epoch 4.76: Loss = 0.632767
Epoch 4.77: Loss = 0.885849
Epoch 4.78: Loss = 0.531952
Epoch 4.79: Loss = 0.740021
Epoch 4.80: Loss = 0.570847
Epoch 4.81: Loss = 0.722061
Epoch 4.82: Loss = 0.635254
Epoch 4.83: Loss = 0.761368
Epoch 4.84: Loss = 0.608841
Epoch 4.85: Loss = 0.756851
Epoch 4.86: Loss = 0.685547
Epoch 4.87: Loss = 0.714401
Epoch 4.88: Loss = 0.775101
Epoch 4.89: Loss = 0.58017
Epoch 4.90: Loss = 0.607361
Epoch 4.91: Loss = 0.645447
Epoch 4.92: Loss = 0.678513
Epoch 4.93: Loss = 0.62677
Epoch 4.94: Loss = 0.690628
Epoch 4.95: Loss = 0.670792
Epoch 4.96: Loss = 0.636917
Epoch 4.97: Loss = 0.730148
Epoch 4.98: Loss = 0.713867
Epoch 4.99: Loss = 0.651962
Epoch 4.100: Loss = 0.623795
Epoch 4.101: Loss = 0.606445
Epoch 4.102: Loss = 0.60643
Epoch 4.103: Loss = 0.756607
Epoch 4.104: Loss = 0.637024
Epoch 4.105: Loss = 0.645554
Epoch 4.106: Loss = 0.590942
Epoch 4.107: Loss = 0.629074
Epoch 4.108: Loss = 0.64267
Epoch 4.109: Loss = 0.583557
Epoch 4.110: Loss = 0.737152
Epoch 4.111: Loss = 0.743423
Epoch 4.112: Loss = 0.57843
Epoch 4.113: Loss = 0.634079
Epoch 4.114: Loss = 0.664093
Epoch 4.115: Loss = 0.640823
Epoch 4.116: Loss = 0.750961
Epoch 4.117: Loss = 0.656235
Epoch 4.118: Loss = 0.630783
Epoch 4.119: Loss = 0.543579
Epoch 4.120: Loss = 0.622055
TRAIN LOSS = 0.665054
TRAIN ACC = 79.6021 % (47763/60000)
Loss = 0.576126
Loss = 0.679657
Loss = 0.635162
Loss = 0.586166
Loss = 0.58963
Loss = 0.770966
Loss = 0.838547
Loss = 0.751358
Loss = 0.666885
Loss = 0.598892
Loss = 0.788223
Loss = 0.761566
Loss = 0.67424
Loss = 0.697784
Loss = 0.647552
Loss = 0.692566
Loss = 0.657608
Loss = 0.700531
Loss = 0.756546
Loss = 0.689606
TEST LOSS = 0.68798
TEST ACC = 477.629 % (7874/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.673645
Epoch 5.2: Loss = 0.674957
Epoch 5.3: Loss = 0.626282
Epoch 5.4: Loss = 0.7202
Epoch 5.5: Loss = 0.620468
Epoch 5.6: Loss = 0.641846
Epoch 5.7: Loss = 0.59491
Epoch 5.8: Loss = 0.637863
Epoch 5.9: Loss = 0.6595
Epoch 5.10: Loss = 0.672287
Epoch 5.11: Loss = 0.79158
Epoch 5.12: Loss = 0.681671
Epoch 5.13: Loss = 0.627838
Epoch 5.14: Loss = 0.598679
Epoch 5.15: Loss = 0.666946
Epoch 5.16: Loss = 0.757858
Epoch 5.17: Loss = 0.711166
Epoch 5.18: Loss = 0.629181
Epoch 5.19: Loss = 0.625229
Epoch 5.20: Loss = 0.70105
Epoch 5.21: Loss = 0.748016
Epoch 5.22: Loss = 0.520584
Epoch 5.23: Loss = 0.651749
Epoch 5.24: Loss = 0.615448
Epoch 5.25: Loss = 0.657211
Epoch 5.26: Loss = 0.567596
Epoch 5.27: Loss = 0.659561
Epoch 5.28: Loss = 0.753647
Epoch 5.29: Loss = 0.707733
Epoch 5.30: Loss = 0.595428
Epoch 5.31: Loss = 0.714111
Epoch 5.32: Loss = 0.637665
Epoch 5.33: Loss = 0.566269
Epoch 5.34: Loss = 0.568756
Epoch 5.35: Loss = 0.731644
Epoch 5.36: Loss = 0.616928
Epoch 5.37: Loss = 0.61055
Epoch 5.38: Loss = 0.62471
Epoch 5.39: Loss = 0.70813
Epoch 5.40: Loss = 0.693787
Epoch 5.41: Loss = 0.706909
Epoch 5.42: Loss = 0.682877
Epoch 5.43: Loss = 0.654999
Epoch 5.44: Loss = 0.592773
Epoch 5.45: Loss = 0.617004
Epoch 5.46: Loss = 0.59314
Epoch 5.47: Loss = 0.751556
Epoch 5.48: Loss = 0.736389
Epoch 5.49: Loss = 0.636383
Epoch 5.50: Loss = 0.606949
Epoch 5.51: Loss = 0.732086
Epoch 5.52: Loss = 0.626068
Epoch 5.53: Loss = 0.666443
Epoch 5.54: Loss = 0.654587
Epoch 5.55: Loss = 0.73024
Epoch 5.56: Loss = 0.65892
Epoch 5.57: Loss = 0.546707
Epoch 5.58: Loss = 0.671509
Epoch 5.59: Loss = 0.648285
Epoch 5.60: Loss = 0.624008
Epoch 5.61: Loss = 0.566513
Epoch 5.62: Loss = 0.639893
Epoch 5.63: Loss = 0.605591
Epoch 5.64: Loss = 0.645126
Epoch 5.65: Loss = 0.658218
Epoch 5.66: Loss = 0.684937
Epoch 5.67: Loss = 0.592209
Epoch 5.68: Loss = 0.713959
Epoch 5.69: Loss = 0.611389
Epoch 5.70: Loss = 0.697739
Epoch 5.71: Loss = 0.720978
Epoch 5.72: Loss = 0.747284
Epoch 5.73: Loss = 0.679733
Epoch 5.74: Loss = 0.644196
Epoch 5.75: Loss = 0.629822
Epoch 5.76: Loss = 0.606293
Epoch 5.77: Loss = 0.604843
Epoch 5.78: Loss = 0.627518
Epoch 5.79: Loss = 0.571106
Epoch 5.80: Loss = 0.719589
Epoch 5.81: Loss = 0.855515
Epoch 5.82: Loss = 0.646835
Epoch 5.83: Loss = 0.678146
Epoch 5.84: Loss = 0.691971
Epoch 5.85: Loss = 0.749466
Epoch 5.86: Loss = 0.682037
Epoch 5.87: Loss = 0.734177
Epoch 5.88: Loss = 0.679306
Epoch 5.89: Loss = 0.7267
Epoch 5.90: Loss = 0.652283
Epoch 5.91: Loss = 0.73201
Epoch 5.92: Loss = 0.636688
Epoch 5.93: Loss = 0.616119
Epoch 5.94: Loss = 0.706467
Epoch 5.95: Loss = 0.787613
Epoch 5.96: Loss = 0.66629
Epoch 5.97: Loss = 0.737976
Epoch 5.98: Loss = 0.697601
Epoch 5.99: Loss = 0.683807
Epoch 5.100: Loss = 0.789597
Epoch 5.101: Loss = 0.646439
Epoch 5.102: Loss = 0.560669
Epoch 5.103: Loss = 0.740936
Epoch 5.104: Loss = 0.598267
Epoch 5.105: Loss = 0.749054
Epoch 5.106: Loss = 0.672012
Epoch 5.107: Loss = 0.702179
Epoch 5.108: Loss = 0.609314
Epoch 5.109: Loss = 0.748734
Epoch 5.110: Loss = 0.664917
Epoch 5.111: Loss = 0.646072
Epoch 5.112: Loss = 0.639038
Epoch 5.113: Loss = 0.669296
Epoch 5.114: Loss = 0.715195
Epoch 5.115: Loss = 0.67807
Epoch 5.116: Loss = 0.653625
Epoch 5.117: Loss = 0.622375
Epoch 5.118: Loss = 0.607834
Epoch 5.119: Loss = 0.733505
Epoch 5.120: Loss = 0.655457
TRAIN LOSS = 0.665283
TRAIN ACC = 80.2185 % (48133/60000)
Loss = 0.583023
Loss = 0.710464
Loss = 0.640594
Loss = 0.60405
Loss = 0.624985
Loss = 0.773315
Loss = 0.832642
Loss = 0.746704
Loss = 0.681046
Loss = 0.617233
Loss = 0.836517
Loss = 0.793304
Loss = 0.706116
Loss = 0.700928
Loss = 0.658524
Loss = 0.714584
Loss = 0.661743
Loss = 0.715012
Loss = 0.761185
Loss = 0.69928
TEST LOSS = 0.703062
TEST ACC = 481.329 % (7897/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.629608
Epoch 6.2: Loss = 0.642258
Epoch 6.3: Loss = 0.761444
Epoch 6.4: Loss = 0.627975
Epoch 6.5: Loss = 0.613007
Epoch 6.6: Loss = 0.72673
Epoch 6.7: Loss = 0.735153
Epoch 6.8: Loss = 0.584518
Epoch 6.9: Loss = 0.658051
Epoch 6.10: Loss = 0.653671
Epoch 6.11: Loss = 0.702484
Epoch 6.12: Loss = 0.732803
Epoch 6.13: Loss = 0.634216
Epoch 6.14: Loss = 0.678818
Epoch 6.15: Loss = 0.670242
Epoch 6.16: Loss = 0.685303
Epoch 6.17: Loss = 0.647415
Epoch 6.18: Loss = 0.752579
Epoch 6.19: Loss = 0.616577
Epoch 6.20: Loss = 0.664993
Epoch 6.21: Loss = 0.695236
Epoch 6.22: Loss = 0.625229
Epoch 6.23: Loss = 0.751251
Epoch 6.24: Loss = 0.721222
Epoch 6.25: Loss = 0.563126
Epoch 6.26: Loss = 0.644791
Epoch 6.27: Loss = 0.687973
Epoch 6.28: Loss = 0.60025
Epoch 6.29: Loss = 0.693832
Epoch 6.30: Loss = 0.67717
Epoch 6.31: Loss = 0.660141
Epoch 6.32: Loss = 0.688126
Epoch 6.33: Loss = 0.631729
Epoch 6.34: Loss = 0.655426
Epoch 6.35: Loss = 0.644394
Epoch 6.36: Loss = 0.737686
Epoch 6.37: Loss = 0.738937
Epoch 6.38: Loss = 0.642456
Epoch 6.39: Loss = 0.644638
Epoch 6.40: Loss = 0.602753
Epoch 6.41: Loss = 0.694565
Epoch 6.42: Loss = 0.578278
Epoch 6.43: Loss = 0.658783
Epoch 6.44: Loss = 0.683792
Epoch 6.45: Loss = 0.659943
Epoch 6.46: Loss = 0.741486
Epoch 6.47: Loss = 0.651611
Epoch 6.48: Loss = 0.698334
Epoch 6.49: Loss = 0.573776
Epoch 6.50: Loss = 0.575516
Epoch 6.51: Loss = 0.508026
Epoch 6.52: Loss = 0.718307
Epoch 6.53: Loss = 0.728256
Epoch 6.54: Loss = 0.625549
Epoch 6.55: Loss = 0.733505
Epoch 6.56: Loss = 0.634735
Epoch 6.57: Loss = 0.676758
Epoch 6.58: Loss = 0.720718
Epoch 6.59: Loss = 0.535019
Epoch 6.60: Loss = 0.598694
Epoch 6.61: Loss = 0.728836
Epoch 6.62: Loss = 0.618332
Epoch 6.63: Loss = 0.681274
Epoch 6.64: Loss = 0.652954
Epoch 6.65: Loss = 0.750702
Epoch 6.66: Loss = 0.747833
Epoch 6.67: Loss = 0.67099
Epoch 6.68: Loss = 0.608276
Epoch 6.69: Loss = 0.728546
Epoch 6.70: Loss = 0.609329
Epoch 6.71: Loss = 0.682175
Epoch 6.72: Loss = 0.763611
Epoch 6.73: Loss = 0.684067
Epoch 6.74: Loss = 0.654846
Epoch 6.75: Loss = 0.614349
Epoch 6.76: Loss = 0.676056
Epoch 6.77: Loss = 0.658569
Epoch 6.78: Loss = 0.574783
Epoch 6.79: Loss = 0.672928
Epoch 6.80: Loss = 0.719513
Epoch 6.81: Loss = 0.635071
Epoch 6.82: Loss = 0.619263
Epoch 6.83: Loss = 0.656937
Epoch 6.84: Loss = 0.704117
Epoch 6.85: Loss = 0.61351
Epoch 6.86: Loss = 0.490814
Epoch 6.87: Loss = 0.618546
Epoch 6.88: Loss = 0.711655
Epoch 6.89: Loss = 0.550156
Epoch 6.90: Loss = 0.662384
Epoch 6.91: Loss = 0.684357
Epoch 6.92: Loss = 0.651199
Epoch 6.93: Loss = 0.646072
Epoch 6.94: Loss = 0.67247
Epoch 6.95: Loss = 0.634842
Epoch 6.96: Loss = 0.538391
Epoch 6.97: Loss = 0.709564
Epoch 6.98: Loss = 0.703629
Epoch 6.99: Loss = 0.588074
Epoch 6.100: Loss = 0.734146
Epoch 6.101: Loss = 0.704437
Epoch 6.102: Loss = 0.610367
Epoch 6.103: Loss = 0.689819
Epoch 6.104: Loss = 0.659485
Epoch 6.105: Loss = 0.560196
Epoch 6.106: Loss = 0.786179
Epoch 6.107: Loss = 0.619003
Epoch 6.108: Loss = 0.630249
Epoch 6.109: Loss = 0.664383
Epoch 6.110: Loss = 0.625381
Epoch 6.111: Loss = 0.837082
Epoch 6.112: Loss = 0.560638
Epoch 6.113: Loss = 0.68277
Epoch 6.114: Loss = 0.634323
Epoch 6.115: Loss = 0.710571
Epoch 6.116: Loss = 0.57193
Epoch 6.117: Loss = 0.59523
Epoch 6.118: Loss = 0.634567
Epoch 6.119: Loss = 0.751465
Epoch 6.120: Loss = 0.570984
TRAIN LOSS = 0.659546
TRAIN ACC = 80.4916 % (48297/60000)
Loss = 0.577194
Loss = 0.686859
Loss = 0.646805
Loss = 0.592331
Loss = 0.621964
Loss = 0.784851
Loss = 0.85878
Loss = 0.770584
Loss = 0.685364
Loss = 0.629807
Loss = 0.841202
Loss = 0.813858
Loss = 0.703644
Loss = 0.713791
Loss = 0.668793
Loss = 0.70137
Loss = 0.715881
Loss = 0.712158
Loss = 0.790451
Loss = 0.723984
TEST LOSS = 0.711983
TEST ACC = 482.97 % (7927/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.662842
Epoch 7.2: Loss = 0.811356
Epoch 7.3: Loss = 0.681992
Epoch 7.4: Loss = 0.701935
Epoch 7.5: Loss = 0.795624
Epoch 7.6: Loss = 0.766724
Epoch 7.7: Loss = 0.591797
Epoch 7.8: Loss = 0.715363
Epoch 7.9: Loss = 0.691589
Epoch 7.10: Loss = 0.760727
Epoch 7.11: Loss = 0.658981
Epoch 7.12: Loss = 0.542755
Epoch 7.13: Loss = 0.551208
Epoch 7.14: Loss = 0.613693
Epoch 7.15: Loss = 0.634537
Epoch 7.16: Loss = 0.572205
Epoch 7.17: Loss = 0.631042
Epoch 7.18: Loss = 0.65892
Epoch 7.19: Loss = 0.667145
Epoch 7.20: Loss = 0.72551
Epoch 7.21: Loss = 0.62468
Epoch 7.22: Loss = 0.689758
Epoch 7.23: Loss = 0.554901
Epoch 7.24: Loss = 0.647339
Epoch 7.25: Loss = 0.535446
Epoch 7.26: Loss = 0.708755
Epoch 7.27: Loss = 0.603073
Epoch 7.28: Loss = 0.669434
Epoch 7.29: Loss = 0.631241
Epoch 7.30: Loss = 0.615829
Epoch 7.31: Loss = 0.660568
Epoch 7.32: Loss = 0.717514
Epoch 7.33: Loss = 0.690002
Epoch 7.34: Loss = 0.743759
Epoch 7.35: Loss = 0.696136
Epoch 7.36: Loss = 0.700394
Epoch 7.37: Loss = 0.653336
Epoch 7.38: Loss = 0.617157
Epoch 7.39: Loss = 0.714157
Epoch 7.40: Loss = 0.785324
Epoch 7.41: Loss = 0.880661
Epoch 7.42: Loss = 0.69458
Epoch 7.43: Loss = 0.72757
Epoch 7.44: Loss = 0.673706
Epoch 7.45: Loss = 0.806686
Epoch 7.46: Loss = 0.585358
Epoch 7.47: Loss = 0.579468
Epoch 7.48: Loss = 0.681717
Epoch 7.49: Loss = 0.550186
Epoch 7.50: Loss = 0.702988
Epoch 7.51: Loss = 0.671906
Epoch 7.52: Loss = 0.571152
Epoch 7.53: Loss = 0.719788
Epoch 7.54: Loss = 0.574875
Epoch 7.55: Loss = 0.591858
Epoch 7.56: Loss = 0.672409
Epoch 7.57: Loss = 0.738632
Epoch 7.58: Loss = 0.793762
Epoch 7.59: Loss = 0.559555
Epoch 7.60: Loss = 0.745422
Epoch 7.61: Loss = 0.732788
Epoch 7.62: Loss = 0.714005
Epoch 7.63: Loss = 0.627304
Epoch 7.64: Loss = 0.680786
Epoch 7.65: Loss = 0.618439
Epoch 7.66: Loss = 0.707779
Epoch 7.67: Loss = 0.676224
Epoch 7.68: Loss = 0.645096
Epoch 7.69: Loss = 0.711411
Epoch 7.70: Loss = 0.681427
Epoch 7.71: Loss = 0.570007
Epoch 7.72: Loss = 0.705185
Epoch 7.73: Loss = 0.695129
Epoch 7.74: Loss = 0.709244
Epoch 7.75: Loss = 0.700851
Epoch 7.76: Loss = 0.649643
Epoch 7.77: Loss = 0.726013
Epoch 7.78: Loss = 0.732452
Epoch 7.79: Loss = 0.669647
Epoch 7.80: Loss = 0.63092
Epoch 7.81: Loss = 0.65213
Epoch 7.82: Loss = 0.667725
Epoch 7.83: Loss = 0.639145
Epoch 7.84: Loss = 0.599426
Epoch 7.85: Loss = 0.631912
Epoch 7.86: Loss = 0.741516
Epoch 7.87: Loss = 0.643494
Epoch 7.88: Loss = 0.615829
Epoch 7.89: Loss = 0.719101
Epoch 7.90: Loss = 0.759842
Epoch 7.91: Loss = 0.627243
Epoch 7.92: Loss = 0.727432
Epoch 7.93: Loss = 0.531906
Epoch 7.94: Loss = 0.556671
Epoch 7.95: Loss = 0.744156
Epoch 7.96: Loss = 0.489395
Epoch 7.97: Loss = 0.757675
Epoch 7.98: Loss = 0.604691
Epoch 7.99: Loss = 0.640121
Epoch 7.100: Loss = 0.544418
Epoch 7.101: Loss = 0.748444
Epoch 7.102: Loss = 0.718719
Epoch 7.103: Loss = 0.687286
Epoch 7.104: Loss = 0.72113
Epoch 7.105: Loss = 0.729218
Epoch 7.106: Loss = 0.588715
Epoch 7.107: Loss = 0.70108
Epoch 7.108: Loss = 0.616516
Epoch 7.109: Loss = 0.749283
Epoch 7.110: Loss = 0.656448
Epoch 7.111: Loss = 0.641602
Epoch 7.112: Loss = 0.54631
Epoch 7.113: Loss = 0.752106
Epoch 7.114: Loss = 0.606995
Epoch 7.115: Loss = 0.686981
Epoch 7.116: Loss = 0.641647
Epoch 7.117: Loss = 0.721329
Epoch 7.118: Loss = 0.656754
Epoch 7.119: Loss = 0.681
Epoch 7.120: Loss = 0.735092
TRAIN LOSS = 0.669037
TRAIN ACC = 80.8029 % (48484/60000)
Loss = 0.577499
Loss = 0.697113
Loss = 0.646774
Loss = 0.573959
Loss = 0.646118
Loss = 0.788528
Loss = 0.865356
Loss = 0.756866
Loss = 0.692703
Loss = 0.620834
Loss = 0.846085
Loss = 0.833115
Loss = 0.703461
Loss = 0.697739
Loss = 0.679214
Loss = 0.694702
Loss = 0.698944
Loss = 0.722
Loss = 0.785843
Loss = 0.705276
TEST LOSS = 0.711606
TEST ACC = 484.839 % (7993/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.782822
Epoch 8.2: Loss = 0.609467
Epoch 8.3: Loss = 0.744431
Epoch 8.4: Loss = 0.766556
Epoch 8.5: Loss = 0.644836
Epoch 8.6: Loss = 0.568512
Epoch 8.7: Loss = 0.685349
Epoch 8.8: Loss = 0.708649
Epoch 8.9: Loss = 0.630035
Epoch 8.10: Loss = 0.760193
Epoch 8.11: Loss = 0.597702
Epoch 8.12: Loss = 0.715302
Epoch 8.13: Loss = 0.742889
Epoch 8.14: Loss = 0.679794
Epoch 8.15: Loss = 0.63382
Epoch 8.16: Loss = 0.602386
Epoch 8.17: Loss = 0.602539
Epoch 8.18: Loss = 0.741226
Epoch 8.19: Loss = 0.855225
Epoch 8.20: Loss = 0.665543
Epoch 8.21: Loss = 0.593002
Epoch 8.22: Loss = 0.540894
Epoch 8.23: Loss = 0.559723
Epoch 8.24: Loss = 0.614883
Epoch 8.25: Loss = 0.638016
Epoch 8.26: Loss = 0.586487
Epoch 8.27: Loss = 0.711365
Epoch 8.28: Loss = 0.815323
Epoch 8.29: Loss = 0.721741
Epoch 8.30: Loss = 0.718704
Epoch 8.31: Loss = 0.601257
Epoch 8.32: Loss = 0.583496
Epoch 8.33: Loss = 0.632141
Epoch 8.34: Loss = 0.54332
Epoch 8.35: Loss = 0.69101
Epoch 8.36: Loss = 0.542801
Epoch 8.37: Loss = 0.642975
Epoch 8.38: Loss = 0.589966
Epoch 8.39: Loss = 0.64064
Epoch 8.40: Loss = 0.73967
Epoch 8.41: Loss = 0.741394
Epoch 8.42: Loss = 0.675598
Epoch 8.43: Loss = 0.581223
Epoch 8.44: Loss = 0.738205
Epoch 8.45: Loss = 0.607407
Epoch 8.46: Loss = 0.671341
Epoch 8.47: Loss = 0.666809
Epoch 8.48: Loss = 0.677444
Epoch 8.49: Loss = 0.658127
Epoch 8.50: Loss = 0.677246
Epoch 8.51: Loss = 0.751923
Epoch 8.52: Loss = 0.716995
Epoch 8.53: Loss = 0.59021
Epoch 8.54: Loss = 0.654327
Epoch 8.55: Loss = 0.618378
Epoch 8.56: Loss = 0.664429
Epoch 8.57: Loss = 0.622009
Epoch 8.58: Loss = 0.681442
Epoch 8.59: Loss = 0.615494
Epoch 8.60: Loss = 0.661789
Epoch 8.61: Loss = 0.732132
Epoch 8.62: Loss = 0.681152
Epoch 8.63: Loss = 0.580582
Epoch 8.64: Loss = 0.727707
Epoch 8.65: Loss = 0.701981
Epoch 8.66: Loss = 0.698669
Epoch 8.67: Loss = 0.674698
Epoch 8.68: Loss = 0.759109
Epoch 8.69: Loss = 0.722488
Epoch 8.70: Loss = 0.593613
Epoch 8.71: Loss = 0.653214
Epoch 8.72: Loss = 0.612625
Epoch 8.73: Loss = 0.679626
Epoch 8.74: Loss = 0.683548
Epoch 8.75: Loss = 0.649948
Epoch 8.76: Loss = 0.740952
Epoch 8.77: Loss = 0.589874
Epoch 8.78: Loss = 0.749329
Epoch 8.79: Loss = 0.65448
Epoch 8.80: Loss = 0.538177
Epoch 8.81: Loss = 0.727097
Epoch 8.82: Loss = 0.64801
Epoch 8.83: Loss = 0.735687
Epoch 8.84: Loss = 0.684082
Epoch 8.85: Loss = 0.718597
Epoch 8.86: Loss = 0.580917
Epoch 8.87: Loss = 0.71701
Epoch 8.88: Loss = 0.661179
Epoch 8.89: Loss = 0.707489
Epoch 8.90: Loss = 0.63385
Epoch 8.91: Loss = 0.544922
Epoch 8.92: Loss = 0.569656
Epoch 8.93: Loss = 0.72345
Epoch 8.94: Loss = 0.673462
Epoch 8.95: Loss = 0.696121
Epoch 8.96: Loss = 0.637238
Epoch 8.97: Loss = 0.526917
Epoch 8.98: Loss = 0.772034
Epoch 8.99: Loss = 0.57634
Epoch 8.100: Loss = 0.612167
Epoch 8.101: Loss = 0.633453
Epoch 8.102: Loss = 0.656555
Epoch 8.103: Loss = 0.669846
Epoch 8.104: Loss = 0.651855
Epoch 8.105: Loss = 0.675461
Epoch 8.106: Loss = 0.477753
Epoch 8.107: Loss = 0.550064
Epoch 8.108: Loss = 0.707535
Epoch 8.109: Loss = 0.614258
Epoch 8.110: Loss = 0.771759
Epoch 8.111: Loss = 0.649826
Epoch 8.112: Loss = 0.548492
Epoch 8.113: Loss = 0.705063
Epoch 8.114: Loss = 0.680298
Epoch 8.115: Loss = 0.537308
Epoch 8.116: Loss = 0.610947
Epoch 8.117: Loss = 0.674515
Epoch 8.118: Loss = 0.642502
Epoch 8.119: Loss = 0.705475
Epoch 8.120: Loss = 0.728806
TRAIN LOSS = 0.659836
TRAIN ACC = 81.1081 % (48667/60000)
Loss = 0.565308
Loss = 0.671509
Loss = 0.643494
Loss = 0.568634
Loss = 0.637253
Loss = 0.759979
Loss = 0.861404
Loss = 0.745148
Loss = 0.655014
Loss = 0.608643
Loss = 0.828888
Loss = 0.828705
Loss = 0.696075
Loss = 0.684494
Loss = 0.658325
Loss = 0.68364
Loss = 0.693771
Loss = 0.722427
Loss = 0.758759
Loss = 0.693481
TEST LOSS = 0.698247
TEST ACC = 486.67 % (8019/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.733017
Epoch 9.2: Loss = 0.697739
Epoch 9.3: Loss = 0.686157
Epoch 9.4: Loss = 0.692856
Epoch 9.5: Loss = 0.738525
Epoch 9.6: Loss = 0.586395
Epoch 9.7: Loss = 0.657333
Epoch 9.8: Loss = 0.572647
Epoch 9.9: Loss = 0.702774
Epoch 9.10: Loss = 0.641983
Epoch 9.11: Loss = 0.779694
Epoch 9.12: Loss = 0.645111
Epoch 9.13: Loss = 0.585159
Epoch 9.14: Loss = 0.671234
Epoch 9.15: Loss = 0.65126
Epoch 9.16: Loss = 0.547516
Epoch 9.17: Loss = 0.706573
Epoch 9.18: Loss = 0.691025
Epoch 9.19: Loss = 0.73288
Epoch 9.20: Loss = 0.596603
Epoch 9.21: Loss = 0.682816
Epoch 9.22: Loss = 0.622604
Epoch 9.23: Loss = 0.617416
Epoch 9.24: Loss = 0.614212
Epoch 9.25: Loss = 0.722336
Epoch 9.26: Loss = 0.612198
Epoch 9.27: Loss = 0.539963
Epoch 9.28: Loss = 0.754089
Epoch 9.29: Loss = 0.760513
Epoch 9.30: Loss = 0.511276
Epoch 9.31: Loss = 0.643082
Epoch 9.32: Loss = 0.63327
Epoch 9.33: Loss = 0.703491
Epoch 9.34: Loss = 0.659882
Epoch 9.35: Loss = 0.528351
Epoch 9.36: Loss = 0.702713
Epoch 9.37: Loss = 0.68515
Epoch 9.38: Loss = 0.675186
Epoch 9.39: Loss = 0.573074
Epoch 9.40: Loss = 0.685059
Epoch 9.41: Loss = 0.579025
Epoch 9.42: Loss = 0.617935
Epoch 9.43: Loss = 0.607559
Epoch 9.44: Loss = 0.652634
Epoch 9.45: Loss = 0.580414
Epoch 9.46: Loss = 0.645523
Epoch 9.47: Loss = 0.701279
Epoch 9.48: Loss = 0.675003
Epoch 9.49: Loss = 0.669678
Epoch 9.50: Loss = 0.540878
Epoch 9.51: Loss = 0.929672
Epoch 9.52: Loss = 0.57399
Epoch 9.53: Loss = 0.755615
Epoch 9.54: Loss = 0.627792
Epoch 9.55: Loss = 0.598328
Epoch 9.56: Loss = 0.606354
Epoch 9.57: Loss = 0.659576
Epoch 9.58: Loss = 0.675446
Epoch 9.59: Loss = 0.664154
Epoch 9.60: Loss = 0.579071
Epoch 9.61: Loss = 0.662292
Epoch 9.62: Loss = 0.599579
Epoch 9.63: Loss = 0.715988
Epoch 9.64: Loss = 0.675903
Epoch 9.65: Loss = 0.635742
Epoch 9.66: Loss = 0.714996
Epoch 9.67: Loss = 0.732147
Epoch 9.68: Loss = 0.617065
Epoch 9.69: Loss = 0.694397
Epoch 9.70: Loss = 0.679581
Epoch 9.71: Loss = 0.733261
Epoch 9.72: Loss = 0.608078
Epoch 9.73: Loss = 0.703964
Epoch 9.74: Loss = 0.689651
Epoch 9.75: Loss = 0.532867
Epoch 9.76: Loss = 0.705765
Epoch 9.77: Loss = 0.626419
Epoch 9.78: Loss = 0.663422
Epoch 9.79: Loss = 0.822998
Epoch 9.80: Loss = 0.696335
Epoch 9.81: Loss = 0.717117
Epoch 9.82: Loss = 0.670395
Epoch 9.83: Loss = 0.699249
Epoch 9.84: Loss = 0.710342
Epoch 9.85: Loss = 0.722702
Epoch 9.86: Loss = 0.565292
Epoch 9.87: Loss = 0.791901
Epoch 9.88: Loss = 0.735016
Epoch 9.89: Loss = 0.723587
Epoch 9.90: Loss = 0.480209
Epoch 9.91: Loss = 0.797516
Epoch 9.92: Loss = 0.717224
Epoch 9.93: Loss = 0.75441
Epoch 9.94: Loss = 0.566727
Epoch 9.95: Loss = 0.62558
Epoch 9.96: Loss = 0.523254
Epoch 9.97: Loss = 0.687073
Epoch 9.98: Loss = 0.723038
Epoch 9.99: Loss = 0.599136
Epoch 9.100: Loss = 0.606628
Epoch 9.101: Loss = 0.658112
Epoch 9.102: Loss = 0.707123
Epoch 9.103: Loss = 0.607849
Epoch 9.104: Loss = 0.660828
Epoch 9.105: Loss = 0.698502
Epoch 9.106: Loss = 0.645401
Epoch 9.107: Loss = 0.615463
Epoch 9.108: Loss = 0.648758
Epoch 9.109: Loss = 0.809204
Epoch 9.110: Loss = 0.772629
Epoch 9.111: Loss = 0.76947
Epoch 9.112: Loss = 0.527573
Epoch 9.113: Loss = 0.853455
Epoch 9.114: Loss = 0.69519
Epoch 9.115: Loss = 0.759918
Epoch 9.116: Loss = 0.629883
Epoch 9.117: Loss = 0.640884
Epoch 9.118: Loss = 0.632233
Epoch 9.119: Loss = 0.645767
Epoch 9.120: Loss = 0.594681
TRAIN LOSS = 0.664047
TRAIN ACC = 81.3278 % (48799/60000)
Loss = 0.599518
Loss = 0.682953
Loss = 0.671021
Loss = 0.600311
Loss = 0.67276
Loss = 0.795624
Loss = 0.883453
Loss = 0.775604
Loss = 0.672607
Loss = 0.663391
Loss = 0.86795
Loss = 0.874146
Loss = 0.716934
Loss = 0.718292
Loss = 0.679749
Loss = 0.679947
Loss = 0.731827
Loss = 0.773605
Loss = 0.772781
Loss = 0.717392
TEST LOSS = 0.727493
TEST ACC = 487.99 % (8002/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.731232
Epoch 10.2: Loss = 0.52861
Epoch 10.3: Loss = 0.72467
Epoch 10.4: Loss = 0.637833
Epoch 10.5: Loss = 0.688889
Epoch 10.6: Loss = 0.734634
Epoch 10.7: Loss = 0.766418
Epoch 10.8: Loss = 0.57074
Epoch 10.9: Loss = 0.604691
Epoch 10.10: Loss = 0.771622
Epoch 10.11: Loss = 0.738342
Epoch 10.12: Loss = 0.618347
Epoch 10.13: Loss = 0.708893
Epoch 10.14: Loss = 0.63765
Epoch 10.15: Loss = 0.660126
Epoch 10.16: Loss = 0.805252
Epoch 10.17: Loss = 0.77243
Epoch 10.18: Loss = 0.539093
Epoch 10.19: Loss = 0.656387
Epoch 10.20: Loss = 0.611053
Epoch 10.21: Loss = 0.614929
Epoch 10.22: Loss = 0.644379
Epoch 10.23: Loss = 0.637314
Epoch 10.24: Loss = 0.605255
Epoch 10.25: Loss = 0.800507
Epoch 10.26: Loss = 0.677231
Epoch 10.27: Loss = 0.854523
Epoch 10.28: Loss = 0.675842
Epoch 10.29: Loss = 0.659012
Epoch 10.30: Loss = 0.655914
Epoch 10.31: Loss = 0.736557
Epoch 10.32: Loss = 0.777786
Epoch 10.33: Loss = 0.753418
Epoch 10.34: Loss = 0.618042
Epoch 10.35: Loss = 0.656052
Epoch 10.36: Loss = 0.709961
Epoch 10.37: Loss = 0.663956
Epoch 10.38: Loss = 0.619797
Epoch 10.39: Loss = 0.741852
Epoch 10.40: Loss = 0.77536
Epoch 10.41: Loss = 0.637054
Epoch 10.42: Loss = 0.559219
Epoch 10.43: Loss = 0.710281
Epoch 10.44: Loss = 0.466278
Epoch 10.45: Loss = 0.819412
Epoch 10.46: Loss = 0.802658
Epoch 10.47: Loss = 0.629257
Epoch 10.48: Loss = 0.683441
Epoch 10.49: Loss = 0.611008
Epoch 10.50: Loss = 0.600571
Epoch 10.51: Loss = 0.783173
Epoch 10.52: Loss = 0.570419
Epoch 10.53: Loss = 0.619461
Epoch 10.54: Loss = 0.805359
Epoch 10.55: Loss = 0.660156
Epoch 10.56: Loss = 0.670227
Epoch 10.57: Loss = 0.672455
Epoch 10.58: Loss = 0.580566
Epoch 10.59: Loss = 0.76799
Epoch 10.60: Loss = 0.680756
Epoch 10.61: Loss = 0.741776
Epoch 10.62: Loss = 0.638962
Epoch 10.63: Loss = 0.651474
Epoch 10.64: Loss = 0.725418
Epoch 10.65: Loss = 0.703033
Epoch 10.66: Loss = 0.661789
Epoch 10.67: Loss = 0.678391
Epoch 10.68: Loss = 0.646042
Epoch 10.69: Loss = 0.624496
Epoch 10.70: Loss = 0.604736
Epoch 10.71: Loss = 0.533356
Epoch 10.72: Loss = 0.749344
Epoch 10.73: Loss = 0.677292
Epoch 10.74: Loss = 0.647018
Epoch 10.75: Loss = 0.624817
Epoch 10.76: Loss = 0.628189
Epoch 10.77: Loss = 0.691299
Epoch 10.78: Loss = 0.564041
Epoch 10.79: Loss = 0.697891
Epoch 10.80: Loss = 0.687729
Epoch 10.81: Loss = 0.64386
Epoch 10.82: Loss = 0.706482
Epoch 10.83: Loss = 0.650223
Epoch 10.84: Loss = 0.594345
Epoch 10.85: Loss = 0.612076
Epoch 10.86: Loss = 0.719818
Epoch 10.87: Loss = 0.652161
Epoch 10.88: Loss = 0.554291
Epoch 10.89: Loss = 0.61647
Epoch 10.90: Loss = 0.710892
Epoch 10.91: Loss = 0.648117
Epoch 10.92: Loss = 0.534561
Epoch 10.93: Loss = 0.588211
Epoch 10.94: Loss = 0.578537
Epoch 10.95: Loss = 0.576431
Epoch 10.96: Loss = 0.670181
Epoch 10.97: Loss = 0.703583
Epoch 10.98: Loss = 0.703125
Epoch 10.99: Loss = 0.68071
Epoch 10.100: Loss = 0.78717
Epoch 10.101: Loss = 0.696732
Epoch 10.102: Loss = 0.623474
Epoch 10.103: Loss = 0.767303
Epoch 10.104: Loss = 0.777039
Epoch 10.105: Loss = 0.700592
Epoch 10.106: Loss = 0.792099
Epoch 10.107: Loss = 0.680099
Epoch 10.108: Loss = 0.615295
Epoch 10.109: Loss = 0.703705
Epoch 10.110: Loss = 0.79184
Epoch 10.111: Loss = 0.677216
Epoch 10.112: Loss = 0.588852
Epoch 10.113: Loss = 0.756943
Epoch 10.114: Loss = 0.617599
Epoch 10.115: Loss = 0.564621
Epoch 10.116: Loss = 0.512512
Epoch 10.117: Loss = 0.732758
Epoch 10.118: Loss = 0.689331
Epoch 10.119: Loss = 0.519348
Epoch 10.120: Loss = 0.744873
TRAIN LOSS = 0.669815
TRAIN ACC = 81.5186 % (48913/60000)
Loss = 0.592697
Loss = 0.656296
Loss = 0.650986
Loss = 0.579025
Loss = 0.629257
Loss = 0.766525
Loss = 0.880081
Loss = 0.758057
Loss = 0.644928
Loss = 0.642731
Loss = 0.854218
Loss = 0.845993
Loss = 0.702255
Loss = 0.680878
Loss = 0.661407
Loss = 0.656372
Loss = 0.716766
Loss = 0.743591
Loss = 0.740417
Loss = 0.68544
TEST LOSS = 0.704396
TEST ACC = 489.13 % (8054/10000)

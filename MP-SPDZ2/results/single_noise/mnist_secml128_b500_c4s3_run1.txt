Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.33354
Epoch 1.2: Loss = 2.26218
Epoch 1.3: Loss = 2.27083
Epoch 1.4: Loss = 2.23863
Epoch 1.5: Loss = 2.21634
Epoch 1.6: Loss = 2.19025
Epoch 1.7: Loss = 2.17358
Epoch 1.8: Loss = 2.1662
Epoch 1.9: Loss = 2.13985
Epoch 1.10: Loss = 2.10466
Epoch 1.11: Loss = 2.09108
Epoch 1.12: Loss = 2.04477
Epoch 1.13: Loss = 2.01782
Epoch 1.14: Loss = 2.00246
Epoch 1.15: Loss = 1.95537
Epoch 1.16: Loss = 1.92123
Epoch 1.17: Loss = 1.92548
Epoch 1.18: Loss = 1.87277
Epoch 1.19: Loss = 1.84705
Epoch 1.20: Loss = 1.836
Epoch 1.21: Loss = 1.81407
Epoch 1.22: Loss = 1.72957
Epoch 1.23: Loss = 1.77345
Epoch 1.24: Loss = 1.70381
Epoch 1.25: Loss = 1.71193
Epoch 1.26: Loss = 1.66554
Epoch 1.27: Loss = 1.62747
Epoch 1.28: Loss = 1.59807
Epoch 1.29: Loss = 1.56091
Epoch 1.30: Loss = 1.54546
Epoch 1.31: Loss = 1.51421
Epoch 1.32: Loss = 1.51006
Epoch 1.33: Loss = 1.42596
Epoch 1.34: Loss = 1.46109
Epoch 1.35: Loss = 1.38206
Epoch 1.36: Loss = 1.38451
Epoch 1.37: Loss = 1.37752
Epoch 1.38: Loss = 1.35989
Epoch 1.39: Loss = 1.27493
Epoch 1.40: Loss = 1.27106
Epoch 1.41: Loss = 1.26271
Epoch 1.42: Loss = 1.25565
Epoch 1.43: Loss = 1.22488
Epoch 1.44: Loss = 1.16174
Epoch 1.45: Loss = 1.1996
Epoch 1.46: Loss = 1.16489
Epoch 1.47: Loss = 1.14813
Epoch 1.48: Loss = 1.14015
Epoch 1.49: Loss = 1.17819
Epoch 1.50: Loss = 1.11485
Epoch 1.51: Loss = 1.11609
Epoch 1.52: Loss = 1.05656
Epoch 1.53: Loss = 1.09909
Epoch 1.54: Loss = 1.05025
Epoch 1.55: Loss = 1.04472
Epoch 1.56: Loss = 1.00218
Epoch 1.57: Loss = 0.976471
Epoch 1.58: Loss = 0.919296
Epoch 1.59: Loss = 0.999069
Epoch 1.60: Loss = 0.956757
Epoch 1.61: Loss = 0.935638
Epoch 1.62: Loss = 0.922516
Epoch 1.63: Loss = 0.948822
Epoch 1.64: Loss = 0.8517
Epoch 1.65: Loss = 0.90358
Epoch 1.66: Loss = 0.875336
Epoch 1.67: Loss = 0.86319
Epoch 1.68: Loss = 0.818329
Epoch 1.69: Loss = 0.824234
Epoch 1.70: Loss = 0.81752
Epoch 1.71: Loss = 0.844543
Epoch 1.72: Loss = 0.810852
Epoch 1.73: Loss = 0.823807
Epoch 1.74: Loss = 0.780289
Epoch 1.75: Loss = 0.816238
Epoch 1.76: Loss = 0.773407
Epoch 1.77: Loss = 0.805969
Epoch 1.78: Loss = 0.812698
Epoch 1.79: Loss = 0.725342
Epoch 1.80: Loss = 0.75737
Epoch 1.81: Loss = 0.750031
Epoch 1.82: Loss = 0.747314
Epoch 1.83: Loss = 0.700623
Epoch 1.84: Loss = 0.779602
Epoch 1.85: Loss = 0.698303
Epoch 1.86: Loss = 0.773148
Epoch 1.87: Loss = 0.716492
Epoch 1.88: Loss = 0.709412
Epoch 1.89: Loss = 0.799377
Epoch 1.90: Loss = 0.689255
Epoch 1.91: Loss = 0.722885
Epoch 1.92: Loss = 0.694321
Epoch 1.93: Loss = 0.720779
Epoch 1.94: Loss = 0.609726
Epoch 1.95: Loss = 0.730743
Epoch 1.96: Loss = 0.694046
Epoch 1.97: Loss = 0.668793
Epoch 1.98: Loss = 0.663239
Epoch 1.99: Loss = 0.71434
Epoch 1.100: Loss = 0.639877
Epoch 1.101: Loss = 0.635742
Epoch 1.102: Loss = 0.682846
Epoch 1.103: Loss = 0.680023
Epoch 1.104: Loss = 0.69194
Epoch 1.105: Loss = 0.638901
Epoch 1.106: Loss = 0.631073
Epoch 1.107: Loss = 0.609787
Epoch 1.108: Loss = 0.550476
Epoch 1.109: Loss = 0.584259
Epoch 1.110: Loss = 0.628326
Epoch 1.111: Loss = 0.646255
Epoch 1.112: Loss = 0.576355
Epoch 1.113: Loss = 0.737061
Epoch 1.114: Loss = 0.595673
Epoch 1.115: Loss = 0.56572
Epoch 1.116: Loss = 0.594559
Epoch 1.117: Loss = 0.563965
Epoch 1.118: Loss = 0.66745
Epoch 1.119: Loss = 0.690262
Epoch 1.120: Loss = 0.559479
TRAIN LOSS = 1.14865
TRAIN ACC = 67.775 % (40667/60000)
Loss = 0.595612
Loss = 0.649307
Loss = 0.731689
Loss = 0.72113
Loss = 0.74144
Loss = 0.604294
Loss = 0.586563
Loss = 0.764954
Loss = 0.721191
Loss = 0.674149
Loss = 0.348648
Loss = 0.498428
Loss = 0.39151
Loss = 0.517014
Loss = 0.427734
Loss = 0.512451
Loss = 0.399841
Loss = 0.232101
Loss = 0.430328
Loss = 0.648743
TEST LOSS = 0.559856
TEST ACC = 406.67 % (8322/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.573044
Epoch 2.2: Loss = 0.56813
Epoch 2.3: Loss = 0.647659
Epoch 2.4: Loss = 0.600464
Epoch 2.5: Loss = 0.610565
Epoch 2.6: Loss = 0.564407
Epoch 2.7: Loss = 0.571594
Epoch 2.8: Loss = 0.564468
Epoch 2.9: Loss = 0.555435
Epoch 2.10: Loss = 0.545685
Epoch 2.11: Loss = 0.514404
Epoch 2.12: Loss = 0.581528
Epoch 2.13: Loss = 0.522598
Epoch 2.14: Loss = 0.556061
Epoch 2.15: Loss = 0.603867
Epoch 2.16: Loss = 0.532867
Epoch 2.17: Loss = 0.561157
Epoch 2.18: Loss = 0.481964
Epoch 2.19: Loss = 0.559174
Epoch 2.20: Loss = 0.528091
Epoch 2.21: Loss = 0.547836
Epoch 2.22: Loss = 0.575714
Epoch 2.23: Loss = 0.472946
Epoch 2.24: Loss = 0.554001
Epoch 2.25: Loss = 0.472992
Epoch 2.26: Loss = 0.520569
Epoch 2.27: Loss = 0.495041
Epoch 2.28: Loss = 0.629593
Epoch 2.29: Loss = 0.499298
Epoch 2.30: Loss = 0.589279
Epoch 2.31: Loss = 0.486526
Epoch 2.32: Loss = 0.450287
Epoch 2.33: Loss = 0.539322
Epoch 2.34: Loss = 0.505386
Epoch 2.35: Loss = 0.537842
Epoch 2.36: Loss = 0.504425
Epoch 2.37: Loss = 0.482529
Epoch 2.38: Loss = 0.528519
Epoch 2.39: Loss = 0.545807
Epoch 2.40: Loss = 0.457611
Epoch 2.41: Loss = 0.469025
Epoch 2.42: Loss = 0.586304
Epoch 2.43: Loss = 0.501968
Epoch 2.44: Loss = 0.562546
Epoch 2.45: Loss = 0.466095
Epoch 2.46: Loss = 0.55246
Epoch 2.47: Loss = 0.519196
Epoch 2.48: Loss = 0.489563
Epoch 2.49: Loss = 0.476379
Epoch 2.50: Loss = 0.476486
Epoch 2.51: Loss = 0.450714
Epoch 2.52: Loss = 0.51384
Epoch 2.53: Loss = 0.520538
Epoch 2.54: Loss = 0.488495
Epoch 2.55: Loss = 0.480377
Epoch 2.56: Loss = 0.439911
Epoch 2.57: Loss = 0.491531
Epoch 2.58: Loss = 0.43248
Epoch 2.59: Loss = 0.496796
Epoch 2.60: Loss = 0.539764
Epoch 2.61: Loss = 0.512161
Epoch 2.62: Loss = 0.551849
Epoch 2.63: Loss = 0.466095
Epoch 2.64: Loss = 0.450882
Epoch 2.65: Loss = 0.49707
Epoch 2.66: Loss = 0.617844
Epoch 2.67: Loss = 0.435638
Epoch 2.68: Loss = 0.542084
Epoch 2.69: Loss = 0.469513
Epoch 2.70: Loss = 0.48674
Epoch 2.71: Loss = 0.549469
Epoch 2.72: Loss = 0.505554
Epoch 2.73: Loss = 0.466736
Epoch 2.74: Loss = 0.455872
Epoch 2.75: Loss = 0.501236
Epoch 2.76: Loss = 0.456207
Epoch 2.77: Loss = 0.424667
Epoch 2.78: Loss = 0.566177
Epoch 2.79: Loss = 0.508301
Epoch 2.80: Loss = 0.484482
Epoch 2.81: Loss = 0.436752
Epoch 2.82: Loss = 0.458023
Epoch 2.83: Loss = 0.481628
Epoch 2.84: Loss = 0.599915
Epoch 2.85: Loss = 0.521149
Epoch 2.86: Loss = 0.489868
Epoch 2.87: Loss = 0.480911
Epoch 2.88: Loss = 0.516479
Epoch 2.89: Loss = 0.47934
Epoch 2.90: Loss = 0.50444
Epoch 2.91: Loss = 0.506836
Epoch 2.92: Loss = 0.498154
Epoch 2.93: Loss = 0.429001
Epoch 2.94: Loss = 0.437119
Epoch 2.95: Loss = 0.456223
Epoch 2.96: Loss = 0.433838
Epoch 2.97: Loss = 0.402985
Epoch 2.98: Loss = 0.523392
Epoch 2.99: Loss = 0.468277
Epoch 2.100: Loss = 0.436691
Epoch 2.101: Loss = 0.420395
Epoch 2.102: Loss = 0.436142
Epoch 2.103: Loss = 0.544525
Epoch 2.104: Loss = 0.464188
Epoch 2.105: Loss = 0.520126
Epoch 2.106: Loss = 0.391891
Epoch 2.107: Loss = 0.407715
Epoch 2.108: Loss = 0.529892
Epoch 2.109: Loss = 0.574692
Epoch 2.110: Loss = 0.475723
Epoch 2.111: Loss = 0.481903
Epoch 2.112: Loss = 0.436905
Epoch 2.113: Loss = 0.414063
Epoch 2.114: Loss = 0.500366
Epoch 2.115: Loss = 0.37532
Epoch 2.116: Loss = 0.454956
Epoch 2.117: Loss = 0.479187
Epoch 2.118: Loss = 0.430008
Epoch 2.119: Loss = 0.447281
Epoch 2.120: Loss = 0.452682
TRAIN LOSS = 0.502853
TRAIN ACC = 84.8923 % (50938/60000)
Loss = 0.429123
Loss = 0.523041
Loss = 0.582916
Loss = 0.57576
Loss = 0.62236
Loss = 0.44426
Loss = 0.436172
Loss = 0.646515
Loss = 0.575577
Loss = 0.53479
Loss = 0.220459
Loss = 0.365021
Loss = 0.308685
Loss = 0.378693
Loss = 0.256027
Loss = 0.388947
Loss = 0.262344
Loss = 0.113968
Loss = 0.276031
Loss = 0.520325
TEST LOSS = 0.423051
TEST ACC = 509.38 % (8705/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.465225
Epoch 3.2: Loss = 0.446869
Epoch 3.3: Loss = 0.422882
Epoch 3.4: Loss = 0.437393
Epoch 3.5: Loss = 0.498398
Epoch 3.6: Loss = 0.359467
Epoch 3.7: Loss = 0.447021
Epoch 3.8: Loss = 0.307739
Epoch 3.9: Loss = 0.338287
Epoch 3.10: Loss = 0.424698
Epoch 3.11: Loss = 0.470718
Epoch 3.12: Loss = 0.412827
Epoch 3.13: Loss = 0.41922
Epoch 3.14: Loss = 0.420593
Epoch 3.15: Loss = 0.489365
Epoch 3.16: Loss = 0.471848
Epoch 3.17: Loss = 0.375351
Epoch 3.18: Loss = 0.473297
Epoch 3.19: Loss = 0.414642
Epoch 3.20: Loss = 0.495255
Epoch 3.21: Loss = 0.496994
Epoch 3.22: Loss = 0.413727
Epoch 3.23: Loss = 0.531128
Epoch 3.24: Loss = 0.426559
Epoch 3.25: Loss = 0.440964
Epoch 3.26: Loss = 0.369308
Epoch 3.27: Loss = 0.416107
Epoch 3.28: Loss = 0.468857
Epoch 3.29: Loss = 0.458527
Epoch 3.30: Loss = 0.454208
Epoch 3.31: Loss = 0.462234
Epoch 3.32: Loss = 0.480469
Epoch 3.33: Loss = 0.431763
Epoch 3.34: Loss = 0.442474
Epoch 3.35: Loss = 0.430511
Epoch 3.36: Loss = 0.451385
Epoch 3.37: Loss = 0.528122
Epoch 3.38: Loss = 0.505386
Epoch 3.39: Loss = 0.335617
Epoch 3.40: Loss = 0.472351
Epoch 3.41: Loss = 0.479645
Epoch 3.42: Loss = 0.431427
Epoch 3.43: Loss = 0.460403
Epoch 3.44: Loss = 0.41655
Epoch 3.45: Loss = 0.360916
Epoch 3.46: Loss = 0.452576
Epoch 3.47: Loss = 0.456985
Epoch 3.48: Loss = 0.507736
Epoch 3.49: Loss = 0.3797
Epoch 3.50: Loss = 0.391571
Epoch 3.51: Loss = 0.454895
Epoch 3.52: Loss = 0.508942
Epoch 3.53: Loss = 0.445969
Epoch 3.54: Loss = 0.403992
Epoch 3.55: Loss = 0.461365
Epoch 3.56: Loss = 0.323929
Epoch 3.57: Loss = 0.334641
Epoch 3.58: Loss = 0.462173
Epoch 3.59: Loss = 0.449478
Epoch 3.60: Loss = 0.443237
Epoch 3.61: Loss = 0.446274
Epoch 3.62: Loss = 0.412949
Epoch 3.63: Loss = 0.345261
Epoch 3.64: Loss = 0.423325
Epoch 3.65: Loss = 0.462433
Epoch 3.66: Loss = 0.372086
Epoch 3.67: Loss = 0.514069
Epoch 3.68: Loss = 0.390228
Epoch 3.69: Loss = 0.52327
Epoch 3.70: Loss = 0.484741
Epoch 3.71: Loss = 0.550781
Epoch 3.72: Loss = 0.386414
Epoch 3.73: Loss = 0.463058
Epoch 3.74: Loss = 0.456802
Epoch 3.75: Loss = 0.430344
Epoch 3.76: Loss = 0.427261
Epoch 3.77: Loss = 0.43718
Epoch 3.78: Loss = 0.609787
Epoch 3.79: Loss = 0.467987
Epoch 3.80: Loss = 0.56189
Epoch 3.81: Loss = 0.476151
Epoch 3.82: Loss = 0.404816
Epoch 3.83: Loss = 0.380676
Epoch 3.84: Loss = 0.395325
Epoch 3.85: Loss = 0.476974
Epoch 3.86: Loss = 0.50705
Epoch 3.87: Loss = 0.391663
Epoch 3.88: Loss = 0.435974
Epoch 3.89: Loss = 0.398987
Epoch 3.90: Loss = 0.466293
Epoch 3.91: Loss = 0.422379
Epoch 3.92: Loss = 0.403595
Epoch 3.93: Loss = 0.427582
Epoch 3.94: Loss = 0.505219
Epoch 3.95: Loss = 0.407822
Epoch 3.96: Loss = 0.406845
Epoch 3.97: Loss = 0.465942
Epoch 3.98: Loss = 0.434586
Epoch 3.99: Loss = 0.560593
Epoch 3.100: Loss = 0.486084
Epoch 3.101: Loss = 0.467361
Epoch 3.102: Loss = 0.418823
Epoch 3.103: Loss = 0.419647
Epoch 3.104: Loss = 0.421722
Epoch 3.105: Loss = 0.346237
Epoch 3.106: Loss = 0.431183
Epoch 3.107: Loss = 0.4048
Epoch 3.108: Loss = 0.379181
Epoch 3.109: Loss = 0.508926
Epoch 3.110: Loss = 0.415619
Epoch 3.111: Loss = 0.390991
Epoch 3.112: Loss = 0.351913
Epoch 3.113: Loss = 0.359528
Epoch 3.114: Loss = 0.462051
Epoch 3.115: Loss = 0.410507
Epoch 3.116: Loss = 0.350113
Epoch 3.117: Loss = 0.383865
Epoch 3.118: Loss = 0.348114
Epoch 3.119: Loss = 0.461899
Epoch 3.120: Loss = 0.339905
TRAIN LOSS = 0.436066
TRAIN ACC = 87.085 % (52253/60000)
Loss = 0.395554
Loss = 0.509323
Loss = 0.556961
Loss = 0.567627
Loss = 0.617355
Loss = 0.431519
Loss = 0.395416
Loss = 0.64296
Loss = 0.564285
Loss = 0.496002
Loss = 0.216965
Loss = 0.336823
Loss = 0.264069
Loss = 0.351395
Loss = 0.231491
Loss = 0.36467
Loss = 0.23024
Loss = 0.0828857
Loss = 0.253021
Loss = 0.517319
TEST LOSS = 0.401294
TEST ACC = 522.53 % (8819/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.421738
Epoch 4.2: Loss = 0.366806
Epoch 4.3: Loss = 0.419449
Epoch 4.4: Loss = 0.325348
Epoch 4.5: Loss = 0.405853
Epoch 4.6: Loss = 0.382309
Epoch 4.7: Loss = 0.512238
Epoch 4.8: Loss = 0.490128
Epoch 4.9: Loss = 0.433578
Epoch 4.10: Loss = 0.39592
Epoch 4.11: Loss = 0.38768
Epoch 4.12: Loss = 0.378906
Epoch 4.13: Loss = 0.497406
Epoch 4.14: Loss = 0.431702
Epoch 4.15: Loss = 0.409073
Epoch 4.16: Loss = 0.415314
Epoch 4.17: Loss = 0.336639
Epoch 4.18: Loss = 0.461197
Epoch 4.19: Loss = 0.494583
Epoch 4.20: Loss = 0.481888
Epoch 4.21: Loss = 0.410049
Epoch 4.22: Loss = 0.454086
Epoch 4.23: Loss = 0.398788
Epoch 4.24: Loss = 0.353012
Epoch 4.25: Loss = 0.335724
Epoch 4.26: Loss = 0.395386
Epoch 4.27: Loss = 0.408691
Epoch 4.28: Loss = 0.410004
Epoch 4.29: Loss = 0.463913
Epoch 4.30: Loss = 0.434525
Epoch 4.31: Loss = 0.34993
Epoch 4.32: Loss = 0.437393
Epoch 4.33: Loss = 0.423645
Epoch 4.34: Loss = 0.404816
Epoch 4.35: Loss = 0.471725
Epoch 4.36: Loss = 0.465271
Epoch 4.37: Loss = 0.388184
Epoch 4.38: Loss = 0.411942
Epoch 4.39: Loss = 0.448105
Epoch 4.40: Loss = 0.450882
Epoch 4.41: Loss = 0.346985
Epoch 4.42: Loss = 0.350037
Epoch 4.43: Loss = 0.487473
Epoch 4.44: Loss = 0.364792
Epoch 4.45: Loss = 0.303116
Epoch 4.46: Loss = 0.323151
Epoch 4.47: Loss = 0.438126
Epoch 4.48: Loss = 0.51416
Epoch 4.49: Loss = 0.468323
Epoch 4.50: Loss = 0.470444
Epoch 4.51: Loss = 0.39267
Epoch 4.52: Loss = 0.470673
Epoch 4.53: Loss = 0.480453
Epoch 4.54: Loss = 0.423233
Epoch 4.55: Loss = 0.339432
Epoch 4.56: Loss = 0.415802
Epoch 4.57: Loss = 0.44281
Epoch 4.58: Loss = 0.456177
Epoch 4.59: Loss = 0.399933
Epoch 4.60: Loss = 0.588287
Epoch 4.61: Loss = 0.460724
Epoch 4.62: Loss = 0.418777
Epoch 4.63: Loss = 0.419022
Epoch 4.64: Loss = 0.350342
Epoch 4.65: Loss = 0.419556
Epoch 4.66: Loss = 0.356125
Epoch 4.67: Loss = 0.397278
Epoch 4.68: Loss = 0.388794
Epoch 4.69: Loss = 0.437943
Epoch 4.70: Loss = 0.414581
Epoch 4.71: Loss = 0.411652
Epoch 4.72: Loss = 0.362152
Epoch 4.73: Loss = 0.426758
Epoch 4.74: Loss = 0.377426
Epoch 4.75: Loss = 0.377487
Epoch 4.76: Loss = 0.435593
Epoch 4.77: Loss = 0.441376
Epoch 4.78: Loss = 0.420654
Epoch 4.79: Loss = 0.44899
Epoch 4.80: Loss = 0.383682
Epoch 4.81: Loss = 0.403824
Epoch 4.82: Loss = 0.418381
Epoch 4.83: Loss = 0.285233
Epoch 4.84: Loss = 0.47023
Epoch 4.85: Loss = 0.422333
Epoch 4.86: Loss = 0.486191
Epoch 4.87: Loss = 0.443741
Epoch 4.88: Loss = 0.450378
Epoch 4.89: Loss = 0.419556
Epoch 4.90: Loss = 0.399246
Epoch 4.91: Loss = 0.456284
Epoch 4.92: Loss = 0.374817
Epoch 4.93: Loss = 0.535736
Epoch 4.94: Loss = 0.440063
Epoch 4.95: Loss = 0.416748
Epoch 4.96: Loss = 0.438675
Epoch 4.97: Loss = 0.527939
Epoch 4.98: Loss = 0.34874
Epoch 4.99: Loss = 0.385559
Epoch 4.100: Loss = 0.42659
Epoch 4.101: Loss = 0.464462
Epoch 4.102: Loss = 0.395599
Epoch 4.103: Loss = 0.408127
Epoch 4.104: Loss = 0.420319
Epoch 4.105: Loss = 0.385071
Epoch 4.106: Loss = 0.482849
Epoch 4.107: Loss = 0.362885
Epoch 4.108: Loss = 0.387146
Epoch 4.109: Loss = 0.392578
Epoch 4.110: Loss = 0.373184
Epoch 4.111: Loss = 0.425003
Epoch 4.112: Loss = 0.442978
Epoch 4.113: Loss = 0.364395
Epoch 4.114: Loss = 0.408112
Epoch 4.115: Loss = 0.360199
Epoch 4.116: Loss = 0.410599
Epoch 4.117: Loss = 0.394882
Epoch 4.118: Loss = 0.30368
Epoch 4.119: Loss = 0.420654
Epoch 4.120: Loss = 0.453217
TRAIN LOSS = 0.416626
TRAIN ACC = 88.0768 % (52849/60000)
Loss = 0.384811
Loss = 0.500183
Loss = 0.54451
Loss = 0.571609
Loss = 0.6008
Loss = 0.39389
Loss = 0.392715
Loss = 0.650833
Loss = 0.564423
Loss = 0.495987
Loss = 0.20192
Loss = 0.293152
Loss = 0.294754
Loss = 0.336105
Loss = 0.190521
Loss = 0.334045
Loss = 0.220764
Loss = 0.0665894
Loss = 0.263885
Loss = 0.507858
TEST LOSS = 0.390468
TEST ACC = 528.49 % (8901/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.461685
Epoch 5.2: Loss = 0.410233
Epoch 5.3: Loss = 0.406876
Epoch 5.4: Loss = 0.340439
Epoch 5.5: Loss = 0.437836
Epoch 5.6: Loss = 0.375961
Epoch 5.7: Loss = 0.38855
Epoch 5.8: Loss = 0.473862
Epoch 5.9: Loss = 0.434525
Epoch 5.10: Loss = 0.498154
Epoch 5.11: Loss = 0.412598
Epoch 5.12: Loss = 0.384232
Epoch 5.13: Loss = 0.423111
Epoch 5.14: Loss = 0.384003
Epoch 5.15: Loss = 0.398376
Epoch 5.16: Loss = 0.390732
Epoch 5.17: Loss = 0.330948
Epoch 5.18: Loss = 0.462845
Epoch 5.19: Loss = 0.403992
Epoch 5.20: Loss = 0.360001
Epoch 5.21: Loss = 0.486511
Epoch 5.22: Loss = 0.404999
Epoch 5.23: Loss = 0.421753
Epoch 5.24: Loss = 0.450363
Epoch 5.25: Loss = 0.41127
Epoch 5.26: Loss = 0.367645
Epoch 5.27: Loss = 0.438293
Epoch 5.28: Loss = 0.416519
Epoch 5.29: Loss = 0.360199
Epoch 5.30: Loss = 0.527069
Epoch 5.31: Loss = 0.405273
Epoch 5.32: Loss = 0.39769
Epoch 5.33: Loss = 0.341522
Epoch 5.34: Loss = 0.40979
Epoch 5.35: Loss = 0.397705
Epoch 5.36: Loss = 0.408905
Epoch 5.37: Loss = 0.348465
Epoch 5.38: Loss = 0.407257
Epoch 5.39: Loss = 0.428955
Epoch 5.40: Loss = 0.423477
Epoch 5.41: Loss = 0.431931
Epoch 5.42: Loss = 0.376617
Epoch 5.43: Loss = 0.337753
Epoch 5.44: Loss = 0.406296
Epoch 5.45: Loss = 0.305954
Epoch 5.46: Loss = 0.409256
Epoch 5.47: Loss = 0.366425
Epoch 5.48: Loss = 0.387817
Epoch 5.49: Loss = 0.506363
Epoch 5.50: Loss = 0.428146
Epoch 5.51: Loss = 0.365402
Epoch 5.52: Loss = 0.357956
Epoch 5.53: Loss = 0.376297
Epoch 5.54: Loss = 0.402481
Epoch 5.55: Loss = 0.450333
Epoch 5.56: Loss = 0.431534
Epoch 5.57: Loss = 0.38559
Epoch 5.58: Loss = 0.484146
Epoch 5.59: Loss = 0.389038
Epoch 5.60: Loss = 0.37059
Epoch 5.61: Loss = 0.440353
Epoch 5.62: Loss = 0.366425
Epoch 5.63: Loss = 0.428497
Epoch 5.64: Loss = 0.44397
Epoch 5.65: Loss = 0.418213
Epoch 5.66: Loss = 0.49646
Epoch 5.67: Loss = 0.408768
Epoch 5.68: Loss = 0.37265
Epoch 5.69: Loss = 0.417099
Epoch 5.70: Loss = 0.356979
Epoch 5.71: Loss = 0.377869
Epoch 5.72: Loss = 0.628967
Epoch 5.73: Loss = 0.365189
Epoch 5.74: Loss = 0.422394
Epoch 5.75: Loss = 0.446823
Epoch 5.76: Loss = 0.444962
Epoch 5.77: Loss = 0.453506
Epoch 5.78: Loss = 0.359528
Epoch 5.79: Loss = 0.38475
Epoch 5.80: Loss = 0.334732
Epoch 5.81: Loss = 0.399139
Epoch 5.82: Loss = 0.404785
Epoch 5.83: Loss = 0.485535
Epoch 5.84: Loss = 0.441071
Epoch 5.85: Loss = 0.457794
Epoch 5.86: Loss = 0.340134
Epoch 5.87: Loss = 0.384659
Epoch 5.88: Loss = 0.324951
Epoch 5.89: Loss = 0.394379
Epoch 5.90: Loss = 0.425446
Epoch 5.91: Loss = 0.310532
Epoch 5.92: Loss = 0.479431
Epoch 5.93: Loss = 0.376144
Epoch 5.94: Loss = 0.431641
Epoch 5.95: Loss = 0.417465
Epoch 5.96: Loss = 0.364288
Epoch 5.97: Loss = 0.355713
Epoch 5.98: Loss = 0.457626
Epoch 5.99: Loss = 0.418808
Epoch 5.100: Loss = 0.369965
Epoch 5.101: Loss = 0.387039
Epoch 5.102: Loss = 0.348663
Epoch 5.103: Loss = 0.492935
Epoch 5.104: Loss = 0.396866
Epoch 5.105: Loss = 0.428116
Epoch 5.106: Loss = 0.346985
Epoch 5.107: Loss = 0.375336
Epoch 5.108: Loss = 0.341293
Epoch 5.109: Loss = 0.480103
Epoch 5.110: Loss = 0.333878
Epoch 5.111: Loss = 0.418991
Epoch 5.112: Loss = 0.433044
Epoch 5.113: Loss = 0.435471
Epoch 5.114: Loss = 0.381836
Epoch 5.115: Loss = 0.371277
Epoch 5.116: Loss = 0.426605
Epoch 5.117: Loss = 0.357285
Epoch 5.118: Loss = 0.389816
Epoch 5.119: Loss = 0.502121
Epoch 5.120: Loss = 0.409088
TRAIN LOSS = 0.407227
TRAIN ACC = 88.6887 % (53216/60000)
Loss = 0.3591
Loss = 0.461731
Loss = 0.540283
Loss = 0.557739
Loss = 0.613922
Loss = 0.38382
Loss = 0.360214
Loss = 0.625809
Loss = 0.522369
Loss = 0.465775
Loss = 0.192291
Loss = 0.29924
Loss = 0.309509
Loss = 0.350342
Loss = 0.195755
Loss = 0.346848
Loss = 0.21846
Loss = 0.0579834
Loss = 0.248245
Loss = 0.531128
TEST LOSS = 0.382028
TEST ACC = 532.159 % (8919/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.386703
Epoch 6.2: Loss = 0.434601
Epoch 6.3: Loss = 0.343979
Epoch 6.4: Loss = 0.414001
Epoch 6.5: Loss = 0.43045
Epoch 6.6: Loss = 0.44928
Epoch 6.7: Loss = 0.522888
Epoch 6.8: Loss = 0.443054
Epoch 6.9: Loss = 0.442795
Epoch 6.10: Loss = 0.389435
Epoch 6.11: Loss = 0.336746
Epoch 6.12: Loss = 0.394791
Epoch 6.13: Loss = 0.433334
Epoch 6.14: Loss = 0.364136
Epoch 6.15: Loss = 0.465195
Epoch 6.16: Loss = 0.391663
Epoch 6.17: Loss = 0.391876
Epoch 6.18: Loss = 0.368576
Epoch 6.19: Loss = 0.354477
Epoch 6.20: Loss = 0.409393
Epoch 6.21: Loss = 0.428848
Epoch 6.22: Loss = 0.580063
Epoch 6.23: Loss = 0.408997
Epoch 6.24: Loss = 0.420425
Epoch 6.25: Loss = 0.366669
Epoch 6.26: Loss = 0.456451
Epoch 6.27: Loss = 0.449753
Epoch 6.28: Loss = 0.280075
Epoch 6.29: Loss = 0.418335
Epoch 6.30: Loss = 0.39064
Epoch 6.31: Loss = 0.307022
Epoch 6.32: Loss = 0.327454
Epoch 6.33: Loss = 0.320602
Epoch 6.34: Loss = 0.549072
Epoch 6.35: Loss = 0.390076
Epoch 6.36: Loss = 0.469543
Epoch 6.37: Loss = 0.432449
Epoch 6.38: Loss = 0.363358
Epoch 6.39: Loss = 0.342987
Epoch 6.40: Loss = 0.472183
Epoch 6.41: Loss = 0.35495
Epoch 6.42: Loss = 0.366867
Epoch 6.43: Loss = 0.40271
Epoch 6.44: Loss = 0.332748
Epoch 6.45: Loss = 0.395172
Epoch 6.46: Loss = 0.471069
Epoch 6.47: Loss = 0.384918
Epoch 6.48: Loss = 0.411316
Epoch 6.49: Loss = 0.437103
Epoch 6.50: Loss = 0.420105
Epoch 6.51: Loss = 0.478378
Epoch 6.52: Loss = 0.423508
Epoch 6.53: Loss = 0.426514
Epoch 6.54: Loss = 0.34671
Epoch 6.55: Loss = 0.355026
Epoch 6.56: Loss = 0.403915
Epoch 6.57: Loss = 0.336685
Epoch 6.58: Loss = 0.329056
Epoch 6.59: Loss = 0.524933
Epoch 6.60: Loss = 0.329132
Epoch 6.61: Loss = 0.346207
Epoch 6.62: Loss = 0.351242
Epoch 6.63: Loss = 0.295715
Epoch 6.64: Loss = 0.324631
Epoch 6.65: Loss = 0.404861
Epoch 6.66: Loss = 0.29863
Epoch 6.67: Loss = 0.326706
Epoch 6.68: Loss = 0.375473
Epoch 6.69: Loss = 0.385788
Epoch 6.70: Loss = 0.388855
Epoch 6.71: Loss = 0.338669
Epoch 6.72: Loss = 0.411865
Epoch 6.73: Loss = 0.393356
Epoch 6.74: Loss = 0.439682
Epoch 6.75: Loss = 0.454315
Epoch 6.76: Loss = 0.380371
Epoch 6.77: Loss = 0.413239
Epoch 6.78: Loss = 0.432602
Epoch 6.79: Loss = 0.479279
Epoch 6.80: Loss = 0.385498
Epoch 6.81: Loss = 0.458984
Epoch 6.82: Loss = 0.336945
Epoch 6.83: Loss = 0.448761
Epoch 6.84: Loss = 0.272705
Epoch 6.85: Loss = 0.491241
Epoch 6.86: Loss = 0.315796
Epoch 6.87: Loss = 0.439499
Epoch 6.88: Loss = 0.463943
Epoch 6.89: Loss = 0.329636
Epoch 6.90: Loss = 0.419586
Epoch 6.91: Loss = 0.333969
Epoch 6.92: Loss = 0.443466
Epoch 6.93: Loss = 0.403625
Epoch 6.94: Loss = 0.46431
Epoch 6.95: Loss = 0.319839
Epoch 6.96: Loss = 0.366089
Epoch 6.97: Loss = 0.522186
Epoch 6.98: Loss = 0.332504
Epoch 6.99: Loss = 0.433075
Epoch 6.100: Loss = 0.365189
Epoch 6.101: Loss = 0.303131
Epoch 6.102: Loss = 0.448288
Epoch 6.103: Loss = 0.355713
Epoch 6.104: Loss = 0.422653
Epoch 6.105: Loss = 0.392853
Epoch 6.106: Loss = 0.404999
Epoch 6.107: Loss = 0.445404
Epoch 6.108: Loss = 0.352371
Epoch 6.109: Loss = 0.283646
Epoch 6.110: Loss = 0.442108
Epoch 6.111: Loss = 0.291046
Epoch 6.112: Loss = 0.446442
Epoch 6.113: Loss = 0.276352
Epoch 6.114: Loss = 0.253128
Epoch 6.115: Loss = 0.434753
Epoch 6.116: Loss = 0.429443
Epoch 6.117: Loss = 0.396622
Epoch 6.118: Loss = 0.4711
Epoch 6.119: Loss = 0.412476
Epoch 6.120: Loss = 0.433121
TRAIN LOSS = 0.396301
TRAIN ACC = 89.2426 % (53548/60000)
Loss = 0.332474
Loss = 0.458099
Loss = 0.552353
Loss = 0.538086
Loss = 0.601395
Loss = 0.373856
Loss = 0.355164
Loss = 0.624283
Loss = 0.538574
Loss = 0.443207
Loss = 0.181976
Loss = 0.289856
Loss = 0.288574
Loss = 0.337723
Loss = 0.170349
Loss = 0.316254
Loss = 0.207443
Loss = 0.0485535
Loss = 0.255356
Loss = 0.496277
TEST LOSS = 0.370492
TEST ACC = 535.48 % (9009/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.403641
Epoch 7.2: Loss = 0.40274
Epoch 7.3: Loss = 0.429855
Epoch 7.4: Loss = 0.395599
Epoch 7.5: Loss = 0.451965
Epoch 7.6: Loss = 0.379364
Epoch 7.7: Loss = 0.490097
Epoch 7.8: Loss = 0.358521
Epoch 7.9: Loss = 0.384979
Epoch 7.10: Loss = 0.347061
Epoch 7.11: Loss = 0.419022
Epoch 7.12: Loss = 0.344513
Epoch 7.13: Loss = 0.398972
Epoch 7.14: Loss = 0.401291
Epoch 7.15: Loss = 0.463425
Epoch 7.16: Loss = 0.346603
Epoch 7.17: Loss = 0.351364
Epoch 7.18: Loss = 0.375443
Epoch 7.19: Loss = 0.363815
Epoch 7.20: Loss = 0.387634
Epoch 7.21: Loss = 0.464706
Epoch 7.22: Loss = 0.369705
Epoch 7.23: Loss = 0.45752
Epoch 7.24: Loss = 0.362595
Epoch 7.25: Loss = 0.398758
Epoch 7.26: Loss = 0.411987
Epoch 7.27: Loss = 0.427185
Epoch 7.28: Loss = 0.36972
Epoch 7.29: Loss = 0.371841
Epoch 7.30: Loss = 0.307816
Epoch 7.31: Loss = 0.426239
Epoch 7.32: Loss = 0.467209
Epoch 7.33: Loss = 0.332123
Epoch 7.34: Loss = 0.457108
Epoch 7.35: Loss = 0.50914
Epoch 7.36: Loss = 0.363022
Epoch 7.37: Loss = 0.311157
Epoch 7.38: Loss = 0.328598
Epoch 7.39: Loss = 0.355957
Epoch 7.40: Loss = 0.471405
Epoch 7.41: Loss = 0.419205
Epoch 7.42: Loss = 0.415726
Epoch 7.43: Loss = 0.430801
Epoch 7.44: Loss = 0.379623
Epoch 7.45: Loss = 0.338669
Epoch 7.46: Loss = 0.348038
Epoch 7.47: Loss = 0.422104
Epoch 7.48: Loss = 0.370132
Epoch 7.49: Loss = 0.342026
Epoch 7.50: Loss = 0.383347
Epoch 7.51: Loss = 0.370911
Epoch 7.52: Loss = 0.322647
Epoch 7.53: Loss = 0.310287
Epoch 7.54: Loss = 0.372208
Epoch 7.55: Loss = 0.489914
Epoch 7.56: Loss = 0.479416
Epoch 7.57: Loss = 0.478531
Epoch 7.58: Loss = 0.395432
Epoch 7.59: Loss = 0.409943
Epoch 7.60: Loss = 0.380737
Epoch 7.61: Loss = 0.358353
Epoch 7.62: Loss = 0.315598
Epoch 7.63: Loss = 0.417999
Epoch 7.64: Loss = 0.395004
Epoch 7.65: Loss = 0.41626
Epoch 7.66: Loss = 0.454437
Epoch 7.67: Loss = 0.373093
Epoch 7.68: Loss = 0.47702
Epoch 7.69: Loss = 0.342453
Epoch 7.70: Loss = 0.393372
Epoch 7.71: Loss = 0.393509
Epoch 7.72: Loss = 0.375244
Epoch 7.73: Loss = 0.429916
Epoch 7.74: Loss = 0.373428
Epoch 7.75: Loss = 0.375946
Epoch 7.76: Loss = 0.337952
Epoch 7.77: Loss = 0.335159
Epoch 7.78: Loss = 0.515976
Epoch 7.79: Loss = 0.448242
Epoch 7.80: Loss = 0.397186
Epoch 7.81: Loss = 0.32959
Epoch 7.82: Loss = 0.293808
Epoch 7.83: Loss = 0.433563
Epoch 7.84: Loss = 0.325058
Epoch 7.85: Loss = 0.412628
Epoch 7.86: Loss = 0.433441
Epoch 7.87: Loss = 0.536087
Epoch 7.88: Loss = 0.381149
Epoch 7.89: Loss = 0.347702
Epoch 7.90: Loss = 0.545654
Epoch 7.91: Loss = 0.467438
Epoch 7.92: Loss = 0.364731
Epoch 7.93: Loss = 0.380859
Epoch 7.94: Loss = 0.44873
Epoch 7.95: Loss = 0.5224
Epoch 7.96: Loss = 0.442734
Epoch 7.97: Loss = 0.285507
Epoch 7.98: Loss = 0.243561
Epoch 7.99: Loss = 0.405838
Epoch 7.100: Loss = 0.3004
Epoch 7.101: Loss = 0.366089
Epoch 7.102: Loss = 0.326187
Epoch 7.103: Loss = 0.343948
Epoch 7.104: Loss = 0.317841
Epoch 7.105: Loss = 0.338531
Epoch 7.106: Loss = 0.469009
Epoch 7.107: Loss = 0.427963
Epoch 7.108: Loss = 0.444321
Epoch 7.109: Loss = 0.418884
Epoch 7.110: Loss = 0.352097
Epoch 7.111: Loss = 0.411118
Epoch 7.112: Loss = 0.368378
Epoch 7.113: Loss = 0.348175
Epoch 7.114: Loss = 0.360199
Epoch 7.115: Loss = 0.437485
Epoch 7.116: Loss = 0.366318
Epoch 7.117: Loss = 0.352127
Epoch 7.118: Loss = 0.381271
Epoch 7.119: Loss = 0.483627
Epoch 7.120: Loss = 0.488434
TRAIN LOSS = 0.394562
TRAIN ACC = 89.566 % (53742/60000)
Loss = 0.36055
Loss = 0.484573
Loss = 0.559158
Loss = 0.595673
Loss = 0.614838
Loss = 0.395721
Loss = 0.371277
Loss = 0.65625
Loss = 0.535843
Loss = 0.47641
Loss = 0.17717
Loss = 0.290665
Loss = 0.305679
Loss = 0.344193
Loss = 0.166443
Loss = 0.310165
Loss = 0.210449
Loss = 0.0519562
Loss = 0.255478
Loss = 0.502182
TEST LOSS = 0.383234
TEST ACC = 537.419 % (9007/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.335831
Epoch 8.2: Loss = 0.437927
Epoch 8.3: Loss = 0.375015
Epoch 8.4: Loss = 0.453873
Epoch 8.5: Loss = 0.3871
Epoch 8.6: Loss = 0.418976
Epoch 8.7: Loss = 0.312469
Epoch 8.8: Loss = 0.366638
Epoch 8.9: Loss = 0.371796
Epoch 8.10: Loss = 0.430511
Epoch 8.11: Loss = 0.352585
Epoch 8.12: Loss = 0.404785
Epoch 8.13: Loss = 0.458618
Epoch 8.14: Loss = 0.391953
Epoch 8.15: Loss = 0.36441
Epoch 8.16: Loss = 0.396118
Epoch 8.17: Loss = 0.421829
Epoch 8.18: Loss = 0.444611
Epoch 8.19: Loss = 0.349136
Epoch 8.20: Loss = 0.502975
Epoch 8.21: Loss = 0.341003
Epoch 8.22: Loss = 0.378754
Epoch 8.23: Loss = 0.348175
Epoch 8.24: Loss = 0.381592
Epoch 8.25: Loss = 0.365799
Epoch 8.26: Loss = 0.469345
Epoch 8.27: Loss = 0.439789
Epoch 8.28: Loss = 0.376617
Epoch 8.29: Loss = 0.349701
Epoch 8.30: Loss = 0.39975
Epoch 8.31: Loss = 0.353745
Epoch 8.32: Loss = 0.379532
Epoch 8.33: Loss = 0.348846
Epoch 8.34: Loss = 0.481934
Epoch 8.35: Loss = 0.481598
Epoch 8.36: Loss = 0.29071
Epoch 8.37: Loss = 0.358017
Epoch 8.38: Loss = 0.328705
Epoch 8.39: Loss = 0.369003
Epoch 8.40: Loss = 0.387558
Epoch 8.41: Loss = 0.479858
Epoch 8.42: Loss = 0.447372
Epoch 8.43: Loss = 0.538422
Epoch 8.44: Loss = 0.379822
Epoch 8.45: Loss = 0.336136
Epoch 8.46: Loss = 0.413544
Epoch 8.47: Loss = 0.400497
Epoch 8.48: Loss = 0.438309
Epoch 8.49: Loss = 0.38797
Epoch 8.50: Loss = 0.379395
Epoch 8.51: Loss = 0.453522
Epoch 8.52: Loss = 0.346024
Epoch 8.53: Loss = 0.466904
Epoch 8.54: Loss = 0.326782
Epoch 8.55: Loss = 0.397995
Epoch 8.56: Loss = 0.498276
Epoch 8.57: Loss = 0.45787
Epoch 8.58: Loss = 0.353607
Epoch 8.59: Loss = 0.425873
Epoch 8.60: Loss = 0.411148
Epoch 8.61: Loss = 0.361832
Epoch 8.62: Loss = 0.398315
Epoch 8.63: Loss = 0.308411
Epoch 8.64: Loss = 0.423492
Epoch 8.65: Loss = 0.372345
Epoch 8.66: Loss = 0.326508
Epoch 8.67: Loss = 0.525696
Epoch 8.68: Loss = 0.44725
Epoch 8.69: Loss = 0.450714
Epoch 8.70: Loss = 0.394547
Epoch 8.71: Loss = 0.480621
Epoch 8.72: Loss = 0.530106
Epoch 8.73: Loss = 0.419296
Epoch 8.74: Loss = 0.397736
Epoch 8.75: Loss = 0.393188
Epoch 8.76: Loss = 0.36203
Epoch 8.77: Loss = 0.392487
Epoch 8.78: Loss = 0.373505
Epoch 8.79: Loss = 0.343979
Epoch 8.80: Loss = 0.425903
Epoch 8.81: Loss = 0.380051
Epoch 8.82: Loss = 0.377487
Epoch 8.83: Loss = 0.33226
Epoch 8.84: Loss = 0.336166
Epoch 8.85: Loss = 0.389923
Epoch 8.86: Loss = 0.395935
Epoch 8.87: Loss = 0.514832
Epoch 8.88: Loss = 0.390244
Epoch 8.89: Loss = 0.447754
Epoch 8.90: Loss = 0.46701
Epoch 8.91: Loss = 0.439911
Epoch 8.92: Loss = 0.489227
Epoch 8.93: Loss = 0.406219
Epoch 8.94: Loss = 0.5242
Epoch 8.95: Loss = 0.401093
Epoch 8.96: Loss = 0.312881
Epoch 8.97: Loss = 0.333237
Epoch 8.98: Loss = 0.332565
Epoch 8.99: Loss = 0.416885
Epoch 8.100: Loss = 0.454407
Epoch 8.101: Loss = 0.432968
Epoch 8.102: Loss = 0.408875
Epoch 8.103: Loss = 0.223694
Epoch 8.104: Loss = 0.484177
Epoch 8.105: Loss = 0.431503
Epoch 8.106: Loss = 0.431183
Epoch 8.107: Loss = 0.395416
Epoch 8.108: Loss = 0.288147
Epoch 8.109: Loss = 0.364151
Epoch 8.110: Loss = 0.366486
Epoch 8.111: Loss = 0.490845
Epoch 8.112: Loss = 0.456924
Epoch 8.113: Loss = 0.324265
Epoch 8.114: Loss = 0.319336
Epoch 8.115: Loss = 0.394684
Epoch 8.116: Loss = 0.350708
Epoch 8.117: Loss = 0.312729
Epoch 8.118: Loss = 0.494934
Epoch 8.119: Loss = 0.368271
Epoch 8.120: Loss = 0.48468
TRAIN LOSS = 0.40033
TRAIN ACC = 89.7644 % (53861/60000)
Loss = 0.346985
Loss = 0.496475
Loss = 0.562546
Loss = 0.593506
Loss = 0.620621
Loss = 0.395447
Loss = 0.367599
Loss = 0.642807
Loss = 0.541183
Loss = 0.481598
Loss = 0.164215
Loss = 0.297562
Loss = 0.293243
Loss = 0.347549
Loss = 0.174271
Loss = 0.344757
Loss = 0.224319
Loss = 0.0446472
Loss = 0.267853
Loss = 0.496765
TEST LOSS = 0.385197
TEST ACC = 538.609 % (9025/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.490753
Epoch 9.2: Loss = 0.424515
Epoch 9.3: Loss = 0.333298
Epoch 9.4: Loss = 0.47673
Epoch 9.5: Loss = 0.33754
Epoch 9.6: Loss = 0.411041
Epoch 9.7: Loss = 0.440262
Epoch 9.8: Loss = 0.282913
Epoch 9.9: Loss = 0.421692
Epoch 9.10: Loss = 0.445801
Epoch 9.11: Loss = 0.499084
Epoch 9.12: Loss = 0.415253
Epoch 9.13: Loss = 0.349365
Epoch 9.14: Loss = 0.351349
Epoch 9.15: Loss = 0.52153
Epoch 9.16: Loss = 0.338242
Epoch 9.17: Loss = 0.446045
Epoch 9.18: Loss = 0.447128
Epoch 9.19: Loss = 0.441147
Epoch 9.20: Loss = 0.352966
Epoch 9.21: Loss = 0.304993
Epoch 9.22: Loss = 0.530594
Epoch 9.23: Loss = 0.465302
Epoch 9.24: Loss = 0.465347
Epoch 9.25: Loss = 0.431351
Epoch 9.26: Loss = 0.34227
Epoch 9.27: Loss = 0.393112
Epoch 9.28: Loss = 0.38913
Epoch 9.29: Loss = 0.522736
Epoch 9.30: Loss = 0.553848
Epoch 9.31: Loss = 0.399551
Epoch 9.32: Loss = 0.32251
Epoch 9.33: Loss = 0.490906
Epoch 9.34: Loss = 0.320175
Epoch 9.35: Loss = 0.374146
Epoch 9.36: Loss = 0.319382
Epoch 9.37: Loss = 0.324921
Epoch 9.38: Loss = 0.39299
Epoch 9.39: Loss = 0.378326
Epoch 9.40: Loss = 0.393356
Epoch 9.41: Loss = 0.377686
Epoch 9.42: Loss = 0.531784
Epoch 9.43: Loss = 0.426895
Epoch 9.44: Loss = 0.399048
Epoch 9.45: Loss = 0.310516
Epoch 9.46: Loss = 0.334702
Epoch 9.47: Loss = 0.406769
Epoch 9.48: Loss = 0.563644
Epoch 9.49: Loss = 0.389389
Epoch 9.50: Loss = 0.368881
Epoch 9.51: Loss = 0.524048
Epoch 9.52: Loss = 0.348618
Epoch 9.53: Loss = 0.378494
Epoch 9.54: Loss = 0.529922
Epoch 9.55: Loss = 0.356522
Epoch 9.56: Loss = 0.444321
Epoch 9.57: Loss = 0.330643
Epoch 9.58: Loss = 0.384171
Epoch 9.59: Loss = 0.391083
Epoch 9.60: Loss = 0.350967
Epoch 9.61: Loss = 0.405869
Epoch 9.62: Loss = 0.348969
Epoch 9.63: Loss = 0.441574
Epoch 9.64: Loss = 0.465271
Epoch 9.65: Loss = 0.437485
Epoch 9.66: Loss = 0.511276
Epoch 9.67: Loss = 0.40416
Epoch 9.68: Loss = 0.333344
Epoch 9.69: Loss = 0.347961
Epoch 9.70: Loss = 0.361984
Epoch 9.71: Loss = 0.407288
Epoch 9.72: Loss = 0.409317
Epoch 9.73: Loss = 0.305267
Epoch 9.74: Loss = 0.343323
Epoch 9.75: Loss = 0.362823
Epoch 9.76: Loss = 0.44194
Epoch 9.77: Loss = 0.3452
Epoch 9.78: Loss = 0.446106
Epoch 9.79: Loss = 0.519928
Epoch 9.80: Loss = 0.421967
Epoch 9.81: Loss = 0.378311
Epoch 9.82: Loss = 0.429886
Epoch 9.83: Loss = 0.334091
Epoch 9.84: Loss = 0.374588
Epoch 9.85: Loss = 0.467697
Epoch 9.86: Loss = 0.292252
Epoch 9.87: Loss = 0.329651
Epoch 9.88: Loss = 0.415573
Epoch 9.89: Loss = 0.43866
Epoch 9.90: Loss = 0.334732
Epoch 9.91: Loss = 0.366119
Epoch 9.92: Loss = 0.338303
Epoch 9.93: Loss = 0.378845
Epoch 9.94: Loss = 0.399658
Epoch 9.95: Loss = 0.298386
Epoch 9.96: Loss = 0.33107
Epoch 9.97: Loss = 0.405991
Epoch 9.98: Loss = 0.323868
Epoch 9.99: Loss = 0.404434
Epoch 9.100: Loss = 0.494904
Epoch 9.101: Loss = 0.323318
Epoch 9.102: Loss = 0.281906
Epoch 9.103: Loss = 0.360046
Epoch 9.104: Loss = 0.341263
Epoch 9.105: Loss = 0.440414
Epoch 9.106: Loss = 0.386169
Epoch 9.107: Loss = 0.376633
Epoch 9.108: Loss = 0.449738
Epoch 9.109: Loss = 0.486511
Epoch 9.110: Loss = 0.382645
Epoch 9.111: Loss = 0.487854
Epoch 9.112: Loss = 0.33194
Epoch 9.113: Loss = 0.465256
Epoch 9.114: Loss = 0.39595
Epoch 9.115: Loss = 0.317627
Epoch 9.116: Loss = 0.439194
Epoch 9.117: Loss = 0.493851
Epoch 9.118: Loss = 0.483932
Epoch 9.119: Loss = 0.257034
Epoch 9.120: Loss = 0.426468
TRAIN LOSS = 0.400116
TRAIN ACC = 90.0131 % (54011/60000)
Loss = 0.349884
Loss = 0.492279
Loss = 0.550034
Loss = 0.58786
Loss = 0.612732
Loss = 0.399399
Loss = 0.34436
Loss = 0.672836
Loss = 0.553726
Loss = 0.489853
Loss = 0.188309
Loss = 0.301224
Loss = 0.278778
Loss = 0.348679
Loss = 0.177017
Loss = 0.323807
Loss = 0.204102
Loss = 0.0397186
Loss = 0.278564
Loss = 0.521225
TEST LOSS = 0.385719
TEST ACC = 540.109 % (9042/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.374069
Epoch 10.2: Loss = 0.423355
Epoch 10.3: Loss = 0.391479
Epoch 10.4: Loss = 0.305801
Epoch 10.5: Loss = 0.463409
Epoch 10.6: Loss = 0.427444
Epoch 10.7: Loss = 0.3078
Epoch 10.8: Loss = 0.436127
Epoch 10.9: Loss = 0.432724
Epoch 10.10: Loss = 0.376633
Epoch 10.11: Loss = 0.381012
Epoch 10.12: Loss = 0.46405
Epoch 10.13: Loss = 0.366974
Epoch 10.14: Loss = 0.347961
Epoch 10.15: Loss = 0.274139
Epoch 10.16: Loss = 0.37767
Epoch 10.17: Loss = 0.595886
Epoch 10.18: Loss = 0.330582
Epoch 10.19: Loss = 0.302261
Epoch 10.20: Loss = 0.469528
Epoch 10.21: Loss = 0.341614
Epoch 10.22: Loss = 0.442047
Epoch 10.23: Loss = 0.396469
Epoch 10.24: Loss = 0.538818
Epoch 10.25: Loss = 0.327057
Epoch 10.26: Loss = 0.328339
Epoch 10.27: Loss = 0.281677
Epoch 10.28: Loss = 0.328613
Epoch 10.29: Loss = 0.408463
Epoch 10.30: Loss = 0.317062
Epoch 10.31: Loss = 0.459808
Epoch 10.32: Loss = 0.406967
Epoch 10.33: Loss = 0.450211
Epoch 10.34: Loss = 0.454544
Epoch 10.35: Loss = 0.523544
Epoch 10.36: Loss = 0.361542
Epoch 10.37: Loss = 0.450546
Epoch 10.38: Loss = 0.390182
Epoch 10.39: Loss = 0.522964
Epoch 10.40: Loss = 0.450775
Epoch 10.41: Loss = 0.380005
Epoch 10.42: Loss = 0.332016
Epoch 10.43: Loss = 0.464264
Epoch 10.44: Loss = 0.324738
Epoch 10.45: Loss = 0.42041
Epoch 10.46: Loss = 0.397827
Epoch 10.47: Loss = 0.372513
Epoch 10.48: Loss = 0.44342
Epoch 10.49: Loss = 0.371674
Epoch 10.50: Loss = 0.411652
Epoch 10.51: Loss = 0.416672
Epoch 10.52: Loss = 0.491745
Epoch 10.53: Loss = 0.374222
Epoch 10.54: Loss = 0.446182
Epoch 10.55: Loss = 0.409973
Epoch 10.56: Loss = 0.410965
Epoch 10.57: Loss = 0.40509
Epoch 10.58: Loss = 0.266495
Epoch 10.59: Loss = 0.402084
Epoch 10.60: Loss = 0.487427
Epoch 10.61: Loss = 0.545776
Epoch 10.62: Loss = 0.393036
Epoch 10.63: Loss = 0.339188
Epoch 10.64: Loss = 0.350159
Epoch 10.65: Loss = 0.366531
Epoch 10.66: Loss = 0.468521
Epoch 10.67: Loss = 0.321411
Epoch 10.68: Loss = 0.372711
Epoch 10.69: Loss = 0.405716
Epoch 10.70: Loss = 0.383514
Epoch 10.71: Loss = 0.398773
Epoch 10.72: Loss = 0.444183
Epoch 10.73: Loss = 0.5009
Epoch 10.74: Loss = 0.360291
Epoch 10.75: Loss = 0.323792
Epoch 10.76: Loss = 0.363937
Epoch 10.77: Loss = 0.437027
Epoch 10.78: Loss = 0.361664
Epoch 10.79: Loss = 0.361481
Epoch 10.80: Loss = 0.427658
Epoch 10.81: Loss = 0.442291
Epoch 10.82: Loss = 0.404037
Epoch 10.83: Loss = 0.438538
Epoch 10.84: Loss = 0.463318
Epoch 10.85: Loss = 0.369522
Epoch 10.86: Loss = 0.507553
Epoch 10.87: Loss = 0.539749
Epoch 10.88: Loss = 0.464676
Epoch 10.89: Loss = 0.434494
Epoch 10.90: Loss = 0.375931
Epoch 10.91: Loss = 0.373215
Epoch 10.92: Loss = 0.503662
Epoch 10.93: Loss = 0.414124
Epoch 10.94: Loss = 0.393875
Epoch 10.95: Loss = 0.415207
Epoch 10.96: Loss = 0.549408
Epoch 10.97: Loss = 0.406464
Epoch 10.98: Loss = 0.379837
Epoch 10.99: Loss = 0.418625
Epoch 10.100: Loss = 0.449341
Epoch 10.101: Loss = 0.40358
Epoch 10.102: Loss = 0.420929
Epoch 10.103: Loss = 0.500229
Epoch 10.104: Loss = 0.51236
Epoch 10.105: Loss = 0.526245
Epoch 10.106: Loss = 0.321274
Epoch 10.107: Loss = 0.42421
Epoch 10.108: Loss = 0.333344
Epoch 10.109: Loss = 0.410294
Epoch 10.110: Loss = 0.366638
Epoch 10.111: Loss = 0.421341
Epoch 10.112: Loss = 0.469543
Epoch 10.113: Loss = 0.484833
Epoch 10.114: Loss = 0.463486
Epoch 10.115: Loss = 0.256317
Epoch 10.116: Loss = 0.332932
Epoch 10.117: Loss = 0.453354
Epoch 10.118: Loss = 0.321442
Epoch 10.119: Loss = 0.367798
Epoch 10.120: Loss = 0.442932
TRAIN LOSS = 0.408035
TRAIN ACC = 90.0299 % (54021/60000)
Loss = 0.341705
Loss = 0.486282
Loss = 0.543152
Loss = 0.60257
Loss = 0.611359
Loss = 0.376877
Loss = 0.325241
Loss = 0.679596
Loss = 0.591019
Loss = 0.507599
Loss = 0.188461
Loss = 0.304214
Loss = 0.292068
Loss = 0.363876
Loss = 0.195023
Loss = 0.308456
Loss = 0.21434
Loss = 0.0399017
Loss = 0.279617
Loss = 0.557358
TEST LOSS = 0.390436
TEST ACC = 540.208 % (9030/10000)

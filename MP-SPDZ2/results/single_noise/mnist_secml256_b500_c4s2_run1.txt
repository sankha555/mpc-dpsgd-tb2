Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.29973
Epoch 1.2: Loss = 2.28157
Epoch 1.3: Loss = 2.2462
Epoch 1.4: Loss = 2.22795
Epoch 1.5: Loss = 2.1893
Epoch 1.6: Loss = 2.18103
Epoch 1.7: Loss = 2.13373
Epoch 1.8: Loss = 2.10847
Epoch 1.9: Loss = 2.06133
Epoch 1.10: Loss = 2.04858
Epoch 1.11: Loss = 2.02002
Epoch 1.12: Loss = 2.00282
Epoch 1.13: Loss = 1.97653
Epoch 1.14: Loss = 1.91972
Epoch 1.15: Loss = 1.92093
Epoch 1.16: Loss = 1.88541
Epoch 1.17: Loss = 1.88821
Epoch 1.18: Loss = 1.84189
Epoch 1.19: Loss = 1.79274
Epoch 1.20: Loss = 1.77242
Epoch 1.21: Loss = 1.70488
Epoch 1.22: Loss = 1.72318
Epoch 1.23: Loss = 1.64536
Epoch 1.24: Loss = 1.61421
Epoch 1.25: Loss = 1.60948
Epoch 1.26: Loss = 1.59071
Epoch 1.27: Loss = 1.58212
Epoch 1.28: Loss = 1.51453
Epoch 1.29: Loss = 1.46515
Epoch 1.30: Loss = 1.52022
Epoch 1.31: Loss = 1.46193
Epoch 1.32: Loss = 1.41943
Epoch 1.33: Loss = 1.4351
Epoch 1.34: Loss = 1.37662
Epoch 1.35: Loss = 1.35068
Epoch 1.36: Loss = 1.295
Epoch 1.37: Loss = 1.24762
Epoch 1.38: Loss = 1.30098
Epoch 1.39: Loss = 1.24832
Epoch 1.40: Loss = 1.20695
Epoch 1.41: Loss = 1.19734
Epoch 1.42: Loss = 1.23112
Epoch 1.43: Loss = 1.18896
Epoch 1.44: Loss = 1.10945
Epoch 1.45: Loss = 1.11858
Epoch 1.46: Loss = 1.0649
Epoch 1.47: Loss = 1.04817
Epoch 1.48: Loss = 1.03427
Epoch 1.49: Loss = 0.974899
Epoch 1.50: Loss = 1.0752
Epoch 1.51: Loss = 1.0186
Epoch 1.52: Loss = 0.984268
Epoch 1.53: Loss = 1.01331
Epoch 1.54: Loss = 1.01694
Epoch 1.55: Loss = 0.9608
Epoch 1.56: Loss = 0.915298
Epoch 1.57: Loss = 0.88475
Epoch 1.58: Loss = 0.884781
Epoch 1.59: Loss = 0.917892
Epoch 1.60: Loss = 0.893967
Epoch 1.61: Loss = 0.921188
Epoch 1.62: Loss = 0.869385
Epoch 1.63: Loss = 0.900146
Epoch 1.64: Loss = 0.844681
Epoch 1.65: Loss = 0.800095
Epoch 1.66: Loss = 0.846771
Epoch 1.67: Loss = 0.837524
Epoch 1.68: Loss = 0.792252
Epoch 1.69: Loss = 0.830185
Epoch 1.70: Loss = 0.76236
Epoch 1.71: Loss = 0.833466
Epoch 1.72: Loss = 0.76561
Epoch 1.73: Loss = 0.741241
Epoch 1.74: Loss = 0.769455
Epoch 1.75: Loss = 0.803085
Epoch 1.76: Loss = 0.742142
Epoch 1.77: Loss = 0.713745
Epoch 1.78: Loss = 0.717056
Epoch 1.79: Loss = 0.68103
Epoch 1.80: Loss = 0.741714
Epoch 1.81: Loss = 0.703857
Epoch 1.82: Loss = 0.67395
Epoch 1.83: Loss = 0.697739
Epoch 1.84: Loss = 0.691696
Epoch 1.85: Loss = 0.713547
Epoch 1.86: Loss = 0.615646
Epoch 1.87: Loss = 0.669525
Epoch 1.88: Loss = 0.644226
Epoch 1.89: Loss = 0.669281
Epoch 1.90: Loss = 0.667709
Epoch 1.91: Loss = 0.653381
Epoch 1.92: Loss = 0.683182
Epoch 1.93: Loss = 0.62764
Epoch 1.94: Loss = 0.592712
Epoch 1.95: Loss = 0.663528
Epoch 1.96: Loss = 0.617966
Epoch 1.97: Loss = 0.607407
Epoch 1.98: Loss = 0.637436
Epoch 1.99: Loss = 0.605011
Epoch 1.100: Loss = 0.625122
Epoch 1.101: Loss = 0.567398
Epoch 1.102: Loss = 0.564621
Epoch 1.103: Loss = 0.643738
Epoch 1.104: Loss = 0.590759
Epoch 1.105: Loss = 0.597519
Epoch 1.106: Loss = 0.607285
Epoch 1.107: Loss = 0.554153
Epoch 1.108: Loss = 0.519577
Epoch 1.109: Loss = 0.553391
Epoch 1.110: Loss = 0.571152
Epoch 1.111: Loss = 0.536942
Epoch 1.112: Loss = 0.538254
Epoch 1.113: Loss = 0.590668
Epoch 1.114: Loss = 0.52887
Epoch 1.115: Loss = 0.559082
Epoch 1.116: Loss = 0.51976
Epoch 1.117: Loss = 0.539703
Epoch 1.118: Loss = 0.531204
Epoch 1.119: Loss = 0.479507
Epoch 1.120: Loss = 0.538467
TRAIN LOSS = 1.0896
TRAIN ACC = 71.5454 % (42930/60000)
Loss = 0.567947
Loss = 0.569946
Loss = 0.697037
Loss = 0.652206
Loss = 0.689423
Loss = 0.586182
Loss = 0.537415
Loss = 0.715485
Loss = 0.656479
Loss = 0.621323
Loss = 0.32164
Loss = 0.484009
Loss = 0.346313
Loss = 0.530029
Loss = 0.400986
Loss = 0.429108
Loss = 0.368744
Loss = 0.21228
Loss = 0.370819
Loss = 0.63649
TEST LOSS = 0.519693
TEST ACC = 429.3 % (8435/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.626495
Epoch 2.2: Loss = 0.575287
Epoch 2.3: Loss = 0.596466
Epoch 2.4: Loss = 0.53244
Epoch 2.5: Loss = 0.516464
Epoch 2.6: Loss = 0.530167
Epoch 2.7: Loss = 0.53717
Epoch 2.8: Loss = 0.491409
Epoch 2.9: Loss = 0.577423
Epoch 2.10: Loss = 0.554276
Epoch 2.11: Loss = 0.546539
Epoch 2.12: Loss = 0.480042
Epoch 2.13: Loss = 0.488312
Epoch 2.14: Loss = 0.569061
Epoch 2.15: Loss = 0.498413
Epoch 2.16: Loss = 0.475677
Epoch 2.17: Loss = 0.549515
Epoch 2.18: Loss = 0.491089
Epoch 2.19: Loss = 0.436905
Epoch 2.20: Loss = 0.422623
Epoch 2.21: Loss = 0.549622
Epoch 2.22: Loss = 0.446259
Epoch 2.23: Loss = 0.5168
Epoch 2.24: Loss = 0.466537
Epoch 2.25: Loss = 0.544022
Epoch 2.26: Loss = 0.629272
Epoch 2.27: Loss = 0.540726
Epoch 2.28: Loss = 0.494064
Epoch 2.29: Loss = 0.449722
Epoch 2.30: Loss = 0.509354
Epoch 2.31: Loss = 0.535568
Epoch 2.32: Loss = 0.499207
Epoch 2.33: Loss = 0.49176
Epoch 2.34: Loss = 0.475784
Epoch 2.35: Loss = 0.521835
Epoch 2.36: Loss = 0.401413
Epoch 2.37: Loss = 0.458694
Epoch 2.38: Loss = 0.439377
Epoch 2.39: Loss = 0.443069
Epoch 2.40: Loss = 0.433731
Epoch 2.41: Loss = 0.523392
Epoch 2.42: Loss = 0.434677
Epoch 2.43: Loss = 0.454178
Epoch 2.44: Loss = 0.463593
Epoch 2.45: Loss = 0.496033
Epoch 2.46: Loss = 0.380264
Epoch 2.47: Loss = 0.503082
Epoch 2.48: Loss = 0.480499
Epoch 2.49: Loss = 0.395508
Epoch 2.50: Loss = 0.622726
Epoch 2.51: Loss = 0.428024
Epoch 2.52: Loss = 0.436523
Epoch 2.53: Loss = 0.453949
Epoch 2.54: Loss = 0.420898
Epoch 2.55: Loss = 0.378647
Epoch 2.56: Loss = 0.503769
Epoch 2.57: Loss = 0.48114
Epoch 2.58: Loss = 0.468506
Epoch 2.59: Loss = 0.469162
Epoch 2.60: Loss = 0.460159
Epoch 2.61: Loss = 0.436615
Epoch 2.62: Loss = 0.500366
Epoch 2.63: Loss = 0.536606
Epoch 2.64: Loss = 0.508041
Epoch 2.65: Loss = 0.409515
Epoch 2.66: Loss = 0.433334
Epoch 2.67: Loss = 0.428467
Epoch 2.68: Loss = 0.462952
Epoch 2.69: Loss = 0.454559
Epoch 2.70: Loss = 0.41655
Epoch 2.71: Loss = 0.427383
Epoch 2.72: Loss = 0.429855
Epoch 2.73: Loss = 0.527786
Epoch 2.74: Loss = 0.484116
Epoch 2.75: Loss = 0.467224
Epoch 2.76: Loss = 0.520172
Epoch 2.77: Loss = 0.373596
Epoch 2.78: Loss = 0.416641
Epoch 2.79: Loss = 0.396881
Epoch 2.80: Loss = 0.503326
Epoch 2.81: Loss = 0.409531
Epoch 2.82: Loss = 0.462845
Epoch 2.83: Loss = 0.450531
Epoch 2.84: Loss = 0.427475
Epoch 2.85: Loss = 0.431122
Epoch 2.86: Loss = 0.45694
Epoch 2.87: Loss = 0.376434
Epoch 2.88: Loss = 0.530289
Epoch 2.89: Loss = 0.488373
Epoch 2.90: Loss = 0.431854
Epoch 2.91: Loss = 0.421234
Epoch 2.92: Loss = 0.43895
Epoch 2.93: Loss = 0.445328
Epoch 2.94: Loss = 0.423767
Epoch 2.95: Loss = 0.448486
Epoch 2.96: Loss = 0.487396
Epoch 2.97: Loss = 0.418457
Epoch 2.98: Loss = 0.432465
Epoch 2.99: Loss = 0.356094
Epoch 2.100: Loss = 0.437057
Epoch 2.101: Loss = 0.418762
Epoch 2.102: Loss = 0.429092
Epoch 2.103: Loss = 0.329483
Epoch 2.104: Loss = 0.402908
Epoch 2.105: Loss = 0.467682
Epoch 2.106: Loss = 0.408005
Epoch 2.107: Loss = 0.472931
Epoch 2.108: Loss = 0.445206
Epoch 2.109: Loss = 0.332565
Epoch 2.110: Loss = 0.428131
Epoch 2.111: Loss = 0.410919
Epoch 2.112: Loss = 0.554077
Epoch 2.113: Loss = 0.507568
Epoch 2.114: Loss = 0.353592
Epoch 2.115: Loss = 0.505539
Epoch 2.116: Loss = 0.405029
Epoch 2.117: Loss = 0.426071
Epoch 2.118: Loss = 0.35498
Epoch 2.119: Loss = 0.465042
Epoch 2.120: Loss = 0.435043
TRAIN LOSS = 0.467148
TRAIN ACC = 85.7407 % (51447/60000)
Loss = 0.410019
Loss = 0.448013
Loss = 0.54895
Loss = 0.533279
Loss = 0.569839
Loss = 0.42894
Loss = 0.403671
Loss = 0.611801
Loss = 0.528
Loss = 0.510651
Loss = 0.202927
Loss = 0.345413
Loss = 0.272018
Loss = 0.403503
Loss = 0.244904
Loss = 0.336273
Loss = 0.252075
Loss = 0.097168
Loss = 0.245514
Loss = 0.512939
TEST LOSS = 0.395295
TEST ACC = 514.468 % (8814/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.33165
Epoch 3.2: Loss = 0.450516
Epoch 3.3: Loss = 0.351639
Epoch 3.4: Loss = 0.406418
Epoch 3.5: Loss = 0.44252
Epoch 3.6: Loss = 0.417633
Epoch 3.7: Loss = 0.412445
Epoch 3.8: Loss = 0.362991
Epoch 3.9: Loss = 0.293518
Epoch 3.10: Loss = 0.301743
Epoch 3.11: Loss = 0.372284
Epoch 3.12: Loss = 0.410522
Epoch 3.13: Loss = 0.403091
Epoch 3.14: Loss = 0.403458
Epoch 3.15: Loss = 0.411636
Epoch 3.16: Loss = 0.485397
Epoch 3.17: Loss = 0.459991
Epoch 3.18: Loss = 0.395233
Epoch 3.19: Loss = 0.447952
Epoch 3.20: Loss = 0.377075
Epoch 3.21: Loss = 0.385498
Epoch 3.22: Loss = 0.403519
Epoch 3.23: Loss = 0.414368
Epoch 3.24: Loss = 0.41832
Epoch 3.25: Loss = 0.425171
Epoch 3.26: Loss = 0.414703
Epoch 3.27: Loss = 0.50563
Epoch 3.28: Loss = 0.493698
Epoch 3.29: Loss = 0.473236
Epoch 3.30: Loss = 0.375244
Epoch 3.31: Loss = 0.45047
Epoch 3.32: Loss = 0.447937
Epoch 3.33: Loss = 0.34729
Epoch 3.34: Loss = 0.45134
Epoch 3.35: Loss = 0.432098
Epoch 3.36: Loss = 0.399796
Epoch 3.37: Loss = 0.350327
Epoch 3.38: Loss = 0.337204
Epoch 3.39: Loss = 0.53833
Epoch 3.40: Loss = 0.401001
Epoch 3.41: Loss = 0.403488
Epoch 3.42: Loss = 0.417923
Epoch 3.43: Loss = 0.365814
Epoch 3.44: Loss = 0.442688
Epoch 3.45: Loss = 0.355194
Epoch 3.46: Loss = 0.392731
Epoch 3.47: Loss = 0.349533
Epoch 3.48: Loss = 0.372528
Epoch 3.49: Loss = 0.429779
Epoch 3.50: Loss = 0.42778
Epoch 3.51: Loss = 0.410416
Epoch 3.52: Loss = 0.40387
Epoch 3.53: Loss = 0.459091
Epoch 3.54: Loss = 0.463501
Epoch 3.55: Loss = 0.447632
Epoch 3.56: Loss = 0.362198
Epoch 3.57: Loss = 0.354828
Epoch 3.58: Loss = 0.427216
Epoch 3.59: Loss = 0.410706
Epoch 3.60: Loss = 0.367096
Epoch 3.61: Loss = 0.412247
Epoch 3.62: Loss = 0.394165
Epoch 3.63: Loss = 0.367844
Epoch 3.64: Loss = 0.411194
Epoch 3.65: Loss = 0.389633
Epoch 3.66: Loss = 0.422668
Epoch 3.67: Loss = 0.309616
Epoch 3.68: Loss = 0.408005
Epoch 3.69: Loss = 0.404327
Epoch 3.70: Loss = 0.263275
Epoch 3.71: Loss = 0.333054
Epoch 3.72: Loss = 0.410568
Epoch 3.73: Loss = 0.386108
Epoch 3.74: Loss = 0.388138
Epoch 3.75: Loss = 0.326324
Epoch 3.76: Loss = 0.382935
Epoch 3.77: Loss = 0.489105
Epoch 3.78: Loss = 0.381622
Epoch 3.79: Loss = 0.460602
Epoch 3.80: Loss = 0.369141
Epoch 3.81: Loss = 0.496262
Epoch 3.82: Loss = 0.428223
Epoch 3.83: Loss = 0.364044
Epoch 3.84: Loss = 0.423737
Epoch 3.85: Loss = 0.450241
Epoch 3.86: Loss = 0.463318
Epoch 3.87: Loss = 0.474869
Epoch 3.88: Loss = 0.390533
Epoch 3.89: Loss = 0.34845
Epoch 3.90: Loss = 0.379868
Epoch 3.91: Loss = 0.445724
Epoch 3.92: Loss = 0.436768
Epoch 3.93: Loss = 0.424225
Epoch 3.94: Loss = 0.382141
Epoch 3.95: Loss = 0.421066
Epoch 3.96: Loss = 0.363174
Epoch 3.97: Loss = 0.382843
Epoch 3.98: Loss = 0.42894
Epoch 3.99: Loss = 0.392838
Epoch 3.100: Loss = 0.394745
Epoch 3.101: Loss = 0.439148
Epoch 3.102: Loss = 0.411957
Epoch 3.103: Loss = 0.425659
Epoch 3.104: Loss = 0.464417
Epoch 3.105: Loss = 0.39975
Epoch 3.106: Loss = 0.440872
Epoch 3.107: Loss = 0.373093
Epoch 3.108: Loss = 0.421616
Epoch 3.109: Loss = 0.477158
Epoch 3.110: Loss = 0.430176
Epoch 3.111: Loss = 0.348389
Epoch 3.112: Loss = 0.42038
Epoch 3.113: Loss = 0.420944
Epoch 3.114: Loss = 0.432739
Epoch 3.115: Loss = 0.337814
Epoch 3.116: Loss = 0.425934
Epoch 3.117: Loss = 0.364899
Epoch 3.118: Loss = 0.351685
Epoch 3.119: Loss = 0.343353
Epoch 3.120: Loss = 0.385071
TRAIN LOSS = 0.404861
TRAIN ACC = 87.9883 % (52795/60000)
Loss = 0.368774
Loss = 0.413544
Loss = 0.514221
Loss = 0.514557
Loss = 0.526016
Loss = 0.387741
Loss = 0.350555
Loss = 0.593964
Loss = 0.495377
Loss = 0.473663
Loss = 0.179565
Loss = 0.312073
Loss = 0.267914
Loss = 0.355347
Loss = 0.197998
Loss = 0.310287
Loss = 0.215073
Loss = 0.0659485
Loss = 0.219376
Loss = 0.467072
TEST LOSS = 0.361453
TEST ACC = 527.95 % (8944/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.502747
Epoch 4.2: Loss = 0.375305
Epoch 4.3: Loss = 0.345032
Epoch 4.4: Loss = 0.407257
Epoch 4.5: Loss = 0.368408
Epoch 4.6: Loss = 0.436584
Epoch 4.7: Loss = 0.362183
Epoch 4.8: Loss = 0.408661
Epoch 4.9: Loss = 0.390381
Epoch 4.10: Loss = 0.36322
Epoch 4.11: Loss = 0.337326
Epoch 4.12: Loss = 0.372894
Epoch 4.13: Loss = 0.432632
Epoch 4.14: Loss = 0.291992
Epoch 4.15: Loss = 0.360748
Epoch 4.16: Loss = 0.409607
Epoch 4.17: Loss = 0.396347
Epoch 4.18: Loss = 0.309036
Epoch 4.19: Loss = 0.308334
Epoch 4.20: Loss = 0.429077
Epoch 4.21: Loss = 0.355835
Epoch 4.22: Loss = 0.360886
Epoch 4.23: Loss = 0.407486
Epoch 4.24: Loss = 0.396759
Epoch 4.25: Loss = 0.389343
Epoch 4.26: Loss = 0.318802
Epoch 4.27: Loss = 0.37323
Epoch 4.28: Loss = 0.412643
Epoch 4.29: Loss = 0.454407
Epoch 4.30: Loss = 0.431595
Epoch 4.31: Loss = 0.37001
Epoch 4.32: Loss = 0.355026
Epoch 4.33: Loss = 0.384689
Epoch 4.34: Loss = 0.325378
Epoch 4.35: Loss = 0.462448
Epoch 4.36: Loss = 0.409882
Epoch 4.37: Loss = 0.363831
Epoch 4.38: Loss = 0.319641
Epoch 4.39: Loss = 0.526291
Epoch 4.40: Loss = 0.420914
Epoch 4.41: Loss = 0.527237
Epoch 4.42: Loss = 0.381912
Epoch 4.43: Loss = 0.380219
Epoch 4.44: Loss = 0.330765
Epoch 4.45: Loss = 0.460251
Epoch 4.46: Loss = 0.441101
Epoch 4.47: Loss = 0.305695
Epoch 4.48: Loss = 0.439331
Epoch 4.49: Loss = 0.393951
Epoch 4.50: Loss = 0.349335
Epoch 4.51: Loss = 0.435394
Epoch 4.52: Loss = 0.426346
Epoch 4.53: Loss = 0.34346
Epoch 4.54: Loss = 0.350723
Epoch 4.55: Loss = 0.39975
Epoch 4.56: Loss = 0.362076
Epoch 4.57: Loss = 0.366501
Epoch 4.58: Loss = 0.377151
Epoch 4.59: Loss = 0.437088
Epoch 4.60: Loss = 0.4151
Epoch 4.61: Loss = 0.444183
Epoch 4.62: Loss = 0.370956
Epoch 4.63: Loss = 0.308029
Epoch 4.64: Loss = 0.350739
Epoch 4.65: Loss = 0.435425
Epoch 4.66: Loss = 0.344376
Epoch 4.67: Loss = 0.503769
Epoch 4.68: Loss = 0.415421
Epoch 4.69: Loss = 0.396622
Epoch 4.70: Loss = 0.399918
Epoch 4.71: Loss = 0.471802
Epoch 4.72: Loss = 0.319077
Epoch 4.73: Loss = 0.345108
Epoch 4.74: Loss = 0.453583
Epoch 4.75: Loss = 0.371872
Epoch 4.76: Loss = 0.291367
Epoch 4.77: Loss = 0.360519
Epoch 4.78: Loss = 0.286499
Epoch 4.79: Loss = 0.364655
Epoch 4.80: Loss = 0.427795
Epoch 4.81: Loss = 0.393936
Epoch 4.82: Loss = 0.328491
Epoch 4.83: Loss = 0.382721
Epoch 4.84: Loss = 0.341324
Epoch 4.85: Loss = 0.398102
Epoch 4.86: Loss = 0.445465
Epoch 4.87: Loss = 0.407043
Epoch 4.88: Loss = 0.328644
Epoch 4.89: Loss = 0.459381
Epoch 4.90: Loss = 0.331192
Epoch 4.91: Loss = 0.343277
Epoch 4.92: Loss = 0.335907
Epoch 4.93: Loss = 0.363739
Epoch 4.94: Loss = 0.443878
Epoch 4.95: Loss = 0.397186
Epoch 4.96: Loss = 0.383392
Epoch 4.97: Loss = 0.487
Epoch 4.98: Loss = 0.292633
Epoch 4.99: Loss = 0.33876
Epoch 4.100: Loss = 0.405991
Epoch 4.101: Loss = 0.417389
Epoch 4.102: Loss = 0.358704
Epoch 4.103: Loss = 0.286789
Epoch 4.104: Loss = 0.35759
Epoch 4.105: Loss = 0.382553
Epoch 4.106: Loss = 0.35878
Epoch 4.107: Loss = 0.440582
Epoch 4.108: Loss = 0.453476
Epoch 4.109: Loss = 0.40596
Epoch 4.110: Loss = 0.382065
Epoch 4.111: Loss = 0.336136
Epoch 4.112: Loss = 0.402405
Epoch 4.113: Loss = 0.326416
Epoch 4.114: Loss = 0.39679
Epoch 4.115: Loss = 0.389191
Epoch 4.116: Loss = 0.359146
Epoch 4.117: Loss = 0.415527
Epoch 4.118: Loss = 0.381592
Epoch 4.119: Loss = 0.342865
Epoch 4.120: Loss = 0.365967
TRAIN LOSS = 0.384995
TRAIN ACC = 88.8855 % (53334/60000)
Loss = 0.354218
Loss = 0.413971
Loss = 0.493347
Loss = 0.511993
Loss = 0.519135
Loss = 0.375671
Loss = 0.332779
Loss = 0.595291
Loss = 0.488464
Loss = 0.472183
Loss = 0.156433
Loss = 0.300537
Loss = 0.262268
Loss = 0.337677
Loss = 0.174759
Loss = 0.280731
Loss = 0.196426
Loss = 0.0551605
Loss = 0.196274
Loss = 0.46994
TEST LOSS = 0.349363
TEST ACC = 533.339 % (8993/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.388412
Epoch 5.2: Loss = 0.365952
Epoch 5.3: Loss = 0.379288
Epoch 5.4: Loss = 0.445572
Epoch 5.5: Loss = 0.439713
Epoch 5.6: Loss = 0.364197
Epoch 5.7: Loss = 0.402206
Epoch 5.8: Loss = 0.343063
Epoch 5.9: Loss = 0.348511
Epoch 5.10: Loss = 0.299377
Epoch 5.11: Loss = 0.414001
Epoch 5.12: Loss = 0.420959
Epoch 5.13: Loss = 0.357712
Epoch 5.14: Loss = 0.377533
Epoch 5.15: Loss = 0.412827
Epoch 5.16: Loss = 0.362549
Epoch 5.17: Loss = 0.463623
Epoch 5.18: Loss = 0.313858
Epoch 5.19: Loss = 0.346558
Epoch 5.20: Loss = 0.390076
Epoch 5.21: Loss = 0.406464
Epoch 5.22: Loss = 0.428818
Epoch 5.23: Loss = 0.39621
Epoch 5.24: Loss = 0.387054
Epoch 5.25: Loss = 0.392654
Epoch 5.26: Loss = 0.327347
Epoch 5.27: Loss = 0.42923
Epoch 5.28: Loss = 0.43808
Epoch 5.29: Loss = 0.405777
Epoch 5.30: Loss = 0.386032
Epoch 5.31: Loss = 0.464355
Epoch 5.32: Loss = 0.233368
Epoch 5.33: Loss = 0.364838
Epoch 5.34: Loss = 0.486542
Epoch 5.35: Loss = 0.445068
Epoch 5.36: Loss = 0.356079
Epoch 5.37: Loss = 0.359619
Epoch 5.38: Loss = 0.365234
Epoch 5.39: Loss = 0.489014
Epoch 5.40: Loss = 0.423309
Epoch 5.41: Loss = 0.392746
Epoch 5.42: Loss = 0.392517
Epoch 5.43: Loss = 0.389694
Epoch 5.44: Loss = 0.308701
Epoch 5.45: Loss = 0.249527
Epoch 5.46: Loss = 0.420944
Epoch 5.47: Loss = 0.315323
Epoch 5.48: Loss = 0.401062
Epoch 5.49: Loss = 0.322937
Epoch 5.50: Loss = 0.392853
Epoch 5.51: Loss = 0.344284
Epoch 5.52: Loss = 0.372864
Epoch 5.53: Loss = 0.339981
Epoch 5.54: Loss = 0.319489
Epoch 5.55: Loss = 0.346695
Epoch 5.56: Loss = 0.316498
Epoch 5.57: Loss = 0.335327
Epoch 5.58: Loss = 0.38176
Epoch 5.59: Loss = 0.419434
Epoch 5.60: Loss = 0.335602
Epoch 5.61: Loss = 0.272598
Epoch 5.62: Loss = 0.298035
Epoch 5.63: Loss = 0.358643
Epoch 5.64: Loss = 0.342728
Epoch 5.65: Loss = 0.502487
Epoch 5.66: Loss = 0.406494
Epoch 5.67: Loss = 0.379013
Epoch 5.68: Loss = 0.406616
Epoch 5.69: Loss = 0.413605
Epoch 5.70: Loss = 0.449783
Epoch 5.71: Loss = 0.42215
Epoch 5.72: Loss = 0.405258
Epoch 5.73: Loss = 0.333832
Epoch 5.74: Loss = 0.38475
Epoch 5.75: Loss = 0.411896
Epoch 5.76: Loss = 0.407196
Epoch 5.77: Loss = 0.320328
Epoch 5.78: Loss = 0.342438
Epoch 5.79: Loss = 0.352112
Epoch 5.80: Loss = 0.398148
Epoch 5.81: Loss = 0.479813
Epoch 5.82: Loss = 0.33783
Epoch 5.83: Loss = 0.408981
Epoch 5.84: Loss = 0.358093
Epoch 5.85: Loss = 0.479889
Epoch 5.86: Loss = 0.372604
Epoch 5.87: Loss = 0.336227
Epoch 5.88: Loss = 0.427963
Epoch 5.89: Loss = 0.309891
Epoch 5.90: Loss = 0.415924
Epoch 5.91: Loss = 0.335007
Epoch 5.92: Loss = 0.38652
Epoch 5.93: Loss = 0.493759
Epoch 5.94: Loss = 0.323975
Epoch 5.95: Loss = 0.307632
Epoch 5.96: Loss = 0.31636
Epoch 5.97: Loss = 0.342316
Epoch 5.98: Loss = 0.318039
Epoch 5.99: Loss = 0.42662
Epoch 5.100: Loss = 0.407959
Epoch 5.101: Loss = 0.309113
Epoch 5.102: Loss = 0.338791
Epoch 5.103: Loss = 0.410278
Epoch 5.104: Loss = 0.352097
Epoch 5.105: Loss = 0.333527
Epoch 5.106: Loss = 0.410736
Epoch 5.107: Loss = 0.3591
Epoch 5.108: Loss = 0.373642
Epoch 5.109: Loss = 0.345047
Epoch 5.110: Loss = 0.391205
Epoch 5.111: Loss = 0.423782
Epoch 5.112: Loss = 0.383514
Epoch 5.113: Loss = 0.35289
Epoch 5.114: Loss = 0.341766
Epoch 5.115: Loss = 0.434158
Epoch 5.116: Loss = 0.44397
Epoch 5.117: Loss = 0.354538
Epoch 5.118: Loss = 0.378662
Epoch 5.119: Loss = 0.260498
Epoch 5.120: Loss = 0.318268
TRAIN LOSS = 0.376907
TRAIN ACC = 89.4745 % (53687/60000)
Loss = 0.347809
Loss = 0.401978
Loss = 0.51123
Loss = 0.519394
Loss = 0.509628
Loss = 0.374161
Loss = 0.332977
Loss = 0.604568
Loss = 0.491074
Loss = 0.475494
Loss = 0.153519
Loss = 0.302032
Loss = 0.262589
Loss = 0.335144
Loss = 0.168213
Loss = 0.295486
Loss = 0.191956
Loss = 0.0511475
Loss = 0.191559
Loss = 0.459488
TEST LOSS = 0.348972
TEST ACC = 536.87 % (9011/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.389267
Epoch 6.2: Loss = 0.359924
Epoch 6.3: Loss = 0.431351
Epoch 6.4: Loss = 0.429993
Epoch 6.5: Loss = 0.416107
Epoch 6.6: Loss = 0.343079
Epoch 6.7: Loss = 0.338455
Epoch 6.8: Loss = 0.311615
Epoch 6.9: Loss = 0.433884
Epoch 6.10: Loss = 0.382263
Epoch 6.11: Loss = 0.363693
Epoch 6.12: Loss = 0.386703
Epoch 6.13: Loss = 0.398926
Epoch 6.14: Loss = 0.259811
Epoch 6.15: Loss = 0.424408
Epoch 6.16: Loss = 0.37056
Epoch 6.17: Loss = 0.348145
Epoch 6.18: Loss = 0.384949
Epoch 6.19: Loss = 0.394882
Epoch 6.20: Loss = 0.311127
Epoch 6.21: Loss = 0.38208
Epoch 6.22: Loss = 0.433899
Epoch 6.23: Loss = 0.386566
Epoch 6.24: Loss = 0.336746
Epoch 6.25: Loss = 0.398666
Epoch 6.26: Loss = 0.449493
Epoch 6.27: Loss = 0.370605
Epoch 6.28: Loss = 0.294006
Epoch 6.29: Loss = 0.414978
Epoch 6.30: Loss = 0.384842
Epoch 6.31: Loss = 0.446701
Epoch 6.32: Loss = 0.376358
Epoch 6.33: Loss = 0.430908
Epoch 6.34: Loss = 0.392075
Epoch 6.35: Loss = 0.395325
Epoch 6.36: Loss = 0.488129
Epoch 6.37: Loss = 0.397125
Epoch 6.38: Loss = 0.377914
Epoch 6.39: Loss = 0.331467
Epoch 6.40: Loss = 0.369629
Epoch 6.41: Loss = 0.310135
Epoch 6.42: Loss = 0.42012
Epoch 6.43: Loss = 0.38269
Epoch 6.44: Loss = 0.406403
Epoch 6.45: Loss = 0.477493
Epoch 6.46: Loss = 0.267319
Epoch 6.47: Loss = 0.347855
Epoch 6.48: Loss = 0.383789
Epoch 6.49: Loss = 0.328049
Epoch 6.50: Loss = 0.331955
Epoch 6.51: Loss = 0.400299
Epoch 6.52: Loss = 0.362061
Epoch 6.53: Loss = 0.424255
Epoch 6.54: Loss = 0.440506
Epoch 6.55: Loss = 0.301025
Epoch 6.56: Loss = 0.444809
Epoch 6.57: Loss = 0.331177
Epoch 6.58: Loss = 0.38884
Epoch 6.59: Loss = 0.323669
Epoch 6.60: Loss = 0.400513
Epoch 6.61: Loss = 0.327744
Epoch 6.62: Loss = 0.343765
Epoch 6.63: Loss = 0.386993
Epoch 6.64: Loss = 0.387482
Epoch 6.65: Loss = 0.386536
Epoch 6.66: Loss = 0.326935
Epoch 6.67: Loss = 0.412613
Epoch 6.68: Loss = 0.368332
Epoch 6.69: Loss = 0.415741
Epoch 6.70: Loss = 0.523499
Epoch 6.71: Loss = 0.383606
Epoch 6.72: Loss = 0.339722
Epoch 6.73: Loss = 0.45636
Epoch 6.74: Loss = 0.287628
Epoch 6.75: Loss = 0.38504
Epoch 6.76: Loss = 0.296463
Epoch 6.77: Loss = 0.346039
Epoch 6.78: Loss = 0.294739
Epoch 6.79: Loss = 0.364182
Epoch 6.80: Loss = 0.251694
Epoch 6.81: Loss = 0.350922
Epoch 6.82: Loss = 0.380707
Epoch 6.83: Loss = 0.395264
Epoch 6.84: Loss = 0.259155
Epoch 6.85: Loss = 0.375946
Epoch 6.86: Loss = 0.40065
Epoch 6.87: Loss = 0.266953
Epoch 6.88: Loss = 0.357986
Epoch 6.89: Loss = 0.38768
Epoch 6.90: Loss = 0.46228
Epoch 6.91: Loss = 0.464859
Epoch 6.92: Loss = 0.384033
Epoch 6.93: Loss = 0.370865
Epoch 6.94: Loss = 0.320236
Epoch 6.95: Loss = 0.370636
Epoch 6.96: Loss = 0.337158
Epoch 6.97: Loss = 0.39798
Epoch 6.98: Loss = 0.274277
Epoch 6.99: Loss = 0.301987
Epoch 6.100: Loss = 0.438965
Epoch 6.101: Loss = 0.458725
Epoch 6.102: Loss = 0.445709
Epoch 6.103: Loss = 0.3909
Epoch 6.104: Loss = 0.368271
Epoch 6.105: Loss = 0.400833
Epoch 6.106: Loss = 0.303787
Epoch 6.107: Loss = 0.328293
Epoch 6.108: Loss = 0.336502
Epoch 6.109: Loss = 0.503296
Epoch 6.110: Loss = 0.351227
Epoch 6.111: Loss = 0.314957
Epoch 6.112: Loss = 0.288483
Epoch 6.113: Loss = 0.369781
Epoch 6.114: Loss = 0.356277
Epoch 6.115: Loss = 0.437332
Epoch 6.116: Loss = 0.231964
Epoch 6.117: Loss = 0.235687
Epoch 6.118: Loss = 0.314529
Epoch 6.119: Loss = 0.448471
Epoch 6.120: Loss = 0.358826
TRAIN LOSS = 0.371979
TRAIN ACC = 89.8666 % (53922/60000)
Loss = 0.329239
Loss = 0.387085
Loss = 0.483826
Loss = 0.527756
Loss = 0.492706
Loss = 0.371536
Loss = 0.32486
Loss = 0.593872
Loss = 0.482529
Loss = 0.457932
Loss = 0.131393
Loss = 0.293396
Loss = 0.26741
Loss = 0.331619
Loss = 0.162659
Loss = 0.292496
Loss = 0.18338
Loss = 0.0493622
Loss = 0.195587
Loss = 0.45845
TEST LOSS = 0.340855
TEST ACC = 539.22 % (9065/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.376236
Epoch 7.2: Loss = 0.401657
Epoch 7.3: Loss = 0.372864
Epoch 7.4: Loss = 0.275146
Epoch 7.5: Loss = 0.434402
Epoch 7.6: Loss = 0.32872
Epoch 7.7: Loss = 0.367569
Epoch 7.8: Loss = 0.383972
Epoch 7.9: Loss = 0.400024
Epoch 7.10: Loss = 0.328812
Epoch 7.11: Loss = 0.36293
Epoch 7.12: Loss = 0.325729
Epoch 7.13: Loss = 0.383728
Epoch 7.14: Loss = 0.394577
Epoch 7.15: Loss = 0.334747
Epoch 7.16: Loss = 0.487778
Epoch 7.17: Loss = 0.406754
Epoch 7.18: Loss = 0.376831
Epoch 7.19: Loss = 0.362473
Epoch 7.20: Loss = 0.332901
Epoch 7.21: Loss = 0.291199
Epoch 7.22: Loss = 0.388275
Epoch 7.23: Loss = 0.453522
Epoch 7.24: Loss = 0.497574
Epoch 7.25: Loss = 0.321869
Epoch 7.26: Loss = 0.252563
Epoch 7.27: Loss = 0.41069
Epoch 7.28: Loss = 0.352982
Epoch 7.29: Loss = 0.304749
Epoch 7.30: Loss = 0.309204
Epoch 7.31: Loss = 0.367722
Epoch 7.32: Loss = 0.388229
Epoch 7.33: Loss = 0.446854
Epoch 7.34: Loss = 0.451218
Epoch 7.35: Loss = 0.386124
Epoch 7.36: Loss = 0.345108
Epoch 7.37: Loss = 0.431854
Epoch 7.38: Loss = 0.396332
Epoch 7.39: Loss = 0.386078
Epoch 7.40: Loss = 0.279282
Epoch 7.41: Loss = 0.380463
Epoch 7.42: Loss = 0.362122
Epoch 7.43: Loss = 0.372131
Epoch 7.44: Loss = 0.317886
Epoch 7.45: Loss = 0.360687
Epoch 7.46: Loss = 0.310715
Epoch 7.47: Loss = 0.400024
Epoch 7.48: Loss = 0.337143
Epoch 7.49: Loss = 0.365494
Epoch 7.50: Loss = 0.365433
Epoch 7.51: Loss = 0.337906
Epoch 7.52: Loss = 0.410583
Epoch 7.53: Loss = 0.408539
Epoch 7.54: Loss = 0.368256
Epoch 7.55: Loss = 0.345779
Epoch 7.56: Loss = 0.408905
Epoch 7.57: Loss = 0.276382
Epoch 7.58: Loss = 0.43515
Epoch 7.59: Loss = 0.369659
Epoch 7.60: Loss = 0.453186
Epoch 7.61: Loss = 0.307907
Epoch 7.62: Loss = 0.418091
Epoch 7.63: Loss = 0.244339
Epoch 7.64: Loss = 0.345413
Epoch 7.65: Loss = 0.300003
Epoch 7.66: Loss = 0.379684
Epoch 7.67: Loss = 0.430069
Epoch 7.68: Loss = 0.385803
Epoch 7.69: Loss = 0.365158
Epoch 7.70: Loss = 0.409271
Epoch 7.71: Loss = 0.364334
Epoch 7.72: Loss = 0.37915
Epoch 7.73: Loss = 0.428314
Epoch 7.74: Loss = 0.335938
Epoch 7.75: Loss = 0.416595
Epoch 7.76: Loss = 0.296402
Epoch 7.77: Loss = 0.254959
Epoch 7.78: Loss = 0.429596
Epoch 7.79: Loss = 0.411057
Epoch 7.80: Loss = 0.358826
Epoch 7.81: Loss = 0.403885
Epoch 7.82: Loss = 0.399628
Epoch 7.83: Loss = 0.319794
Epoch 7.84: Loss = 0.407349
Epoch 7.85: Loss = 0.280579
Epoch 7.86: Loss = 0.485184
Epoch 7.87: Loss = 0.28067
Epoch 7.88: Loss = 0.334641
Epoch 7.89: Loss = 0.329117
Epoch 7.90: Loss = 0.348969
Epoch 7.91: Loss = 0.285278
Epoch 7.92: Loss = 0.50148
Epoch 7.93: Loss = 0.276382
Epoch 7.94: Loss = 0.371445
Epoch 7.95: Loss = 0.402679
Epoch 7.96: Loss = 0.421921
Epoch 7.97: Loss = 0.301666
Epoch 7.98: Loss = 0.310135
Epoch 7.99: Loss = 0.317703
Epoch 7.100: Loss = 0.477707
Epoch 7.101: Loss = 0.33403
Epoch 7.102: Loss = 0.310089
Epoch 7.103: Loss = 0.300339
Epoch 7.104: Loss = 0.462128
Epoch 7.105: Loss = 0.338699
Epoch 7.106: Loss = 0.371948
Epoch 7.107: Loss = 0.268494
Epoch 7.108: Loss = 0.319565
Epoch 7.109: Loss = 0.31044
Epoch 7.110: Loss = 0.347595
Epoch 7.111: Loss = 0.376511
Epoch 7.112: Loss = 0.298203
Epoch 7.113: Loss = 0.381683
Epoch 7.114: Loss = 0.390808
Epoch 7.115: Loss = 0.348755
Epoch 7.116: Loss = 0.508514
Epoch 7.117: Loss = 0.314407
Epoch 7.118: Loss = 0.278427
Epoch 7.119: Loss = 0.441681
Epoch 7.120: Loss = 0.30484
TRAIN LOSS = 0.365082
TRAIN ACC = 90.2283 % (54140/60000)
Loss = 0.32692
Loss = 0.383316
Loss = 0.473953
Loss = 0.517685
Loss = 0.485336
Loss = 0.368744
Loss = 0.314667
Loss = 0.572693
Loss = 0.486282
Loss = 0.445953
Loss = 0.140091
Loss = 0.303207
Loss = 0.259903
Loss = 0.329285
Loss = 0.156723
Loss = 0.26445
Loss = 0.184158
Loss = 0.0393982
Loss = 0.193619
Loss = 0.469513
TEST LOSS = 0.335795
TEST ACC = 541.399 % (9086/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.380692
Epoch 8.2: Loss = 0.39975
Epoch 8.3: Loss = 0.326599
Epoch 8.4: Loss = 0.411713
Epoch 8.5: Loss = 0.314636
Epoch 8.6: Loss = 0.391342
Epoch 8.7: Loss = 0.348663
Epoch 8.8: Loss = 0.434341
Epoch 8.9: Loss = 0.276535
Epoch 8.10: Loss = 0.294189
Epoch 8.11: Loss = 0.293945
Epoch 8.12: Loss = 0.336624
Epoch 8.13: Loss = 0.299026
Epoch 8.14: Loss = 0.43251
Epoch 8.15: Loss = 0.363785
Epoch 8.16: Loss = 0.359543
Epoch 8.17: Loss = 0.364319
Epoch 8.18: Loss = 0.298355
Epoch 8.19: Loss = 0.327637
Epoch 8.20: Loss = 0.29776
Epoch 8.21: Loss = 0.382126
Epoch 8.22: Loss = 0.373276
Epoch 8.23: Loss = 0.309967
Epoch 8.24: Loss = 0.377136
Epoch 8.25: Loss = 0.410797
Epoch 8.26: Loss = 0.334686
Epoch 8.27: Loss = 0.422379
Epoch 8.28: Loss = 0.305191
Epoch 8.29: Loss = 0.461121
Epoch 8.30: Loss = 0.382568
Epoch 8.31: Loss = 0.413132
Epoch 8.32: Loss = 0.325256
Epoch 8.33: Loss = 0.345718
Epoch 8.34: Loss = 0.272552
Epoch 8.35: Loss = 0.33252
Epoch 8.36: Loss = 0.384659
Epoch 8.37: Loss = 0.319153
Epoch 8.38: Loss = 0.26416
Epoch 8.39: Loss = 0.448166
Epoch 8.40: Loss = 0.294296
Epoch 8.41: Loss = 0.492172
Epoch 8.42: Loss = 0.290359
Epoch 8.43: Loss = 0.396683
Epoch 8.44: Loss = 0.330826
Epoch 8.45: Loss = 0.423599
Epoch 8.46: Loss = 0.413712
Epoch 8.47: Loss = 0.397263
Epoch 8.48: Loss = 0.362091
Epoch 8.49: Loss = 0.367844
Epoch 8.50: Loss = 0.383286
Epoch 8.51: Loss = 0.397064
Epoch 8.52: Loss = 0.355743
Epoch 8.53: Loss = 0.350616
Epoch 8.54: Loss = 0.4077
Epoch 8.55: Loss = 0.486069
Epoch 8.56: Loss = 0.335876
Epoch 8.57: Loss = 0.308975
Epoch 8.58: Loss = 0.313187
Epoch 8.59: Loss = 0.353851
Epoch 8.60: Loss = 0.316513
Epoch 8.61: Loss = 0.27655
Epoch 8.62: Loss = 0.287949
Epoch 8.63: Loss = 0.376633
Epoch 8.64: Loss = 0.430695
Epoch 8.65: Loss = 0.276428
Epoch 8.66: Loss = 0.412735
Epoch 8.67: Loss = 0.307892
Epoch 8.68: Loss = 0.299347
Epoch 8.69: Loss = 0.264099
Epoch 8.70: Loss = 0.369415
Epoch 8.71: Loss = 0.397263
Epoch 8.72: Loss = 0.373642
Epoch 8.73: Loss = 0.234894
Epoch 8.74: Loss = 0.370499
Epoch 8.75: Loss = 0.283035
Epoch 8.76: Loss = 0.34494
Epoch 8.77: Loss = 0.484772
Epoch 8.78: Loss = 0.325684
Epoch 8.79: Loss = 0.378616
Epoch 8.80: Loss = 0.345612
Epoch 8.81: Loss = 0.318069
Epoch 8.82: Loss = 0.349823
Epoch 8.83: Loss = 0.393234
Epoch 8.84: Loss = 0.26886
Epoch 8.85: Loss = 0.394089
Epoch 8.86: Loss = 0.43634
Epoch 8.87: Loss = 0.350952
Epoch 8.88: Loss = 0.367752
Epoch 8.89: Loss = 0.357147
Epoch 8.90: Loss = 0.343979
Epoch 8.91: Loss = 0.439377
Epoch 8.92: Loss = 0.400238
Epoch 8.93: Loss = 0.356842
Epoch 8.94: Loss = 0.417664
Epoch 8.95: Loss = 0.373474
Epoch 8.96: Loss = 0.278
Epoch 8.97: Loss = 0.397491
Epoch 8.98: Loss = 0.344589
Epoch 8.99: Loss = 0.457336
Epoch 8.100: Loss = 0.308334
Epoch 8.101: Loss = 0.288086
Epoch 8.102: Loss = 0.270035
Epoch 8.103: Loss = 0.478607
Epoch 8.104: Loss = 0.277374
Epoch 8.105: Loss = 0.432709
Epoch 8.106: Loss = 0.503937
Epoch 8.107: Loss = 0.407578
Epoch 8.108: Loss = 0.327362
Epoch 8.109: Loss = 0.311768
Epoch 8.110: Loss = 0.505035
Epoch 8.111: Loss = 0.417496
Epoch 8.112: Loss = 0.476944
Epoch 8.113: Loss = 0.31279
Epoch 8.114: Loss = 0.380402
Epoch 8.115: Loss = 0.461807
Epoch 8.116: Loss = 0.3022
Epoch 8.117: Loss = 0.457382
Epoch 8.118: Loss = 0.364761
Epoch 8.119: Loss = 0.359894
Epoch 8.120: Loss = 0.416901
TRAIN LOSS = 0.362854
TRAIN ACC = 90.4236 % (54257/60000)
Loss = 0.317795
Loss = 0.408951
Loss = 0.457504
Loss = 0.511505
Loss = 0.483017
Loss = 0.365814
Loss = 0.312332
Loss = 0.574463
Loss = 0.483978
Loss = 0.437836
Loss = 0.13266
Loss = 0.306442
Loss = 0.286026
Loss = 0.32048
Loss = 0.153229
Loss = 0.285492
Loss = 0.174149
Loss = 0.0403595
Loss = 0.20787
Loss = 0.496063
TEST LOSS = 0.337798
TEST ACC = 542.569 % (9109/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.326508
Epoch 9.2: Loss = 0.39859
Epoch 9.3: Loss = 0.335495
Epoch 9.4: Loss = 0.317886
Epoch 9.5: Loss = 0.346634
Epoch 9.6: Loss = 0.258667
Epoch 9.7: Loss = 0.323441
Epoch 9.8: Loss = 0.365936
Epoch 9.9: Loss = 0.446518
Epoch 9.10: Loss = 0.281265
Epoch 9.11: Loss = 0.295334
Epoch 9.12: Loss = 0.385651
Epoch 9.13: Loss = 0.459625
Epoch 9.14: Loss = 0.425629
Epoch 9.15: Loss = 0.363922
Epoch 9.16: Loss = 0.371857
Epoch 9.17: Loss = 0.307953
Epoch 9.18: Loss = 0.341873
Epoch 9.19: Loss = 0.319641
Epoch 9.20: Loss = 0.233337
Epoch 9.21: Loss = 0.39447
Epoch 9.22: Loss = 0.459106
Epoch 9.23: Loss = 0.395004
Epoch 9.24: Loss = 0.395569
Epoch 9.25: Loss = 0.37764
Epoch 9.26: Loss = 0.277924
Epoch 9.27: Loss = 0.347229
Epoch 9.28: Loss = 0.346405
Epoch 9.29: Loss = 0.492065
Epoch 9.30: Loss = 0.288559
Epoch 9.31: Loss = 0.405121
Epoch 9.32: Loss = 0.360168
Epoch 9.33: Loss = 0.436111
Epoch 9.34: Loss = 0.311554
Epoch 9.35: Loss = 0.375992
Epoch 9.36: Loss = 0.334091
Epoch 9.37: Loss = 0.40155
Epoch 9.38: Loss = 0.390488
Epoch 9.39: Loss = 0.364853
Epoch 9.40: Loss = 0.41188
Epoch 9.41: Loss = 0.34584
Epoch 9.42: Loss = 0.331497
Epoch 9.43: Loss = 0.503723
Epoch 9.44: Loss = 0.314102
Epoch 9.45: Loss = 0.429245
Epoch 9.46: Loss = 0.31015
Epoch 9.47: Loss = 0.426941
Epoch 9.48: Loss = 0.400955
Epoch 9.49: Loss = 0.255127
Epoch 9.50: Loss = 0.333618
Epoch 9.51: Loss = 0.37236
Epoch 9.52: Loss = 0.365051
Epoch 9.53: Loss = 0.420609
Epoch 9.54: Loss = 0.312866
Epoch 9.55: Loss = 0.280746
Epoch 9.56: Loss = 0.418182
Epoch 9.57: Loss = 0.286957
Epoch 9.58: Loss = 0.319397
Epoch 9.59: Loss = 0.336166
Epoch 9.60: Loss = 0.289261
Epoch 9.61: Loss = 0.386658
Epoch 9.62: Loss = 0.352722
Epoch 9.63: Loss = 0.298233
Epoch 9.64: Loss = 0.348221
Epoch 9.65: Loss = 0.41539
Epoch 9.66: Loss = 0.501251
Epoch 9.67: Loss = 0.390945
Epoch 9.68: Loss = 0.345428
Epoch 9.69: Loss = 0.372665
Epoch 9.70: Loss = 0.360687
Epoch 9.71: Loss = 0.468552
Epoch 9.72: Loss = 0.363754
Epoch 9.73: Loss = 0.404816
Epoch 9.74: Loss = 0.352203
Epoch 9.75: Loss = 0.335968
Epoch 9.76: Loss = 0.389221
Epoch 9.77: Loss = 0.309357
Epoch 9.78: Loss = 0.349136
Epoch 9.79: Loss = 0.3638
Epoch 9.80: Loss = 0.284882
Epoch 9.81: Loss = 0.287476
Epoch 9.82: Loss = 0.480713
Epoch 9.83: Loss = 0.367554
Epoch 9.84: Loss = 0.402969
Epoch 9.85: Loss = 0.414078
Epoch 9.86: Loss = 0.422119
Epoch 9.87: Loss = 0.455917
Epoch 9.88: Loss = 0.412384
Epoch 9.89: Loss = 0.27829
Epoch 9.90: Loss = 0.379303
Epoch 9.91: Loss = 0.439331
Epoch 9.92: Loss = 0.353317
Epoch 9.93: Loss = 0.454803
Epoch 9.94: Loss = 0.297272
Epoch 9.95: Loss = 0.39119
Epoch 9.96: Loss = 0.3358
Epoch 9.97: Loss = 0.430115
Epoch 9.98: Loss = 0.359573
Epoch 9.99: Loss = 0.325485
Epoch 9.100: Loss = 0.352859
Epoch 9.101: Loss = 0.36087
Epoch 9.102: Loss = 0.298477
Epoch 9.103: Loss = 0.315491
Epoch 9.104: Loss = 0.301132
Epoch 9.105: Loss = 0.254501
Epoch 9.106: Loss = 0.40329
Epoch 9.107: Loss = 0.442215
Epoch 9.108: Loss = 0.325729
Epoch 9.109: Loss = 0.358902
Epoch 9.110: Loss = 0.435532
Epoch 9.111: Loss = 0.383591
Epoch 9.112: Loss = 0.345917
Epoch 9.113: Loss = 0.373825
Epoch 9.114: Loss = 0.318222
Epoch 9.115: Loss = 0.37915
Epoch 9.116: Loss = 0.415146
Epoch 9.117: Loss = 0.326553
Epoch 9.118: Loss = 0.345963
Epoch 9.119: Loss = 0.316315
Epoch 9.120: Loss = 0.396881
TRAIN LOSS = 0.3638
TRAIN ACC = 90.6586 % (54398/60000)
Loss = 0.331711
Loss = 0.404907
Loss = 0.465408
Loss = 0.520432
Loss = 0.498062
Loss = 0.359802
Loss = 0.315704
Loss = 0.590439
Loss = 0.488754
Loss = 0.460861
Loss = 0.145355
Loss = 0.282425
Loss = 0.29097
Loss = 0.339325
Loss = 0.153519
Loss = 0.275024
Loss = 0.183334
Loss = 0.0388031
Loss = 0.184052
Loss = 0.491089
TEST LOSS = 0.340999
TEST ACC = 543.979 % (9108/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.291855
Epoch 10.2: Loss = 0.447388
Epoch 10.3: Loss = 0.435913
Epoch 10.4: Loss = 0.293549
Epoch 10.5: Loss = 0.293915
Epoch 10.6: Loss = 0.430527
Epoch 10.7: Loss = 0.457153
Epoch 10.8: Loss = 0.304596
Epoch 10.9: Loss = 0.363602
Epoch 10.10: Loss = 0.263107
Epoch 10.11: Loss = 0.248047
Epoch 10.12: Loss = 0.307343
Epoch 10.13: Loss = 0.31456
Epoch 10.14: Loss = 0.38736
Epoch 10.15: Loss = 0.335999
Epoch 10.16: Loss = 0.47998
Epoch 10.17: Loss = 0.421463
Epoch 10.18: Loss = 0.380493
Epoch 10.19: Loss = 0.374908
Epoch 10.20: Loss = 0.371613
Epoch 10.21: Loss = 0.281036
Epoch 10.22: Loss = 0.288452
Epoch 10.23: Loss = 0.307983
Epoch 10.24: Loss = 0.446442
Epoch 10.25: Loss = 0.289612
Epoch 10.26: Loss = 0.421875
Epoch 10.27: Loss = 0.372772
Epoch 10.28: Loss = 0.422989
Epoch 10.29: Loss = 0.268951
Epoch 10.30: Loss = 0.347961
Epoch 10.31: Loss = 0.334015
Epoch 10.32: Loss = 0.386581
Epoch 10.33: Loss = 0.379242
Epoch 10.34: Loss = 0.517227
Epoch 10.35: Loss = 0.375748
Epoch 10.36: Loss = 0.399063
Epoch 10.37: Loss = 0.431183
Epoch 10.38: Loss = 0.275574
Epoch 10.39: Loss = 0.424973
Epoch 10.40: Loss = 0.302353
Epoch 10.41: Loss = 0.30098
Epoch 10.42: Loss = 0.409836
Epoch 10.43: Loss = 0.421219
Epoch 10.44: Loss = 0.417526
Epoch 10.45: Loss = 0.24649
Epoch 10.46: Loss = 0.390182
Epoch 10.47: Loss = 0.326477
Epoch 10.48: Loss = 0.45343
Epoch 10.49: Loss = 0.264725
Epoch 10.50: Loss = 0.313995
Epoch 10.51: Loss = 0.435181
Epoch 10.52: Loss = 0.44397
Epoch 10.53: Loss = 0.452408
Epoch 10.54: Loss = 0.362671
Epoch 10.55: Loss = 0.363358
Epoch 10.56: Loss = 0.333099
Epoch 10.57: Loss = 0.31897
Epoch 10.58: Loss = 0.342072
Epoch 10.59: Loss = 0.387787
Epoch 10.60: Loss = 0.390106
Epoch 10.61: Loss = 0.434891
Epoch 10.62: Loss = 0.358231
Epoch 10.63: Loss = 0.322174
Epoch 10.64: Loss = 0.390335
Epoch 10.65: Loss = 0.425415
Epoch 10.66: Loss = 0.371765
Epoch 10.67: Loss = 0.387314
Epoch 10.68: Loss = 0.337997
Epoch 10.69: Loss = 0.465378
Epoch 10.70: Loss = 0.340302
Epoch 10.71: Loss = 0.418533
Epoch 10.72: Loss = 0.338074
Epoch 10.73: Loss = 0.297653
Epoch 10.74: Loss = 0.381805
Epoch 10.75: Loss = 0.389008
Epoch 10.76: Loss = 0.34671
Epoch 10.77: Loss = 0.357101
Epoch 10.78: Loss = 0.42717
Epoch 10.79: Loss = 0.389557
Epoch 10.80: Loss = 0.290878
Epoch 10.81: Loss = 0.372467
Epoch 10.82: Loss = 0.371597
Epoch 10.83: Loss = 0.384827
Epoch 10.84: Loss = 0.349426
Epoch 10.85: Loss = 0.43367
Epoch 10.86: Loss = 0.327728
Epoch 10.87: Loss = 0.362701
Epoch 10.88: Loss = 0.346146
Epoch 10.89: Loss = 0.355209
Epoch 10.90: Loss = 0.398651
Epoch 10.91: Loss = 0.351196
Epoch 10.92: Loss = 0.307068
Epoch 10.93: Loss = 0.353561
Epoch 10.94: Loss = 0.332123
Epoch 10.95: Loss = 0.338821
Epoch 10.96: Loss = 0.368698
Epoch 10.97: Loss = 0.364212
Epoch 10.98: Loss = 0.374771
Epoch 10.99: Loss = 0.271713
Epoch 10.100: Loss = 0.438538
Epoch 10.101: Loss = 0.374481
Epoch 10.102: Loss = 0.369598
Epoch 10.103: Loss = 0.313461
Epoch 10.104: Loss = 0.462158
Epoch 10.105: Loss = 0.281082
Epoch 10.106: Loss = 0.377304
Epoch 10.107: Loss = 0.392395
Epoch 10.108: Loss = 0.341797
Epoch 10.109: Loss = 0.319794
Epoch 10.110: Loss = 0.381241
Epoch 10.111: Loss = 0.380524
Epoch 10.112: Loss = 0.32695
Epoch 10.113: Loss = 0.336288
Epoch 10.114: Loss = 0.281448
Epoch 10.115: Loss = 0.36412
Epoch 10.116: Loss = 0.351135
Epoch 10.117: Loss = 0.351379
Epoch 10.118: Loss = 0.428375
Epoch 10.119: Loss = 0.225571
Epoch 10.120: Loss = 0.389313
TRAIN LOSS = 0.363113
TRAIN ACC = 90.8264 % (54498/60000)
Loss = 0.302551
Loss = 0.397125
Loss = 0.451645
Loss = 0.501709
Loss = 0.485794
Loss = 0.351166
Loss = 0.309586
Loss = 0.56813
Loss = 0.469635
Loss = 0.429474
Loss = 0.162415
Loss = 0.289413
Loss = 0.294571
Loss = 0.336502
Loss = 0.150818
Loss = 0.285446
Loss = 0.184113
Loss = 0.0406036
Loss = 0.180283
Loss = 0.475662
TEST LOSS = 0.333332
TEST ACC = 544.978 % (9141/10000)

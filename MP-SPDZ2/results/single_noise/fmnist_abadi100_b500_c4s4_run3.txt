Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.48318
Epoch 1.2: Loss = 2.31656
Epoch 1.3: Loss = 2.17494
Epoch 1.4: Loss = 2.10588
Epoch 1.5: Loss = 2.03825
Epoch 1.6: Loss = 2.00955
Epoch 1.7: Loss = 1.95732
Epoch 1.8: Loss = 1.90152
Epoch 1.9: Loss = 1.86646
Epoch 1.10: Loss = 1.81055
Epoch 1.11: Loss = 1.7794
Epoch 1.12: Loss = 1.71385
Epoch 1.13: Loss = 1.67917
Epoch 1.14: Loss = 1.62309
Epoch 1.15: Loss = 1.63721
Epoch 1.16: Loss = 1.59584
Epoch 1.17: Loss = 1.53392
Epoch 1.18: Loss = 1.50407
Epoch 1.19: Loss = 1.50435
Epoch 1.20: Loss = 1.46042
Epoch 1.21: Loss = 1.42024
Epoch 1.22: Loss = 1.39799
Epoch 1.23: Loss = 1.41237
Epoch 1.24: Loss = 1.40067
Epoch 1.25: Loss = 1.31905
Epoch 1.26: Loss = 1.33197
Epoch 1.27: Loss = 1.32416
Epoch 1.28: Loss = 1.34837
Epoch 1.29: Loss = 1.26819
Epoch 1.30: Loss = 1.25099
Epoch 1.31: Loss = 1.25877
Epoch 1.32: Loss = 1.18326
Epoch 1.33: Loss = 1.23364
Epoch 1.34: Loss = 1.20351
Epoch 1.35: Loss = 1.20532
Epoch 1.36: Loss = 1.11703
Epoch 1.37: Loss = 1.12436
Epoch 1.38: Loss = 1.12099
Epoch 1.39: Loss = 1.13516
Epoch 1.40: Loss = 1.08681
Epoch 1.41: Loss = 1.13217
Epoch 1.42: Loss = 1.13094
Epoch 1.43: Loss = 1.20056
Epoch 1.44: Loss = 1.08661
Epoch 1.45: Loss = 1.07416
Epoch 1.46: Loss = 0.996445
Epoch 1.47: Loss = 1.01518
Epoch 1.48: Loss = 1.02319
Epoch 1.49: Loss = 1.05086
Epoch 1.50: Loss = 1.03004
Epoch 1.51: Loss = 0.947952
Epoch 1.52: Loss = 0.940033
Epoch 1.53: Loss = 0.949661
Epoch 1.54: Loss = 0.977966
Epoch 1.55: Loss = 1.05576
Epoch 1.56: Loss = 0.980408
Epoch 1.57: Loss = 1.02388
Epoch 1.58: Loss = 0.95816
Epoch 1.59: Loss = 0.99736
Epoch 1.60: Loss = 0.974716
Epoch 1.61: Loss = 0.933136
Epoch 1.62: Loss = 0.919296
Epoch 1.63: Loss = 0.968094
Epoch 1.64: Loss = 0.966949
Epoch 1.65: Loss = 0.927582
Epoch 1.66: Loss = 0.941193
Epoch 1.67: Loss = 0.978378
Epoch 1.68: Loss = 0.91597
Epoch 1.69: Loss = 0.960938
Epoch 1.70: Loss = 0.96994
Epoch 1.71: Loss = 0.910934
Epoch 1.72: Loss = 0.914719
Epoch 1.73: Loss = 0.895752
Epoch 1.74: Loss = 0.876938
Epoch 1.75: Loss = 0.862106
Epoch 1.76: Loss = 0.908722
Epoch 1.77: Loss = 0.825455
Epoch 1.78: Loss = 0.882446
Epoch 1.79: Loss = 0.845352
Epoch 1.80: Loss = 0.902115
Epoch 1.81: Loss = 0.837296
Epoch 1.82: Loss = 0.88652
Epoch 1.83: Loss = 0.865723
Epoch 1.84: Loss = 0.778519
Epoch 1.85: Loss = 0.846298
Epoch 1.86: Loss = 0.830963
Epoch 1.87: Loss = 0.838531
Epoch 1.88: Loss = 0.820587
Epoch 1.89: Loss = 0.845978
Epoch 1.90: Loss = 0.855469
Epoch 1.91: Loss = 0.895554
Epoch 1.92: Loss = 0.910034
Epoch 1.93: Loss = 0.80751
Epoch 1.94: Loss = 0.829987
Epoch 1.95: Loss = 0.839264
Epoch 1.96: Loss = 0.82016
Epoch 1.97: Loss = 0.812576
Epoch 1.98: Loss = 0.890961
Epoch 1.99: Loss = 0.811081
Epoch 1.100: Loss = 0.838303
Epoch 1.101: Loss = 0.828629
Epoch 1.102: Loss = 0.826141
Epoch 1.103: Loss = 0.822678
Epoch 1.104: Loss = 0.82254
Epoch 1.105: Loss = 0.819199
Epoch 1.106: Loss = 0.798004
Epoch 1.107: Loss = 0.837067
Epoch 1.108: Loss = 0.799667
Epoch 1.109: Loss = 0.82872
Epoch 1.110: Loss = 0.833481
Epoch 1.111: Loss = 0.845154
Epoch 1.112: Loss = 0.696106
Epoch 1.113: Loss = 0.81636
Epoch 1.114: Loss = 0.829575
Epoch 1.115: Loss = 0.809784
Epoch 1.116: Loss = 0.797791
Epoch 1.117: Loss = 0.783981
Epoch 1.118: Loss = 0.819885
Epoch 1.119: Loss = 0.755371
Epoch 1.120: Loss = 0.778793
TRAIN LOSS = 1.1142
TRAIN ACC = 61.9278 % (37158/60000)
Loss = 0.719498
Loss = 0.829071
Loss = 0.837143
Loss = 0.720032
Loss = 0.764008
Loss = 0.891434
Loss = 0.8936
Loss = 0.841827
Loss = 0.772186
Loss = 0.739532
Loss = 0.838577
Loss = 0.805481
Loss = 0.815247
Loss = 0.831589
Loss = 0.777115
Loss = 0.832733
Loss = 0.758331
Loss = 0.78949
Loss = 0.844116
Loss = 0.793671
TEST LOSS = 0.804734
TEST ACC = 371.579 % (7125/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.749969
Epoch 2.2: Loss = 0.882202
Epoch 2.3: Loss = 0.836853
Epoch 2.4: Loss = 0.788269
Epoch 2.5: Loss = 0.700623
Epoch 2.6: Loss = 0.778854
Epoch 2.7: Loss = 0.81131
Epoch 2.8: Loss = 0.695862
Epoch 2.9: Loss = 0.716202
Epoch 2.10: Loss = 0.694672
Epoch 2.11: Loss = 0.803787
Epoch 2.12: Loss = 0.759689
Epoch 2.13: Loss = 0.716614
Epoch 2.14: Loss = 0.771622
Epoch 2.15: Loss = 0.741653
Epoch 2.16: Loss = 0.742188
Epoch 2.17: Loss = 0.743958
Epoch 2.18: Loss = 0.698257
Epoch 2.19: Loss = 0.824112
Epoch 2.20: Loss = 0.783752
Epoch 2.21: Loss = 0.713837
Epoch 2.22: Loss = 0.81871
Epoch 2.23: Loss = 0.795197
Epoch 2.24: Loss = 0.790955
Epoch 2.25: Loss = 0.700714
Epoch 2.26: Loss = 0.739563
Epoch 2.27: Loss = 0.788651
Epoch 2.28: Loss = 0.768524
Epoch 2.29: Loss = 0.715179
Epoch 2.30: Loss = 0.766724
Epoch 2.31: Loss = 0.661087
Epoch 2.32: Loss = 0.786057
Epoch 2.33: Loss = 0.773514
Epoch 2.34: Loss = 0.75444
Epoch 2.35: Loss = 0.762558
Epoch 2.36: Loss = 0.764984
Epoch 2.37: Loss = 0.775879
Epoch 2.38: Loss = 0.776794
Epoch 2.39: Loss = 0.748184
Epoch 2.40: Loss = 0.771255
Epoch 2.41: Loss = 0.736633
Epoch 2.42: Loss = 0.751617
Epoch 2.43: Loss = 0.715027
Epoch 2.44: Loss = 0.697937
Epoch 2.45: Loss = 0.737961
Epoch 2.46: Loss = 0.748077
Epoch 2.47: Loss = 0.745972
Epoch 2.48: Loss = 0.805893
Epoch 2.49: Loss = 0.634979
Epoch 2.50: Loss = 0.675537
Epoch 2.51: Loss = 0.693268
Epoch 2.52: Loss = 0.652313
Epoch 2.53: Loss = 0.782883
Epoch 2.54: Loss = 0.729111
Epoch 2.55: Loss = 0.738037
Epoch 2.56: Loss = 0.687546
Epoch 2.57: Loss = 0.755112
Epoch 2.58: Loss = 0.694733
Epoch 2.59: Loss = 0.686035
Epoch 2.60: Loss = 0.692383
Epoch 2.61: Loss = 0.777023
Epoch 2.62: Loss = 0.703979
Epoch 2.63: Loss = 0.697632
Epoch 2.64: Loss = 0.650024
Epoch 2.65: Loss = 0.730133
Epoch 2.66: Loss = 0.670792
Epoch 2.67: Loss = 0.774017
Epoch 2.68: Loss = 0.696609
Epoch 2.69: Loss = 0.744415
Epoch 2.70: Loss = 0.752823
Epoch 2.71: Loss = 0.712082
Epoch 2.72: Loss = 0.759293
Epoch 2.73: Loss = 0.751343
Epoch 2.74: Loss = 0.706375
Epoch 2.75: Loss = 0.708801
Epoch 2.76: Loss = 0.670441
Epoch 2.77: Loss = 0.680908
Epoch 2.78: Loss = 0.732605
Epoch 2.79: Loss = 0.692398
Epoch 2.80: Loss = 0.695343
Epoch 2.81: Loss = 0.72403
Epoch 2.82: Loss = 0.727539
Epoch 2.83: Loss = 0.644547
Epoch 2.84: Loss = 0.740265
Epoch 2.85: Loss = 0.775406
Epoch 2.86: Loss = 0.718063
Epoch 2.87: Loss = 0.714615
Epoch 2.88: Loss = 0.680984
Epoch 2.89: Loss = 0.749176
Epoch 2.90: Loss = 0.788422
Epoch 2.91: Loss = 0.637421
Epoch 2.92: Loss = 0.684265
Epoch 2.93: Loss = 0.712601
Epoch 2.94: Loss = 0.738937
Epoch 2.95: Loss = 0.650848
Epoch 2.96: Loss = 0.668945
Epoch 2.97: Loss = 0.671555
Epoch 2.98: Loss = 0.627731
Epoch 2.99: Loss = 0.713821
Epoch 2.100: Loss = 0.680389
Epoch 2.101: Loss = 0.789658
Epoch 2.102: Loss = 0.652008
Epoch 2.103: Loss = 0.725479
Epoch 2.104: Loss = 0.716019
Epoch 2.105: Loss = 0.73143
Epoch 2.106: Loss = 0.694
Epoch 2.107: Loss = 0.702591
Epoch 2.108: Loss = 0.832489
Epoch 2.109: Loss = 0.698776
Epoch 2.110: Loss = 0.649475
Epoch 2.111: Loss = 0.708557
Epoch 2.112: Loss = 0.740082
Epoch 2.113: Loss = 0.634003
Epoch 2.114: Loss = 0.669174
Epoch 2.115: Loss = 0.611832
Epoch 2.116: Loss = 0.700653
Epoch 2.117: Loss = 0.702896
Epoch 2.118: Loss = 0.659576
Epoch 2.119: Loss = 0.557373
Epoch 2.120: Loss = 0.690842
TRAIN LOSS = 0.725006
TRAIN ACC = 74.7681 % (44863/60000)
Loss = 0.625473
Loss = 0.740967
Loss = 0.714432
Loss = 0.608749
Loss = 0.654861
Loss = 0.830582
Loss = 0.816833
Loss = 0.758316
Loss = 0.704468
Loss = 0.640625
Loss = 0.767395
Loss = 0.736206
Loss = 0.718155
Loss = 0.736145
Loss = 0.697601
Loss = 0.746582
Loss = 0.660248
Loss = 0.726608
Loss = 0.761276
Loss = 0.705292
TEST LOSS = 0.717541
TEST ACC = 448.63 % (7535/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.68782
Epoch 3.2: Loss = 0.656143
Epoch 3.3: Loss = 0.736084
Epoch 3.4: Loss = 0.689133
Epoch 3.5: Loss = 0.692123
Epoch 3.6: Loss = 0.592148
Epoch 3.7: Loss = 0.593521
Epoch 3.8: Loss = 0.733154
Epoch 3.9: Loss = 0.739609
Epoch 3.10: Loss = 0.758209
Epoch 3.11: Loss = 0.686081
Epoch 3.12: Loss = 0.717392
Epoch 3.13: Loss = 0.666946
Epoch 3.14: Loss = 0.74913
Epoch 3.15: Loss = 0.679123
Epoch 3.16: Loss = 0.762421
Epoch 3.17: Loss = 0.64917
Epoch 3.18: Loss = 0.636337
Epoch 3.19: Loss = 0.648102
Epoch 3.20: Loss = 0.632751
Epoch 3.21: Loss = 0.80838
Epoch 3.22: Loss = 0.689163
Epoch 3.23: Loss = 0.716141
Epoch 3.24: Loss = 0.631424
Epoch 3.25: Loss = 0.6604
Epoch 3.26: Loss = 0.64917
Epoch 3.27: Loss = 0.658752
Epoch 3.28: Loss = 0.682953
Epoch 3.29: Loss = 0.654449
Epoch 3.30: Loss = 0.650177
Epoch 3.31: Loss = 0.675049
Epoch 3.32: Loss = 0.769699
Epoch 3.33: Loss = 0.558746
Epoch 3.34: Loss = 0.603226
Epoch 3.35: Loss = 0.628632
Epoch 3.36: Loss = 0.718384
Epoch 3.37: Loss = 0.65271
Epoch 3.38: Loss = 0.629669
Epoch 3.39: Loss = 0.761063
Epoch 3.40: Loss = 0.666504
Epoch 3.41: Loss = 0.738022
Epoch 3.42: Loss = 0.741653
Epoch 3.43: Loss = 0.706375
Epoch 3.44: Loss = 0.712006
Epoch 3.45: Loss = 0.7388
Epoch 3.46: Loss = 0.625702
Epoch 3.47: Loss = 0.582367
Epoch 3.48: Loss = 0.615845
Epoch 3.49: Loss = 0.625992
Epoch 3.50: Loss = 0.704895
Epoch 3.51: Loss = 0.642502
Epoch 3.52: Loss = 0.654633
Epoch 3.53: Loss = 0.597626
Epoch 3.54: Loss = 0.64679
Epoch 3.55: Loss = 0.593658
Epoch 3.56: Loss = 0.721298
Epoch 3.57: Loss = 0.618332
Epoch 3.58: Loss = 0.668182
Epoch 3.59: Loss = 0.679642
Epoch 3.60: Loss = 0.685623
Epoch 3.61: Loss = 0.63475
Epoch 3.62: Loss = 0.71489
Epoch 3.63: Loss = 0.750214
Epoch 3.64: Loss = 0.705856
Epoch 3.65: Loss = 0.696121
Epoch 3.66: Loss = 0.654846
Epoch 3.67: Loss = 0.678452
Epoch 3.68: Loss = 0.706833
Epoch 3.69: Loss = 0.666733
Epoch 3.70: Loss = 0.60965
Epoch 3.71: Loss = 0.71199
Epoch 3.72: Loss = 0.706055
Epoch 3.73: Loss = 0.703369
Epoch 3.74: Loss = 0.626053
Epoch 3.75: Loss = 0.63562
Epoch 3.76: Loss = 0.651169
Epoch 3.77: Loss = 0.658051
Epoch 3.78: Loss = 0.740082
Epoch 3.79: Loss = 0.645325
Epoch 3.80: Loss = 0.627243
Epoch 3.81: Loss = 0.702148
Epoch 3.82: Loss = 0.684601
Epoch 3.83: Loss = 0.666168
Epoch 3.84: Loss = 0.817261
Epoch 3.85: Loss = 0.727646
Epoch 3.86: Loss = 0.723663
Epoch 3.87: Loss = 0.632538
Epoch 3.88: Loss = 0.677689
Epoch 3.89: Loss = 0.638199
Epoch 3.90: Loss = 0.625427
Epoch 3.91: Loss = 0.650467
Epoch 3.92: Loss = 0.748199
Epoch 3.93: Loss = 0.557632
Epoch 3.94: Loss = 0.692734
Epoch 3.95: Loss = 0.606064
Epoch 3.96: Loss = 0.707977
Epoch 3.97: Loss = 0.650635
Epoch 3.98: Loss = 0.635971
Epoch 3.99: Loss = 0.728226
Epoch 3.100: Loss = 0.62674
Epoch 3.101: Loss = 0.619354
Epoch 3.102: Loss = 0.626755
Epoch 3.103: Loss = 0.632553
Epoch 3.104: Loss = 0.638031
Epoch 3.105: Loss = 0.761078
Epoch 3.106: Loss = 0.683853
Epoch 3.107: Loss = 0.585693
Epoch 3.108: Loss = 0.70433
Epoch 3.109: Loss = 0.705368
Epoch 3.110: Loss = 0.769318
Epoch 3.111: Loss = 0.649139
Epoch 3.112: Loss = 0.643585
Epoch 3.113: Loss = 0.619141
Epoch 3.114: Loss = 0.641953
Epoch 3.115: Loss = 0.65419
Epoch 3.116: Loss = 0.613632
Epoch 3.117: Loss = 0.676941
Epoch 3.118: Loss = 0.71817
Epoch 3.119: Loss = 0.644989
Epoch 3.120: Loss = 0.678299
TRAIN LOSS = 0.673019
TRAIN ACC = 77.4261 % (46458/60000)
Loss = 0.580734
Loss = 0.695267
Loss = 0.663681
Loss = 0.562836
Loss = 0.629272
Loss = 0.774872
Loss = 0.796768
Loss = 0.724518
Loss = 0.665482
Loss = 0.607285
Loss = 0.718521
Loss = 0.693787
Loss = 0.694092
Loss = 0.700348
Loss = 0.656067
Loss = 0.681229
Loss = 0.625488
Loss = 0.684067
Loss = 0.704193
Loss = 0.666824
TEST LOSS = 0.676266
TEST ACC = 464.58 % (7700/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.697693
Epoch 4.2: Loss = 0.686295
Epoch 4.3: Loss = 0.692291
Epoch 4.4: Loss = 0.692291
Epoch 4.5: Loss = 0.581467
Epoch 4.6: Loss = 0.658356
Epoch 4.7: Loss = 0.724518
Epoch 4.8: Loss = 0.671204
Epoch 4.9: Loss = 0.659515
Epoch 4.10: Loss = 0.771332
Epoch 4.11: Loss = 0.736893
Epoch 4.12: Loss = 0.66124
Epoch 4.13: Loss = 0.697998
Epoch 4.14: Loss = 0.770813
Epoch 4.15: Loss = 0.766495
Epoch 4.16: Loss = 0.65976
Epoch 4.17: Loss = 0.691925
Epoch 4.18: Loss = 0.734406
Epoch 4.19: Loss = 0.589005
Epoch 4.20: Loss = 0.627777
Epoch 4.21: Loss = 0.588196
Epoch 4.22: Loss = 0.66185
Epoch 4.23: Loss = 0.673996
Epoch 4.24: Loss = 0.681549
Epoch 4.25: Loss = 0.687088
Epoch 4.26: Loss = 0.597168
Epoch 4.27: Loss = 0.718964
Epoch 4.28: Loss = 0.675842
Epoch 4.29: Loss = 0.637939
Epoch 4.30: Loss = 0.6922
Epoch 4.31: Loss = 0.65477
Epoch 4.32: Loss = 0.575012
Epoch 4.33: Loss = 0.628906
Epoch 4.34: Loss = 0.624207
Epoch 4.35: Loss = 0.735748
Epoch 4.36: Loss = 0.562851
Epoch 4.37: Loss = 0.673294
Epoch 4.38: Loss = 0.562317
Epoch 4.39: Loss = 0.542923
Epoch 4.40: Loss = 0.667755
Epoch 4.41: Loss = 0.651077
Epoch 4.42: Loss = 0.659088
Epoch 4.43: Loss = 0.653336
Epoch 4.44: Loss = 0.56012
Epoch 4.45: Loss = 0.613281
Epoch 4.46: Loss = 0.634232
Epoch 4.47: Loss = 0.635559
Epoch 4.48: Loss = 0.654984
Epoch 4.49: Loss = 0.714676
Epoch 4.50: Loss = 0.729111
Epoch 4.51: Loss = 0.592636
Epoch 4.52: Loss = 0.609421
Epoch 4.53: Loss = 0.657379
Epoch 4.54: Loss = 0.623901
Epoch 4.55: Loss = 0.736145
Epoch 4.56: Loss = 0.585693
Epoch 4.57: Loss = 0.761887
Epoch 4.58: Loss = 0.654388
Epoch 4.59: Loss = 0.558105
Epoch 4.60: Loss = 0.610184
Epoch 4.61: Loss = 0.737701
Epoch 4.62: Loss = 0.541183
Epoch 4.63: Loss = 0.66684
Epoch 4.64: Loss = 0.76178
Epoch 4.65: Loss = 0.663956
Epoch 4.66: Loss = 0.634003
Epoch 4.67: Loss = 0.708572
Epoch 4.68: Loss = 0.675079
Epoch 4.69: Loss = 0.704926
Epoch 4.70: Loss = 0.621338
Epoch 4.71: Loss = 0.595993
Epoch 4.72: Loss = 0.651535
Epoch 4.73: Loss = 0.646652
Epoch 4.74: Loss = 0.662598
Epoch 4.75: Loss = 0.696411
Epoch 4.76: Loss = 0.740112
Epoch 4.77: Loss = 0.629486
Epoch 4.78: Loss = 0.676819
Epoch 4.79: Loss = 0.634338
Epoch 4.80: Loss = 0.619385
Epoch 4.81: Loss = 0.620636
Epoch 4.82: Loss = 0.589676
Epoch 4.83: Loss = 0.594574
Epoch 4.84: Loss = 0.60817
Epoch 4.85: Loss = 0.690414
Epoch 4.86: Loss = 0.687744
Epoch 4.87: Loss = 0.651367
Epoch 4.88: Loss = 0.763535
Epoch 4.89: Loss = 0.588333
Epoch 4.90: Loss = 0.613434
Epoch 4.91: Loss = 0.691162
Epoch 4.92: Loss = 0.612
Epoch 4.93: Loss = 0.57309
Epoch 4.94: Loss = 0.699722
Epoch 4.95: Loss = 0.780701
Epoch 4.96: Loss = 0.532852
Epoch 4.97: Loss = 0.68483
Epoch 4.98: Loss = 0.639969
Epoch 4.99: Loss = 0.67511
Epoch 4.100: Loss = 0.765991
Epoch 4.101: Loss = 0.604248
Epoch 4.102: Loss = 0.581314
Epoch 4.103: Loss = 0.618073
Epoch 4.104: Loss = 0.627121
Epoch 4.105: Loss = 0.604156
Epoch 4.106: Loss = 0.680801
Epoch 4.107: Loss = 0.658066
Epoch 4.108: Loss = 0.543457
Epoch 4.109: Loss = 0.636017
Epoch 4.110: Loss = 0.558029
Epoch 4.111: Loss = 0.643845
Epoch 4.112: Loss = 0.599274
Epoch 4.113: Loss = 0.603943
Epoch 4.114: Loss = 0.595139
Epoch 4.115: Loss = 0.596924
Epoch 4.116: Loss = 0.60704
Epoch 4.117: Loss = 0.629959
Epoch 4.118: Loss = 0.722198
Epoch 4.119: Loss = 0.646561
Epoch 4.120: Loss = 0.628677
TRAIN LOSS = 0.651886
TRAIN ACC = 78.6606 % (47198/60000)
Loss = 0.561722
Loss = 0.697067
Loss = 0.639633
Loss = 0.545227
Loss = 0.639908
Loss = 0.777359
Loss = 0.7789
Loss = 0.71814
Loss = 0.6595
Loss = 0.60434
Loss = 0.737793
Loss = 0.706711
Loss = 0.684723
Loss = 0.696503
Loss = 0.653625
Loss = 0.670074
Loss = 0.617508
Loss = 0.686691
Loss = 0.713242
Loss = 0.656418
TEST LOSS = 0.672254
TEST ACC = 471.979 % (7836/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.668594
Epoch 5.2: Loss = 0.592117
Epoch 5.3: Loss = 0.540924
Epoch 5.4: Loss = 0.673599
Epoch 5.5: Loss = 0.627869
Epoch 5.6: Loss = 0.706589
Epoch 5.7: Loss = 0.581848
Epoch 5.8: Loss = 0.666611
Epoch 5.9: Loss = 0.668198
Epoch 5.10: Loss = 0.683578
Epoch 5.11: Loss = 0.625305
Epoch 5.12: Loss = 0.595963
Epoch 5.13: Loss = 0.67009
Epoch 5.14: Loss = 0.561462
Epoch 5.15: Loss = 0.600357
Epoch 5.16: Loss = 0.619293
Epoch 5.17: Loss = 0.566727
Epoch 5.18: Loss = 0.630936
Epoch 5.19: Loss = 0.68779
Epoch 5.20: Loss = 0.581863
Epoch 5.21: Loss = 0.622086
Epoch 5.22: Loss = 0.59491
Epoch 5.23: Loss = 0.677444
Epoch 5.24: Loss = 0.665329
Epoch 5.25: Loss = 0.593185
Epoch 5.26: Loss = 0.67511
Epoch 5.27: Loss = 0.628922
Epoch 5.28: Loss = 0.594406
Epoch 5.29: Loss = 0.551117
Epoch 5.30: Loss = 0.78331
Epoch 5.31: Loss = 0.698242
Epoch 5.32: Loss = 0.690262
Epoch 5.33: Loss = 0.705231
Epoch 5.34: Loss = 0.778015
Epoch 5.35: Loss = 0.666748
Epoch 5.36: Loss = 0.630432
Epoch 5.37: Loss = 0.655853
Epoch 5.38: Loss = 0.61998
Epoch 5.39: Loss = 0.614746
Epoch 5.40: Loss = 0.594376
Epoch 5.41: Loss = 0.643677
Epoch 5.42: Loss = 0.581482
Epoch 5.43: Loss = 0.673676
Epoch 5.44: Loss = 0.502228
Epoch 5.45: Loss = 0.722122
Epoch 5.46: Loss = 0.541489
Epoch 5.47: Loss = 0.583862
Epoch 5.48: Loss = 0.596359
Epoch 5.49: Loss = 0.696716
Epoch 5.50: Loss = 0.644135
Epoch 5.51: Loss = 0.64357
Epoch 5.52: Loss = 0.647675
Epoch 5.53: Loss = 0.597336
Epoch 5.54: Loss = 0.704132
Epoch 5.55: Loss = 0.706467
Epoch 5.56: Loss = 0.699722
Epoch 5.57: Loss = 0.643066
Epoch 5.58: Loss = 0.587494
Epoch 5.59: Loss = 0.657974
Epoch 5.60: Loss = 0.677032
Epoch 5.61: Loss = 0.514175
Epoch 5.62: Loss = 0.635345
Epoch 5.63: Loss = 0.683533
Epoch 5.64: Loss = 0.562912
Epoch 5.65: Loss = 0.584152
Epoch 5.66: Loss = 0.584412
Epoch 5.67: Loss = 0.615692
Epoch 5.68: Loss = 0.691895
Epoch 5.69: Loss = 0.657272
Epoch 5.70: Loss = 0.590012
Epoch 5.71: Loss = 0.668289
Epoch 5.72: Loss = 0.531982
Epoch 5.73: Loss = 0.626373
Epoch 5.74: Loss = 0.801025
Epoch 5.75: Loss = 0.710007
Epoch 5.76: Loss = 0.726151
Epoch 5.77: Loss = 0.746811
Epoch 5.78: Loss = 0.684875
Epoch 5.79: Loss = 0.599045
Epoch 5.80: Loss = 0.621292
Epoch 5.81: Loss = 0.582916
Epoch 5.82: Loss = 0.611954
Epoch 5.83: Loss = 0.754471
Epoch 5.84: Loss = 0.548492
Epoch 5.85: Loss = 0.562408
Epoch 5.86: Loss = 0.647614
Epoch 5.87: Loss = 0.70491
Epoch 5.88: Loss = 0.64978
Epoch 5.89: Loss = 0.668076
Epoch 5.90: Loss = 0.688187
Epoch 5.91: Loss = 0.634949
Epoch 5.92: Loss = 0.676651
Epoch 5.93: Loss = 0.576691
Epoch 5.94: Loss = 0.611404
Epoch 5.95: Loss = 0.675415
Epoch 5.96: Loss = 0.621811
Epoch 5.97: Loss = 0.544128
Epoch 5.98: Loss = 0.551102
Epoch 5.99: Loss = 0.658249
Epoch 5.100: Loss = 0.584167
Epoch 5.101: Loss = 0.531357
Epoch 5.102: Loss = 0.60704
Epoch 5.103: Loss = 0.647659
Epoch 5.104: Loss = 0.688995
Epoch 5.105: Loss = 0.764999
Epoch 5.106: Loss = 0.664734
Epoch 5.107: Loss = 0.563293
Epoch 5.108: Loss = 0.682053
Epoch 5.109: Loss = 0.703705
Epoch 5.110: Loss = 0.694809
Epoch 5.111: Loss = 0.593185
Epoch 5.112: Loss = 0.65332
Epoch 5.113: Loss = 0.665298
Epoch 5.114: Loss = 0.623001
Epoch 5.115: Loss = 0.606705
Epoch 5.116: Loss = 0.590485
Epoch 5.117: Loss = 0.725586
Epoch 5.118: Loss = 0.642288
Epoch 5.119: Loss = 0.632294
Epoch 5.120: Loss = 0.597534
TRAIN LOSS = 0.638397
TRAIN ACC = 79.4571 % (47677/60000)
Loss = 0.561142
Loss = 0.684967
Loss = 0.648224
Loss = 0.551041
Loss = 0.644318
Loss = 0.765808
Loss = 0.77623
Loss = 0.711838
Loss = 0.66391
Loss = 0.601379
Loss = 0.742783
Loss = 0.721771
Loss = 0.68251
Loss = 0.711578
Loss = 0.639877
Loss = 0.658005
Loss = 0.613831
Loss = 0.678589
Loss = 0.697067
Loss = 0.646866
TEST LOSS = 0.670086
TEST ACC = 476.768 % (7902/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.545914
Epoch 6.2: Loss = 0.702789
Epoch 6.3: Loss = 0.613159
Epoch 6.4: Loss = 0.719269
Epoch 6.5: Loss = 0.585739
Epoch 6.6: Loss = 0.635178
Epoch 6.7: Loss = 0.591034
Epoch 6.8: Loss = 0.604294
Epoch 6.9: Loss = 0.676605
Epoch 6.10: Loss = 0.679703
Epoch 6.11: Loss = 0.617203
Epoch 6.12: Loss = 0.74118
Epoch 6.13: Loss = 0.597244
Epoch 6.14: Loss = 0.558243
Epoch 6.15: Loss = 0.668106
Epoch 6.16: Loss = 0.720856
Epoch 6.17: Loss = 0.723129
Epoch 6.18: Loss = 0.71434
Epoch 6.19: Loss = 0.72197
Epoch 6.20: Loss = 0.661453
Epoch 6.21: Loss = 0.657578
Epoch 6.22: Loss = 0.464355
Epoch 6.23: Loss = 0.706009
Epoch 6.24: Loss = 0.694809
Epoch 6.25: Loss = 0.700378
Epoch 6.26: Loss = 0.6707
Epoch 6.27: Loss = 0.629288
Epoch 6.28: Loss = 0.626114
Epoch 6.29: Loss = 0.604431
Epoch 6.30: Loss = 0.655426
Epoch 6.31: Loss = 0.623291
Epoch 6.32: Loss = 0.593689
Epoch 6.33: Loss = 0.671249
Epoch 6.34: Loss = 0.551804
Epoch 6.35: Loss = 0.626648
Epoch 6.36: Loss = 0.568298
Epoch 6.37: Loss = 0.640518
Epoch 6.38: Loss = 0.584732
Epoch 6.39: Loss = 0.6073
Epoch 6.40: Loss = 0.589447
Epoch 6.41: Loss = 0.662781
Epoch 6.42: Loss = 0.64064
Epoch 6.43: Loss = 0.577728
Epoch 6.44: Loss = 0.570114
Epoch 6.45: Loss = 0.660614
Epoch 6.46: Loss = 0.605698
Epoch 6.47: Loss = 0.611328
Epoch 6.48: Loss = 0.626404
Epoch 6.49: Loss = 0.589188
Epoch 6.50: Loss = 0.611725
Epoch 6.51: Loss = 0.626923
Epoch 6.52: Loss = 0.605911
Epoch 6.53: Loss = 0.634018
Epoch 6.54: Loss = 0.581955
Epoch 6.55: Loss = 0.634521
Epoch 6.56: Loss = 0.594986
Epoch 6.57: Loss = 0.662918
Epoch 6.58: Loss = 0.58873
Epoch 6.59: Loss = 0.643356
Epoch 6.60: Loss = 0.589417
Epoch 6.61: Loss = 0.677307
Epoch 6.62: Loss = 0.533127
Epoch 6.63: Loss = 0.632721
Epoch 6.64: Loss = 0.633972
Epoch 6.65: Loss = 0.612213
Epoch 6.66: Loss = 0.606644
Epoch 6.67: Loss = 0.604126
Epoch 6.68: Loss = 0.649353
Epoch 6.69: Loss = 0.487167
Epoch 6.70: Loss = 0.589767
Epoch 6.71: Loss = 0.650009
Epoch 6.72: Loss = 0.569519
Epoch 6.73: Loss = 0.630524
Epoch 6.74: Loss = 0.629517
Epoch 6.75: Loss = 0.660477
Epoch 6.76: Loss = 0.601074
Epoch 6.77: Loss = 0.629181
Epoch 6.78: Loss = 0.632858
Epoch 6.79: Loss = 0.606659
Epoch 6.80: Loss = 0.778809
Epoch 6.81: Loss = 0.583618
Epoch 6.82: Loss = 0.611557
Epoch 6.83: Loss = 0.61412
Epoch 6.84: Loss = 0.591049
Epoch 6.85: Loss = 0.610748
Epoch 6.86: Loss = 0.739655
Epoch 6.87: Loss = 0.672318
Epoch 6.88: Loss = 0.630234
Epoch 6.89: Loss = 0.541931
Epoch 6.90: Loss = 0.632629
Epoch 6.91: Loss = 0.684341
Epoch 6.92: Loss = 0.720398
Epoch 6.93: Loss = 0.608444
Epoch 6.94: Loss = 0.581116
Epoch 6.95: Loss = 0.615707
Epoch 6.96: Loss = 0.590775
Epoch 6.97: Loss = 0.721985
Epoch 6.98: Loss = 0.56369
Epoch 6.99: Loss = 0.585114
Epoch 6.100: Loss = 0.564133
Epoch 6.101: Loss = 0.645004
Epoch 6.102: Loss = 0.682846
Epoch 6.103: Loss = 0.765564
Epoch 6.104: Loss = 0.70195
Epoch 6.105: Loss = 0.525513
Epoch 6.106: Loss = 0.542953
Epoch 6.107: Loss = 0.624329
Epoch 6.108: Loss = 0.606735
Epoch 6.109: Loss = 0.623901
Epoch 6.110: Loss = 0.607025
Epoch 6.111: Loss = 0.582642
Epoch 6.112: Loss = 0.711044
Epoch 6.113: Loss = 0.614731
Epoch 6.114: Loss = 0.617783
Epoch 6.115: Loss = 0.669479
Epoch 6.116: Loss = 0.527115
Epoch 6.117: Loss = 0.69873
Epoch 6.118: Loss = 0.643234
Epoch 6.119: Loss = 0.608704
Epoch 6.120: Loss = 0.679276
TRAIN LOSS = 0.62851
TRAIN ACC = 80.22 % (48134/60000)
Loss = 0.567169
Loss = 0.659744
Loss = 0.624481
Loss = 0.544418
Loss = 0.627502
Loss = 0.740921
Loss = 0.773163
Loss = 0.718887
Loss = 0.653473
Loss = 0.615814
Loss = 0.775314
Loss = 0.733444
Loss = 0.691528
Loss = 0.710342
Loss = 0.628708
Loss = 0.652145
Loss = 0.617294
Loss = 0.690994
Loss = 0.697098
Loss = 0.658859
TEST LOSS = 0.669065
TEST ACC = 481.339 % (7970/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.651062
Epoch 7.2: Loss = 0.69632
Epoch 7.3: Loss = 0.691269
Epoch 7.4: Loss = 0.650726
Epoch 7.5: Loss = 0.613647
Epoch 7.6: Loss = 0.658356
Epoch 7.7: Loss = 0.565048
Epoch 7.8: Loss = 0.5793
Epoch 7.9: Loss = 0.72438
Epoch 7.10: Loss = 0.6745
Epoch 7.11: Loss = 0.604065
Epoch 7.12: Loss = 0.702789
Epoch 7.13: Loss = 0.689667
Epoch 7.14: Loss = 0.664474
Epoch 7.15: Loss = 0.662827
Epoch 7.16: Loss = 0.670685
Epoch 7.17: Loss = 0.638336
Epoch 7.18: Loss = 0.738998
Epoch 7.19: Loss = 0.582458
Epoch 7.20: Loss = 0.569168
Epoch 7.21: Loss = 0.73613
Epoch 7.22: Loss = 0.560211
Epoch 7.23: Loss = 0.745895
Epoch 7.24: Loss = 0.764877
Epoch 7.25: Loss = 0.724197
Epoch 7.26: Loss = 0.676926
Epoch 7.27: Loss = 0.580276
Epoch 7.28: Loss = 0.561707
Epoch 7.29: Loss = 0.595734
Epoch 7.30: Loss = 0.50386
Epoch 7.31: Loss = 0.633179
Epoch 7.32: Loss = 0.624359
Epoch 7.33: Loss = 0.59552
Epoch 7.34: Loss = 0.690659
Epoch 7.35: Loss = 0.585403
Epoch 7.36: Loss = 0.661743
Epoch 7.37: Loss = 0.689545
Epoch 7.38: Loss = 0.676788
Epoch 7.39: Loss = 0.622375
Epoch 7.40: Loss = 0.591141
Epoch 7.41: Loss = 0.719604
Epoch 7.42: Loss = 0.663025
Epoch 7.43: Loss = 0.623032
Epoch 7.44: Loss = 0.495087
Epoch 7.45: Loss = 0.714233
Epoch 7.46: Loss = 0.617584
Epoch 7.47: Loss = 0.679871
Epoch 7.48: Loss = 0.578583
Epoch 7.49: Loss = 0.560806
Epoch 7.50: Loss = 0.656967
Epoch 7.51: Loss = 0.559799
Epoch 7.52: Loss = 0.638412
Epoch 7.53: Loss = 0.635483
Epoch 7.54: Loss = 0.733109
Epoch 7.55: Loss = 0.630493
Epoch 7.56: Loss = 0.61795
Epoch 7.57: Loss = 0.561401
Epoch 7.58: Loss = 0.597092
Epoch 7.59: Loss = 0.555374
Epoch 7.60: Loss = 0.536728
Epoch 7.61: Loss = 0.561111
Epoch 7.62: Loss = 0.643616
Epoch 7.63: Loss = 0.565292
Epoch 7.64: Loss = 0.614258
Epoch 7.65: Loss = 0.641159
Epoch 7.66: Loss = 0.624405
Epoch 7.67: Loss = 0.697983
Epoch 7.68: Loss = 0.601227
Epoch 7.69: Loss = 0.557312
Epoch 7.70: Loss = 0.740082
Epoch 7.71: Loss = 0.689163
Epoch 7.72: Loss = 0.665497
Epoch 7.73: Loss = 0.664444
Epoch 7.74: Loss = 0.651276
Epoch 7.75: Loss = 0.588318
Epoch 7.76: Loss = 0.682678
Epoch 7.77: Loss = 0.545624
Epoch 7.78: Loss = 0.617706
Epoch 7.79: Loss = 0.559799
Epoch 7.80: Loss = 0.662796
Epoch 7.81: Loss = 0.620987
Epoch 7.82: Loss = 0.616043
Epoch 7.83: Loss = 0.641449
Epoch 7.84: Loss = 0.547333
Epoch 7.85: Loss = 0.651398
Epoch 7.86: Loss = 0.575241
Epoch 7.87: Loss = 0.67778
Epoch 7.88: Loss = 0.508347
Epoch 7.89: Loss = 0.742081
Epoch 7.90: Loss = 0.517014
Epoch 7.91: Loss = 0.671265
Epoch 7.92: Loss = 0.612259
Epoch 7.93: Loss = 0.594238
Epoch 7.94: Loss = 0.631912
Epoch 7.95: Loss = 0.620026
Epoch 7.96: Loss = 0.57637
Epoch 7.97: Loss = 0.768234
Epoch 7.98: Loss = 0.624512
Epoch 7.99: Loss = 0.817474
Epoch 7.100: Loss = 0.648468
Epoch 7.101: Loss = 0.773727
Epoch 7.102: Loss = 0.63826
Epoch 7.103: Loss = 0.705154
Epoch 7.104: Loss = 0.644852
Epoch 7.105: Loss = 0.694122
Epoch 7.106: Loss = 0.714722
Epoch 7.107: Loss = 0.574097
Epoch 7.108: Loss = 0.656982
Epoch 7.109: Loss = 0.615204
Epoch 7.110: Loss = 0.584259
Epoch 7.111: Loss = 0.596039
Epoch 7.112: Loss = 0.543793
Epoch 7.113: Loss = 0.750641
Epoch 7.114: Loss = 0.587296
Epoch 7.115: Loss = 0.581223
Epoch 7.116: Loss = 0.648788
Epoch 7.117: Loss = 0.753754
Epoch 7.118: Loss = 0.621994
Epoch 7.119: Loss = 0.588272
Epoch 7.120: Loss = 0.619171
TRAIN LOSS = 0.636276
TRAIN ACC = 80.5298 % (48320/60000)
Loss = 0.5504
Loss = 0.651306
Loss = 0.609131
Loss = 0.550293
Loss = 0.606979
Loss = 0.714447
Loss = 0.801147
Loss = 0.717163
Loss = 0.656631
Loss = 0.603348
Loss = 0.76796
Loss = 0.713593
Loss = 0.698578
Loss = 0.681519
Loss = 0.610809
Loss = 0.644348
Loss = 0.608719
Loss = 0.694199
Loss = 0.674774
Loss = 0.651001
TEST LOSS = 0.660317
TEST ACC = 483.199 % (8010/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.655121
Epoch 8.2: Loss = 0.637009
Epoch 8.3: Loss = 0.619461
Epoch 8.4: Loss = 0.666092
Epoch 8.5: Loss = 0.649918
Epoch 8.6: Loss = 0.663544
Epoch 8.7: Loss = 0.69577
Epoch 8.8: Loss = 0.601181
Epoch 8.9: Loss = 0.651505
Epoch 8.10: Loss = 0.555389
Epoch 8.11: Loss = 0.616852
Epoch 8.12: Loss = 0.645676
Epoch 8.13: Loss = 0.631149
Epoch 8.14: Loss = 0.648773
Epoch 8.15: Loss = 0.543243
Epoch 8.16: Loss = 0.601501
Epoch 8.17: Loss = 0.566544
Epoch 8.18: Loss = 0.681854
Epoch 8.19: Loss = 0.667953
Epoch 8.20: Loss = 0.691528
Epoch 8.21: Loss = 0.672501
Epoch 8.22: Loss = 0.707397
Epoch 8.23: Loss = 0.575836
Epoch 8.24: Loss = 0.621597
Epoch 8.25: Loss = 0.627411
Epoch 8.26: Loss = 0.55455
Epoch 8.27: Loss = 0.703842
Epoch 8.28: Loss = 0.552094
Epoch 8.29: Loss = 0.653778
Epoch 8.30: Loss = 0.628403
Epoch 8.31: Loss = 0.62706
Epoch 8.32: Loss = 0.577896
Epoch 8.33: Loss = 0.74707
Epoch 8.34: Loss = 0.698242
Epoch 8.35: Loss = 0.685135
Epoch 8.36: Loss = 0.685669
Epoch 8.37: Loss = 0.665146
Epoch 8.38: Loss = 0.644974
Epoch 8.39: Loss = 0.681793
Epoch 8.40: Loss = 0.536652
Epoch 8.41: Loss = 0.590851
Epoch 8.42: Loss = 0.696396
Epoch 8.43: Loss = 0.59729
Epoch 8.44: Loss = 0.58696
Epoch 8.45: Loss = 0.649582
Epoch 8.46: Loss = 0.659332
Epoch 8.47: Loss = 0.706726
Epoch 8.48: Loss = 0.677536
Epoch 8.49: Loss = 0.558487
Epoch 8.50: Loss = 0.623795
Epoch 8.51: Loss = 0.563568
Epoch 8.52: Loss = 0.570831
Epoch 8.53: Loss = 0.702866
Epoch 8.54: Loss = 0.506409
Epoch 8.55: Loss = 0.682617
Epoch 8.56: Loss = 0.516861
Epoch 8.57: Loss = 0.695923
Epoch 8.58: Loss = 0.779755
Epoch 8.59: Loss = 0.666489
Epoch 8.60: Loss = 0.684387
Epoch 8.61: Loss = 0.528412
Epoch 8.62: Loss = 0.647583
Epoch 8.63: Loss = 0.714386
Epoch 8.64: Loss = 0.609909
Epoch 8.65: Loss = 0.638535
Epoch 8.66: Loss = 0.661209
Epoch 8.67: Loss = 0.645462
Epoch 8.68: Loss = 0.7108
Epoch 8.69: Loss = 0.61171
Epoch 8.70: Loss = 0.60672
Epoch 8.71: Loss = 0.604553
Epoch 8.72: Loss = 0.622665
Epoch 8.73: Loss = 0.505432
Epoch 8.74: Loss = 0.567291
Epoch 8.75: Loss = 0.671249
Epoch 8.76: Loss = 0.609573
Epoch 8.77: Loss = 0.748306
Epoch 8.78: Loss = 0.643784
Epoch 8.79: Loss = 0.637436
Epoch 8.80: Loss = 0.580597
Epoch 8.81: Loss = 0.643326
Epoch 8.82: Loss = 0.602509
Epoch 8.83: Loss = 0.657379
Epoch 8.84: Loss = 0.527832
Epoch 8.85: Loss = 0.597229
Epoch 8.86: Loss = 0.62796
Epoch 8.87: Loss = 0.761993
Epoch 8.88: Loss = 0.732819
Epoch 8.89: Loss = 0.620163
Epoch 8.90: Loss = 0.65094
Epoch 8.91: Loss = 0.726273
Epoch 8.92: Loss = 0.701996
Epoch 8.93: Loss = 0.542496
Epoch 8.94: Loss = 0.732605
Epoch 8.95: Loss = 0.688797
Epoch 8.96: Loss = 0.714386
Epoch 8.97: Loss = 0.569656
Epoch 8.98: Loss = 0.598999
Epoch 8.99: Loss = 0.628052
Epoch 8.100: Loss = 0.649597
Epoch 8.101: Loss = 0.642563
Epoch 8.102: Loss = 0.60791
Epoch 8.103: Loss = 0.673798
Epoch 8.104: Loss = 0.691483
Epoch 8.105: Loss = 0.544968
Epoch 8.106: Loss = 0.537476
Epoch 8.107: Loss = 0.698715
Epoch 8.108: Loss = 0.501602
Epoch 8.109: Loss = 0.679367
Epoch 8.110: Loss = 0.620499
Epoch 8.111: Loss = 0.623093
Epoch 8.112: Loss = 0.634689
Epoch 8.113: Loss = 0.52449
Epoch 8.114: Loss = 0.610992
Epoch 8.115: Loss = 0.739441
Epoch 8.116: Loss = 0.59082
Epoch 8.117: Loss = 0.69519
Epoch 8.118: Loss = 0.672318
Epoch 8.119: Loss = 0.665939
Epoch 8.120: Loss = 0.737564
TRAIN LOSS = 0.637405
TRAIN ACC = 80.7632 % (48460/60000)
Loss = 0.540054
Loss = 0.667938
Loss = 0.611725
Loss = 0.545746
Loss = 0.599197
Loss = 0.743576
Loss = 0.793961
Loss = 0.70517
Loss = 0.636566
Loss = 0.604004
Loss = 0.775314
Loss = 0.732117
Loss = 0.699661
Loss = 0.692169
Loss = 0.610794
Loss = 0.653336
Loss = 0.610855
Loss = 0.700073
Loss = 0.673141
Loss = 0.657928
TEST LOSS = 0.662666
TEST ACC = 484.599 % (7996/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.689133
Epoch 9.2: Loss = 0.664383
Epoch 9.3: Loss = 0.805679
Epoch 9.4: Loss = 0.580399
Epoch 9.5: Loss = 0.575287
Epoch 9.6: Loss = 0.653366
Epoch 9.7: Loss = 0.677094
Epoch 9.8: Loss = 0.560883
Epoch 9.9: Loss = 0.57341
Epoch 9.10: Loss = 0.706375
Epoch 9.11: Loss = 0.583954
Epoch 9.12: Loss = 0.534485
Epoch 9.13: Loss = 0.635361
Epoch 9.14: Loss = 0.736099
Epoch 9.15: Loss = 0.664017
Epoch 9.16: Loss = 0.613281
Epoch 9.17: Loss = 0.533295
Epoch 9.18: Loss = 0.582153
Epoch 9.19: Loss = 0.606857
Epoch 9.20: Loss = 0.644867
Epoch 9.21: Loss = 0.648849
Epoch 9.22: Loss = 0.81868
Epoch 9.23: Loss = 0.699905
Epoch 9.24: Loss = 0.612488
Epoch 9.25: Loss = 0.658768
Epoch 9.26: Loss = 0.591003
Epoch 9.27: Loss = 0.605133
Epoch 9.28: Loss = 0.668121
Epoch 9.29: Loss = 0.671188
Epoch 9.30: Loss = 0.604172
Epoch 9.31: Loss = 0.631851
Epoch 9.32: Loss = 0.786041
Epoch 9.33: Loss = 0.679291
Epoch 9.34: Loss = 0.603699
Epoch 9.35: Loss = 0.702911
Epoch 9.36: Loss = 0.639786
Epoch 9.37: Loss = 0.581207
Epoch 9.38: Loss = 0.63382
Epoch 9.39: Loss = 0.656052
Epoch 9.40: Loss = 0.586639
Epoch 9.41: Loss = 0.556656
Epoch 9.42: Loss = 0.492462
Epoch 9.43: Loss = 0.737823
Epoch 9.44: Loss = 0.60347
Epoch 9.45: Loss = 0.630295
Epoch 9.46: Loss = 0.555161
Epoch 9.47: Loss = 0.757462
Epoch 9.48: Loss = 0.570465
Epoch 9.49: Loss = 0.630005
Epoch 9.50: Loss = 0.607071
Epoch 9.51: Loss = 0.786499
Epoch 9.52: Loss = 0.603745
Epoch 9.53: Loss = 0.58725
Epoch 9.54: Loss = 0.674103
Epoch 9.55: Loss = 0.693253
Epoch 9.56: Loss = 0.600815
Epoch 9.57: Loss = 0.589584
Epoch 9.58: Loss = 0.567139
Epoch 9.59: Loss = 0.553299
Epoch 9.60: Loss = 0.661087
Epoch 9.61: Loss = 0.846252
Epoch 9.62: Loss = 0.762665
Epoch 9.63: Loss = 0.637344
Epoch 9.64: Loss = 0.716858
Epoch 9.65: Loss = 0.679672
Epoch 9.66: Loss = 0.654449
Epoch 9.67: Loss = 0.620041
Epoch 9.68: Loss = 0.632401
Epoch 9.69: Loss = 0.607239
Epoch 9.70: Loss = 0.737518
Epoch 9.71: Loss = 0.669266
Epoch 9.72: Loss = 0.519485
Epoch 9.73: Loss = 0.564819
Epoch 9.74: Loss = 0.709442
Epoch 9.75: Loss = 0.663315
Epoch 9.76: Loss = 0.585907
Epoch 9.77: Loss = 0.688431
Epoch 9.78: Loss = 0.631409
Epoch 9.79: Loss = 0.56398
Epoch 9.80: Loss = 0.570251
Epoch 9.81: Loss = 0.645493
Epoch 9.82: Loss = 0.55719
Epoch 9.83: Loss = 0.566162
Epoch 9.84: Loss = 0.653061
Epoch 9.85: Loss = 0.665344
Epoch 9.86: Loss = 0.561905
Epoch 9.87: Loss = 0.681549
Epoch 9.88: Loss = 0.590775
Epoch 9.89: Loss = 0.580917
Epoch 9.90: Loss = 0.716064
Epoch 9.91: Loss = 0.648727
Epoch 9.92: Loss = 0.569763
Epoch 9.93: Loss = 0.698837
Epoch 9.94: Loss = 0.662735
Epoch 9.95: Loss = 0.668884
Epoch 9.96: Loss = 0.607925
Epoch 9.97: Loss = 0.601181
Epoch 9.98: Loss = 0.639862
Epoch 9.99: Loss = 0.709732
Epoch 9.100: Loss = 0.541183
Epoch 9.101: Loss = 0.536407
Epoch 9.102: Loss = 0.597153
Epoch 9.103: Loss = 0.621353
Epoch 9.104: Loss = 0.583313
Epoch 9.105: Loss = 0.799881
Epoch 9.106: Loss = 0.748413
Epoch 9.107: Loss = 0.454254
Epoch 9.108: Loss = 0.787903
Epoch 9.109: Loss = 0.55011
Epoch 9.110: Loss = 0.717606
Epoch 9.111: Loss = 0.53685
Epoch 9.112: Loss = 0.65976
Epoch 9.113: Loss = 0.596451
Epoch 9.114: Loss = 0.60318
Epoch 9.115: Loss = 0.545517
Epoch 9.116: Loss = 0.69223
Epoch 9.117: Loss = 0.586929
Epoch 9.118: Loss = 0.601151
Epoch 9.119: Loss = 0.71933
Epoch 9.120: Loss = 0.543137
TRAIN LOSS = 0.635574
TRAIN ACC = 81.0516 % (48633/60000)
Loss = 0.559006
Loss = 0.656494
Loss = 0.597092
Loss = 0.558136
Loss = 0.620453
Loss = 0.717133
Loss = 0.802094
Loss = 0.702194
Loss = 0.631104
Loss = 0.614212
Loss = 0.775116
Loss = 0.762985
Loss = 0.691238
Loss = 0.678284
Loss = 0.638519
Loss = 0.675522
Loss = 0.609924
Loss = 0.691711
Loss = 0.677567
Loss = 0.666214
TEST LOSS = 0.66625
TEST ACC = 486.33 % (8028/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.684235
Epoch 10.2: Loss = 0.516998
Epoch 10.3: Loss = 0.586334
Epoch 10.4: Loss = 0.577408
Epoch 10.5: Loss = 0.689468
Epoch 10.6: Loss = 0.530319
Epoch 10.7: Loss = 0.723862
Epoch 10.8: Loss = 0.779068
Epoch 10.9: Loss = 0.663101
Epoch 10.10: Loss = 0.562119
Epoch 10.11: Loss = 0.593277
Epoch 10.12: Loss = 0.676865
Epoch 10.13: Loss = 0.641891
Epoch 10.14: Loss = 0.695587
Epoch 10.15: Loss = 0.62323
Epoch 10.16: Loss = 0.657547
Epoch 10.17: Loss = 0.623596
Epoch 10.18: Loss = 0.50148
Epoch 10.19: Loss = 0.572205
Epoch 10.20: Loss = 0.647629
Epoch 10.21: Loss = 0.555206
Epoch 10.22: Loss = 0.833237
Epoch 10.23: Loss = 0.603897
Epoch 10.24: Loss = 0.6371
Epoch 10.25: Loss = 0.592194
Epoch 10.26: Loss = 0.664536
Epoch 10.27: Loss = 0.74295
Epoch 10.28: Loss = 0.677353
Epoch 10.29: Loss = 0.730347
Epoch 10.30: Loss = 0.590179
Epoch 10.31: Loss = 0.633255
Epoch 10.32: Loss = 0.64064
Epoch 10.33: Loss = 0.668564
Epoch 10.34: Loss = 0.67981
Epoch 10.35: Loss = 0.507339
Epoch 10.36: Loss = 0.600662
Epoch 10.37: Loss = 0.691742
Epoch 10.38: Loss = 0.620026
Epoch 10.39: Loss = 0.73941
Epoch 10.40: Loss = 0.603424
Epoch 10.41: Loss = 0.619537
Epoch 10.42: Loss = 0.629166
Epoch 10.43: Loss = 0.588043
Epoch 10.44: Loss = 0.635666
Epoch 10.45: Loss = 0.537979
Epoch 10.46: Loss = 0.700165
Epoch 10.47: Loss = 0.550049
Epoch 10.48: Loss = 0.611465
Epoch 10.49: Loss = 0.672501
Epoch 10.50: Loss = 0.637131
Epoch 10.51: Loss = 0.72403
Epoch 10.52: Loss = 0.684067
Epoch 10.53: Loss = 0.603485
Epoch 10.54: Loss = 0.631851
Epoch 10.55: Loss = 0.585434
Epoch 10.56: Loss = 0.769135
Epoch 10.57: Loss = 0.650131
Epoch 10.58: Loss = 0.670013
Epoch 10.59: Loss = 0.629959
Epoch 10.60: Loss = 0.718002
Epoch 10.61: Loss = 0.722855
Epoch 10.62: Loss = 0.500961
Epoch 10.63: Loss = 0.620422
Epoch 10.64: Loss = 0.646362
Epoch 10.65: Loss = 0.62468
Epoch 10.66: Loss = 0.736191
Epoch 10.67: Loss = 0.644028
Epoch 10.68: Loss = 0.722183
Epoch 10.69: Loss = 0.648544
Epoch 10.70: Loss = 0.587143
Epoch 10.71: Loss = 0.636093
Epoch 10.72: Loss = 0.569427
Epoch 10.73: Loss = 0.634323
Epoch 10.74: Loss = 0.660477
Epoch 10.75: Loss = 0.560989
Epoch 10.76: Loss = 0.576279
Epoch 10.77: Loss = 0.695557
Epoch 10.78: Loss = 0.526306
Epoch 10.79: Loss = 0.735672
Epoch 10.80: Loss = 0.639008
Epoch 10.81: Loss = 0.646515
Epoch 10.82: Loss = 0.587662
Epoch 10.83: Loss = 0.66655
Epoch 10.84: Loss = 0.644424
Epoch 10.85: Loss = 0.645782
Epoch 10.86: Loss = 0.633896
Epoch 10.87: Loss = 0.645065
Epoch 10.88: Loss = 0.681122
Epoch 10.89: Loss = 0.691559
Epoch 10.90: Loss = 0.660278
Epoch 10.91: Loss = 0.649994
Epoch 10.92: Loss = 0.647751
Epoch 10.93: Loss = 0.641815
Epoch 10.94: Loss = 0.683945
Epoch 10.95: Loss = 0.690674
Epoch 10.96: Loss = 0.718216
Epoch 10.97: Loss = 0.673065
Epoch 10.98: Loss = 0.551941
Epoch 10.99: Loss = 0.670792
Epoch 10.100: Loss = 0.612503
Epoch 10.101: Loss = 0.711212
Epoch 10.102: Loss = 0.638702
Epoch 10.103: Loss = 0.612579
Epoch 10.104: Loss = 0.702393
Epoch 10.105: Loss = 0.653015
Epoch 10.106: Loss = 0.667084
Epoch 10.107: Loss = 0.600952
Epoch 10.108: Loss = 0.62146
Epoch 10.109: Loss = 0.587219
Epoch 10.110: Loss = 0.624664
Epoch 10.111: Loss = 0.613586
Epoch 10.112: Loss = 0.617798
Epoch 10.113: Loss = 0.713364
Epoch 10.114: Loss = 0.713715
Epoch 10.115: Loss = 0.52301
Epoch 10.116: Loss = 0.660141
Epoch 10.117: Loss = 0.718811
Epoch 10.118: Loss = 0.754578
Epoch 10.119: Loss = 0.562485
Epoch 10.120: Loss = 0.608475
TRAIN LOSS = 0.642029
TRAIN ACC = 81.0043 % (48605/60000)
Loss = 0.557556
Loss = 0.691345
Loss = 0.621765
Loss = 0.562836
Loss = 0.647247
Loss = 0.750397
Loss = 0.819748
Loss = 0.721573
Loss = 0.64122
Loss = 0.631012
Loss = 0.800507
Loss = 0.797379
Loss = 0.714066
Loss = 0.703842
Loss = 0.672668
Loss = 0.698105
Loss = 0.621872
Loss = 0.718613
Loss = 0.708633
Loss = 0.685028
TEST LOSS = 0.68827
TEST ACC = 486.049 % (8046/10000)

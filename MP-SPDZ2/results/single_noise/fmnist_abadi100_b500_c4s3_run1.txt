Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.54622
Epoch 1.2: Loss = 2.43912
Epoch 1.3: Loss = 2.33928
Epoch 1.4: Loss = 2.2513
Epoch 1.5: Loss = 2.18738
Epoch 1.6: Loss = 2.1636
Epoch 1.7: Loss = 2.07059
Epoch 1.8: Loss = 2.01801
Epoch 1.9: Loss = 1.96432
Epoch 1.10: Loss = 1.93268
Epoch 1.11: Loss = 1.86494
Epoch 1.12: Loss = 1.8369
Epoch 1.13: Loss = 1.76416
Epoch 1.14: Loss = 1.71548
Epoch 1.15: Loss = 1.71207
Epoch 1.16: Loss = 1.67375
Epoch 1.17: Loss = 1.60941
Epoch 1.18: Loss = 1.57635
Epoch 1.19: Loss = 1.53775
Epoch 1.20: Loss = 1.48146
Epoch 1.21: Loss = 1.49324
Epoch 1.22: Loss = 1.52461
Epoch 1.23: Loss = 1.42787
Epoch 1.24: Loss = 1.3924
Epoch 1.25: Loss = 1.45953
Epoch 1.26: Loss = 1.37819
Epoch 1.27: Loss = 1.36145
Epoch 1.28: Loss = 1.31032
Epoch 1.29: Loss = 1.31728
Epoch 1.30: Loss = 1.29672
Epoch 1.31: Loss = 1.29739
Epoch 1.32: Loss = 1.31931
Epoch 1.33: Loss = 1.22702
Epoch 1.34: Loss = 1.2399
Epoch 1.35: Loss = 1.26573
Epoch 1.36: Loss = 1.19882
Epoch 1.37: Loss = 1.2444
Epoch 1.38: Loss = 1.21321
Epoch 1.39: Loss = 1.19411
Epoch 1.40: Loss = 1.15358
Epoch 1.41: Loss = 1.09146
Epoch 1.42: Loss = 1.13219
Epoch 1.43: Loss = 1.10451
Epoch 1.44: Loss = 1.12064
Epoch 1.45: Loss = 1.09283
Epoch 1.46: Loss = 1.10323
Epoch 1.47: Loss = 1.09048
Epoch 1.48: Loss = 1.095
Epoch 1.49: Loss = 1.10167
Epoch 1.50: Loss = 1.09813
Epoch 1.51: Loss = 1.08675
Epoch 1.52: Loss = 1.04327
Epoch 1.53: Loss = 1.03908
Epoch 1.54: Loss = 0.990219
Epoch 1.55: Loss = 0.96669
Epoch 1.56: Loss = 0.997543
Epoch 1.57: Loss = 0.984863
Epoch 1.58: Loss = 0.942612
Epoch 1.59: Loss = 0.99733
Epoch 1.60: Loss = 0.916702
Epoch 1.61: Loss = 0.903854
Epoch 1.62: Loss = 1.07774
Epoch 1.63: Loss = 1.02104
Epoch 1.64: Loss = 0.99472
Epoch 1.65: Loss = 0.950821
Epoch 1.66: Loss = 0.94957
Epoch 1.67: Loss = 0.970764
Epoch 1.68: Loss = 0.849976
Epoch 1.69: Loss = 0.954895
Epoch 1.70: Loss = 0.946991
Epoch 1.71: Loss = 0.895187
Epoch 1.72: Loss = 0.927673
Epoch 1.73: Loss = 0.93129
Epoch 1.74: Loss = 0.920059
Epoch 1.75: Loss = 0.96402
Epoch 1.76: Loss = 0.887375
Epoch 1.77: Loss = 0.865601
Epoch 1.78: Loss = 0.821259
Epoch 1.79: Loss = 0.853149
Epoch 1.80: Loss = 0.946518
Epoch 1.81: Loss = 0.898178
Epoch 1.82: Loss = 0.880768
Epoch 1.83: Loss = 0.891693
Epoch 1.84: Loss = 0.926315
Epoch 1.85: Loss = 0.852234
Epoch 1.86: Loss = 0.925064
Epoch 1.87: Loss = 0.875259
Epoch 1.88: Loss = 0.916443
Epoch 1.89: Loss = 0.871277
Epoch 1.90: Loss = 0.844925
Epoch 1.91: Loss = 0.860123
Epoch 1.92: Loss = 0.801834
Epoch 1.93: Loss = 0.902786
Epoch 1.94: Loss = 0.826691
Epoch 1.95: Loss = 0.842285
Epoch 1.96: Loss = 0.844696
Epoch 1.97: Loss = 0.90184
Epoch 1.98: Loss = 0.820419
Epoch 1.99: Loss = 0.748108
Epoch 1.100: Loss = 0.875687
Epoch 1.101: Loss = 0.852325
Epoch 1.102: Loss = 0.876099
Epoch 1.103: Loss = 0.759949
Epoch 1.104: Loss = 0.840347
Epoch 1.105: Loss = 0.853806
Epoch 1.106: Loss = 0.870514
Epoch 1.107: Loss = 0.806503
Epoch 1.108: Loss = 0.791306
Epoch 1.109: Loss = 0.875595
Epoch 1.110: Loss = 0.800735
Epoch 1.111: Loss = 0.781952
Epoch 1.112: Loss = 0.857498
Epoch 1.113: Loss = 0.82222
Epoch 1.114: Loss = 0.804688
Epoch 1.115: Loss = 0.851715
Epoch 1.116: Loss = 0.859207
Epoch 1.117: Loss = 0.838242
Epoch 1.118: Loss = 0.813049
Epoch 1.119: Loss = 0.893356
Epoch 1.120: Loss = 0.779831
TRAIN LOSS = 1.15469
TRAIN ACC = 60.5911 % (36356/60000)
Loss = 0.737762
Loss = 0.819901
Loss = 0.835266
Loss = 0.74147
Loss = 0.73555
Loss = 0.873917
Loss = 0.90712
Loss = 0.853104
Loss = 0.780212
Loss = 0.726334
Loss = 0.876404
Loss = 0.802979
Loss = 0.827225
Loss = 0.829163
Loss = 0.780685
Loss = 0.853699
Loss = 0.752121
Loss = 0.801346
Loss = 0.832275
Loss = 0.793427
TEST LOSS = 0.807998
TEST ACC = 363.559 % (7047/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.80513
Epoch 2.2: Loss = 0.770676
Epoch 2.3: Loss = 0.818878
Epoch 2.4: Loss = 0.802551
Epoch 2.5: Loss = 0.746063
Epoch 2.6: Loss = 0.779907
Epoch 2.7: Loss = 0.735977
Epoch 2.8: Loss = 0.770844
Epoch 2.9: Loss = 0.808228
Epoch 2.10: Loss = 0.750916
Epoch 2.11: Loss = 0.744553
Epoch 2.12: Loss = 0.77655
Epoch 2.13: Loss = 0.794266
Epoch 2.14: Loss = 0.747986
Epoch 2.15: Loss = 0.759399
Epoch 2.16: Loss = 0.713318
Epoch 2.17: Loss = 0.752716
Epoch 2.18: Loss = 0.778595
Epoch 2.19: Loss = 0.752335
Epoch 2.20: Loss = 0.817429
Epoch 2.21: Loss = 0.758667
Epoch 2.22: Loss = 0.789841
Epoch 2.23: Loss = 0.755844
Epoch 2.24: Loss = 0.758652
Epoch 2.25: Loss = 0.74913
Epoch 2.26: Loss = 0.795853
Epoch 2.27: Loss = 0.669937
Epoch 2.28: Loss = 0.745605
Epoch 2.29: Loss = 0.770157
Epoch 2.30: Loss = 0.664948
Epoch 2.31: Loss = 0.760269
Epoch 2.32: Loss = 0.848312
Epoch 2.33: Loss = 0.719604
Epoch 2.34: Loss = 0.731323
Epoch 2.35: Loss = 0.727448
Epoch 2.36: Loss = 0.854431
Epoch 2.37: Loss = 0.654739
Epoch 2.38: Loss = 0.721848
Epoch 2.39: Loss = 0.710754
Epoch 2.40: Loss = 0.765686
Epoch 2.41: Loss = 0.75116
Epoch 2.42: Loss = 0.684082
Epoch 2.43: Loss = 0.782898
Epoch 2.44: Loss = 0.73877
Epoch 2.45: Loss = 0.721893
Epoch 2.46: Loss = 0.709152
Epoch 2.47: Loss = 0.757446
Epoch 2.48: Loss = 0.793594
Epoch 2.49: Loss = 0.790955
Epoch 2.50: Loss = 0.736771
Epoch 2.51: Loss = 0.800247
Epoch 2.52: Loss = 0.76091
Epoch 2.53: Loss = 0.705093
Epoch 2.54: Loss = 0.707413
Epoch 2.55: Loss = 0.776611
Epoch 2.56: Loss = 0.768341
Epoch 2.57: Loss = 0.679459
Epoch 2.58: Loss = 0.843689
Epoch 2.59: Loss = 0.752518
Epoch 2.60: Loss = 0.70697
Epoch 2.61: Loss = 0.723572
Epoch 2.62: Loss = 0.709473
Epoch 2.63: Loss = 0.726547
Epoch 2.64: Loss = 0.776535
Epoch 2.65: Loss = 0.798492
Epoch 2.66: Loss = 0.723373
Epoch 2.67: Loss = 0.746292
Epoch 2.68: Loss = 0.776962
Epoch 2.69: Loss = 0.730362
Epoch 2.70: Loss = 0.738373
Epoch 2.71: Loss = 0.743225
Epoch 2.72: Loss = 0.699692
Epoch 2.73: Loss = 0.664673
Epoch 2.74: Loss = 0.660553
Epoch 2.75: Loss = 0.714142
Epoch 2.76: Loss = 0.734543
Epoch 2.77: Loss = 0.709213
Epoch 2.78: Loss = 0.731979
Epoch 2.79: Loss = 0.638672
Epoch 2.80: Loss = 0.698181
Epoch 2.81: Loss = 0.726593
Epoch 2.82: Loss = 0.751205
Epoch 2.83: Loss = 0.73642
Epoch 2.84: Loss = 0.802368
Epoch 2.85: Loss = 0.709549
Epoch 2.86: Loss = 0.748489
Epoch 2.87: Loss = 0.69603
Epoch 2.88: Loss = 0.710266
Epoch 2.89: Loss = 0.656021
Epoch 2.90: Loss = 0.764175
Epoch 2.91: Loss = 0.733292
Epoch 2.92: Loss = 0.611801
Epoch 2.93: Loss = 0.678345
Epoch 2.94: Loss = 0.712997
Epoch 2.95: Loss = 0.766357
Epoch 2.96: Loss = 0.667328
Epoch 2.97: Loss = 0.751434
Epoch 2.98: Loss = 0.736221
Epoch 2.99: Loss = 0.724716
Epoch 2.100: Loss = 0.777283
Epoch 2.101: Loss = 0.777985
Epoch 2.102: Loss = 0.679947
Epoch 2.103: Loss = 0.714981
Epoch 2.104: Loss = 0.7108
Epoch 2.105: Loss = 0.863205
Epoch 2.106: Loss = 0.651459
Epoch 2.107: Loss = 0.714645
Epoch 2.108: Loss = 0.716507
Epoch 2.109: Loss = 0.726181
Epoch 2.110: Loss = 0.713013
Epoch 2.111: Loss = 0.719528
Epoch 2.112: Loss = 0.741302
Epoch 2.113: Loss = 0.640503
Epoch 2.114: Loss = 0.632141
Epoch 2.115: Loss = 0.60553
Epoch 2.116: Loss = 0.66478
Epoch 2.117: Loss = 0.673889
Epoch 2.118: Loss = 0.645172
Epoch 2.119: Loss = 0.662704
Epoch 2.120: Loss = 0.662689
TRAIN LOSS = 0.734772
TRAIN ACC = 74.1348 % (44483/60000)
Loss = 0.648071
Loss = 0.77182
Loss = 0.727051
Loss = 0.630707
Loss = 0.647491
Loss = 0.802963
Loss = 0.822739
Loss = 0.773071
Loss = 0.710861
Loss = 0.649658
Loss = 0.82193
Loss = 0.752686
Loss = 0.746368
Loss = 0.736206
Loss = 0.705383
Loss = 0.770172
Loss = 0.665924
Loss = 0.731155
Loss = 0.765778
Loss = 0.720108
TEST LOSS = 0.730007
TEST ACC = 444.829 % (7433/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.704285
Epoch 3.2: Loss = 0.743256
Epoch 3.3: Loss = 0.779419
Epoch 3.4: Loss = 0.683517
Epoch 3.5: Loss = 0.711182
Epoch 3.6: Loss = 0.663071
Epoch 3.7: Loss = 0.720734
Epoch 3.8: Loss = 0.655212
Epoch 3.9: Loss = 0.673599
Epoch 3.10: Loss = 0.737961
Epoch 3.11: Loss = 0.631638
Epoch 3.12: Loss = 0.639221
Epoch 3.13: Loss = 0.759552
Epoch 3.14: Loss = 0.691208
Epoch 3.15: Loss = 0.749863
Epoch 3.16: Loss = 0.686005
Epoch 3.17: Loss = 0.811981
Epoch 3.18: Loss = 0.757355
Epoch 3.19: Loss = 0.666016
Epoch 3.20: Loss = 0.711777
Epoch 3.21: Loss = 0.736298
Epoch 3.22: Loss = 0.707321
Epoch 3.23: Loss = 0.732254
Epoch 3.24: Loss = 0.621796
Epoch 3.25: Loss = 0.643723
Epoch 3.26: Loss = 0.660034
Epoch 3.27: Loss = 0.583405
Epoch 3.28: Loss = 0.739258
Epoch 3.29: Loss = 0.671265
Epoch 3.30: Loss = 0.665588
Epoch 3.31: Loss = 0.72757
Epoch 3.32: Loss = 0.697403
Epoch 3.33: Loss = 0.752853
Epoch 3.34: Loss = 0.615677
Epoch 3.35: Loss = 0.677322
Epoch 3.36: Loss = 0.667496
Epoch 3.37: Loss = 0.7686
Epoch 3.38: Loss = 0.69696
Epoch 3.39: Loss = 0.711258
Epoch 3.40: Loss = 0.727722
Epoch 3.41: Loss = 0.711304
Epoch 3.42: Loss = 0.645279
Epoch 3.43: Loss = 0.667618
Epoch 3.44: Loss = 0.595871
Epoch 3.45: Loss = 0.681808
Epoch 3.46: Loss = 0.664612
Epoch 3.47: Loss = 0.615219
Epoch 3.48: Loss = 0.729279
Epoch 3.49: Loss = 0.693848
Epoch 3.50: Loss = 0.616974
Epoch 3.51: Loss = 0.655426
Epoch 3.52: Loss = 0.723221
Epoch 3.53: Loss = 0.659149
Epoch 3.54: Loss = 0.665619
Epoch 3.55: Loss = 0.702621
Epoch 3.56: Loss = 0.71756
Epoch 3.57: Loss = 0.647095
Epoch 3.58: Loss = 0.651917
Epoch 3.59: Loss = 0.63475
Epoch 3.60: Loss = 0.684189
Epoch 3.61: Loss = 0.729095
Epoch 3.62: Loss = 0.784286
Epoch 3.63: Loss = 0.724976
Epoch 3.64: Loss = 0.599426
Epoch 3.65: Loss = 0.648911
Epoch 3.66: Loss = 0.719101
Epoch 3.67: Loss = 0.640778
Epoch 3.68: Loss = 0.661606
Epoch 3.69: Loss = 0.553558
Epoch 3.70: Loss = 0.655396
Epoch 3.71: Loss = 0.681778
Epoch 3.72: Loss = 0.656036
Epoch 3.73: Loss = 0.659332
Epoch 3.74: Loss = 0.625854
Epoch 3.75: Loss = 0.733871
Epoch 3.76: Loss = 0.671021
Epoch 3.77: Loss = 0.609772
Epoch 3.78: Loss = 0.752487
Epoch 3.79: Loss = 0.692535
Epoch 3.80: Loss = 0.709122
Epoch 3.81: Loss = 0.610809
Epoch 3.82: Loss = 0.698578
Epoch 3.83: Loss = 0.627472
Epoch 3.84: Loss = 0.740265
Epoch 3.85: Loss = 0.652435
Epoch 3.86: Loss = 0.774704
Epoch 3.87: Loss = 0.673965
Epoch 3.88: Loss = 0.647919
Epoch 3.89: Loss = 0.690063
Epoch 3.90: Loss = 0.649124
Epoch 3.91: Loss = 0.61586
Epoch 3.92: Loss = 0.72998
Epoch 3.93: Loss = 0.690063
Epoch 3.94: Loss = 0.67688
Epoch 3.95: Loss = 0.668549
Epoch 3.96: Loss = 0.613693
Epoch 3.97: Loss = 0.725967
Epoch 3.98: Loss = 0.625504
Epoch 3.99: Loss = 0.63736
Epoch 3.100: Loss = 0.693298
Epoch 3.101: Loss = 0.64238
Epoch 3.102: Loss = 0.649368
Epoch 3.103: Loss = 0.651108
Epoch 3.104: Loss = 0.706421
Epoch 3.105: Loss = 0.673035
Epoch 3.106: Loss = 0.576462
Epoch 3.107: Loss = 0.597076
Epoch 3.108: Loss = 0.691772
Epoch 3.109: Loss = 0.694046
Epoch 3.110: Loss = 0.644974
Epoch 3.111: Loss = 0.692276
Epoch 3.112: Loss = 0.727951
Epoch 3.113: Loss = 0.616501
Epoch 3.114: Loss = 0.59726
Epoch 3.115: Loss = 0.620773
Epoch 3.116: Loss = 0.598129
Epoch 3.117: Loss = 0.724548
Epoch 3.118: Loss = 0.594101
Epoch 3.119: Loss = 0.654114
Epoch 3.120: Loss = 0.645645
TRAIN LOSS = 0.67746
TRAIN ACC = 77.2324 % (46341/60000)
Loss = 0.606613
Loss = 0.701889
Loss = 0.633301
Loss = 0.585114
Loss = 0.582489
Loss = 0.743073
Loss = 0.806808
Loss = 0.719772
Loss = 0.674362
Loss = 0.610855
Loss = 0.763229
Loss = 0.729263
Loss = 0.678284
Loss = 0.695129
Loss = 0.656631
Loss = 0.725433
Loss = 0.61644
Loss = 0.670776
Loss = 0.720871
Loss = 0.663376
TEST LOSS = 0.679185
TEST ACC = 463.409 % (7688/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.61055
Epoch 4.2: Loss = 0.616959
Epoch 4.3: Loss = 0.625595
Epoch 4.4: Loss = 0.617844
Epoch 4.5: Loss = 0.735413
Epoch 4.6: Loss = 0.628799
Epoch 4.7: Loss = 0.591843
Epoch 4.8: Loss = 0.634048
Epoch 4.9: Loss = 0.666595
Epoch 4.10: Loss = 0.714828
Epoch 4.11: Loss = 0.667709
Epoch 4.12: Loss = 0.670807
Epoch 4.13: Loss = 0.731216
Epoch 4.14: Loss = 0.582504
Epoch 4.15: Loss = 0.653305
Epoch 4.16: Loss = 0.703583
Epoch 4.17: Loss = 0.569473
Epoch 4.18: Loss = 0.673935
Epoch 4.19: Loss = 0.724579
Epoch 4.20: Loss = 0.703644
Epoch 4.21: Loss = 0.611481
Epoch 4.22: Loss = 0.623718
Epoch 4.23: Loss = 0.598648
Epoch 4.24: Loss = 0.677933
Epoch 4.25: Loss = 0.699142
Epoch 4.26: Loss = 0.596466
Epoch 4.27: Loss = 0.595764
Epoch 4.28: Loss = 0.511719
Epoch 4.29: Loss = 0.556351
Epoch 4.30: Loss = 0.713791
Epoch 4.31: Loss = 0.737076
Epoch 4.32: Loss = 0.615463
Epoch 4.33: Loss = 0.600479
Epoch 4.34: Loss = 0.585083
Epoch 4.35: Loss = 0.744415
Epoch 4.36: Loss = 0.613358
Epoch 4.37: Loss = 0.683594
Epoch 4.38: Loss = 0.776703
Epoch 4.39: Loss = 0.71669
Epoch 4.40: Loss = 0.782867
Epoch 4.41: Loss = 0.522675
Epoch 4.42: Loss = 0.691803
Epoch 4.43: Loss = 0.64296
Epoch 4.44: Loss = 0.590302
Epoch 4.45: Loss = 0.684006
Epoch 4.46: Loss = 0.62944
Epoch 4.47: Loss = 0.653839
Epoch 4.48: Loss = 0.676392
Epoch 4.49: Loss = 0.652084
Epoch 4.50: Loss = 0.584137
Epoch 4.51: Loss = 0.652664
Epoch 4.52: Loss = 0.621704
Epoch 4.53: Loss = 0.63588
Epoch 4.54: Loss = 0.649902
Epoch 4.55: Loss = 0.719711
Epoch 4.56: Loss = 0.681458
Epoch 4.57: Loss = 0.787231
Epoch 4.58: Loss = 0.669586
Epoch 4.59: Loss = 0.650742
Epoch 4.60: Loss = 0.693558
Epoch 4.61: Loss = 0.645294
Epoch 4.62: Loss = 0.643188
Epoch 4.63: Loss = 0.565842
Epoch 4.64: Loss = 0.679199
Epoch 4.65: Loss = 0.615784
Epoch 4.66: Loss = 0.645096
Epoch 4.67: Loss = 0.652359
Epoch 4.68: Loss = 0.759857
Epoch 4.69: Loss = 0.646454
Epoch 4.70: Loss = 0.618118
Epoch 4.71: Loss = 0.584808
Epoch 4.72: Loss = 0.651794
Epoch 4.73: Loss = 0.673203
Epoch 4.74: Loss = 0.659149
Epoch 4.75: Loss = 0.639099
Epoch 4.76: Loss = 0.763535
Epoch 4.77: Loss = 0.683899
Epoch 4.78: Loss = 0.596054
Epoch 4.79: Loss = 0.659744
Epoch 4.80: Loss = 0.618759
Epoch 4.81: Loss = 0.651535
Epoch 4.82: Loss = 0.552963
Epoch 4.83: Loss = 0.646866
Epoch 4.84: Loss = 0.644653
Epoch 4.85: Loss = 0.676559
Epoch 4.86: Loss = 0.634598
Epoch 4.87: Loss = 0.72139
Epoch 4.88: Loss = 0.525772
Epoch 4.89: Loss = 0.712921
Epoch 4.90: Loss = 0.619553
Epoch 4.91: Loss = 0.597076
Epoch 4.92: Loss = 0.648483
Epoch 4.93: Loss = 0.675049
Epoch 4.94: Loss = 0.713745
Epoch 4.95: Loss = 0.605042
Epoch 4.96: Loss = 0.716583
Epoch 4.97: Loss = 0.606308
Epoch 4.98: Loss = 0.635605
Epoch 4.99: Loss = 0.660065
Epoch 4.100: Loss = 0.750229
Epoch 4.101: Loss = 0.624725
Epoch 4.102: Loss = 0.70639
Epoch 4.103: Loss = 0.70668
Epoch 4.104: Loss = 0.582397
Epoch 4.105: Loss = 0.583313
Epoch 4.106: Loss = 0.677658
Epoch 4.107: Loss = 0.74794
Epoch 4.108: Loss = 0.659607
Epoch 4.109: Loss = 0.577957
Epoch 4.110: Loss = 0.640472
Epoch 4.111: Loss = 0.660965
Epoch 4.112: Loss = 0.520782
Epoch 4.113: Loss = 0.727997
Epoch 4.114: Loss = 0.749161
Epoch 4.115: Loss = 0.714706
Epoch 4.116: Loss = 0.703171
Epoch 4.117: Loss = 0.538544
Epoch 4.118: Loss = 0.625763
Epoch 4.119: Loss = 0.682953
Epoch 4.120: Loss = 0.586395
TRAIN LOSS = 0.652237
TRAIN ACC = 78.7247 % (47237/60000)
Loss = 0.580032
Loss = 0.698547
Loss = 0.621964
Loss = 0.569321
Loss = 0.591446
Loss = 0.742554
Loss = 0.799591
Loss = 0.71843
Loss = 0.670578
Loss = 0.601257
Loss = 0.760986
Loss = 0.734634
Loss = 0.675919
Loss = 0.684753
Loss = 0.667084
Loss = 0.714478
Loss = 0.619095
Loss = 0.65834
Loss = 0.727921
Loss = 0.654526
TEST LOSS = 0.674573
TEST ACC = 472.369 % (7821/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.780334
Epoch 5.2: Loss = 0.583893
Epoch 5.3: Loss = 0.667969
Epoch 5.4: Loss = 0.599716
Epoch 5.5: Loss = 0.57254
Epoch 5.6: Loss = 0.778885
Epoch 5.7: Loss = 0.597321
Epoch 5.8: Loss = 0.640823
Epoch 5.9: Loss = 0.609848
Epoch 5.10: Loss = 0.732452
Epoch 5.11: Loss = 0.666138
Epoch 5.12: Loss = 0.629593
Epoch 5.13: Loss = 0.671524
Epoch 5.14: Loss = 0.717758
Epoch 5.15: Loss = 0.668716
Epoch 5.16: Loss = 0.674179
Epoch 5.17: Loss = 0.660233
Epoch 5.18: Loss = 0.614975
Epoch 5.19: Loss = 0.801071
Epoch 5.20: Loss = 0.683762
Epoch 5.21: Loss = 0.623886
Epoch 5.22: Loss = 0.687485
Epoch 5.23: Loss = 0.549377
Epoch 5.24: Loss = 0.743683
Epoch 5.25: Loss = 0.556274
Epoch 5.26: Loss = 0.649185
Epoch 5.27: Loss = 0.576035
Epoch 5.28: Loss = 0.661179
Epoch 5.29: Loss = 0.609222
Epoch 5.30: Loss = 0.716675
Epoch 5.31: Loss = 0.618393
Epoch 5.32: Loss = 0.629715
Epoch 5.33: Loss = 0.583344
Epoch 5.34: Loss = 0.625153
Epoch 5.35: Loss = 0.703659
Epoch 5.36: Loss = 0.694366
Epoch 5.37: Loss = 0.67247
Epoch 5.38: Loss = 0.58139
Epoch 5.39: Loss = 0.733658
Epoch 5.40: Loss = 0.660614
Epoch 5.41: Loss = 0.631104
Epoch 5.42: Loss = 0.6884
Epoch 5.43: Loss = 0.592575
Epoch 5.44: Loss = 0.719574
Epoch 5.45: Loss = 0.681107
Epoch 5.46: Loss = 0.655411
Epoch 5.47: Loss = 0.587036
Epoch 5.48: Loss = 0.614822
Epoch 5.49: Loss = 0.634323
Epoch 5.50: Loss = 0.66391
Epoch 5.51: Loss = 0.661148
Epoch 5.52: Loss = 0.632858
Epoch 5.53: Loss = 0.596725
Epoch 5.54: Loss = 0.597839
Epoch 5.55: Loss = 0.602814
Epoch 5.56: Loss = 0.598343
Epoch 5.57: Loss = 0.573502
Epoch 5.58: Loss = 0.696701
Epoch 5.59: Loss = 0.614197
Epoch 5.60: Loss = 0.599777
Epoch 5.61: Loss = 0.70108
Epoch 5.62: Loss = 0.67334
Epoch 5.63: Loss = 0.563339
Epoch 5.64: Loss = 0.667969
Epoch 5.65: Loss = 0.639435
Epoch 5.66: Loss = 0.655746
Epoch 5.67: Loss = 0.651611
Epoch 5.68: Loss = 0.633835
Epoch 5.69: Loss = 0.571518
Epoch 5.70: Loss = 0.594299
Epoch 5.71: Loss = 0.676544
Epoch 5.72: Loss = 0.710403
Epoch 5.73: Loss = 0.647324
Epoch 5.74: Loss = 0.653427
Epoch 5.75: Loss = 0.609589
Epoch 5.76: Loss = 0.693008
Epoch 5.77: Loss = 0.703369
Epoch 5.78: Loss = 0.590744
Epoch 5.79: Loss = 0.620224
Epoch 5.80: Loss = 0.706436
Epoch 5.81: Loss = 0.68277
Epoch 5.82: Loss = 0.613739
Epoch 5.83: Loss = 0.62677
Epoch 5.84: Loss = 0.62117
Epoch 5.85: Loss = 0.626053
Epoch 5.86: Loss = 0.447952
Epoch 5.87: Loss = 0.664169
Epoch 5.88: Loss = 0.612793
Epoch 5.89: Loss = 0.631287
Epoch 5.90: Loss = 0.63533
Epoch 5.91: Loss = 0.730331
Epoch 5.92: Loss = 0.591629
Epoch 5.93: Loss = 0.571304
Epoch 5.94: Loss = 0.654297
Epoch 5.95: Loss = 0.604904
Epoch 5.96: Loss = 0.660126
Epoch 5.97: Loss = 0.619461
Epoch 5.98: Loss = 0.566498
Epoch 5.99: Loss = 0.65152
Epoch 5.100: Loss = 0.733307
Epoch 5.101: Loss = 0.563095
Epoch 5.102: Loss = 0.559021
Epoch 5.103: Loss = 0.590195
Epoch 5.104: Loss = 0.563293
Epoch 5.105: Loss = 0.605255
Epoch 5.106: Loss = 0.657089
Epoch 5.107: Loss = 0.583267
Epoch 5.108: Loss = 0.632736
Epoch 5.109: Loss = 0.618668
Epoch 5.110: Loss = 0.615845
Epoch 5.111: Loss = 0.548355
Epoch 5.112: Loss = 0.624176
Epoch 5.113: Loss = 0.54895
Epoch 5.114: Loss = 0.625504
Epoch 5.115: Loss = 0.608063
Epoch 5.116: Loss = 0.750534
Epoch 5.117: Loss = 0.461868
Epoch 5.118: Loss = 0.613068
Epoch 5.119: Loss = 0.57547
Epoch 5.120: Loss = 0.646439
TRAIN LOSS = 0.636429
TRAIN ACC = 79.7333 % (47842/60000)
Loss = 0.558105
Loss = 0.691376
Loss = 0.611725
Loss = 0.557114
Loss = 0.591492
Loss = 0.712143
Loss = 0.795853
Loss = 0.706985
Loss = 0.663528
Loss = 0.580734
Loss = 0.775452
Loss = 0.726913
Loss = 0.685654
Loss = 0.664566
Loss = 0.657806
Loss = 0.697083
Loss = 0.608566
Loss = 0.640015
Loss = 0.717621
Loss = 0.636627
TEST LOSS = 0.663968
TEST ACC = 478.419 % (7880/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.546463
Epoch 6.2: Loss = 0.533707
Epoch 6.3: Loss = 0.698761
Epoch 6.4: Loss = 0.686279
Epoch 6.5: Loss = 0.643677
Epoch 6.6: Loss = 0.676575
Epoch 6.7: Loss = 0.561905
Epoch 6.8: Loss = 0.673599
Epoch 6.9: Loss = 0.523026
Epoch 6.10: Loss = 0.61705
Epoch 6.11: Loss = 0.577499
Epoch 6.12: Loss = 0.678574
Epoch 6.13: Loss = 0.639282
Epoch 6.14: Loss = 0.626694
Epoch 6.15: Loss = 0.648804
Epoch 6.16: Loss = 0.590317
Epoch 6.17: Loss = 0.648254
Epoch 6.18: Loss = 0.620483
Epoch 6.19: Loss = 0.721191
Epoch 6.20: Loss = 0.568741
Epoch 6.21: Loss = 0.655457
Epoch 6.22: Loss = 0.58374
Epoch 6.23: Loss = 0.597717
Epoch 6.24: Loss = 0.721786
Epoch 6.25: Loss = 0.627228
Epoch 6.26: Loss = 0.597794
Epoch 6.27: Loss = 0.553574
Epoch 6.28: Loss = 0.597733
Epoch 6.29: Loss = 0.617706
Epoch 6.30: Loss = 0.554367
Epoch 6.31: Loss = 0.518845
Epoch 6.32: Loss = 0.648514
Epoch 6.33: Loss = 0.68927
Epoch 6.34: Loss = 0.573914
Epoch 6.35: Loss = 0.655502
Epoch 6.36: Loss = 0.634262
Epoch 6.37: Loss = 0.665604
Epoch 6.38: Loss = 0.535629
Epoch 6.39: Loss = 0.588272
Epoch 6.40: Loss = 0.593369
Epoch 6.41: Loss = 0.58432
Epoch 6.42: Loss = 0.500015
Epoch 6.43: Loss = 0.567169
Epoch 6.44: Loss = 0.632935
Epoch 6.45: Loss = 0.639984
Epoch 6.46: Loss = 0.630081
Epoch 6.47: Loss = 0.582291
Epoch 6.48: Loss = 0.744934
Epoch 6.49: Loss = 0.552689
Epoch 6.50: Loss = 0.771942
Epoch 6.51: Loss = 0.496872
Epoch 6.52: Loss = 0.745987
Epoch 6.53: Loss = 0.569443
Epoch 6.54: Loss = 0.692459
Epoch 6.55: Loss = 0.646317
Epoch 6.56: Loss = 0.562637
Epoch 6.57: Loss = 0.604065
Epoch 6.58: Loss = 0.600876
Epoch 6.59: Loss = 0.521881
Epoch 6.60: Loss = 0.626373
Epoch 6.61: Loss = 0.531128
Epoch 6.62: Loss = 0.664581
Epoch 6.63: Loss = 0.653229
Epoch 6.64: Loss = 0.686996
Epoch 6.65: Loss = 0.589813
Epoch 6.66: Loss = 0.691162
Epoch 6.67: Loss = 0.610199
Epoch 6.68: Loss = 0.518219
Epoch 6.69: Loss = 0.649429
Epoch 6.70: Loss = 0.685272
Epoch 6.71: Loss = 0.583755
Epoch 6.72: Loss = 0.62912
Epoch 6.73: Loss = 0.536804
Epoch 6.74: Loss = 0.585617
Epoch 6.75: Loss = 0.586685
Epoch 6.76: Loss = 0.661728
Epoch 6.77: Loss = 0.718369
Epoch 6.78: Loss = 0.604523
Epoch 6.79: Loss = 0.665115
Epoch 6.80: Loss = 0.556503
Epoch 6.81: Loss = 0.633423
Epoch 6.82: Loss = 0.667374
Epoch 6.83: Loss = 0.640762
Epoch 6.84: Loss = 0.675888
Epoch 6.85: Loss = 0.633759
Epoch 6.86: Loss = 0.635651
Epoch 6.87: Loss = 0.716553
Epoch 6.88: Loss = 0.721756
Epoch 6.89: Loss = 0.616699
Epoch 6.90: Loss = 0.600922
Epoch 6.91: Loss = 0.667892
Epoch 6.92: Loss = 0.680573
Epoch 6.93: Loss = 0.634338
Epoch 6.94: Loss = 0.708099
Epoch 6.95: Loss = 0.61879
Epoch 6.96: Loss = 0.613129
Epoch 6.97: Loss = 0.66011
Epoch 6.98: Loss = 0.619705
Epoch 6.99: Loss = 0.65184
Epoch 6.100: Loss = 0.522018
Epoch 6.101: Loss = 0.640732
Epoch 6.102: Loss = 0.590881
Epoch 6.103: Loss = 0.596207
Epoch 6.104: Loss = 0.656082
Epoch 6.105: Loss = 0.62532
Epoch 6.106: Loss = 0.62056
Epoch 6.107: Loss = 0.668503
Epoch 6.108: Loss = 0.7491
Epoch 6.109: Loss = 0.546509
Epoch 6.110: Loss = 0.636673
Epoch 6.111: Loss = 0.670731
Epoch 6.112: Loss = 0.650253
Epoch 6.113: Loss = 0.604126
Epoch 6.114: Loss = 0.573196
Epoch 6.115: Loss = 0.558472
Epoch 6.116: Loss = 0.671295
Epoch 6.117: Loss = 0.643402
Epoch 6.118: Loss = 0.702087
Epoch 6.119: Loss = 0.580109
Epoch 6.120: Loss = 0.583084
TRAIN LOSS = 0.62384
TRAIN ACC = 80.304 % (48185/60000)
Loss = 0.538361
Loss = 0.673904
Loss = 0.586884
Loss = 0.543198
Loss = 0.578415
Loss = 0.681656
Loss = 0.794876
Loss = 0.698212
Loss = 0.656967
Loss = 0.565674
Loss = 0.761551
Loss = 0.702484
Loss = 0.664932
Loss = 0.649689
Loss = 0.613235
Loss = 0.670776
Loss = 0.605942
Loss = 0.617386
Loss = 0.674896
Loss = 0.620026
TEST LOSS = 0.644953
TEST ACC = 481.85 % (7954/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.559692
Epoch 7.2: Loss = 0.531937
Epoch 7.3: Loss = 0.655441
Epoch 7.4: Loss = 0.543823
Epoch 7.5: Loss = 0.580704
Epoch 7.6: Loss = 0.621231
Epoch 7.7: Loss = 0.671371
Epoch 7.8: Loss = 0.691284
Epoch 7.9: Loss = 0.538177
Epoch 7.10: Loss = 0.622833
Epoch 7.11: Loss = 0.671341
Epoch 7.12: Loss = 0.557114
Epoch 7.13: Loss = 0.597397
Epoch 7.14: Loss = 0.706543
Epoch 7.15: Loss = 0.622803
Epoch 7.16: Loss = 0.579865
Epoch 7.17: Loss = 0.667633
Epoch 7.18: Loss = 0.487656
Epoch 7.19: Loss = 0.6147
Epoch 7.20: Loss = 0.623077
Epoch 7.21: Loss = 0.612076
Epoch 7.22: Loss = 0.475906
Epoch 7.23: Loss = 0.714554
Epoch 7.24: Loss = 0.612442
Epoch 7.25: Loss = 0.652557
Epoch 7.26: Loss = 0.695389
Epoch 7.27: Loss = 0.567078
Epoch 7.28: Loss = 0.611343
Epoch 7.29: Loss = 0.567535
Epoch 7.30: Loss = 0.638992
Epoch 7.31: Loss = 0.706284
Epoch 7.32: Loss = 0.639877
Epoch 7.33: Loss = 0.792419
Epoch 7.34: Loss = 0.656372
Epoch 7.35: Loss = 0.581406
Epoch 7.36: Loss = 0.746658
Epoch 7.37: Loss = 0.495117
Epoch 7.38: Loss = 0.508148
Epoch 7.39: Loss = 0.599457
Epoch 7.40: Loss = 0.643143
Epoch 7.41: Loss = 0.618423
Epoch 7.42: Loss = 0.595779
Epoch 7.43: Loss = 0.462326
Epoch 7.44: Loss = 0.617142
Epoch 7.45: Loss = 0.600601
Epoch 7.46: Loss = 0.772156
Epoch 7.47: Loss = 0.629898
Epoch 7.48: Loss = 0.613144
Epoch 7.49: Loss = 0.597961
Epoch 7.50: Loss = 0.697571
Epoch 7.51: Loss = 0.504272
Epoch 7.52: Loss = 0.619797
Epoch 7.53: Loss = 0.594635
Epoch 7.54: Loss = 0.629715
Epoch 7.55: Loss = 0.5224
Epoch 7.56: Loss = 0.492706
Epoch 7.57: Loss = 0.667328
Epoch 7.58: Loss = 0.673401
Epoch 7.59: Loss = 0.674301
Epoch 7.60: Loss = 0.555283
Epoch 7.61: Loss = 0.671127
Epoch 7.62: Loss = 0.570206
Epoch 7.63: Loss = 0.586792
Epoch 7.64: Loss = 0.625275
Epoch 7.65: Loss = 0.597839
Epoch 7.66: Loss = 0.610092
Epoch 7.67: Loss = 0.534836
Epoch 7.68: Loss = 0.670807
Epoch 7.69: Loss = 0.63884
Epoch 7.70: Loss = 0.584473
Epoch 7.71: Loss = 0.694427
Epoch 7.72: Loss = 0.55394
Epoch 7.73: Loss = 0.771622
Epoch 7.74: Loss = 0.660706
Epoch 7.75: Loss = 0.603119
Epoch 7.76: Loss = 0.578156
Epoch 7.77: Loss = 0.75531
Epoch 7.78: Loss = 0.599121
Epoch 7.79: Loss = 0.607101
Epoch 7.80: Loss = 0.588791
Epoch 7.81: Loss = 0.609665
Epoch 7.82: Loss = 0.746536
Epoch 7.83: Loss = 0.580124
Epoch 7.84: Loss = 0.716461
Epoch 7.85: Loss = 0.618027
Epoch 7.86: Loss = 0.618942
Epoch 7.87: Loss = 0.651398
Epoch 7.88: Loss = 0.564957
Epoch 7.89: Loss = 0.507309
Epoch 7.90: Loss = 0.612534
Epoch 7.91: Loss = 0.741638
Epoch 7.92: Loss = 0.577652
Epoch 7.93: Loss = 0.601578
Epoch 7.94: Loss = 0.619339
Epoch 7.95: Loss = 0.59996
Epoch 7.96: Loss = 0.552383
Epoch 7.97: Loss = 0.564667
Epoch 7.98: Loss = 0.685837
Epoch 7.99: Loss = 0.553513
Epoch 7.100: Loss = 0.589691
Epoch 7.101: Loss = 0.659378
Epoch 7.102: Loss = 0.639069
Epoch 7.103: Loss = 0.657639
Epoch 7.104: Loss = 0.625183
Epoch 7.105: Loss = 0.645111
Epoch 7.106: Loss = 0.66217
Epoch 7.107: Loss = 0.767746
Epoch 7.108: Loss = 0.583618
Epoch 7.109: Loss = 0.567261
Epoch 7.110: Loss = 0.563904
Epoch 7.111: Loss = 0.701157
Epoch 7.112: Loss = 0.688965
Epoch 7.113: Loss = 0.6604
Epoch 7.114: Loss = 0.646286
Epoch 7.115: Loss = 0.468338
Epoch 7.116: Loss = 0.573105
Epoch 7.117: Loss = 0.6996
Epoch 7.118: Loss = 0.68132
Epoch 7.119: Loss = 0.645294
Epoch 7.120: Loss = 0.632034
TRAIN LOSS = 0.619873
TRAIN ACC = 80.8151 % (48491/60000)
Loss = 0.545654
Loss = 0.66803
Loss = 0.588608
Loss = 0.539963
Loss = 0.582291
Loss = 0.693588
Loss = 0.773499
Loss = 0.689011
Loss = 0.662598
Loss = 0.567444
Loss = 0.767258
Loss = 0.698441
Loss = 0.662186
Loss = 0.624756
Loss = 0.620911
Loss = 0.663498
Loss = 0.594666
Loss = 0.619537
Loss = 0.67453
Loss = 0.613983
TEST LOSS = 0.642522
TEST ACC = 484.909 % (7999/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.563812
Epoch 8.2: Loss = 0.584503
Epoch 8.3: Loss = 0.641098
Epoch 8.4: Loss = 0.514847
Epoch 8.5: Loss = 0.754883
Epoch 8.6: Loss = 0.656647
Epoch 8.7: Loss = 0.764145
Epoch 8.8: Loss = 0.719147
Epoch 8.9: Loss = 0.64946
Epoch 8.10: Loss = 0.725266
Epoch 8.11: Loss = 0.596085
Epoch 8.12: Loss = 0.555527
Epoch 8.13: Loss = 0.607819
Epoch 8.14: Loss = 0.551315
Epoch 8.15: Loss = 0.554367
Epoch 8.16: Loss = 0.571136
Epoch 8.17: Loss = 0.546936
Epoch 8.18: Loss = 0.4953
Epoch 8.19: Loss = 0.623657
Epoch 8.20: Loss = 0.542404
Epoch 8.21: Loss = 0.603058
Epoch 8.22: Loss = 0.624832
Epoch 8.23: Loss = 0.56636
Epoch 8.24: Loss = 0.593567
Epoch 8.25: Loss = 0.613525
Epoch 8.26: Loss = 0.574081
Epoch 8.27: Loss = 0.614304
Epoch 8.28: Loss = 0.57074
Epoch 8.29: Loss = 0.615295
Epoch 8.30: Loss = 0.603714
Epoch 8.31: Loss = 0.543411
Epoch 8.32: Loss = 0.601791
Epoch 8.33: Loss = 0.544006
Epoch 8.34: Loss = 0.646179
Epoch 8.35: Loss = 0.588242
Epoch 8.36: Loss = 0.655151
Epoch 8.37: Loss = 0.610855
Epoch 8.38: Loss = 0.569092
Epoch 8.39: Loss = 0.629074
Epoch 8.40: Loss = 0.642822
Epoch 8.41: Loss = 0.55986
Epoch 8.42: Loss = 0.655762
Epoch 8.43: Loss = 0.688156
Epoch 8.44: Loss = 0.773865
Epoch 8.45: Loss = 0.660431
Epoch 8.46: Loss = 0.713684
Epoch 8.47: Loss = 0.706207
Epoch 8.48: Loss = 0.606583
Epoch 8.49: Loss = 0.634476
Epoch 8.50: Loss = 0.600708
Epoch 8.51: Loss = 0.450333
Epoch 8.52: Loss = 0.644424
Epoch 8.53: Loss = 0.561264
Epoch 8.54: Loss = 0.694489
Epoch 8.55: Loss = 0.601089
Epoch 8.56: Loss = 0.668411
Epoch 8.57: Loss = 0.59761
Epoch 8.58: Loss = 0.727097
Epoch 8.59: Loss = 0.663055
Epoch 8.60: Loss = 0.546677
Epoch 8.61: Loss = 0.57962
Epoch 8.62: Loss = 0.640671
Epoch 8.63: Loss = 0.554337
Epoch 8.64: Loss = 0.630478
Epoch 8.65: Loss = 0.526733
Epoch 8.66: Loss = 0.590347
Epoch 8.67: Loss = 0.52597
Epoch 8.68: Loss = 0.690002
Epoch 8.69: Loss = 0.604767
Epoch 8.70: Loss = 0.567657
Epoch 8.71: Loss = 0.602356
Epoch 8.72: Loss = 0.653183
Epoch 8.73: Loss = 0.591553
Epoch 8.74: Loss = 0.604721
Epoch 8.75: Loss = 0.659592
Epoch 8.76: Loss = 0.649033
Epoch 8.77: Loss = 0.606201
Epoch 8.78: Loss = 0.546341
Epoch 8.79: Loss = 0.635971
Epoch 8.80: Loss = 0.628769
Epoch 8.81: Loss = 0.660049
Epoch 8.82: Loss = 0.633118
Epoch 8.83: Loss = 0.618011
Epoch 8.84: Loss = 0.705917
Epoch 8.85: Loss = 0.508728
Epoch 8.86: Loss = 0.602417
Epoch 8.87: Loss = 0.672195
Epoch 8.88: Loss = 0.706543
Epoch 8.89: Loss = 0.645477
Epoch 8.90: Loss = 0.521744
Epoch 8.91: Loss = 0.739014
Epoch 8.92: Loss = 0.647919
Epoch 8.93: Loss = 0.501617
Epoch 8.94: Loss = 0.579788
Epoch 8.95: Loss = 0.670074
Epoch 8.96: Loss = 0.687485
Epoch 8.97: Loss = 0.676407
Epoch 8.98: Loss = 0.558914
Epoch 8.99: Loss = 0.638412
Epoch 8.100: Loss = 0.643219
Epoch 8.101: Loss = 0.641876
Epoch 8.102: Loss = 0.638397
Epoch 8.103: Loss = 0.743073
Epoch 8.104: Loss = 0.533478
Epoch 8.105: Loss = 0.601532
Epoch 8.106: Loss = 0.545486
Epoch 8.107: Loss = 0.565186
Epoch 8.108: Loss = 0.642319
Epoch 8.109: Loss = 0.667953
Epoch 8.110: Loss = 0.62999
Epoch 8.111: Loss = 0.509766
Epoch 8.112: Loss = 0.658325
Epoch 8.113: Loss = 0.464233
Epoch 8.114: Loss = 0.652084
Epoch 8.115: Loss = 0.574326
Epoch 8.116: Loss = 0.726028
Epoch 8.117: Loss = 0.606461
Epoch 8.118: Loss = 0.667206
Epoch 8.119: Loss = 0.527969
Epoch 8.120: Loss = 0.70192
TRAIN LOSS = 0.616562
TRAIN ACC = 81.1081 % (48667/60000)
Loss = 0.551178
Loss = 0.676163
Loss = 0.588837
Loss = 0.533249
Loss = 0.583389
Loss = 0.683395
Loss = 0.778931
Loss = 0.681931
Loss = 0.65033
Loss = 0.568542
Loss = 0.778961
Loss = 0.70961
Loss = 0.67308
Loss = 0.637985
Loss = 0.6138
Loss = 0.653748
Loss = 0.620163
Loss = 0.606918
Loss = 0.667313
Loss = 0.622345
TEST LOSS = 0.643993
TEST ACC = 486.67 % (7988/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.754166
Epoch 9.2: Loss = 0.630005
Epoch 9.3: Loss = 0.674118
Epoch 9.4: Loss = 0.651825
Epoch 9.5: Loss = 0.629379
Epoch 9.6: Loss = 0.752853
Epoch 9.7: Loss = 0.60997
Epoch 9.8: Loss = 0.530167
Epoch 9.9: Loss = 0.530197
Epoch 9.10: Loss = 0.654449
Epoch 9.11: Loss = 0.632355
Epoch 9.12: Loss = 0.550903
Epoch 9.13: Loss = 0.670166
Epoch 9.14: Loss = 0.593811
Epoch 9.15: Loss = 0.528336
Epoch 9.16: Loss = 0.567215
Epoch 9.17: Loss = 0.537888
Epoch 9.18: Loss = 0.635818
Epoch 9.19: Loss = 0.51709
Epoch 9.20: Loss = 0.642197
Epoch 9.21: Loss = 0.69574
Epoch 9.22: Loss = 0.573792
Epoch 9.23: Loss = 0.567047
Epoch 9.24: Loss = 0.542267
Epoch 9.25: Loss = 0.558502
Epoch 9.26: Loss = 0.539474
Epoch 9.27: Loss = 0.625397
Epoch 9.28: Loss = 0.557297
Epoch 9.29: Loss = 0.590149
Epoch 9.30: Loss = 0.627548
Epoch 9.31: Loss = 0.563461
Epoch 9.32: Loss = 0.584061
Epoch 9.33: Loss = 0.489334
Epoch 9.34: Loss = 0.75029
Epoch 9.35: Loss = 0.553528
Epoch 9.36: Loss = 0.579239
Epoch 9.37: Loss = 0.562775
Epoch 9.38: Loss = 0.690521
Epoch 9.39: Loss = 0.634308
Epoch 9.40: Loss = 0.572113
Epoch 9.41: Loss = 0.621994
Epoch 9.42: Loss = 0.566711
Epoch 9.43: Loss = 0.59166
Epoch 9.44: Loss = 0.555634
Epoch 9.45: Loss = 0.591812
Epoch 9.46: Loss = 0.662918
Epoch 9.47: Loss = 0.666565
Epoch 9.48: Loss = 0.578629
Epoch 9.49: Loss = 0.57782
Epoch 9.50: Loss = 0.627106
Epoch 9.51: Loss = 0.59642
Epoch 9.52: Loss = 0.639435
Epoch 9.53: Loss = 0.663162
Epoch 9.54: Loss = 0.693176
Epoch 9.55: Loss = 0.589661
Epoch 9.56: Loss = 0.578751
Epoch 9.57: Loss = 0.731567
Epoch 9.58: Loss = 0.706436
Epoch 9.59: Loss = 0.505386
Epoch 9.60: Loss = 0.581726
Epoch 9.61: Loss = 0.551895
Epoch 9.62: Loss = 0.523895
Epoch 9.63: Loss = 0.678223
Epoch 9.64: Loss = 0.615082
Epoch 9.65: Loss = 0.661011
Epoch 9.66: Loss = 0.644424
Epoch 9.67: Loss = 0.529755
Epoch 9.68: Loss = 0.65097
Epoch 9.69: Loss = 0.588806
Epoch 9.70: Loss = 0.555099
Epoch 9.71: Loss = 0.578796
Epoch 9.72: Loss = 0.648651
Epoch 9.73: Loss = 0.664307
Epoch 9.74: Loss = 0.736069
Epoch 9.75: Loss = 0.502075
Epoch 9.76: Loss = 0.427887
Epoch 9.77: Loss = 0.657959
Epoch 9.78: Loss = 0.56424
Epoch 9.79: Loss = 0.70694
Epoch 9.80: Loss = 0.579468
Epoch 9.81: Loss = 0.698853
Epoch 9.82: Loss = 0.674561
Epoch 9.83: Loss = 0.685394
Epoch 9.84: Loss = 0.622314
Epoch 9.85: Loss = 0.614243
Epoch 9.86: Loss = 0.562347
Epoch 9.87: Loss = 0.604538
Epoch 9.88: Loss = 0.611145
Epoch 9.89: Loss = 0.596741
Epoch 9.90: Loss = 0.730103
Epoch 9.91: Loss = 0.655533
Epoch 9.92: Loss = 0.60759
Epoch 9.93: Loss = 0.621002
Epoch 9.94: Loss = 0.567291
Epoch 9.95: Loss = 0.640488
Epoch 9.96: Loss = 0.600662
Epoch 9.97: Loss = 0.561218
Epoch 9.98: Loss = 0.588058
Epoch 9.99: Loss = 0.573181
Epoch 9.100: Loss = 0.685455
Epoch 9.101: Loss = 0.684143
Epoch 9.102: Loss = 0.600922
Epoch 9.103: Loss = 0.633224
Epoch 9.104: Loss = 0.549713
Epoch 9.105: Loss = 0.651398
Epoch 9.106: Loss = 0.666321
Epoch 9.107: Loss = 0.58194
Epoch 9.108: Loss = 0.542404
Epoch 9.109: Loss = 0.538559
Epoch 9.110: Loss = 0.547989
Epoch 9.111: Loss = 0.619095
Epoch 9.112: Loss = 0.65036
Epoch 9.113: Loss = 0.697113
Epoch 9.114: Loss = 0.629913
Epoch 9.115: Loss = 0.677338
Epoch 9.116: Loss = 0.587753
Epoch 9.117: Loss = 0.524918
Epoch 9.118: Loss = 0.529312
Epoch 9.119: Loss = 0.675552
Epoch 9.120: Loss = 0.729294
TRAIN LOSS = 0.611343
TRAIN ACC = 81.3278 % (48799/60000)
Loss = 0.550873
Loss = 0.672287
Loss = 0.585358
Loss = 0.543472
Loss = 0.589523
Loss = 0.669876
Loss = 0.783722
Loss = 0.679398
Loss = 0.657272
Loss = 0.56134
Loss = 0.781967
Loss = 0.718353
Loss = 0.680481
Loss = 0.647064
Loss = 0.623932
Loss = 0.64653
Loss = 0.603745
Loss = 0.613342
Loss = 0.666199
Loss = 0.614456
TEST LOSS = 0.644459
TEST ACC = 487.99 % (8034/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.607315
Epoch 10.2: Loss = 0.522034
Epoch 10.3: Loss = 0.545517
Epoch 10.4: Loss = 0.717209
Epoch 10.5: Loss = 0.660492
Epoch 10.6: Loss = 0.631668
Epoch 10.7: Loss = 0.666733
Epoch 10.8: Loss = 0.668564
Epoch 10.9: Loss = 0.603989
Epoch 10.10: Loss = 0.575974
Epoch 10.11: Loss = 0.681625
Epoch 10.12: Loss = 0.638885
Epoch 10.13: Loss = 0.646545
Epoch 10.14: Loss = 0.577271
Epoch 10.15: Loss = 0.663071
Epoch 10.16: Loss = 0.615311
Epoch 10.17: Loss = 0.526443
Epoch 10.18: Loss = 0.660721
Epoch 10.19: Loss = 0.572052
Epoch 10.20: Loss = 0.6539
Epoch 10.21: Loss = 0.636337
Epoch 10.22: Loss = 0.630859
Epoch 10.23: Loss = 0.5466
Epoch 10.24: Loss = 0.598511
Epoch 10.25: Loss = 0.547974
Epoch 10.26: Loss = 0.577454
Epoch 10.27: Loss = 0.576233
Epoch 10.28: Loss = 0.716003
Epoch 10.29: Loss = 0.675491
Epoch 10.30: Loss = 0.728622
Epoch 10.31: Loss = 0.649734
Epoch 10.32: Loss = 0.613998
Epoch 10.33: Loss = 0.71727
Epoch 10.34: Loss = 0.519348
Epoch 10.35: Loss = 0.618347
Epoch 10.36: Loss = 0.581177
Epoch 10.37: Loss = 0.561768
Epoch 10.38: Loss = 0.660355
Epoch 10.39: Loss = 0.609222
Epoch 10.40: Loss = 0.603729
Epoch 10.41: Loss = 0.656219
Epoch 10.42: Loss = 0.614899
Epoch 10.43: Loss = 0.582458
Epoch 10.44: Loss = 0.687225
Epoch 10.45: Loss = 0.499176
Epoch 10.46: Loss = 0.464981
Epoch 10.47: Loss = 0.613068
Epoch 10.48: Loss = 0.63797
Epoch 10.49: Loss = 0.480316
Epoch 10.50: Loss = 0.606781
Epoch 10.51: Loss = 0.588165
Epoch 10.52: Loss = 0.600571
Epoch 10.53: Loss = 0.557953
Epoch 10.54: Loss = 0.552292
Epoch 10.55: Loss = 0.543213
Epoch 10.56: Loss = 0.650787
Epoch 10.57: Loss = 0.629059
Epoch 10.58: Loss = 0.638947
Epoch 10.59: Loss = 0.693924
Epoch 10.60: Loss = 0.598572
Epoch 10.61: Loss = 0.547653
Epoch 10.62: Loss = 0.571686
Epoch 10.63: Loss = 0.618408
Epoch 10.64: Loss = 0.64209
Epoch 10.65: Loss = 0.630753
Epoch 10.66: Loss = 0.64415
Epoch 10.67: Loss = 0.616425
Epoch 10.68: Loss = 0.586304
Epoch 10.69: Loss = 0.658142
Epoch 10.70: Loss = 0.610672
Epoch 10.71: Loss = 0.591537
Epoch 10.72: Loss = 0.570786
Epoch 10.73: Loss = 0.595978
Epoch 10.74: Loss = 0.623337
Epoch 10.75: Loss = 0.720245
Epoch 10.76: Loss = 0.626205
Epoch 10.77: Loss = 0.586349
Epoch 10.78: Loss = 0.60611
Epoch 10.79: Loss = 0.752701
Epoch 10.80: Loss = 0.544586
Epoch 10.81: Loss = 0.528
Epoch 10.82: Loss = 0.700851
Epoch 10.83: Loss = 0.642639
Epoch 10.84: Loss = 0.536484
Epoch 10.85: Loss = 0.629776
Epoch 10.86: Loss = 0.636047
Epoch 10.87: Loss = 0.585968
Epoch 10.88: Loss = 0.556458
Epoch 10.89: Loss = 0.624985
Epoch 10.90: Loss = 0.55127
Epoch 10.91: Loss = 0.741348
Epoch 10.92: Loss = 0.547531
Epoch 10.93: Loss = 0.566422
Epoch 10.94: Loss = 0.614365
Epoch 10.95: Loss = 0.592682
Epoch 10.96: Loss = 0.64415
Epoch 10.97: Loss = 0.504684
Epoch 10.98: Loss = 0.715744
Epoch 10.99: Loss = 0.7323
Epoch 10.100: Loss = 0.565582
Epoch 10.101: Loss = 0.442398
Epoch 10.102: Loss = 0.642822
Epoch 10.103: Loss = 0.637802
Epoch 10.104: Loss = 0.509735
Epoch 10.105: Loss = 0.638992
Epoch 10.106: Loss = 0.573242
Epoch 10.107: Loss = 0.609543
Epoch 10.108: Loss = 0.569321
Epoch 10.109: Loss = 0.559952
Epoch 10.110: Loss = 0.52713
Epoch 10.111: Loss = 0.579681
Epoch 10.112: Loss = 0.572662
Epoch 10.113: Loss = 0.600296
Epoch 10.114: Loss = 0.560501
Epoch 10.115: Loss = 0.44783
Epoch 10.116: Loss = 0.560989
Epoch 10.117: Loss = 0.668518
Epoch 10.118: Loss = 0.608414
Epoch 10.119: Loss = 0.610336
Epoch 10.120: Loss = 0.623245
TRAIN LOSS = 0.606735
TRAIN ACC = 81.5414 % (48927/60000)
Loss = 0.522263
Loss = 0.686432
Loss = 0.571228
Loss = 0.53038
Loss = 0.59079
Loss = 0.679626
Loss = 0.786499
Loss = 0.671005
Loss = 0.659363
Loss = 0.566391
Loss = 0.772232
Loss = 0.709564
Loss = 0.686417
Loss = 0.646378
Loss = 0.631012
Loss = 0.649521
Loss = 0.602386
Loss = 0.633301
Loss = 0.679718
Loss = 0.59761
TEST LOSS = 0.643606
TEST ACC = 489.268 % (8054/10000)

Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.39435
Epoch 1.2: Loss = 2.27264
Epoch 1.3: Loss = 2.19649
Epoch 1.4: Loss = 2.11131
Epoch 1.5: Loss = 2.0556
Epoch 1.6: Loss = 1.97017
Epoch 1.7: Loss = 1.93524
Epoch 1.8: Loss = 1.87781
Epoch 1.9: Loss = 1.81972
Epoch 1.10: Loss = 1.77516
Epoch 1.11: Loss = 1.67604
Epoch 1.12: Loss = 1.62177
Epoch 1.13: Loss = 1.61469
Epoch 1.14: Loss = 1.60236
Epoch 1.15: Loss = 1.55235
Epoch 1.16: Loss = 1.51457
Epoch 1.17: Loss = 1.47359
Epoch 1.18: Loss = 1.45241
Epoch 1.19: Loss = 1.43398
Epoch 1.20: Loss = 1.39708
Epoch 1.21: Loss = 1.35979
Epoch 1.22: Loss = 1.34294
Epoch 1.23: Loss = 1.31689
Epoch 1.24: Loss = 1.38434
Epoch 1.25: Loss = 1.21405
Epoch 1.26: Loss = 1.19864
Epoch 1.27: Loss = 1.25426
Epoch 1.28: Loss = 1.20488
Epoch 1.29: Loss = 1.21965
Epoch 1.30: Loss = 1.19554
Epoch 1.31: Loss = 1.16486
Epoch 1.32: Loss = 1.13361
Epoch 1.33: Loss = 1.1387
Epoch 1.34: Loss = 1.14598
Epoch 1.35: Loss = 1.11623
Epoch 1.36: Loss = 1.06546
Epoch 1.37: Loss = 1.04605
Epoch 1.38: Loss = 1.09819
Epoch 1.39: Loss = 1.03665
Epoch 1.40: Loss = 1.02727
Epoch 1.41: Loss = 1.03265
Epoch 1.42: Loss = 1.03618
Epoch 1.43: Loss = 0.989716
Epoch 1.44: Loss = 0.979416
Epoch 1.45: Loss = 1.03455
Epoch 1.46: Loss = 0.974274
Epoch 1.47: Loss = 0.903046
Epoch 1.48: Loss = 0.926895
Epoch 1.49: Loss = 1.006
Epoch 1.50: Loss = 0.995651
Epoch 1.51: Loss = 0.965866
Epoch 1.52: Loss = 0.923096
Epoch 1.53: Loss = 0.909286
Epoch 1.54: Loss = 0.887558
Epoch 1.55: Loss = 0.886276
Epoch 1.56: Loss = 0.97821
Epoch 1.57: Loss = 0.803925
Epoch 1.58: Loss = 0.929504
Epoch 1.59: Loss = 0.915192
Epoch 1.60: Loss = 0.918091
Epoch 1.61: Loss = 0.901123
Epoch 1.62: Loss = 0.827377
Epoch 1.63: Loss = 0.851837
Epoch 1.64: Loss = 0.858154
Epoch 1.65: Loss = 0.936478
Epoch 1.66: Loss = 0.881393
Epoch 1.67: Loss = 0.894943
Epoch 1.68: Loss = 0.834
Epoch 1.69: Loss = 0.803848
Epoch 1.70: Loss = 0.770706
Epoch 1.71: Loss = 0.845367
Epoch 1.72: Loss = 0.821991
Epoch 1.73: Loss = 0.782578
Epoch 1.74: Loss = 0.825226
Epoch 1.75: Loss = 0.805893
Epoch 1.76: Loss = 0.880798
Epoch 1.77: Loss = 0.840698
Epoch 1.78: Loss = 0.812622
Epoch 1.79: Loss = 0.817963
Epoch 1.80: Loss = 0.821487
Epoch 1.81: Loss = 0.778549
Epoch 1.82: Loss = 0.768051
Epoch 1.83: Loss = 0.877045
Epoch 1.84: Loss = 0.789902
Epoch 1.85: Loss = 0.77626
Epoch 1.86: Loss = 0.821365
Epoch 1.87: Loss = 0.85022
Epoch 1.88: Loss = 0.808701
Epoch 1.89: Loss = 0.769958
Epoch 1.90: Loss = 0.785767
Epoch 1.91: Loss = 0.854935
Epoch 1.92: Loss = 0.784653
Epoch 1.93: Loss = 0.807861
Epoch 1.94: Loss = 0.863739
Epoch 1.95: Loss = 0.720779
Epoch 1.96: Loss = 0.71579
Epoch 1.97: Loss = 0.736954
Epoch 1.98: Loss = 0.727356
Epoch 1.99: Loss = 0.793259
Epoch 1.100: Loss = 0.743515
Epoch 1.101: Loss = 0.73909
Epoch 1.102: Loss = 0.811218
Epoch 1.103: Loss = 0.717712
Epoch 1.104: Loss = 0.801651
Epoch 1.105: Loss = 0.729935
Epoch 1.106: Loss = 0.745728
Epoch 1.107: Loss = 0.778671
Epoch 1.108: Loss = 0.704758
Epoch 1.109: Loss = 0.759735
Epoch 1.110: Loss = 0.748764
Epoch 1.111: Loss = 0.714249
Epoch 1.112: Loss = 0.737625
Epoch 1.113: Loss = 0.702286
Epoch 1.114: Loss = 0.827362
Epoch 1.115: Loss = 0.698837
Epoch 1.116: Loss = 0.67981
Epoch 1.117: Loss = 0.723465
Epoch 1.118: Loss = 0.682251
Epoch 1.119: Loss = 0.744583
Epoch 1.120: Loss = 0.710083
TRAIN LOSS = 1.04796
TRAIN ACC = 64.769 % (38863/60000)
Loss = 0.672607
Loss = 0.788071
Loss = 0.775742
Loss = 0.69989
Loss = 0.682892
Loss = 0.826431
Loss = 0.855026
Loss = 0.803314
Loss = 0.729965
Loss = 0.693832
Loss = 0.811279
Loss = 0.781631
Loss = 0.758987
Loss = 0.771454
Loss = 0.734604
Loss = 0.791412
Loss = 0.709702
Loss = 0.771561
Loss = 0.802917
Loss = 0.738922
TEST LOSS = 0.760012
TEST ACC = 388.629 % (7326/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.741104
Epoch 2.2: Loss = 0.706818
Epoch 2.3: Loss = 0.852905
Epoch 2.4: Loss = 0.729111
Epoch 2.5: Loss = 0.712631
Epoch 2.6: Loss = 0.669815
Epoch 2.7: Loss = 0.719193
Epoch 2.8: Loss = 0.678375
Epoch 2.9: Loss = 0.7052
Epoch 2.10: Loss = 0.73764
Epoch 2.11: Loss = 0.727783
Epoch 2.12: Loss = 0.723083
Epoch 2.13: Loss = 0.793152
Epoch 2.14: Loss = 0.680405
Epoch 2.15: Loss = 0.736023
Epoch 2.16: Loss = 0.687195
Epoch 2.17: Loss = 0.691269
Epoch 2.18: Loss = 0.655548
Epoch 2.19: Loss = 0.643799
Epoch 2.20: Loss = 0.679947
Epoch 2.21: Loss = 0.710464
Epoch 2.22: Loss = 0.668228
Epoch 2.23: Loss = 0.695679
Epoch 2.24: Loss = 0.687088
Epoch 2.25: Loss = 0.687668
Epoch 2.26: Loss = 0.657486
Epoch 2.27: Loss = 0.769394
Epoch 2.28: Loss = 0.66745
Epoch 2.29: Loss = 0.654221
Epoch 2.30: Loss = 0.595108
Epoch 2.31: Loss = 0.819626
Epoch 2.32: Loss = 0.727188
Epoch 2.33: Loss = 0.674942
Epoch 2.34: Loss = 0.635498
Epoch 2.35: Loss = 0.768631
Epoch 2.36: Loss = 0.635712
Epoch 2.37: Loss = 0.59938
Epoch 2.38: Loss = 0.642456
Epoch 2.39: Loss = 0.7202
Epoch 2.40: Loss = 0.712555
Epoch 2.41: Loss = 0.735153
Epoch 2.42: Loss = 0.692337
Epoch 2.43: Loss = 0.751846
Epoch 2.44: Loss = 0.771667
Epoch 2.45: Loss = 0.709183
Epoch 2.46: Loss = 0.634888
Epoch 2.47: Loss = 0.67955
Epoch 2.48: Loss = 0.824341
Epoch 2.49: Loss = 0.725677
Epoch 2.50: Loss = 0.615677
Epoch 2.51: Loss = 0.777588
Epoch 2.52: Loss = 0.634033
Epoch 2.53: Loss = 0.735367
Epoch 2.54: Loss = 0.704956
Epoch 2.55: Loss = 0.71402
Epoch 2.56: Loss = 0.656235
Epoch 2.57: Loss = 0.626038
Epoch 2.58: Loss = 0.727142
Epoch 2.59: Loss = 0.735367
Epoch 2.60: Loss = 0.71759
Epoch 2.61: Loss = 0.771057
Epoch 2.62: Loss = 0.694305
Epoch 2.63: Loss = 0.720108
Epoch 2.64: Loss = 0.647491
Epoch 2.65: Loss = 0.674698
Epoch 2.66: Loss = 0.692963
Epoch 2.67: Loss = 0.680435
Epoch 2.68: Loss = 0.696945
Epoch 2.69: Loss = 0.731216
Epoch 2.70: Loss = 0.747986
Epoch 2.71: Loss = 0.606216
Epoch 2.72: Loss = 0.659164
Epoch 2.73: Loss = 0.615112
Epoch 2.74: Loss = 0.66069
Epoch 2.75: Loss = 0.710342
Epoch 2.76: Loss = 0.681961
Epoch 2.77: Loss = 0.713531
Epoch 2.78: Loss = 0.608078
Epoch 2.79: Loss = 0.666534
Epoch 2.80: Loss = 0.728378
Epoch 2.81: Loss = 0.786804
Epoch 2.82: Loss = 0.686981
Epoch 2.83: Loss = 0.6259
Epoch 2.84: Loss = 0.697678
Epoch 2.85: Loss = 0.701553
Epoch 2.86: Loss = 0.622284
Epoch 2.87: Loss = 0.709976
Epoch 2.88: Loss = 0.678802
Epoch 2.89: Loss = 0.681
Epoch 2.90: Loss = 0.633865
Epoch 2.91: Loss = 0.765579
Epoch 2.92: Loss = 0.746857
Epoch 2.93: Loss = 0.63533
Epoch 2.94: Loss = 0.739182
Epoch 2.95: Loss = 0.68924
Epoch 2.96: Loss = 0.674606
Epoch 2.97: Loss = 0.678711
Epoch 2.98: Loss = 0.740173
Epoch 2.99: Loss = 0.687729
Epoch 2.100: Loss = 0.649033
Epoch 2.101: Loss = 0.731262
Epoch 2.102: Loss = 0.683624
Epoch 2.103: Loss = 0.658936
Epoch 2.104: Loss = 0.68869
Epoch 2.105: Loss = 0.736633
Epoch 2.106: Loss = 0.610291
Epoch 2.107: Loss = 0.763733
Epoch 2.108: Loss = 0.766052
Epoch 2.109: Loss = 0.720322
Epoch 2.110: Loss = 0.738388
Epoch 2.111: Loss = 0.627121
Epoch 2.112: Loss = 0.641632
Epoch 2.113: Loss = 0.655396
Epoch 2.114: Loss = 0.691498
Epoch 2.115: Loss = 0.648544
Epoch 2.116: Loss = 0.686951
Epoch 2.117: Loss = 0.694489
Epoch 2.118: Loss = 0.674423
Epoch 2.119: Loss = 0.680252
Epoch 2.120: Loss = 0.659851
TRAIN LOSS = 0.695557
TRAIN ACC = 76.5198 % (45914/60000)
Loss = 0.597153
Loss = 0.704895
Loss = 0.669388
Loss = 0.608307
Loss = 0.61824
Loss = 0.743469
Loss = 0.784195
Loss = 0.741119
Loss = 0.66272
Loss = 0.627289
Loss = 0.748871
Loss = 0.71965
Loss = 0.670349
Loss = 0.6745
Loss = 0.664459
Loss = 0.715836
Loss = 0.630508
Loss = 0.694138
Loss = 0.714966
Loss = 0.6633
TEST LOSS = 0.682667
TEST ACC = 459.138 % (7696/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.624191
Epoch 3.2: Loss = 0.668243
Epoch 3.3: Loss = 0.670853
Epoch 3.4: Loss = 0.639603
Epoch 3.5: Loss = 0.690323
Epoch 3.6: Loss = 0.679382
Epoch 3.7: Loss = 0.656311
Epoch 3.8: Loss = 0.614838
Epoch 3.9: Loss = 0.69519
Epoch 3.10: Loss = 0.64711
Epoch 3.11: Loss = 0.64859
Epoch 3.12: Loss = 0.630417
Epoch 3.13: Loss = 0.724915
Epoch 3.14: Loss = 0.721268
Epoch 3.15: Loss = 0.684845
Epoch 3.16: Loss = 0.66774
Epoch 3.17: Loss = 0.697952
Epoch 3.18: Loss = 0.74382
Epoch 3.19: Loss = 0.67688
Epoch 3.20: Loss = 0.619873
Epoch 3.21: Loss = 0.698013
Epoch 3.22: Loss = 0.69725
Epoch 3.23: Loss = 0.645996
Epoch 3.24: Loss = 0.680756
Epoch 3.25: Loss = 0.631668
Epoch 3.26: Loss = 0.623871
Epoch 3.27: Loss = 0.585495
Epoch 3.28: Loss = 0.716217
Epoch 3.29: Loss = 0.593262
Epoch 3.30: Loss = 0.641129
Epoch 3.31: Loss = 0.593491
Epoch 3.32: Loss = 0.620804
Epoch 3.33: Loss = 0.69017
Epoch 3.34: Loss = 0.681946
Epoch 3.35: Loss = 0.685974
Epoch 3.36: Loss = 0.651764
Epoch 3.37: Loss = 0.711563
Epoch 3.38: Loss = 0.546356
Epoch 3.39: Loss = 0.623322
Epoch 3.40: Loss = 0.623642
Epoch 3.41: Loss = 0.637436
Epoch 3.42: Loss = 0.554779
Epoch 3.43: Loss = 0.597137
Epoch 3.44: Loss = 0.551361
Epoch 3.45: Loss = 0.568405
Epoch 3.46: Loss = 0.616653
Epoch 3.47: Loss = 0.719482
Epoch 3.48: Loss = 0.678665
Epoch 3.49: Loss = 0.663757
Epoch 3.50: Loss = 0.672729
Epoch 3.51: Loss = 0.657852
Epoch 3.52: Loss = 0.770584
Epoch 3.53: Loss = 0.687363
Epoch 3.54: Loss = 0.68483
Epoch 3.55: Loss = 0.637833
Epoch 3.56: Loss = 0.682129
Epoch 3.57: Loss = 0.688568
Epoch 3.58: Loss = 0.642685
Epoch 3.59: Loss = 0.626755
Epoch 3.60: Loss = 0.663956
Epoch 3.61: Loss = 0.607208
Epoch 3.62: Loss = 0.578903
Epoch 3.63: Loss = 0.702957
Epoch 3.64: Loss = 0.681396
Epoch 3.65: Loss = 0.615341
Epoch 3.66: Loss = 0.698334
Epoch 3.67: Loss = 0.642593
Epoch 3.68: Loss = 0.610474
Epoch 3.69: Loss = 0.530167
Epoch 3.70: Loss = 0.713211
Epoch 3.71: Loss = 0.6409
Epoch 3.72: Loss = 0.740784
Epoch 3.73: Loss = 0.674652
Epoch 3.74: Loss = 0.67836
Epoch 3.75: Loss = 0.64444
Epoch 3.76: Loss = 0.684082
Epoch 3.77: Loss = 0.654907
Epoch 3.78: Loss = 0.601898
Epoch 3.79: Loss = 0.610382
Epoch 3.80: Loss = 0.665909
Epoch 3.81: Loss = 0.618103
Epoch 3.82: Loss = 0.657547
Epoch 3.83: Loss = 0.663223
Epoch 3.84: Loss = 0.596893
Epoch 3.85: Loss = 0.689117
Epoch 3.86: Loss = 0.59436
Epoch 3.87: Loss = 0.564331
Epoch 3.88: Loss = 0.693558
Epoch 3.89: Loss = 0.536728
Epoch 3.90: Loss = 0.643555
Epoch 3.91: Loss = 0.64006
Epoch 3.92: Loss = 0.670898
Epoch 3.93: Loss = 0.653107
Epoch 3.94: Loss = 0.691071
Epoch 3.95: Loss = 0.654999
Epoch 3.96: Loss = 0.685928
Epoch 3.97: Loss = 0.669586
Epoch 3.98: Loss = 0.641205
Epoch 3.99: Loss = 0.684235
Epoch 3.100: Loss = 0.613083
Epoch 3.101: Loss = 0.573914
Epoch 3.102: Loss = 0.760818
Epoch 3.103: Loss = 0.682251
Epoch 3.104: Loss = 0.573715
Epoch 3.105: Loss = 0.745697
Epoch 3.106: Loss = 0.699142
Epoch 3.107: Loss = 0.594467
Epoch 3.108: Loss = 0.666443
Epoch 3.109: Loss = 0.718994
Epoch 3.110: Loss = 0.677628
Epoch 3.111: Loss = 0.640396
Epoch 3.112: Loss = 0.655487
Epoch 3.113: Loss = 0.639404
Epoch 3.114: Loss = 0.6082
Epoch 3.115: Loss = 0.647446
Epoch 3.116: Loss = 0.628555
Epoch 3.117: Loss = 0.638351
Epoch 3.118: Loss = 0.547729
Epoch 3.119: Loss = 0.72374
Epoch 3.120: Loss = 0.621063
TRAIN LOSS = 0.651932
TRAIN ACC = 78.7216 % (47235/60000)
Loss = 0.556747
Loss = 0.686188
Loss = 0.633728
Loss = 0.566986
Loss = 0.594894
Loss = 0.735229
Loss = 0.765442
Loss = 0.716125
Loss = 0.636642
Loss = 0.596573
Loss = 0.741714
Loss = 0.715668
Loss = 0.668106
Loss = 0.64592
Loss = 0.640762
Loss = 0.697418
Loss = 0.613998
Loss = 0.679474
Loss = 0.692123
Loss = 0.648544
TEST LOSS = 0.661614
TEST ACC = 472.35 % (7825/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.721008
Epoch 4.2: Loss = 0.532516
Epoch 4.3: Loss = 0.559525
Epoch 4.4: Loss = 0.601563
Epoch 4.5: Loss = 0.677628
Epoch 4.6: Loss = 0.728882
Epoch 4.7: Loss = 0.584808
Epoch 4.8: Loss = 0.678146
Epoch 4.9: Loss = 0.624481
Epoch 4.10: Loss = 0.606415
Epoch 4.11: Loss = 0.684311
Epoch 4.12: Loss = 0.666016
Epoch 4.13: Loss = 0.629074
Epoch 4.14: Loss = 0.626862
Epoch 4.15: Loss = 0.5793
Epoch 4.16: Loss = 0.660782
Epoch 4.17: Loss = 0.649231
Epoch 4.18: Loss = 0.664749
Epoch 4.19: Loss = 0.641983
Epoch 4.20: Loss = 0.626328
Epoch 4.21: Loss = 0.624237
Epoch 4.22: Loss = 0.649841
Epoch 4.23: Loss = 0.637222
Epoch 4.24: Loss = 0.545456
Epoch 4.25: Loss = 0.583984
Epoch 4.26: Loss = 0.707733
Epoch 4.27: Loss = 0.623245
Epoch 4.28: Loss = 0.54364
Epoch 4.29: Loss = 0.554733
Epoch 4.30: Loss = 0.607498
Epoch 4.31: Loss = 0.679672
Epoch 4.32: Loss = 0.537399
Epoch 4.33: Loss = 0.70282
Epoch 4.34: Loss = 0.616028
Epoch 4.35: Loss = 0.542435
Epoch 4.36: Loss = 0.664154
Epoch 4.37: Loss = 0.692947
Epoch 4.38: Loss = 0.697372
Epoch 4.39: Loss = 0.716934
Epoch 4.40: Loss = 0.63942
Epoch 4.41: Loss = 0.592163
Epoch 4.42: Loss = 0.636688
Epoch 4.43: Loss = 0.582336
Epoch 4.44: Loss = 0.575424
Epoch 4.45: Loss = 0.606094
Epoch 4.46: Loss = 0.580719
Epoch 4.47: Loss = 0.780884
Epoch 4.48: Loss = 0.714111
Epoch 4.49: Loss = 0.579224
Epoch 4.50: Loss = 0.616562
Epoch 4.51: Loss = 0.625687
Epoch 4.52: Loss = 0.648972
Epoch 4.53: Loss = 0.672195
Epoch 4.54: Loss = 0.697128
Epoch 4.55: Loss = 0.599014
Epoch 4.56: Loss = 0.630905
Epoch 4.57: Loss = 0.598343
Epoch 4.58: Loss = 0.745239
Epoch 4.59: Loss = 0.602707
Epoch 4.60: Loss = 0.681519
Epoch 4.61: Loss = 0.621262
Epoch 4.62: Loss = 0.594803
Epoch 4.63: Loss = 0.663666
Epoch 4.64: Loss = 0.632553
Epoch 4.65: Loss = 0.672394
Epoch 4.66: Loss = 0.599518
Epoch 4.67: Loss = 0.749298
Epoch 4.68: Loss = 0.550583
Epoch 4.69: Loss = 0.650711
Epoch 4.70: Loss = 0.620758
Epoch 4.71: Loss = 0.54892
Epoch 4.72: Loss = 0.633224
Epoch 4.73: Loss = 0.726059
Epoch 4.74: Loss = 0.552155
Epoch 4.75: Loss = 0.583145
Epoch 4.76: Loss = 0.632858
Epoch 4.77: Loss = 0.556198
Epoch 4.78: Loss = 0.755585
Epoch 4.79: Loss = 0.583786
Epoch 4.80: Loss = 0.600449
Epoch 4.81: Loss = 0.649857
Epoch 4.82: Loss = 0.576859
Epoch 4.83: Loss = 0.615524
Epoch 4.84: Loss = 0.586975
Epoch 4.85: Loss = 0.657227
Epoch 4.86: Loss = 0.576111
Epoch 4.87: Loss = 0.647202
Epoch 4.88: Loss = 0.507751
Epoch 4.89: Loss = 0.617386
Epoch 4.90: Loss = 0.659698
Epoch 4.91: Loss = 0.670547
Epoch 4.92: Loss = 0.648834
Epoch 4.93: Loss = 0.627167
Epoch 4.94: Loss = 0.599167
Epoch 4.95: Loss = 0.735794
Epoch 4.96: Loss = 0.628616
Epoch 4.97: Loss = 0.626953
Epoch 4.98: Loss = 0.604614
Epoch 4.99: Loss = 0.573242
Epoch 4.100: Loss = 0.6362
Epoch 4.101: Loss = 0.586563
Epoch 4.102: Loss = 0.653244
Epoch 4.103: Loss = 0.660339
Epoch 4.104: Loss = 0.654572
Epoch 4.105: Loss = 0.630493
Epoch 4.106: Loss = 0.703842
Epoch 4.107: Loss = 0.63205
Epoch 4.108: Loss = 0.672806
Epoch 4.109: Loss = 0.660675
Epoch 4.110: Loss = 0.654709
Epoch 4.111: Loss = 0.602417
Epoch 4.112: Loss = 0.716019
Epoch 4.113: Loss = 0.583206
Epoch 4.114: Loss = 0.588577
Epoch 4.115: Loss = 0.597916
Epoch 4.116: Loss = 0.633713
Epoch 4.117: Loss = 0.651199
Epoch 4.118: Loss = 0.670273
Epoch 4.119: Loss = 0.547607
Epoch 4.120: Loss = 0.643417
TRAIN LOSS = 0.631622
TRAIN ACC = 79.808 % (47887/60000)
Loss = 0.55542
Loss = 0.677185
Loss = 0.61235
Loss = 0.552307
Loss = 0.583038
Loss = 0.736725
Loss = 0.758652
Loss = 0.715225
Loss = 0.645813
Loss = 0.576004
Loss = 0.757385
Loss = 0.716888
Loss = 0.657303
Loss = 0.627228
Loss = 0.640121
Loss = 0.696457
Loss = 0.615662
Loss = 0.688675
Loss = 0.683182
Loss = 0.644867
TEST LOSS = 0.657024
TEST ACC = 478.87 % (7894/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.558075
Epoch 5.2: Loss = 0.651489
Epoch 5.3: Loss = 0.646439
Epoch 5.4: Loss = 0.599396
Epoch 5.5: Loss = 0.649246
Epoch 5.6: Loss = 0.70816
Epoch 5.7: Loss = 0.763474
Epoch 5.8: Loss = 0.719803
Epoch 5.9: Loss = 0.700531
Epoch 5.10: Loss = 0.638336
Epoch 5.11: Loss = 0.592728
Epoch 5.12: Loss = 0.631561
Epoch 5.13: Loss = 0.720184
Epoch 5.14: Loss = 0.596924
Epoch 5.15: Loss = 0.582581
Epoch 5.16: Loss = 0.684021
Epoch 5.17: Loss = 0.590179
Epoch 5.18: Loss = 0.492752
Epoch 5.19: Loss = 0.664124
Epoch 5.20: Loss = 0.708832
Epoch 5.21: Loss = 0.571228
Epoch 5.22: Loss = 0.76329
Epoch 5.23: Loss = 0.575546
Epoch 5.24: Loss = 0.603699
Epoch 5.25: Loss = 0.585114
Epoch 5.26: Loss = 0.581604
Epoch 5.27: Loss = 0.614578
Epoch 5.28: Loss = 0.641769
Epoch 5.29: Loss = 0.62764
Epoch 5.30: Loss = 0.680176
Epoch 5.31: Loss = 0.698624
Epoch 5.32: Loss = 0.564163
Epoch 5.33: Loss = 0.64209
Epoch 5.34: Loss = 0.592987
Epoch 5.35: Loss = 0.605713
Epoch 5.36: Loss = 0.647232
Epoch 5.37: Loss = 0.647598
Epoch 5.38: Loss = 0.658096
Epoch 5.39: Loss = 0.742783
Epoch 5.40: Loss = 0.707138
Epoch 5.41: Loss = 0.595566
Epoch 5.42: Loss = 0.617188
Epoch 5.43: Loss = 0.567291
Epoch 5.44: Loss = 0.661407
Epoch 5.45: Loss = 0.731522
Epoch 5.46: Loss = 0.598312
Epoch 5.47: Loss = 0.636185
Epoch 5.48: Loss = 0.582397
Epoch 5.49: Loss = 0.598007
Epoch 5.50: Loss = 0.704056
Epoch 5.51: Loss = 0.63446
Epoch 5.52: Loss = 0.59404
Epoch 5.53: Loss = 0.667633
Epoch 5.54: Loss = 0.615921
Epoch 5.55: Loss = 0.627228
Epoch 5.56: Loss = 0.531769
Epoch 5.57: Loss = 0.645782
Epoch 5.58: Loss = 0.592773
Epoch 5.59: Loss = 0.597137
Epoch 5.60: Loss = 0.62561
Epoch 5.61: Loss = 0.584671
Epoch 5.62: Loss = 0.613281
Epoch 5.63: Loss = 0.638275
Epoch 5.64: Loss = 0.5215
Epoch 5.65: Loss = 0.614304
Epoch 5.66: Loss = 0.581482
Epoch 5.67: Loss = 0.610962
Epoch 5.68: Loss = 0.667252
Epoch 5.69: Loss = 0.569183
Epoch 5.70: Loss = 0.597672
Epoch 5.71: Loss = 0.667648
Epoch 5.72: Loss = 0.655258
Epoch 5.73: Loss = 0.550507
Epoch 5.74: Loss = 0.561691
Epoch 5.75: Loss = 0.56543
Epoch 5.76: Loss = 0.67897
Epoch 5.77: Loss = 0.701279
Epoch 5.78: Loss = 0.639267
Epoch 5.79: Loss = 0.567184
Epoch 5.80: Loss = 0.511902
Epoch 5.81: Loss = 0.605972
Epoch 5.82: Loss = 0.752594
Epoch 5.83: Loss = 0.659546
Epoch 5.84: Loss = 0.603317
Epoch 5.85: Loss = 0.621109
Epoch 5.86: Loss = 0.633316
Epoch 5.87: Loss = 0.67955
Epoch 5.88: Loss = 0.615692
Epoch 5.89: Loss = 0.633896
Epoch 5.90: Loss = 0.734146
Epoch 5.91: Loss = 0.595703
Epoch 5.92: Loss = 0.61673
Epoch 5.93: Loss = 0.626709
Epoch 5.94: Loss = 0.634567
Epoch 5.95: Loss = 0.62149
Epoch 5.96: Loss = 0.504547
Epoch 5.97: Loss = 0.594391
Epoch 5.98: Loss = 0.6362
Epoch 5.99: Loss = 0.673691
Epoch 5.100: Loss = 0.638702
Epoch 5.101: Loss = 0.539764
Epoch 5.102: Loss = 0.5242
Epoch 5.103: Loss = 0.605957
Epoch 5.104: Loss = 0.616318
Epoch 5.105: Loss = 0.680847
Epoch 5.106: Loss = 0.524994
Epoch 5.107: Loss = 0.640015
Epoch 5.108: Loss = 0.651596
Epoch 5.109: Loss = 0.673447
Epoch 5.110: Loss = 0.547974
Epoch 5.111: Loss = 0.548172
Epoch 5.112: Loss = 0.69873
Epoch 5.113: Loss = 0.632874
Epoch 5.114: Loss = 0.623352
Epoch 5.115: Loss = 0.587997
Epoch 5.116: Loss = 0.61113
Epoch 5.117: Loss = 0.656921
Epoch 5.118: Loss = 0.614395
Epoch 5.119: Loss = 0.660416
Epoch 5.120: Loss = 0.601196
TRAIN LOSS = 0.625793
TRAIN ACC = 80.3894 % (48235/60000)
Loss = 0.552673
Loss = 0.671646
Loss = 0.609863
Loss = 0.538635
Loss = 0.584702
Loss = 0.703003
Loss = 0.751251
Loss = 0.679367
Loss = 0.65451
Loss = 0.571701
Loss = 0.766632
Loss = 0.732178
Loss = 0.669418
Loss = 0.638519
Loss = 0.638184
Loss = 0.680466
Loss = 0.607529
Loss = 0.692719
Loss = 0.675262
Loss = 0.649353
TEST LOSS = 0.65338
TEST ACC = 482.349 % (7971/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.688766
Epoch 6.2: Loss = 0.760864
Epoch 6.3: Loss = 0.553894
Epoch 6.4: Loss = 0.663239
Epoch 6.5: Loss = 0.562958
Epoch 6.6: Loss = 0.588928
Epoch 6.7: Loss = 0.619537
Epoch 6.8: Loss = 0.638077
Epoch 6.9: Loss = 0.582077
Epoch 6.10: Loss = 0.669342
Epoch 6.11: Loss = 0.582245
Epoch 6.12: Loss = 0.566467
Epoch 6.13: Loss = 0.635239
Epoch 6.14: Loss = 0.707703
Epoch 6.15: Loss = 0.675705
Epoch 6.16: Loss = 0.606003
Epoch 6.17: Loss = 0.59967
Epoch 6.18: Loss = 0.536758
Epoch 6.19: Loss = 0.561707
Epoch 6.20: Loss = 0.649048
Epoch 6.21: Loss = 0.575134
Epoch 6.22: Loss = 0.638321
Epoch 6.23: Loss = 0.761124
Epoch 6.24: Loss = 0.590591
Epoch 6.25: Loss = 0.596283
Epoch 6.26: Loss = 0.568161
Epoch 6.27: Loss = 0.635452
Epoch 6.28: Loss = 0.782135
Epoch 6.29: Loss = 0.615616
Epoch 6.30: Loss = 0.588425
Epoch 6.31: Loss = 0.67334
Epoch 6.32: Loss = 0.65332
Epoch 6.33: Loss = 0.641388
Epoch 6.34: Loss = 0.564087
Epoch 6.35: Loss = 0.59227
Epoch 6.36: Loss = 0.686356
Epoch 6.37: Loss = 0.64888
Epoch 6.38: Loss = 0.699875
Epoch 6.39: Loss = 0.564255
Epoch 6.40: Loss = 0.637177
Epoch 6.41: Loss = 0.763596
Epoch 6.42: Loss = 0.617676
Epoch 6.43: Loss = 0.59465
Epoch 6.44: Loss = 0.594879
Epoch 6.45: Loss = 0.718079
Epoch 6.46: Loss = 0.587799
Epoch 6.47: Loss = 0.536392
Epoch 6.48: Loss = 0.77037
Epoch 6.49: Loss = 0.598801
Epoch 6.50: Loss = 0.626541
Epoch 6.51: Loss = 0.546555
Epoch 6.52: Loss = 0.599838
Epoch 6.53: Loss = 0.525986
Epoch 6.54: Loss = 0.633606
Epoch 6.55: Loss = 0.562378
Epoch 6.56: Loss = 0.656693
Epoch 6.57: Loss = 0.541565
Epoch 6.58: Loss = 0.68335
Epoch 6.59: Loss = 0.556534
Epoch 6.60: Loss = 0.542511
Epoch 6.61: Loss = 0.581161
Epoch 6.62: Loss = 0.656097
Epoch 6.63: Loss = 0.665375
Epoch 6.64: Loss = 0.569824
Epoch 6.65: Loss = 0.540482
Epoch 6.66: Loss = 0.601807
Epoch 6.67: Loss = 0.580048
Epoch 6.68: Loss = 0.569229
Epoch 6.69: Loss = 0.680481
Epoch 6.70: Loss = 0.701416
Epoch 6.71: Loss = 0.694702
Epoch 6.72: Loss = 0.639282
Epoch 6.73: Loss = 0.661926
Epoch 6.74: Loss = 0.575424
Epoch 6.75: Loss = 0.621307
Epoch 6.76: Loss = 0.684433
Epoch 6.77: Loss = 0.525284
Epoch 6.78: Loss = 0.556381
Epoch 6.79: Loss = 0.612411
Epoch 6.80: Loss = 0.539108
Epoch 6.81: Loss = 0.502472
Epoch 6.82: Loss = 0.697388
Epoch 6.83: Loss = 0.544174
Epoch 6.84: Loss = 0.555496
Epoch 6.85: Loss = 0.626663
Epoch 6.86: Loss = 0.602081
Epoch 6.87: Loss = 0.657257
Epoch 6.88: Loss = 0.626236
Epoch 6.89: Loss = 0.588104
Epoch 6.90: Loss = 0.676529
Epoch 6.91: Loss = 0.821045
Epoch 6.92: Loss = 0.535156
Epoch 6.93: Loss = 0.593185
Epoch 6.94: Loss = 0.605026
Epoch 6.95: Loss = 0.562683
Epoch 6.96: Loss = 0.641037
Epoch 6.97: Loss = 0.592728
Epoch 6.98: Loss = 0.675873
Epoch 6.99: Loss = 0.73822
Epoch 6.100: Loss = 0.679413
Epoch 6.101: Loss = 0.649429
Epoch 6.102: Loss = 0.590454
Epoch 6.103: Loss = 0.616486
Epoch 6.104: Loss = 0.61702
Epoch 6.105: Loss = 0.602951
Epoch 6.106: Loss = 0.602417
Epoch 6.107: Loss = 0.520859
Epoch 6.108: Loss = 0.749329
Epoch 6.109: Loss = 0.585907
Epoch 6.110: Loss = 0.576904
Epoch 6.111: Loss = 0.713821
Epoch 6.112: Loss = 0.579407
Epoch 6.113: Loss = 0.593765
Epoch 6.114: Loss = 0.531052
Epoch 6.115: Loss = 0.626282
Epoch 6.116: Loss = 0.61232
Epoch 6.117: Loss = 0.536346
Epoch 6.118: Loss = 0.539459
Epoch 6.119: Loss = 0.721725
Epoch 6.120: Loss = 0.556091
TRAIN LOSS = 0.619324
TRAIN ACC = 80.7465 % (48450/60000)
Loss = 0.542221
Loss = 0.659378
Loss = 0.604324
Loss = 0.539688
Loss = 0.58226
Loss = 0.688583
Loss = 0.758896
Loss = 0.670822
Loss = 0.64032
Loss = 0.570969
Loss = 0.768799
Loss = 0.717178
Loss = 0.663788
Loss = 0.638794
Loss = 0.625961
Loss = 0.652924
Loss = 0.616379
Loss = 0.677444
Loss = 0.656158
Loss = 0.637436
TEST LOSS = 0.645616
TEST ACC = 484.499 % (8005/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.561096
Epoch 7.2: Loss = 0.619858
Epoch 7.3: Loss = 0.550812
Epoch 7.4: Loss = 0.576553
Epoch 7.5: Loss = 0.586349
Epoch 7.6: Loss = 0.647903
Epoch 7.7: Loss = 0.592072
Epoch 7.8: Loss = 0.538712
Epoch 7.9: Loss = 0.541458
Epoch 7.10: Loss = 0.694656
Epoch 7.11: Loss = 0.520081
Epoch 7.12: Loss = 0.586884
Epoch 7.13: Loss = 0.6035
Epoch 7.14: Loss = 0.652191
Epoch 7.15: Loss = 0.64418
Epoch 7.16: Loss = 0.52948
Epoch 7.17: Loss = 0.682739
Epoch 7.18: Loss = 0.649521
Epoch 7.19: Loss = 0.672546
Epoch 7.20: Loss = 0.660553
Epoch 7.21: Loss = 0.545822
Epoch 7.22: Loss = 0.624542
Epoch 7.23: Loss = 0.47258
Epoch 7.24: Loss = 0.51001
Epoch 7.25: Loss = 0.647507
Epoch 7.26: Loss = 0.666824
Epoch 7.27: Loss = 0.648239
Epoch 7.28: Loss = 0.600128
Epoch 7.29: Loss = 0.674957
Epoch 7.30: Loss = 0.5867
Epoch 7.31: Loss = 0.584473
Epoch 7.32: Loss = 0.625336
Epoch 7.33: Loss = 0.533875
Epoch 7.34: Loss = 0.621658
Epoch 7.35: Loss = 0.613144
Epoch 7.36: Loss = 0.647766
Epoch 7.37: Loss = 0.607269
Epoch 7.38: Loss = 0.644119
Epoch 7.39: Loss = 0.625214
Epoch 7.40: Loss = 0.515778
Epoch 7.41: Loss = 0.647583
Epoch 7.42: Loss = 0.670578
Epoch 7.43: Loss = 0.596008
Epoch 7.44: Loss = 0.673508
Epoch 7.45: Loss = 0.50647
Epoch 7.46: Loss = 0.559616
Epoch 7.47: Loss = 0.63501
Epoch 7.48: Loss = 0.581696
Epoch 7.49: Loss = 0.658875
Epoch 7.50: Loss = 0.596832
Epoch 7.51: Loss = 0.555862
Epoch 7.52: Loss = 0.648087
Epoch 7.53: Loss = 0.577225
Epoch 7.54: Loss = 0.60701
Epoch 7.55: Loss = 0.65654
Epoch 7.56: Loss = 0.723953
Epoch 7.57: Loss = 0.590927
Epoch 7.58: Loss = 0.554398
Epoch 7.59: Loss = 0.69928
Epoch 7.60: Loss = 0.627625
Epoch 7.61: Loss = 0.586456
Epoch 7.62: Loss = 0.595856
Epoch 7.63: Loss = 0.59433
Epoch 7.64: Loss = 0.62822
Epoch 7.65: Loss = 0.700027
Epoch 7.66: Loss = 0.631973
Epoch 7.67: Loss = 0.603348
Epoch 7.68: Loss = 0.604553
Epoch 7.69: Loss = 0.604904
Epoch 7.70: Loss = 0.602814
Epoch 7.71: Loss = 0.508423
Epoch 7.72: Loss = 0.539856
Epoch 7.73: Loss = 0.635544
Epoch 7.74: Loss = 0.648911
Epoch 7.75: Loss = 0.488953
Epoch 7.76: Loss = 0.726944
Epoch 7.77: Loss = 0.746841
Epoch 7.78: Loss = 0.629852
Epoch 7.79: Loss = 0.629074
Epoch 7.80: Loss = 0.598969
Epoch 7.81: Loss = 0.61586
Epoch 7.82: Loss = 0.720779
Epoch 7.83: Loss = 0.560028
Epoch 7.84: Loss = 0.743088
Epoch 7.85: Loss = 0.691925
Epoch 7.86: Loss = 0.615479
Epoch 7.87: Loss = 0.616821
Epoch 7.88: Loss = 0.623398
Epoch 7.89: Loss = 0.548004
Epoch 7.90: Loss = 0.660782
Epoch 7.91: Loss = 0.679794
Epoch 7.92: Loss = 0.619064
Epoch 7.93: Loss = 0.639969
Epoch 7.94: Loss = 0.739731
Epoch 7.95: Loss = 0.54422
Epoch 7.96: Loss = 0.623123
Epoch 7.97: Loss = 0.648392
Epoch 7.98: Loss = 0.663574
Epoch 7.99: Loss = 0.614136
Epoch 7.100: Loss = 0.602051
Epoch 7.101: Loss = 0.580383
Epoch 7.102: Loss = 0.666138
Epoch 7.103: Loss = 0.600693
Epoch 7.104: Loss = 0.701385
Epoch 7.105: Loss = 0.563965
Epoch 7.106: Loss = 0.677902
Epoch 7.107: Loss = 0.610428
Epoch 7.108: Loss = 0.588028
Epoch 7.109: Loss = 0.596024
Epoch 7.110: Loss = 0.657806
Epoch 7.111: Loss = 0.519958
Epoch 7.112: Loss = 0.534058
Epoch 7.113: Loss = 0.689102
Epoch 7.114: Loss = 0.671509
Epoch 7.115: Loss = 0.639587
Epoch 7.116: Loss = 0.706039
Epoch 7.117: Loss = 0.633179
Epoch 7.118: Loss = 0.5746
Epoch 7.119: Loss = 0.626083
Epoch 7.120: Loss = 0.613937
TRAIN LOSS = 0.616699
TRAIN ACC = 81.2225 % (48736/60000)
Loss = 0.552887
Loss = 0.691589
Loss = 0.616974
Loss = 0.535217
Loss = 0.588623
Loss = 0.713287
Loss = 0.791031
Loss = 0.68512
Loss = 0.637711
Loss = 0.566666
Loss = 0.805771
Loss = 0.744217
Loss = 0.691437
Loss = 0.649628
Loss = 0.629318
Loss = 0.66655
Loss = 0.63942
Loss = 0.690079
Loss = 0.668137
Loss = 0.63475
TEST LOSS = 0.65992
TEST ACC = 487.36 % (8034/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.597153
Epoch 8.2: Loss = 0.663986
Epoch 8.3: Loss = 0.696136
Epoch 8.4: Loss = 0.643234
Epoch 8.5: Loss = 0.637924
Epoch 8.6: Loss = 0.670166
Epoch 8.7: Loss = 0.656158
Epoch 8.8: Loss = 0.538818
Epoch 8.9: Loss = 0.593643
Epoch 8.10: Loss = 0.5569
Epoch 8.11: Loss = 0.744263
Epoch 8.12: Loss = 0.529587
Epoch 8.13: Loss = 0.668213
Epoch 8.14: Loss = 0.640366
Epoch 8.15: Loss = 0.69281
Epoch 8.16: Loss = 0.66687
Epoch 8.17: Loss = 0.711441
Epoch 8.18: Loss = 0.541107
Epoch 8.19: Loss = 0.595428
Epoch 8.20: Loss = 0.60994
Epoch 8.21: Loss = 0.757462
Epoch 8.22: Loss = 0.600388
Epoch 8.23: Loss = 0.624207
Epoch 8.24: Loss = 0.580063
Epoch 8.25: Loss = 0.588226
Epoch 8.26: Loss = 0.55954
Epoch 8.27: Loss = 0.63945
Epoch 8.28: Loss = 0.557953
Epoch 8.29: Loss = 0.683563
Epoch 8.30: Loss = 0.523132
Epoch 8.31: Loss = 0.606781
Epoch 8.32: Loss = 0.542938
Epoch 8.33: Loss = 0.696426
Epoch 8.34: Loss = 0.623627
Epoch 8.35: Loss = 0.665207
Epoch 8.36: Loss = 0.599869
Epoch 8.37: Loss = 0.540298
Epoch 8.38: Loss = 0.544891
Epoch 8.39: Loss = 0.626633
Epoch 8.40: Loss = 0.642868
Epoch 8.41: Loss = 0.560577
Epoch 8.42: Loss = 0.649612
Epoch 8.43: Loss = 0.621796
Epoch 8.44: Loss = 0.666885
Epoch 8.45: Loss = 0.596756
Epoch 8.46: Loss = 0.557617
Epoch 8.47: Loss = 0.60199
Epoch 8.48: Loss = 0.633926
Epoch 8.49: Loss = 0.717987
Epoch 8.50: Loss = 0.80658
Epoch 8.51: Loss = 0.541718
Epoch 8.52: Loss = 0.65921
Epoch 8.53: Loss = 0.664688
Epoch 8.54: Loss = 0.605957
Epoch 8.55: Loss = 0.589676
Epoch 8.56: Loss = 0.564728
Epoch 8.57: Loss = 0.785065
Epoch 8.58: Loss = 0.608231
Epoch 8.59: Loss = 0.636261
Epoch 8.60: Loss = 0.578064
Epoch 8.61: Loss = 0.667847
Epoch 8.62: Loss = 0.625687
Epoch 8.63: Loss = 0.578812
Epoch 8.64: Loss = 0.685699
Epoch 8.65: Loss = 0.637634
Epoch 8.66: Loss = 0.57663
Epoch 8.67: Loss = 0.676849
Epoch 8.68: Loss = 0.57132
Epoch 8.69: Loss = 0.533646
Epoch 8.70: Loss = 0.671768
Epoch 8.71: Loss = 0.533386
Epoch 8.72: Loss = 0.539063
Epoch 8.73: Loss = 0.661743
Epoch 8.74: Loss = 0.61557
Epoch 8.75: Loss = 0.691025
Epoch 8.76: Loss = 0.62619
Epoch 8.77: Loss = 0.665405
Epoch 8.78: Loss = 0.599274
Epoch 8.79: Loss = 0.566345
Epoch 8.80: Loss = 0.573273
Epoch 8.81: Loss = 0.712555
Epoch 8.82: Loss = 0.557037
Epoch 8.83: Loss = 0.493546
Epoch 8.84: Loss = 0.651047
Epoch 8.85: Loss = 0.563934
Epoch 8.86: Loss = 0.608139
Epoch 8.87: Loss = 0.565292
Epoch 8.88: Loss = 0.582855
Epoch 8.89: Loss = 0.563904
Epoch 8.90: Loss = 0.579376
Epoch 8.91: Loss = 0.556412
Epoch 8.92: Loss = 0.522324
Epoch 8.93: Loss = 0.555298
Epoch 8.94: Loss = 0.635803
Epoch 8.95: Loss = 0.653198
Epoch 8.96: Loss = 0.662521
Epoch 8.97: Loss = 0.581436
Epoch 8.98: Loss = 0.556259
Epoch 8.99: Loss = 0.507065
Epoch 8.100: Loss = 0.645996
Epoch 8.101: Loss = 0.576553
Epoch 8.102: Loss = 0.523575
Epoch 8.103: Loss = 0.541367
Epoch 8.104: Loss = 0.625
Epoch 8.105: Loss = 0.624756
Epoch 8.106: Loss = 0.629761
Epoch 8.107: Loss = 0.636139
Epoch 8.108: Loss = 0.583588
Epoch 8.109: Loss = 0.536118
Epoch 8.110: Loss = 0.542038
Epoch 8.111: Loss = 0.593048
Epoch 8.112: Loss = 0.691254
Epoch 8.113: Loss = 0.535248
Epoch 8.114: Loss = 0.534805
Epoch 8.115: Loss = 0.622711
Epoch 8.116: Loss = 0.561951
Epoch 8.117: Loss = 0.561127
Epoch 8.118: Loss = 0.54512
Epoch 8.119: Loss = 0.517578
Epoch 8.120: Loss = 0.651688
TRAIN LOSS = 0.60994
TRAIN ACC = 81.4667 % (48882/60000)
Loss = 0.532944
Loss = 0.630829
Loss = 0.598465
Loss = 0.508301
Loss = 0.5811
Loss = 0.695938
Loss = 0.758408
Loss = 0.671082
Loss = 0.611008
Loss = 0.540176
Loss = 0.766098
Loss = 0.701248
Loss = 0.660904
Loss = 0.603165
Loss = 0.603928
Loss = 0.644089
Loss = 0.604126
Loss = 0.6754
Loss = 0.641647
Loss = 0.62352
TEST LOSS = 0.632619
TEST ACC = 488.82 % (8058/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.642822
Epoch 9.2: Loss = 0.610214
Epoch 9.3: Loss = 0.5616
Epoch 9.4: Loss = 0.749893
Epoch 9.5: Loss = 0.616577
Epoch 9.6: Loss = 0.592117
Epoch 9.7: Loss = 0.692642
Epoch 9.8: Loss = 0.616226
Epoch 9.9: Loss = 0.575241
Epoch 9.10: Loss = 0.657028
Epoch 9.11: Loss = 0.63887
Epoch 9.12: Loss = 0.693359
Epoch 9.13: Loss = 0.59761
Epoch 9.14: Loss = 0.452744
Epoch 9.15: Loss = 0.556015
Epoch 9.16: Loss = 0.538757
Epoch 9.17: Loss = 0.723129
Epoch 9.18: Loss = 0.660095
Epoch 9.19: Loss = 0.607727
Epoch 9.20: Loss = 0.648087
Epoch 9.21: Loss = 0.576706
Epoch 9.22: Loss = 0.625641
Epoch 9.23: Loss = 0.561462
Epoch 9.24: Loss = 0.594818
Epoch 9.25: Loss = 0.619675
Epoch 9.26: Loss = 0.580154
Epoch 9.27: Loss = 0.552017
Epoch 9.28: Loss = 0.740967
Epoch 9.29: Loss = 0.495865
Epoch 9.30: Loss = 0.617813
Epoch 9.31: Loss = 0.618347
Epoch 9.32: Loss = 0.676468
Epoch 9.33: Loss = 0.49472
Epoch 9.34: Loss = 0.600967
Epoch 9.35: Loss = 0.562775
Epoch 9.36: Loss = 0.623016
Epoch 9.37: Loss = 0.556442
Epoch 9.38: Loss = 0.498734
Epoch 9.39: Loss = 0.631912
Epoch 9.40: Loss = 0.66423
Epoch 9.41: Loss = 0.509476
Epoch 9.42: Loss = 0.690842
Epoch 9.43: Loss = 0.742157
Epoch 9.44: Loss = 0.583923
Epoch 9.45: Loss = 0.602951
Epoch 9.46: Loss = 0.469879
Epoch 9.47: Loss = 0.683701
Epoch 9.48: Loss = 0.720169
Epoch 9.49: Loss = 0.639938
Epoch 9.50: Loss = 0.542038
Epoch 9.51: Loss = 0.618652
Epoch 9.52: Loss = 0.506348
Epoch 9.53: Loss = 0.462219
Epoch 9.54: Loss = 0.628159
Epoch 9.55: Loss = 0.618179
Epoch 9.56: Loss = 0.634537
Epoch 9.57: Loss = 0.612823
Epoch 9.58: Loss = 0.611023
Epoch 9.59: Loss = 0.627518
Epoch 9.60: Loss = 0.55217
Epoch 9.61: Loss = 0.653015
Epoch 9.62: Loss = 0.674789
Epoch 9.63: Loss = 0.567596
Epoch 9.64: Loss = 0.606567
Epoch 9.65: Loss = 0.608307
Epoch 9.66: Loss = 0.504395
Epoch 9.67: Loss = 0.646637
Epoch 9.68: Loss = 0.484619
Epoch 9.69: Loss = 0.598862
Epoch 9.70: Loss = 0.560226
Epoch 9.71: Loss = 0.593872
Epoch 9.72: Loss = 0.605011
Epoch 9.73: Loss = 0.633469
Epoch 9.74: Loss = 0.651733
Epoch 9.75: Loss = 0.55986
Epoch 9.76: Loss = 0.522324
Epoch 9.77: Loss = 0.719437
Epoch 9.78: Loss = 0.741608
Epoch 9.79: Loss = 0.542633
Epoch 9.80: Loss = 0.63559
Epoch 9.81: Loss = 0.659271
Epoch 9.82: Loss = 0.462158
Epoch 9.83: Loss = 0.542709
Epoch 9.84: Loss = 0.629929
Epoch 9.85: Loss = 0.567169
Epoch 9.86: Loss = 0.562317
Epoch 9.87: Loss = 0.606842
Epoch 9.88: Loss = 0.590881
Epoch 9.89: Loss = 0.551315
Epoch 9.90: Loss = 0.621887
Epoch 9.91: Loss = 0.603485
Epoch 9.92: Loss = 0.628036
Epoch 9.93: Loss = 0.705597
Epoch 9.94: Loss = 0.752197
Epoch 9.95: Loss = 0.575317
Epoch 9.96: Loss = 0.60495
Epoch 9.97: Loss = 0.543106
Epoch 9.98: Loss = 0.615112
Epoch 9.99: Loss = 0.636353
Epoch 9.100: Loss = 0.546814
Epoch 9.101: Loss = 0.640701
Epoch 9.102: Loss = 0.600708
Epoch 9.103: Loss = 0.570999
Epoch 9.104: Loss = 0.536102
Epoch 9.105: Loss = 0.695496
Epoch 9.106: Loss = 0.552368
Epoch 9.107: Loss = 0.619156
Epoch 9.108: Loss = 0.685547
Epoch 9.109: Loss = 0.651093
Epoch 9.110: Loss = 0.530075
Epoch 9.111: Loss = 0.680618
Epoch 9.112: Loss = 0.470215
Epoch 9.113: Loss = 0.618546
Epoch 9.114: Loss = 0.589157
Epoch 9.115: Loss = 0.608871
Epoch 9.116: Loss = 0.682632
Epoch 9.117: Loss = 0.585938
Epoch 9.118: Loss = 0.617722
Epoch 9.119: Loss = 0.577301
Epoch 9.120: Loss = 0.56781
TRAIN LOSS = 0.604828
TRAIN ACC = 81.8405 % (49107/60000)
Loss = 0.522125
Loss = 0.654343
Loss = 0.599655
Loss = 0.510101
Loss = 0.590942
Loss = 0.692734
Loss = 0.766098
Loss = 0.661896
Loss = 0.618423
Loss = 0.551041
Loss = 0.774246
Loss = 0.703842
Loss = 0.677536
Loss = 0.623978
Loss = 0.616821
Loss = 0.636063
Loss = 0.622299
Loss = 0.687943
Loss = 0.649963
Loss = 0.625763
TEST LOSS = 0.63929
TEST ACC = 491.069 % (8087/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.49408
Epoch 10.2: Loss = 0.641327
Epoch 10.3: Loss = 0.57869
Epoch 10.4: Loss = 0.6828
Epoch 10.5: Loss = 0.730453
Epoch 10.6: Loss = 0.688461
Epoch 10.7: Loss = 0.537262
Epoch 10.8: Loss = 0.608231
Epoch 10.9: Loss = 0.568176
Epoch 10.10: Loss = 0.648697
Epoch 10.11: Loss = 0.613083
Epoch 10.12: Loss = 0.560684
Epoch 10.13: Loss = 0.588867
Epoch 10.14: Loss = 0.60939
Epoch 10.15: Loss = 0.602264
Epoch 10.16: Loss = 0.552673
Epoch 10.17: Loss = 0.624893
Epoch 10.18: Loss = 0.536545
Epoch 10.19: Loss = 0.693314
Epoch 10.20: Loss = 0.555054
Epoch 10.21: Loss = 0.583054
Epoch 10.22: Loss = 0.655304
Epoch 10.23: Loss = 0.633514
Epoch 10.24: Loss = 0.543304
Epoch 10.25: Loss = 0.558578
Epoch 10.26: Loss = 0.633133
Epoch 10.27: Loss = 0.674408
Epoch 10.28: Loss = 0.489227
Epoch 10.29: Loss = 0.6203
Epoch 10.30: Loss = 0.601181
Epoch 10.31: Loss = 0.719193
Epoch 10.32: Loss = 0.495789
Epoch 10.33: Loss = 0.590149
Epoch 10.34: Loss = 0.58548
Epoch 10.35: Loss = 0.603638
Epoch 10.36: Loss = 0.726685
Epoch 10.37: Loss = 0.641113
Epoch 10.38: Loss = 0.584717
Epoch 10.39: Loss = 0.583481
Epoch 10.40: Loss = 0.597153
Epoch 10.41: Loss = 0.601425
Epoch 10.42: Loss = 0.614655
Epoch 10.43: Loss = 0.635635
Epoch 10.44: Loss = 0.61557
Epoch 10.45: Loss = 0.491257
Epoch 10.46: Loss = 0.642212
Epoch 10.47: Loss = 0.652893
Epoch 10.48: Loss = 0.530762
Epoch 10.49: Loss = 0.519394
Epoch 10.50: Loss = 0.488022
Epoch 10.51: Loss = 0.547363
Epoch 10.52: Loss = 0.598206
Epoch 10.53: Loss = 0.561264
Epoch 10.54: Loss = 0.514023
Epoch 10.55: Loss = 0.628189
Epoch 10.56: Loss = 0.607834
Epoch 10.57: Loss = 0.670547
Epoch 10.58: Loss = 0.619003
Epoch 10.59: Loss = 0.723419
Epoch 10.60: Loss = 0.590195
Epoch 10.61: Loss = 0.606628
Epoch 10.62: Loss = 0.664505
Epoch 10.63: Loss = 0.671387
Epoch 10.64: Loss = 0.596939
Epoch 10.65: Loss = 0.666962
Epoch 10.66: Loss = 0.554581
Epoch 10.67: Loss = 0.645142
Epoch 10.68: Loss = 0.49765
Epoch 10.69: Loss = 0.543182
Epoch 10.70: Loss = 0.655212
Epoch 10.71: Loss = 0.58345
Epoch 10.72: Loss = 0.615219
Epoch 10.73: Loss = 0.563919
Epoch 10.74: Loss = 0.65271
Epoch 10.75: Loss = 0.609253
Epoch 10.76: Loss = 0.503693
Epoch 10.77: Loss = 0.603745
Epoch 10.78: Loss = 0.524124
Epoch 10.79: Loss = 0.697815
Epoch 10.80: Loss = 0.577454
Epoch 10.81: Loss = 0.723648
Epoch 10.82: Loss = 0.577454
Epoch 10.83: Loss = 0.692917
Epoch 10.84: Loss = 0.510468
Epoch 10.85: Loss = 0.537216
Epoch 10.86: Loss = 0.601898
Epoch 10.87: Loss = 0.424011
Epoch 10.88: Loss = 0.727539
Epoch 10.89: Loss = 0.564667
Epoch 10.90: Loss = 0.572464
Epoch 10.91: Loss = 0.684769
Epoch 10.92: Loss = 0.651031
Epoch 10.93: Loss = 0.626389
Epoch 10.94: Loss = 0.665756
Epoch 10.95: Loss = 0.549423
Epoch 10.96: Loss = 0.677185
Epoch 10.97: Loss = 0.553665
Epoch 10.98: Loss = 0.508804
Epoch 10.99: Loss = 0.643417
Epoch 10.100: Loss = 0.734268
Epoch 10.101: Loss = 0.602631
Epoch 10.102: Loss = 0.597214
Epoch 10.103: Loss = 0.592575
Epoch 10.104: Loss = 0.614655
Epoch 10.105: Loss = 0.541107
Epoch 10.106: Loss = 0.672516
Epoch 10.107: Loss = 0.550308
Epoch 10.108: Loss = 0.698471
Epoch 10.109: Loss = 0.604263
Epoch 10.110: Loss = 0.611588
Epoch 10.111: Loss = 0.659653
Epoch 10.112: Loss = 0.691727
Epoch 10.113: Loss = 0.53801
Epoch 10.114: Loss = 0.570755
Epoch 10.115: Loss = 0.544708
Epoch 10.116: Loss = 0.566605
Epoch 10.117: Loss = 0.578491
Epoch 10.118: Loss = 0.692978
Epoch 10.119: Loss = 0.679672
Epoch 10.120: Loss = 0.475418
TRAIN LOSS = 0.603607
TRAIN ACC = 82.0587 % (49238/60000)
Loss = 0.517136
Loss = 0.662872
Loss = 0.60405
Loss = 0.518967
Loss = 0.601028
Loss = 0.719894
Loss = 0.775284
Loss = 0.674667
Loss = 0.603256
Loss = 0.552612
Loss = 0.778824
Loss = 0.7052
Loss = 0.682343
Loss = 0.626526
Loss = 0.628204
Loss = 0.65094
Loss = 0.621658
Loss = 0.698181
Loss = 0.663818
Loss = 0.654755
TEST LOSS = 0.647011
TEST ACC = 492.38 % (8115/10000)

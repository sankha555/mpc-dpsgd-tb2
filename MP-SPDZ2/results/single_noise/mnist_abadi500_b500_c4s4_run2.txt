Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.32809
Epoch 1.2: Loss = 2.29097
Epoch 1.3: Loss = 2.22734
Epoch 1.4: Loss = 2.17552
Epoch 1.5: Loss = 2.1535
Epoch 1.6: Loss = 2.11877
Epoch 1.7: Loss = 2.05247
Epoch 1.8: Loss = 2.01868
Epoch 1.9: Loss = 1.987
Epoch 1.10: Loss = 1.94318
Epoch 1.11: Loss = 1.9211
Epoch 1.12: Loss = 1.87218
Epoch 1.13: Loss = 1.80035
Epoch 1.14: Loss = 1.77574
Epoch 1.15: Loss = 1.78406
Epoch 1.16: Loss = 1.72044
Epoch 1.17: Loss = 1.69812
Epoch 1.18: Loss = 1.6857
Epoch 1.19: Loss = 1.62769
Epoch 1.20: Loss = 1.61319
Epoch 1.21: Loss = 1.59142
Epoch 1.22: Loss = 1.53122
Epoch 1.23: Loss = 1.5672
Epoch 1.24: Loss = 1.48723
Epoch 1.25: Loss = 1.45493
Epoch 1.26: Loss = 1.39201
Epoch 1.27: Loss = 1.4016
Epoch 1.28: Loss = 1.38287
Epoch 1.29: Loss = 1.41708
Epoch 1.30: Loss = 1.34296
Epoch 1.31: Loss = 1.36615
Epoch 1.32: Loss = 1.32689
Epoch 1.33: Loss = 1.27332
Epoch 1.34: Loss = 1.20659
Epoch 1.35: Loss = 1.25252
Epoch 1.36: Loss = 1.21111
Epoch 1.37: Loss = 1.19907
Epoch 1.38: Loss = 1.16267
Epoch 1.39: Loss = 1.15071
Epoch 1.40: Loss = 1.15958
Epoch 1.41: Loss = 1.06219
Epoch 1.42: Loss = 1.1163
Epoch 1.43: Loss = 1.12535
Epoch 1.44: Loss = 1.08691
Epoch 1.45: Loss = 1.07938
Epoch 1.46: Loss = 1.0831
Epoch 1.47: Loss = 1.01057
Epoch 1.48: Loss = 0.996185
Epoch 1.49: Loss = 0.977325
Epoch 1.50: Loss = 0.967194
Epoch 1.51: Loss = 0.973984
Epoch 1.52: Loss = 0.908539
Epoch 1.53: Loss = 0.980301
Epoch 1.54: Loss = 0.951233
Epoch 1.55: Loss = 0.938248
Epoch 1.56: Loss = 0.894714
Epoch 1.57: Loss = 0.92012
Epoch 1.58: Loss = 0.897415
Epoch 1.59: Loss = 0.868729
Epoch 1.60: Loss = 0.852432
Epoch 1.61: Loss = 0.916504
Epoch 1.62: Loss = 0.885849
Epoch 1.63: Loss = 0.868149
Epoch 1.64: Loss = 0.807297
Epoch 1.65: Loss = 0.812347
Epoch 1.66: Loss = 0.78418
Epoch 1.67: Loss = 0.808914
Epoch 1.68: Loss = 0.837051
Epoch 1.69: Loss = 0.836578
Epoch 1.70: Loss = 0.834839
Epoch 1.71: Loss = 0.763489
Epoch 1.72: Loss = 0.813995
Epoch 1.73: Loss = 0.768829
Epoch 1.74: Loss = 0.772369
Epoch 1.75: Loss = 0.744827
Epoch 1.76: Loss = 0.768463
Epoch 1.77: Loss = 0.816116
Epoch 1.78: Loss = 0.701202
Epoch 1.79: Loss = 0.82048
Epoch 1.80: Loss = 0.735229
Epoch 1.81: Loss = 0.729736
Epoch 1.82: Loss = 0.735641
Epoch 1.83: Loss = 0.718124
Epoch 1.84: Loss = 0.75679
Epoch 1.85: Loss = 0.700027
Epoch 1.86: Loss = 0.629501
Epoch 1.87: Loss = 0.729889
Epoch 1.88: Loss = 0.702423
Epoch 1.89: Loss = 0.713669
Epoch 1.90: Loss = 0.712799
Epoch 1.91: Loss = 0.658997
Epoch 1.92: Loss = 0.594711
Epoch 1.93: Loss = 0.684158
Epoch 1.94: Loss = 0.668289
Epoch 1.95: Loss = 0.643402
Epoch 1.96: Loss = 0.682053
Epoch 1.97: Loss = 0.645798
Epoch 1.98: Loss = 0.665848
Epoch 1.99: Loss = 0.689362
Epoch 1.100: Loss = 0.640717
Epoch 1.101: Loss = 0.602997
Epoch 1.102: Loss = 0.64212
Epoch 1.103: Loss = 0.649155
Epoch 1.104: Loss = 0.655243
Epoch 1.105: Loss = 0.62619
Epoch 1.106: Loss = 0.596222
Epoch 1.107: Loss = 0.60054
Epoch 1.108: Loss = 0.628647
Epoch 1.109: Loss = 0.61734
Epoch 1.110: Loss = 0.596664
Epoch 1.111: Loss = 0.60025
Epoch 1.112: Loss = 0.62355
Epoch 1.113: Loss = 0.594116
Epoch 1.114: Loss = 0.646667
Epoch 1.115: Loss = 0.550812
Epoch 1.116: Loss = 0.562119
Epoch 1.117: Loss = 0.602631
Epoch 1.118: Loss = 0.556732
Epoch 1.119: Loss = 0.633209
Epoch 1.120: Loss = 0.600067
TRAIN LOSS = 1.06123
TRAIN ACC = 71.2708 % (42765/60000)
Loss = 0.611755
Loss = 0.645142
Loss = 0.763535
Loss = 0.690659
Loss = 0.737976
Loss = 0.652786
Loss = 0.606995
Loss = 0.763931
Loss = 0.715637
Loss = 0.669601
Loss = 0.344635
Loss = 0.493317
Loss = 0.347458
Loss = 0.556366
Loss = 0.444504
Loss = 0.457581
Loss = 0.389908
Loss = 0.248657
Loss = 0.422409
Loss = 0.648499
TEST LOSS = 0.560567
TEST ACC = 427.649 % (8399/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.636261
Epoch 2.2: Loss = 0.651123
Epoch 2.3: Loss = 0.572968
Epoch 2.4: Loss = 0.584991
Epoch 2.5: Loss = 0.543427
Epoch 2.6: Loss = 0.520721
Epoch 2.7: Loss = 0.556976
Epoch 2.8: Loss = 0.550888
Epoch 2.9: Loss = 0.607056
Epoch 2.10: Loss = 0.606766
Epoch 2.11: Loss = 0.585648
Epoch 2.12: Loss = 0.55864
Epoch 2.13: Loss = 0.539398
Epoch 2.14: Loss = 0.531067
Epoch 2.15: Loss = 0.563889
Epoch 2.16: Loss = 0.528793
Epoch 2.17: Loss = 0.561966
Epoch 2.18: Loss = 0.643967
Epoch 2.19: Loss = 0.578247
Epoch 2.20: Loss = 0.550446
Epoch 2.21: Loss = 0.514923
Epoch 2.22: Loss = 0.540192
Epoch 2.23: Loss = 0.471634
Epoch 2.24: Loss = 0.533386
Epoch 2.25: Loss = 0.536545
Epoch 2.26: Loss = 0.488342
Epoch 2.27: Loss = 0.573242
Epoch 2.28: Loss = 0.551559
Epoch 2.29: Loss = 0.513077
Epoch 2.30: Loss = 0.550369
Epoch 2.31: Loss = 0.579697
Epoch 2.32: Loss = 0.585129
Epoch 2.33: Loss = 0.531403
Epoch 2.34: Loss = 0.443024
Epoch 2.35: Loss = 0.570282
Epoch 2.36: Loss = 0.533768
Epoch 2.37: Loss = 0.489441
Epoch 2.38: Loss = 0.543259
Epoch 2.39: Loss = 0.545944
Epoch 2.40: Loss = 0.533707
Epoch 2.41: Loss = 0.542969
Epoch 2.42: Loss = 0.562256
Epoch 2.43: Loss = 0.52684
Epoch 2.44: Loss = 0.490204
Epoch 2.45: Loss = 0.545761
Epoch 2.46: Loss = 0.484848
Epoch 2.47: Loss = 0.510895
Epoch 2.48: Loss = 0.512009
Epoch 2.49: Loss = 0.454163
Epoch 2.50: Loss = 0.525543
Epoch 2.51: Loss = 0.520584
Epoch 2.52: Loss = 0.467499
Epoch 2.53: Loss = 0.480698
Epoch 2.54: Loss = 0.519272
Epoch 2.55: Loss = 0.560333
Epoch 2.56: Loss = 0.438477
Epoch 2.57: Loss = 0.524567
Epoch 2.58: Loss = 0.520828
Epoch 2.59: Loss = 0.442001
Epoch 2.60: Loss = 0.468781
Epoch 2.61: Loss = 0.459
Epoch 2.62: Loss = 0.474304
Epoch 2.63: Loss = 0.503418
Epoch 2.64: Loss = 0.462143
Epoch 2.65: Loss = 0.49585
Epoch 2.66: Loss = 0.535477
Epoch 2.67: Loss = 0.564636
Epoch 2.68: Loss = 0.550018
Epoch 2.69: Loss = 0.479797
Epoch 2.70: Loss = 0.506424
Epoch 2.71: Loss = 0.427216
Epoch 2.72: Loss = 0.385315
Epoch 2.73: Loss = 0.485931
Epoch 2.74: Loss = 0.43457
Epoch 2.75: Loss = 0.501724
Epoch 2.76: Loss = 0.528015
Epoch 2.77: Loss = 0.445724
Epoch 2.78: Loss = 0.469864
Epoch 2.79: Loss = 0.43576
Epoch 2.80: Loss = 0.447006
Epoch 2.81: Loss = 0.485565
Epoch 2.82: Loss = 0.507874
Epoch 2.83: Loss = 0.4953
Epoch 2.84: Loss = 0.506577
Epoch 2.85: Loss = 0.470459
Epoch 2.86: Loss = 0.420914
Epoch 2.87: Loss = 0.485352
Epoch 2.88: Loss = 0.487015
Epoch 2.89: Loss = 0.453781
Epoch 2.90: Loss = 0.464005
Epoch 2.91: Loss = 0.451874
Epoch 2.92: Loss = 0.443588
Epoch 2.93: Loss = 0.441116
Epoch 2.94: Loss = 0.500076
Epoch 2.95: Loss = 0.47171
Epoch 2.96: Loss = 0.388916
Epoch 2.97: Loss = 0.474686
Epoch 2.98: Loss = 0.475983
Epoch 2.99: Loss = 0.477066
Epoch 2.100: Loss = 0.502182
Epoch 2.101: Loss = 0.422211
Epoch 2.102: Loss = 0.509247
Epoch 2.103: Loss = 0.514801
Epoch 2.104: Loss = 0.475021
Epoch 2.105: Loss = 0.50705
Epoch 2.106: Loss = 0.443893
Epoch 2.107: Loss = 0.409683
Epoch 2.108: Loss = 0.458694
Epoch 2.109: Loss = 0.40802
Epoch 2.110: Loss = 0.430634
Epoch 2.111: Loss = 0.572098
Epoch 2.112: Loss = 0.446365
Epoch 2.113: Loss = 0.485321
Epoch 2.114: Loss = 0.468872
Epoch 2.115: Loss = 0.421204
Epoch 2.116: Loss = 0.427124
Epoch 2.117: Loss = 0.541122
Epoch 2.118: Loss = 0.410049
Epoch 2.119: Loss = 0.460907
Epoch 2.120: Loss = 0.374878
TRAIN LOSS = 0.50322
TRAIN ACC = 85.0052 % (51005/60000)
Loss = 0.438324
Loss = 0.517227
Loss = 0.611603
Loss = 0.56926
Loss = 0.611771
Loss = 0.483551
Loss = 0.461166
Loss = 0.649872
Loss = 0.572815
Loss = 0.5401
Loss = 0.216217
Loss = 0.338303
Loss = 0.242233
Loss = 0.402039
Loss = 0.27594
Loss = 0.326233
Loss = 0.258942
Loss = 0.108765
Loss = 0.289413
Loss = 0.528564
TEST LOSS = 0.422117
TEST ACC = 510.049 % (8761/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.432816
Epoch 3.2: Loss = 0.43132
Epoch 3.3: Loss = 0.507355
Epoch 3.4: Loss = 0.47052
Epoch 3.5: Loss = 0.388596
Epoch 3.6: Loss = 0.404129
Epoch 3.7: Loss = 0.490417
Epoch 3.8: Loss = 0.434982
Epoch 3.9: Loss = 0.480621
Epoch 3.10: Loss = 0.471054
Epoch 3.11: Loss = 0.437744
Epoch 3.12: Loss = 0.458191
Epoch 3.13: Loss = 0.454926
Epoch 3.14: Loss = 0.410721
Epoch 3.15: Loss = 0.467987
Epoch 3.16: Loss = 0.395615
Epoch 3.17: Loss = 0.451141
Epoch 3.18: Loss = 0.477783
Epoch 3.19: Loss = 0.479218
Epoch 3.20: Loss = 0.453445
Epoch 3.21: Loss = 0.465073
Epoch 3.22: Loss = 0.463882
Epoch 3.23: Loss = 0.430695
Epoch 3.24: Loss = 0.406784
Epoch 3.25: Loss = 0.452667
Epoch 3.26: Loss = 0.448395
Epoch 3.27: Loss = 0.417725
Epoch 3.28: Loss = 0.417068
Epoch 3.29: Loss = 0.355865
Epoch 3.30: Loss = 0.510376
Epoch 3.31: Loss = 0.462387
Epoch 3.32: Loss = 0.54512
Epoch 3.33: Loss = 0.397446
Epoch 3.34: Loss = 0.438889
Epoch 3.35: Loss = 0.415466
Epoch 3.36: Loss = 0.452179
Epoch 3.37: Loss = 0.404205
Epoch 3.38: Loss = 0.417343
Epoch 3.39: Loss = 0.513962
Epoch 3.40: Loss = 0.463348
Epoch 3.41: Loss = 0.430038
Epoch 3.42: Loss = 0.42598
Epoch 3.43: Loss = 0.425293
Epoch 3.44: Loss = 0.394073
Epoch 3.45: Loss = 0.435318
Epoch 3.46: Loss = 0.353058
Epoch 3.47: Loss = 0.476212
Epoch 3.48: Loss = 0.478333
Epoch 3.49: Loss = 0.415573
Epoch 3.50: Loss = 0.436752
Epoch 3.51: Loss = 0.385986
Epoch 3.52: Loss = 0.45697
Epoch 3.53: Loss = 0.369736
Epoch 3.54: Loss = 0.473251
Epoch 3.55: Loss = 0.473755
Epoch 3.56: Loss = 0.419495
Epoch 3.57: Loss = 0.498276
Epoch 3.58: Loss = 0.37291
Epoch 3.59: Loss = 0.451981
Epoch 3.60: Loss = 0.355118
Epoch 3.61: Loss = 0.483307
Epoch 3.62: Loss = 0.420151
Epoch 3.63: Loss = 0.415741
Epoch 3.64: Loss = 0.434158
Epoch 3.65: Loss = 0.471725
Epoch 3.66: Loss = 0.454712
Epoch 3.67: Loss = 0.409378
Epoch 3.68: Loss = 0.369034
Epoch 3.69: Loss = 0.432114
Epoch 3.70: Loss = 0.477936
Epoch 3.71: Loss = 0.391769
Epoch 3.72: Loss = 0.485199
Epoch 3.73: Loss = 0.359573
Epoch 3.74: Loss = 0.440735
Epoch 3.75: Loss = 0.384552
Epoch 3.76: Loss = 0.389114
Epoch 3.77: Loss = 0.490952
Epoch 3.78: Loss = 0.449326
Epoch 3.79: Loss = 0.363541
Epoch 3.80: Loss = 0.42601
Epoch 3.81: Loss = 0.371658
Epoch 3.82: Loss = 0.380798
Epoch 3.83: Loss = 0.405212
Epoch 3.84: Loss = 0.410721
Epoch 3.85: Loss = 0.391205
Epoch 3.86: Loss = 0.34787
Epoch 3.87: Loss = 0.422668
Epoch 3.88: Loss = 0.42717
Epoch 3.89: Loss = 0.428467
Epoch 3.90: Loss = 0.336319
Epoch 3.91: Loss = 0.398499
Epoch 3.92: Loss = 0.436462
Epoch 3.93: Loss = 0.386658
Epoch 3.94: Loss = 0.443451
Epoch 3.95: Loss = 0.452393
Epoch 3.96: Loss = 0.31427
Epoch 3.97: Loss = 0.442825
Epoch 3.98: Loss = 0.377762
Epoch 3.99: Loss = 0.389694
Epoch 3.100: Loss = 0.355225
Epoch 3.101: Loss = 0.416916
Epoch 3.102: Loss = 0.411041
Epoch 3.103: Loss = 0.434158
Epoch 3.104: Loss = 0.528946
Epoch 3.105: Loss = 0.456268
Epoch 3.106: Loss = 0.385651
Epoch 3.107: Loss = 0.453049
Epoch 3.108: Loss = 0.395615
Epoch 3.109: Loss = 0.497269
Epoch 3.110: Loss = 0.331467
Epoch 3.111: Loss = 0.494202
Epoch 3.112: Loss = 0.481094
Epoch 3.113: Loss = 0.484909
Epoch 3.114: Loss = 0.436707
Epoch 3.115: Loss = 0.479904
Epoch 3.116: Loss = 0.41066
Epoch 3.117: Loss = 0.35817
Epoch 3.118: Loss = 0.351517
Epoch 3.119: Loss = 0.42897
Epoch 3.120: Loss = 0.357941
TRAIN LOSS = 0.428909
TRAIN ACC = 87.2467 % (52351/60000)
Loss = 0.387207
Loss = 0.475372
Loss = 0.563614
Loss = 0.534668
Loss = 0.576721
Loss = 0.438522
Loss = 0.389954
Loss = 0.604446
Loss = 0.515549
Loss = 0.496292
Loss = 0.184753
Loss = 0.301025
Loss = 0.244781
Loss = 0.386917
Loss = 0.229034
Loss = 0.331009
Loss = 0.227158
Loss = 0.0789185
Loss = 0.256287
Loss = 0.521469
TEST LOSS = 0.387185
TEST ACC = 523.509 % (8848/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.355637
Epoch 4.2: Loss = 0.425156
Epoch 4.3: Loss = 0.433121
Epoch 4.4: Loss = 0.381424
Epoch 4.5: Loss = 0.519302
Epoch 4.6: Loss = 0.483902
Epoch 4.7: Loss = 0.354446
Epoch 4.8: Loss = 0.389816
Epoch 4.9: Loss = 0.311447
Epoch 4.10: Loss = 0.361542
Epoch 4.11: Loss = 0.461334
Epoch 4.12: Loss = 0.362717
Epoch 4.13: Loss = 0.360519
Epoch 4.14: Loss = 0.408157
Epoch 4.15: Loss = 0.415359
Epoch 4.16: Loss = 0.392426
Epoch 4.17: Loss = 0.481888
Epoch 4.18: Loss = 0.43013
Epoch 4.19: Loss = 0.393036
Epoch 4.20: Loss = 0.375153
Epoch 4.21: Loss = 0.471298
Epoch 4.22: Loss = 0.34642
Epoch 4.23: Loss = 0.39267
Epoch 4.24: Loss = 0.355423
Epoch 4.25: Loss = 0.484726
Epoch 4.26: Loss = 0.380142
Epoch 4.27: Loss = 0.409561
Epoch 4.28: Loss = 0.43251
Epoch 4.29: Loss = 0.402496
Epoch 4.30: Loss = 0.493378
Epoch 4.31: Loss = 0.412552
Epoch 4.32: Loss = 0.391525
Epoch 4.33: Loss = 0.48703
Epoch 4.34: Loss = 0.380432
Epoch 4.35: Loss = 0.377975
Epoch 4.36: Loss = 0.332733
Epoch 4.37: Loss = 0.373383
Epoch 4.38: Loss = 0.398178
Epoch 4.39: Loss = 0.390656
Epoch 4.40: Loss = 0.379364
Epoch 4.41: Loss = 0.339371
Epoch 4.42: Loss = 0.378616
Epoch 4.43: Loss = 0.39917
Epoch 4.44: Loss = 0.433014
Epoch 4.45: Loss = 0.425049
Epoch 4.46: Loss = 0.344467
Epoch 4.47: Loss = 0.39801
Epoch 4.48: Loss = 0.426086
Epoch 4.49: Loss = 0.381699
Epoch 4.50: Loss = 0.465149
Epoch 4.51: Loss = 0.433273
Epoch 4.52: Loss = 0.440948
Epoch 4.53: Loss = 0.49379
Epoch 4.54: Loss = 0.517685
Epoch 4.55: Loss = 0.406204
Epoch 4.56: Loss = 0.438477
Epoch 4.57: Loss = 0.47641
Epoch 4.58: Loss = 0.393677
Epoch 4.59: Loss = 0.318054
Epoch 4.60: Loss = 0.363876
Epoch 4.61: Loss = 0.422882
Epoch 4.62: Loss = 0.401825
Epoch 4.63: Loss = 0.424957
Epoch 4.64: Loss = 0.338333
Epoch 4.65: Loss = 0.375549
Epoch 4.66: Loss = 0.39563
Epoch 4.67: Loss = 0.438736
Epoch 4.68: Loss = 0.474442
Epoch 4.69: Loss = 0.39035
Epoch 4.70: Loss = 0.407181
Epoch 4.71: Loss = 0.394852
Epoch 4.72: Loss = 0.433273
Epoch 4.73: Loss = 0.348938
Epoch 4.74: Loss = 0.444717
Epoch 4.75: Loss = 0.455307
Epoch 4.76: Loss = 0.348938
Epoch 4.77: Loss = 0.404678
Epoch 4.78: Loss = 0.407364
Epoch 4.79: Loss = 0.393005
Epoch 4.80: Loss = 0.328156
Epoch 4.81: Loss = 0.385895
Epoch 4.82: Loss = 0.353287
Epoch 4.83: Loss = 0.429825
Epoch 4.84: Loss = 0.473801
Epoch 4.85: Loss = 0.43335
Epoch 4.86: Loss = 0.375168
Epoch 4.87: Loss = 0.336807
Epoch 4.88: Loss = 0.447281
Epoch 4.89: Loss = 0.424118
Epoch 4.90: Loss = 0.472107
Epoch 4.91: Loss = 0.444931
Epoch 4.92: Loss = 0.41008
Epoch 4.93: Loss = 0.307724
Epoch 4.94: Loss = 0.448059
Epoch 4.95: Loss = 0.453903
Epoch 4.96: Loss = 0.389343
Epoch 4.97: Loss = 0.373474
Epoch 4.98: Loss = 0.354095
Epoch 4.99: Loss = 0.329239
Epoch 4.100: Loss = 0.451965
Epoch 4.101: Loss = 0.36496
Epoch 4.102: Loss = 0.391861
Epoch 4.103: Loss = 0.418625
Epoch 4.104: Loss = 0.457367
Epoch 4.105: Loss = 0.406998
Epoch 4.106: Loss = 0.416046
Epoch 4.107: Loss = 0.430481
Epoch 4.108: Loss = 0.375183
Epoch 4.109: Loss = 0.411133
Epoch 4.110: Loss = 0.37413
Epoch 4.111: Loss = 0.390244
Epoch 4.112: Loss = 0.455032
Epoch 4.113: Loss = 0.469772
Epoch 4.114: Loss = 0.383499
Epoch 4.115: Loss = 0.381027
Epoch 4.116: Loss = 0.452713
Epoch 4.117: Loss = 0.432983
Epoch 4.118: Loss = 0.370163
Epoch 4.119: Loss = 0.374207
Epoch 4.120: Loss = 0.447586
TRAIN LOSS = 0.406586
TRAIN ACC = 87.9578 % (52777/60000)
Loss = 0.387558
Loss = 0.468079
Loss = 0.53894
Loss = 0.518555
Loss = 0.586197
Loss = 0.423401
Loss = 0.378082
Loss = 0.626434
Loss = 0.524796
Loss = 0.490417
Loss = 0.176437
Loss = 0.302277
Loss = 0.277283
Loss = 0.359619
Loss = 0.214279
Loss = 0.344543
Loss = 0.251205
Loss = 0.0692291
Loss = 0.243759
Loss = 0.522461
TEST LOSS = 0.385178
TEST ACC = 527.769 % (8877/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.409882
Epoch 5.2: Loss = 0.472137
Epoch 5.3: Loss = 0.404404
Epoch 5.4: Loss = 0.472595
Epoch 5.5: Loss = 0.413681
Epoch 5.6: Loss = 0.377411
Epoch 5.7: Loss = 0.320541
Epoch 5.8: Loss = 0.479904
Epoch 5.9: Loss = 0.380157
Epoch 5.10: Loss = 0.378799
Epoch 5.11: Loss = 0.433212
Epoch 5.12: Loss = 0.376663
Epoch 5.13: Loss = 0.329102
Epoch 5.14: Loss = 0.419022
Epoch 5.15: Loss = 0.404449
Epoch 5.16: Loss = 0.426178
Epoch 5.17: Loss = 0.346252
Epoch 5.18: Loss = 0.432861
Epoch 5.19: Loss = 0.350327
Epoch 5.20: Loss = 0.390747
Epoch 5.21: Loss = 0.527954
Epoch 5.22: Loss = 0.502121
Epoch 5.23: Loss = 0.45163
Epoch 5.24: Loss = 0.374893
Epoch 5.25: Loss = 0.434311
Epoch 5.26: Loss = 0.431839
Epoch 5.27: Loss = 0.540543
Epoch 5.28: Loss = 0.282898
Epoch 5.29: Loss = 0.41301
Epoch 5.30: Loss = 0.368744
Epoch 5.31: Loss = 0.358749
Epoch 5.32: Loss = 0.382202
Epoch 5.33: Loss = 0.401901
Epoch 5.34: Loss = 0.40686
Epoch 5.35: Loss = 0.436935
Epoch 5.36: Loss = 0.429535
Epoch 5.37: Loss = 0.388336
Epoch 5.38: Loss = 0.341446
Epoch 5.39: Loss = 0.420776
Epoch 5.40: Loss = 0.421234
Epoch 5.41: Loss = 0.394073
Epoch 5.42: Loss = 0.410965
Epoch 5.43: Loss = 0.413788
Epoch 5.44: Loss = 0.408524
Epoch 5.45: Loss = 0.413132
Epoch 5.46: Loss = 0.319366
Epoch 5.47: Loss = 0.463623
Epoch 5.48: Loss = 0.417191
Epoch 5.49: Loss = 0.337967
Epoch 5.50: Loss = 0.354248
Epoch 5.51: Loss = 0.387955
Epoch 5.52: Loss = 0.542099
Epoch 5.53: Loss = 0.354324
Epoch 5.54: Loss = 0.45784
Epoch 5.55: Loss = 0.409775
Epoch 5.56: Loss = 0.381073
Epoch 5.57: Loss = 0.395126
Epoch 5.58: Loss = 0.388397
Epoch 5.59: Loss = 0.361664
Epoch 5.60: Loss = 0.452026
Epoch 5.61: Loss = 0.361221
Epoch 5.62: Loss = 0.407791
Epoch 5.63: Loss = 0.350021
Epoch 5.64: Loss = 0.432983
Epoch 5.65: Loss = 0.34845
Epoch 5.66: Loss = 0.424011
Epoch 5.67: Loss = 0.414581
Epoch 5.68: Loss = 0.432022
Epoch 5.69: Loss = 0.418777
Epoch 5.70: Loss = 0.401459
Epoch 5.71: Loss = 0.347519
Epoch 5.72: Loss = 0.427139
Epoch 5.73: Loss = 0.374832
Epoch 5.74: Loss = 0.432449
Epoch 5.75: Loss = 0.348953
Epoch 5.76: Loss = 0.417084
Epoch 5.77: Loss = 0.458221
Epoch 5.78: Loss = 0.316589
Epoch 5.79: Loss = 0.333649
Epoch 5.80: Loss = 0.374359
Epoch 5.81: Loss = 0.377762
Epoch 5.82: Loss = 0.351135
Epoch 5.83: Loss = 0.477264
Epoch 5.84: Loss = 0.323929
Epoch 5.85: Loss = 0.420547
Epoch 5.86: Loss = 0.373077
Epoch 5.87: Loss = 0.398529
Epoch 5.88: Loss = 0.355499
Epoch 5.89: Loss = 0.391098
Epoch 5.90: Loss = 0.35817
Epoch 5.91: Loss = 0.468719
Epoch 5.92: Loss = 0.342758
Epoch 5.93: Loss = 0.360214
Epoch 5.94: Loss = 0.375092
Epoch 5.95: Loss = 0.362732
Epoch 5.96: Loss = 0.315414
Epoch 5.97: Loss = 0.398056
Epoch 5.98: Loss = 0.389847
Epoch 5.99: Loss = 0.336121
Epoch 5.100: Loss = 0.420395
Epoch 5.101: Loss = 0.365097
Epoch 5.102: Loss = 0.388794
Epoch 5.103: Loss = 0.451904
Epoch 5.104: Loss = 0.357254
Epoch 5.105: Loss = 0.427307
Epoch 5.106: Loss = 0.398422
Epoch 5.107: Loss = 0.357101
Epoch 5.108: Loss = 0.349258
Epoch 5.109: Loss = 0.414154
Epoch 5.110: Loss = 0.468597
Epoch 5.111: Loss = 0.442749
Epoch 5.112: Loss = 0.393112
Epoch 5.113: Loss = 0.388412
Epoch 5.114: Loss = 0.524612
Epoch 5.115: Loss = 0.325943
Epoch 5.116: Loss = 0.436417
Epoch 5.117: Loss = 0.367752
Epoch 5.118: Loss = 0.400665
Epoch 5.119: Loss = 0.371246
Epoch 5.120: Loss = 0.305649
TRAIN LOSS = 0.397919
TRAIN ACC = 88.4583 % (53077/60000)
Loss = 0.374786
Loss = 0.459839
Loss = 0.534515
Loss = 0.508926
Loss = 0.558197
Loss = 0.411392
Loss = 0.366257
Loss = 0.621246
Loss = 0.519424
Loss = 0.479187
Loss = 0.179749
Loss = 0.296371
Loss = 0.264847
Loss = 0.352188
Loss = 0.187454
Loss = 0.334045
Loss = 0.254196
Loss = 0.0602875
Loss = 0.251617
Loss = 0.508072
TEST LOSS = 0.37613
TEST ACC = 530.769 % (8933/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.428452
Epoch 6.2: Loss = 0.400589
Epoch 6.3: Loss = 0.397415
Epoch 6.4: Loss = 0.311615
Epoch 6.5: Loss = 0.36528
Epoch 6.6: Loss = 0.352524
Epoch 6.7: Loss = 0.386963
Epoch 6.8: Loss = 0.420715
Epoch 6.9: Loss = 0.472534
Epoch 6.10: Loss = 0.355072
Epoch 6.11: Loss = 0.379288
Epoch 6.12: Loss = 0.397995
Epoch 6.13: Loss = 0.427872
Epoch 6.14: Loss = 0.348053
Epoch 6.15: Loss = 0.339539
Epoch 6.16: Loss = 0.463654
Epoch 6.17: Loss = 0.347031
Epoch 6.18: Loss = 0.419785
Epoch 6.19: Loss = 0.352615
Epoch 6.20: Loss = 0.349304
Epoch 6.21: Loss = 0.420166
Epoch 6.22: Loss = 0.36618
Epoch 6.23: Loss = 0.394073
Epoch 6.24: Loss = 0.410614
Epoch 6.25: Loss = 0.351181
Epoch 6.26: Loss = 0.334198
Epoch 6.27: Loss = 0.384659
Epoch 6.28: Loss = 0.331604
Epoch 6.29: Loss = 0.419067
Epoch 6.30: Loss = 0.40213
Epoch 6.31: Loss = 0.368561
Epoch 6.32: Loss = 0.360077
Epoch 6.33: Loss = 0.44783
Epoch 6.34: Loss = 0.346588
Epoch 6.35: Loss = 0.401474
Epoch 6.36: Loss = 0.423355
Epoch 6.37: Loss = 0.403687
Epoch 6.38: Loss = 0.355515
Epoch 6.39: Loss = 0.396454
Epoch 6.40: Loss = 0.423279
Epoch 6.41: Loss = 0.360641
Epoch 6.42: Loss = 0.391006
Epoch 6.43: Loss = 0.451752
Epoch 6.44: Loss = 0.38855
Epoch 6.45: Loss = 0.464264
Epoch 6.46: Loss = 0.425476
Epoch 6.47: Loss = 0.336273
Epoch 6.48: Loss = 0.401138
Epoch 6.49: Loss = 0.381973
Epoch 6.50: Loss = 0.383636
Epoch 6.51: Loss = 0.5298
Epoch 6.52: Loss = 0.395386
Epoch 6.53: Loss = 0.411255
Epoch 6.54: Loss = 0.428284
Epoch 6.55: Loss = 0.377563
Epoch 6.56: Loss = 0.378464
Epoch 6.57: Loss = 0.429276
Epoch 6.58: Loss = 0.399063
Epoch 6.59: Loss = 0.454803
Epoch 6.60: Loss = 0.350311
Epoch 6.61: Loss = 0.438599
Epoch 6.62: Loss = 0.395493
Epoch 6.63: Loss = 0.502533
Epoch 6.64: Loss = 0.451035
Epoch 6.65: Loss = 0.34375
Epoch 6.66: Loss = 0.368713
Epoch 6.67: Loss = 0.407379
Epoch 6.68: Loss = 0.350922
Epoch 6.69: Loss = 0.391479
Epoch 6.70: Loss = 0.341202
Epoch 6.71: Loss = 0.385788
Epoch 6.72: Loss = 0.344742
Epoch 6.73: Loss = 0.382462
Epoch 6.74: Loss = 0.5728
Epoch 6.75: Loss = 0.333694
Epoch 6.76: Loss = 0.473709
Epoch 6.77: Loss = 0.388412
Epoch 6.78: Loss = 0.472229
Epoch 6.79: Loss = 0.504089
Epoch 6.80: Loss = 0.390045
Epoch 6.81: Loss = 0.384308
Epoch 6.82: Loss = 0.307281
Epoch 6.83: Loss = 0.469559
Epoch 6.84: Loss = 0.341217
Epoch 6.85: Loss = 0.323288
Epoch 6.86: Loss = 0.331512
Epoch 6.87: Loss = 0.353455
Epoch 6.88: Loss = 0.334183
Epoch 6.89: Loss = 0.363007
Epoch 6.90: Loss = 0.436081
Epoch 6.91: Loss = 0.3927
Epoch 6.92: Loss = 0.431564
Epoch 6.93: Loss = 0.474487
Epoch 6.94: Loss = 0.403702
Epoch 6.95: Loss = 0.366943
Epoch 6.96: Loss = 0.414322
Epoch 6.97: Loss = 0.365585
Epoch 6.98: Loss = 0.348267
Epoch 6.99: Loss = 0.363922
Epoch 6.100: Loss = 0.319839
Epoch 6.101: Loss = 0.381271
Epoch 6.102: Loss = 0.463776
Epoch 6.103: Loss = 0.357788
Epoch 6.104: Loss = 0.38121
Epoch 6.105: Loss = 0.373306
Epoch 6.106: Loss = 0.423981
Epoch 6.107: Loss = 0.448578
Epoch 6.108: Loss = 0.414642
Epoch 6.109: Loss = 0.36084
Epoch 6.110: Loss = 0.34761
Epoch 6.111: Loss = 0.44104
Epoch 6.112: Loss = 0.395599
Epoch 6.113: Loss = 0.473831
Epoch 6.114: Loss = 0.316925
Epoch 6.115: Loss = 0.370102
Epoch 6.116: Loss = 0.471237
Epoch 6.117: Loss = 0.298325
Epoch 6.118: Loss = 0.351959
Epoch 6.119: Loss = 0.413727
Epoch 6.120: Loss = 0.359406
TRAIN LOSS = 0.393372
TRAIN ACC = 88.7772 % (53268/60000)
Loss = 0.356735
Loss = 0.442993
Loss = 0.525055
Loss = 0.521759
Loss = 0.557434
Loss = 0.417709
Loss = 0.368927
Loss = 0.637207
Loss = 0.524521
Loss = 0.474808
Loss = 0.173187
Loss = 0.290268
Loss = 0.265808
Loss = 0.344299
Loss = 0.178772
Loss = 0.339096
Loss = 0.225021
Loss = 0.0496063
Loss = 0.244293
Loss = 0.491638
TEST LOSS = 0.371457
TEST ACC = 532.68 % (8976/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.336395
Epoch 7.2: Loss = 0.467621
Epoch 7.3: Loss = 0.468521
Epoch 7.4: Loss = 0.384354
Epoch 7.5: Loss = 0.44252
Epoch 7.6: Loss = 0.421371
Epoch 7.7: Loss = 0.360321
Epoch 7.8: Loss = 0.363144
Epoch 7.9: Loss = 0.383652
Epoch 7.10: Loss = 0.415466
Epoch 7.11: Loss = 0.390579
Epoch 7.12: Loss = 0.360336
Epoch 7.13: Loss = 0.373413
Epoch 7.14: Loss = 0.323151
Epoch 7.15: Loss = 0.40094
Epoch 7.16: Loss = 0.318222
Epoch 7.17: Loss = 0.365265
Epoch 7.18: Loss = 0.417969
Epoch 7.19: Loss = 0.505081
Epoch 7.20: Loss = 0.329346
Epoch 7.21: Loss = 0.374283
Epoch 7.22: Loss = 0.355698
Epoch 7.23: Loss = 0.446014
Epoch 7.24: Loss = 0.34671
Epoch 7.25: Loss = 0.362106
Epoch 7.26: Loss = 0.401199
Epoch 7.27: Loss = 0.417267
Epoch 7.28: Loss = 0.365295
Epoch 7.29: Loss = 0.455978
Epoch 7.30: Loss = 0.415207
Epoch 7.31: Loss = 0.406662
Epoch 7.32: Loss = 0.474655
Epoch 7.33: Loss = 0.356796
Epoch 7.34: Loss = 0.458862
Epoch 7.35: Loss = 0.353241
Epoch 7.36: Loss = 0.287262
Epoch 7.37: Loss = 0.383606
Epoch 7.38: Loss = 0.447296
Epoch 7.39: Loss = 0.379105
Epoch 7.40: Loss = 0.336594
Epoch 7.41: Loss = 0.395157
Epoch 7.42: Loss = 0.392532
Epoch 7.43: Loss = 0.377838
Epoch 7.44: Loss = 0.427399
Epoch 7.45: Loss = 0.426498
Epoch 7.46: Loss = 0.418304
Epoch 7.47: Loss = 0.357529
Epoch 7.48: Loss = 0.359085
Epoch 7.49: Loss = 0.375839
Epoch 7.50: Loss = 0.464859
Epoch 7.51: Loss = 0.335907
Epoch 7.52: Loss = 0.40509
Epoch 7.53: Loss = 0.412216
Epoch 7.54: Loss = 0.391556
Epoch 7.55: Loss = 0.448532
Epoch 7.56: Loss = 0.385651
Epoch 7.57: Loss = 0.4077
Epoch 7.58: Loss = 0.467438
Epoch 7.59: Loss = 0.394745
Epoch 7.60: Loss = 0.351349
Epoch 7.61: Loss = 0.340118
Epoch 7.62: Loss = 0.424561
Epoch 7.63: Loss = 0.434235
Epoch 7.64: Loss = 0.338867
Epoch 7.65: Loss = 0.371536
Epoch 7.66: Loss = 0.396835
Epoch 7.67: Loss = 0.403702
Epoch 7.68: Loss = 0.343811
Epoch 7.69: Loss = 0.527603
Epoch 7.70: Loss = 0.338745
Epoch 7.71: Loss = 0.393143
Epoch 7.72: Loss = 0.386627
Epoch 7.73: Loss = 0.421646
Epoch 7.74: Loss = 0.442047
Epoch 7.75: Loss = 0.372269
Epoch 7.76: Loss = 0.413727
Epoch 7.77: Loss = 0.418396
Epoch 7.78: Loss = 0.438293
Epoch 7.79: Loss = 0.381958
Epoch 7.80: Loss = 0.388626
Epoch 7.81: Loss = 0.33728
Epoch 7.82: Loss = 0.409821
Epoch 7.83: Loss = 0.386093
Epoch 7.84: Loss = 0.4039
Epoch 7.85: Loss = 0.431915
Epoch 7.86: Loss = 0.332932
Epoch 7.87: Loss = 0.36557
Epoch 7.88: Loss = 0.396606
Epoch 7.89: Loss = 0.408188
Epoch 7.90: Loss = 0.368927
Epoch 7.91: Loss = 0.356842
Epoch 7.92: Loss = 0.396866
Epoch 7.93: Loss = 0.365295
Epoch 7.94: Loss = 0.358185
Epoch 7.95: Loss = 0.395554
Epoch 7.96: Loss = 0.368484
Epoch 7.97: Loss = 0.408127
Epoch 7.98: Loss = 0.391434
Epoch 7.99: Loss = 0.340759
Epoch 7.100: Loss = 0.465805
Epoch 7.101: Loss = 0.423019
Epoch 7.102: Loss = 0.412155
Epoch 7.103: Loss = 0.351791
Epoch 7.104: Loss = 0.363388
Epoch 7.105: Loss = 0.312943
Epoch 7.106: Loss = 0.394455
Epoch 7.107: Loss = 0.410156
Epoch 7.108: Loss = 0.367538
Epoch 7.109: Loss = 0.238937
Epoch 7.110: Loss = 0.397919
Epoch 7.111: Loss = 0.447403
Epoch 7.112: Loss = 0.391769
Epoch 7.113: Loss = 0.375275
Epoch 7.114: Loss = 0.29567
Epoch 7.115: Loss = 0.308762
Epoch 7.116: Loss = 0.428818
Epoch 7.117: Loss = 0.328568
Epoch 7.118: Loss = 0.346848
Epoch 7.119: Loss = 0.362259
Epoch 7.120: Loss = 0.425339
TRAIN LOSS = 0.38913
TRAIN ACC = 89.0503 % (53432/60000)
Loss = 0.346237
Loss = 0.431961
Loss = 0.509964
Loss = 0.530289
Loss = 0.561325
Loss = 0.397217
Loss = 0.349091
Loss = 0.644714
Loss = 0.515503
Loss = 0.467163
Loss = 0.149216
Loss = 0.314301
Loss = 0.267334
Loss = 0.344391
Loss = 0.181656
Loss = 0.336685
Loss = 0.212311
Loss = 0.0498199
Loss = 0.22084
Loss = 0.518784
TEST LOSS = 0.36744
TEST ACC = 534.319 % (8992/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.342651
Epoch 8.2: Loss = 0.307678
Epoch 8.3: Loss = 0.539566
Epoch 8.4: Loss = 0.377594
Epoch 8.5: Loss = 0.369476
Epoch 8.6: Loss = 0.351486
Epoch 8.7: Loss = 0.457199
Epoch 8.8: Loss = 0.340302
Epoch 8.9: Loss = 0.372131
Epoch 8.10: Loss = 0.378906
Epoch 8.11: Loss = 0.36409
Epoch 8.12: Loss = 0.317719
Epoch 8.13: Loss = 0.449158
Epoch 8.14: Loss = 0.351349
Epoch 8.15: Loss = 0.359177
Epoch 8.16: Loss = 0.370743
Epoch 8.17: Loss = 0.371078
Epoch 8.18: Loss = 0.358109
Epoch 8.19: Loss = 0.308716
Epoch 8.20: Loss = 0.390228
Epoch 8.21: Loss = 0.364105
Epoch 8.22: Loss = 0.374802
Epoch 8.23: Loss = 0.329956
Epoch 8.24: Loss = 0.386734
Epoch 8.25: Loss = 0.389404
Epoch 8.26: Loss = 0.346527
Epoch 8.27: Loss = 0.313248
Epoch 8.28: Loss = 0.416885
Epoch 8.29: Loss = 0.345963
Epoch 8.30: Loss = 0.446274
Epoch 8.31: Loss = 0.346649
Epoch 8.32: Loss = 0.368896
Epoch 8.33: Loss = 0.415085
Epoch 8.34: Loss = 0.326614
Epoch 8.35: Loss = 0.366241
Epoch 8.36: Loss = 0.367386
Epoch 8.37: Loss = 0.429611
Epoch 8.38: Loss = 0.462479
Epoch 8.39: Loss = 0.373962
Epoch 8.40: Loss = 0.38765
Epoch 8.41: Loss = 0.367966
Epoch 8.42: Loss = 0.352936
Epoch 8.43: Loss = 0.346283
Epoch 8.44: Loss = 0.446167
Epoch 8.45: Loss = 0.424347
Epoch 8.46: Loss = 0.459335
Epoch 8.47: Loss = 0.376251
Epoch 8.48: Loss = 0.497711
Epoch 8.49: Loss = 0.380508
Epoch 8.50: Loss = 0.342834
Epoch 8.51: Loss = 0.389755
Epoch 8.52: Loss = 0.402466
Epoch 8.53: Loss = 0.376724
Epoch 8.54: Loss = 0.315979
Epoch 8.55: Loss = 0.343567
Epoch 8.56: Loss = 0.458572
Epoch 8.57: Loss = 0.339813
Epoch 8.58: Loss = 0.511993
Epoch 8.59: Loss = 0.501541
Epoch 8.60: Loss = 0.339798
Epoch 8.61: Loss = 0.30014
Epoch 8.62: Loss = 0.428558
Epoch 8.63: Loss = 0.354813
Epoch 8.64: Loss = 0.419739
Epoch 8.65: Loss = 0.454834
Epoch 8.66: Loss = 0.414398
Epoch 8.67: Loss = 0.412231
Epoch 8.68: Loss = 0.372833
Epoch 8.69: Loss = 0.388931
Epoch 8.70: Loss = 0.431946
Epoch 8.71: Loss = 0.37677
Epoch 8.72: Loss = 0.348679
Epoch 8.73: Loss = 0.486267
Epoch 8.74: Loss = 0.369278
Epoch 8.75: Loss = 0.460922
Epoch 8.76: Loss = 0.34671
Epoch 8.77: Loss = 0.426544
Epoch 8.78: Loss = 0.329224
Epoch 8.79: Loss = 0.369308
Epoch 8.80: Loss = 0.3974
Epoch 8.81: Loss = 0.499023
Epoch 8.82: Loss = 0.377213
Epoch 8.83: Loss = 0.338501
Epoch 8.84: Loss = 0.400772
Epoch 8.85: Loss = 0.367035
Epoch 8.86: Loss = 0.37709
Epoch 8.87: Loss = 0.321579
Epoch 8.88: Loss = 0.409454
Epoch 8.89: Loss = 0.36618
Epoch 8.90: Loss = 0.422638
Epoch 8.91: Loss = 0.464569
Epoch 8.92: Loss = 0.369522
Epoch 8.93: Loss = 0.341904
Epoch 8.94: Loss = 0.435715
Epoch 8.95: Loss = 0.506653
Epoch 8.96: Loss = 0.443344
Epoch 8.97: Loss = 0.316666
Epoch 8.98: Loss = 0.352509
Epoch 8.99: Loss = 0.431183
Epoch 8.100: Loss = 0.531509
Epoch 8.101: Loss = 0.41011
Epoch 8.102: Loss = 0.399597
Epoch 8.103: Loss = 0.349411
Epoch 8.104: Loss = 0.330627
Epoch 8.105: Loss = 0.379898
Epoch 8.106: Loss = 0.447968
Epoch 8.107: Loss = 0.350967
Epoch 8.108: Loss = 0.446091
Epoch 8.109: Loss = 0.375107
Epoch 8.110: Loss = 0.330673
Epoch 8.111: Loss = 0.425079
Epoch 8.112: Loss = 0.372269
Epoch 8.113: Loss = 0.406799
Epoch 8.114: Loss = 0.347458
Epoch 8.115: Loss = 0.390991
Epoch 8.116: Loss = 0.391235
Epoch 8.117: Loss = 0.521255
Epoch 8.118: Loss = 0.384369
Epoch 8.119: Loss = 0.497299
Epoch 8.120: Loss = 0.382919
TRAIN LOSS = 0.390945
TRAIN ACC = 89.2731 % (53566/60000)
Loss = 0.348663
Loss = 0.439331
Loss = 0.518677
Loss = 0.539642
Loss = 0.563416
Loss = 0.372849
Loss = 0.353226
Loss = 0.632034
Loss = 0.517929
Loss = 0.478363
Loss = 0.166428
Loss = 0.316788
Loss = 0.277573
Loss = 0.334717
Loss = 0.166733
Loss = 0.344482
Loss = 0.202011
Loss = 0.0516663
Loss = 0.219864
Loss = 0.48288
TEST LOSS = 0.366363
TEST ACC = 535.66 % (8984/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.37236
Epoch 9.2: Loss = 0.343491
Epoch 9.3: Loss = 0.443542
Epoch 9.4: Loss = 0.466064
Epoch 9.5: Loss = 0.343079
Epoch 9.6: Loss = 0.334473
Epoch 9.7: Loss = 0.370087
Epoch 9.8: Loss = 0.314835
Epoch 9.9: Loss = 0.473526
Epoch 9.10: Loss = 0.44754
Epoch 9.11: Loss = 0.457565
Epoch 9.12: Loss = 0.353592
Epoch 9.13: Loss = 0.461594
Epoch 9.14: Loss = 0.48288
Epoch 9.15: Loss = 0.431122
Epoch 9.16: Loss = 0.435654
Epoch 9.17: Loss = 0.325546
Epoch 9.18: Loss = 0.401016
Epoch 9.19: Loss = 0.336517
Epoch 9.20: Loss = 0.323883
Epoch 9.21: Loss = 0.340424
Epoch 9.22: Loss = 0.361511
Epoch 9.23: Loss = 0.389053
Epoch 9.24: Loss = 0.429047
Epoch 9.25: Loss = 0.431534
Epoch 9.26: Loss = 0.418701
Epoch 9.27: Loss = 0.470398
Epoch 9.28: Loss = 0.435913
Epoch 9.29: Loss = 0.333954
Epoch 9.30: Loss = 0.401825
Epoch 9.31: Loss = 0.231674
Epoch 9.32: Loss = 0.407547
Epoch 9.33: Loss = 0.423523
Epoch 9.34: Loss = 0.381485
Epoch 9.35: Loss = 0.338486
Epoch 9.36: Loss = 0.53949
Epoch 9.37: Loss = 0.34671
Epoch 9.38: Loss = 0.407349
Epoch 9.39: Loss = 0.429382
Epoch 9.40: Loss = 0.347244
Epoch 9.41: Loss = 0.362411
Epoch 9.42: Loss = 0.412277
Epoch 9.43: Loss = 0.311295
Epoch 9.44: Loss = 0.433655
Epoch 9.45: Loss = 0.400528
Epoch 9.46: Loss = 0.342987
Epoch 9.47: Loss = 0.429703
Epoch 9.48: Loss = 0.306015
Epoch 9.49: Loss = 0.310516
Epoch 9.50: Loss = 0.315018
Epoch 9.51: Loss = 0.404022
Epoch 9.52: Loss = 0.340958
Epoch 9.53: Loss = 0.473083
Epoch 9.54: Loss = 0.323502
Epoch 9.55: Loss = 0.352081
Epoch 9.56: Loss = 0.349365
Epoch 9.57: Loss = 0.486954
Epoch 9.58: Loss = 0.311707
Epoch 9.59: Loss = 0.44313
Epoch 9.60: Loss = 0.377716
Epoch 9.61: Loss = 0.323441
Epoch 9.62: Loss = 0.353149
Epoch 9.63: Loss = 0.411804
Epoch 9.64: Loss = 0.357605
Epoch 9.65: Loss = 0.342636
Epoch 9.66: Loss = 0.346298
Epoch 9.67: Loss = 0.450882
Epoch 9.68: Loss = 0.314957
Epoch 9.69: Loss = 0.451645
Epoch 9.70: Loss = 0.370193
Epoch 9.71: Loss = 0.446609
Epoch 9.72: Loss = 0.355957
Epoch 9.73: Loss = 0.434448
Epoch 9.74: Loss = 0.37294
Epoch 9.75: Loss = 0.454697
Epoch 9.76: Loss = 0.441223
Epoch 9.77: Loss = 0.490204
Epoch 9.78: Loss = 0.477478
Epoch 9.79: Loss = 0.353271
Epoch 9.80: Loss = 0.500381
Epoch 9.81: Loss = 0.402435
Epoch 9.82: Loss = 0.417755
Epoch 9.83: Loss = 0.418198
Epoch 9.84: Loss = 0.420609
Epoch 9.85: Loss = 0.437027
Epoch 9.86: Loss = 0.29306
Epoch 9.87: Loss = 0.402573
Epoch 9.88: Loss = 0.469879
Epoch 9.89: Loss = 0.344528
Epoch 9.90: Loss = 0.3647
Epoch 9.91: Loss = 0.407059
Epoch 9.92: Loss = 0.418686
Epoch 9.93: Loss = 0.372147
Epoch 9.94: Loss = 0.445541
Epoch 9.95: Loss = 0.299103
Epoch 9.96: Loss = 0.43663
Epoch 9.97: Loss = 0.363266
Epoch 9.98: Loss = 0.484924
Epoch 9.99: Loss = 0.37114
Epoch 9.100: Loss = 0.410477
Epoch 9.101: Loss = 0.367935
Epoch 9.102: Loss = 0.352692
Epoch 9.103: Loss = 0.369476
Epoch 9.104: Loss = 0.441254
Epoch 9.105: Loss = 0.392853
Epoch 9.106: Loss = 0.388458
Epoch 9.107: Loss = 0.398941
Epoch 9.108: Loss = 0.363892
Epoch 9.109: Loss = 0.380295
Epoch 9.110: Loss = 0.333832
Epoch 9.111: Loss = 0.483063
Epoch 9.112: Loss = 0.330963
Epoch 9.113: Loss = 0.46579
Epoch 9.114: Loss = 0.441757
Epoch 9.115: Loss = 0.428711
Epoch 9.116: Loss = 0.433426
Epoch 9.117: Loss = 0.372208
Epoch 9.118: Loss = 0.448151
Epoch 9.119: Loss = 0.452148
Epoch 9.120: Loss = 0.485672
TRAIN LOSS = 0.395264
TRAIN ACC = 89.3494 % (53612/60000)
Loss = 0.34877
Loss = 0.448517
Loss = 0.528107
Loss = 0.545197
Loss = 0.576218
Loss = 0.374329
Loss = 0.342728
Loss = 0.658051
Loss = 0.538971
Loss = 0.456741
Loss = 0.150803
Loss = 0.321609
Loss = 0.249863
Loss = 0.358871
Loss = 0.169998
Loss = 0.340378
Loss = 0.22229
Loss = 0.0525665
Loss = 0.242661
Loss = 0.488724
TEST LOSS = 0.370769
TEST ACC = 536.119 % (9014/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.393997
Epoch 10.2: Loss = 0.368973
Epoch 10.3: Loss = 0.36528
Epoch 10.4: Loss = 0.325562
Epoch 10.5: Loss = 0.454483
Epoch 10.6: Loss = 0.453461
Epoch 10.7: Loss = 0.406464
Epoch 10.8: Loss = 0.470169
Epoch 10.9: Loss = 0.444809
Epoch 10.10: Loss = 0.374176
Epoch 10.11: Loss = 0.424088
Epoch 10.12: Loss = 0.39682
Epoch 10.13: Loss = 0.46286
Epoch 10.14: Loss = 0.403122
Epoch 10.15: Loss = 0.395004
Epoch 10.16: Loss = 0.360168
Epoch 10.17: Loss = 0.494995
Epoch 10.18: Loss = 0.375519
Epoch 10.19: Loss = 0.331238
Epoch 10.20: Loss = 0.386978
Epoch 10.21: Loss = 0.338379
Epoch 10.22: Loss = 0.356018
Epoch 10.23: Loss = 0.428833
Epoch 10.24: Loss = 0.329605
Epoch 10.25: Loss = 0.400146
Epoch 10.26: Loss = 0.479736
Epoch 10.27: Loss = 0.394394
Epoch 10.28: Loss = 0.55719
Epoch 10.29: Loss = 0.387772
Epoch 10.30: Loss = 0.368683
Epoch 10.31: Loss = 0.367432
Epoch 10.32: Loss = 0.378113
Epoch 10.33: Loss = 0.316986
Epoch 10.34: Loss = 0.403625
Epoch 10.35: Loss = 0.327911
Epoch 10.36: Loss = 0.41954
Epoch 10.37: Loss = 0.45845
Epoch 10.38: Loss = 0.375015
Epoch 10.39: Loss = 0.491852
Epoch 10.40: Loss = 0.337601
Epoch 10.41: Loss = 0.401657
Epoch 10.42: Loss = 0.300293
Epoch 10.43: Loss = 0.406921
Epoch 10.44: Loss = 0.302521
Epoch 10.45: Loss = 0.437973
Epoch 10.46: Loss = 0.398331
Epoch 10.47: Loss = 0.451599
Epoch 10.48: Loss = 0.372147
Epoch 10.49: Loss = 0.360458
Epoch 10.50: Loss = 0.381897
Epoch 10.51: Loss = 0.468475
Epoch 10.52: Loss = 0.33046
Epoch 10.53: Loss = 0.351349
Epoch 10.54: Loss = 0.423523
Epoch 10.55: Loss = 0.403168
Epoch 10.56: Loss = 0.434189
Epoch 10.57: Loss = 0.400421
Epoch 10.58: Loss = 0.420898
Epoch 10.59: Loss = 0.412247
Epoch 10.60: Loss = 0.315948
Epoch 10.61: Loss = 0.355133
Epoch 10.62: Loss = 0.322235
Epoch 10.63: Loss = 0.340317
Epoch 10.64: Loss = 0.563812
Epoch 10.65: Loss = 0.441193
Epoch 10.66: Loss = 0.447098
Epoch 10.67: Loss = 0.200684
Epoch 10.68: Loss = 0.433777
Epoch 10.69: Loss = 0.53064
Epoch 10.70: Loss = 0.412994
Epoch 10.71: Loss = 0.456848
Epoch 10.72: Loss = 0.323822
Epoch 10.73: Loss = 0.407318
Epoch 10.74: Loss = 0.309555
Epoch 10.75: Loss = 0.28717
Epoch 10.76: Loss = 0.292709
Epoch 10.77: Loss = 0.252228
Epoch 10.78: Loss = 0.460464
Epoch 10.79: Loss = 0.541306
Epoch 10.80: Loss = 0.338531
Epoch 10.81: Loss = 0.366867
Epoch 10.82: Loss = 0.447784
Epoch 10.83: Loss = 0.433273
Epoch 10.84: Loss = 0.317825
Epoch 10.85: Loss = 0.436951
Epoch 10.86: Loss = 0.375381
Epoch 10.87: Loss = 0.365891
Epoch 10.88: Loss = 0.347183
Epoch 10.89: Loss = 0.299149
Epoch 10.90: Loss = 0.410782
Epoch 10.91: Loss = 0.394547
Epoch 10.92: Loss = 0.460587
Epoch 10.93: Loss = 0.432709
Epoch 10.94: Loss = 0.358246
Epoch 10.95: Loss = 0.363403
Epoch 10.96: Loss = 0.440735
Epoch 10.97: Loss = 0.429901
Epoch 10.98: Loss = 0.382111
Epoch 10.99: Loss = 0.457123
Epoch 10.100: Loss = 0.505768
Epoch 10.101: Loss = 0.444077
Epoch 10.102: Loss = 0.416931
Epoch 10.103: Loss = 0.481842
Epoch 10.104: Loss = 0.353973
Epoch 10.105: Loss = 0.335495
Epoch 10.106: Loss = 0.484131
Epoch 10.107: Loss = 0.489868
Epoch 10.108: Loss = 0.41124
Epoch 10.109: Loss = 0.365875
Epoch 10.110: Loss = 0.479385
Epoch 10.111: Loss = 0.435501
Epoch 10.112: Loss = 0.420624
Epoch 10.113: Loss = 0.405701
Epoch 10.114: Loss = 0.343475
Epoch 10.115: Loss = 0.378815
Epoch 10.116: Loss = 0.5401
Epoch 10.117: Loss = 0.397461
Epoch 10.118: Loss = 0.378891
Epoch 10.119: Loss = 0.444473
Epoch 10.120: Loss = 0.321701
TRAIN LOSS = 0.398773
TRAIN ACC = 89.3814 % (53632/60000)
Loss = 0.362
Loss = 0.460495
Loss = 0.533417
Loss = 0.554108
Loss = 0.594086
Loss = 0.40892
Loss = 0.334946
Loss = 0.682281
Loss = 0.55777
Loss = 0.458466
Loss = 0.154694
Loss = 0.317917
Loss = 0.261032
Loss = 0.359207
Loss = 0.166519
Loss = 0.312775
Loss = 0.193481
Loss = 0.0466614
Loss = 0.249786
Loss = 0.485504
TEST LOSS = 0.374703
TEST ACC = 536.319 % (9021/10000)

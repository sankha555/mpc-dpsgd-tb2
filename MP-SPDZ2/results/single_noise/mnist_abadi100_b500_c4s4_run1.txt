Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.37396
Epoch 1.2: Loss = 2.35722
Epoch 1.3: Loss = 2.32518
Epoch 1.4: Loss = 2.26903
Epoch 1.5: Loss = 2.23802
Epoch 1.6: Loss = 2.16776
Epoch 1.7: Loss = 2.14064
Epoch 1.8: Loss = 2.10312
Epoch 1.9: Loss = 2.07965
Epoch 1.10: Loss = 2.04893
Epoch 1.11: Loss = 2.03833
Epoch 1.12: Loss = 1.98906
Epoch 1.13: Loss = 1.96524
Epoch 1.14: Loss = 1.91809
Epoch 1.15: Loss = 1.88416
Epoch 1.16: Loss = 1.88437
Epoch 1.17: Loss = 1.84631
Epoch 1.18: Loss = 1.7924
Epoch 1.19: Loss = 1.76537
Epoch 1.20: Loss = 1.769
Epoch 1.21: Loss = 1.73534
Epoch 1.22: Loss = 1.70303
Epoch 1.23: Loss = 1.65611
Epoch 1.24: Loss = 1.6292
Epoch 1.25: Loss = 1.58514
Epoch 1.26: Loss = 1.59186
Epoch 1.27: Loss = 1.55728
Epoch 1.28: Loss = 1.4969
Epoch 1.29: Loss = 1.51396
Epoch 1.30: Loss = 1.44772
Epoch 1.31: Loss = 1.47301
Epoch 1.32: Loss = 1.41876
Epoch 1.33: Loss = 1.39384
Epoch 1.34: Loss = 1.36325
Epoch 1.35: Loss = 1.33788
Epoch 1.36: Loss = 1.33585
Epoch 1.37: Loss = 1.30246
Epoch 1.38: Loss = 1.33319
Epoch 1.39: Loss = 1.28201
Epoch 1.40: Loss = 1.27615
Epoch 1.41: Loss = 1.22348
Epoch 1.42: Loss = 1.2478
Epoch 1.43: Loss = 1.21912
Epoch 1.44: Loss = 1.16072
Epoch 1.45: Loss = 1.13196
Epoch 1.46: Loss = 1.14648
Epoch 1.47: Loss = 1.07506
Epoch 1.48: Loss = 1.08376
Epoch 1.49: Loss = 1.0674
Epoch 1.50: Loss = 1.0694
Epoch 1.51: Loss = 1.05873
Epoch 1.52: Loss = 1.00876
Epoch 1.53: Loss = 1.01189
Epoch 1.54: Loss = 0.971664
Epoch 1.55: Loss = 0.943054
Epoch 1.56: Loss = 1.00063
Epoch 1.57: Loss = 0.959671
Epoch 1.58: Loss = 0.99147
Epoch 1.59: Loss = 0.972794
Epoch 1.60: Loss = 0.945938
Epoch 1.61: Loss = 0.911652
Epoch 1.62: Loss = 0.929092
Epoch 1.63: Loss = 0.887085
Epoch 1.64: Loss = 0.881882
Epoch 1.65: Loss = 0.820313
Epoch 1.66: Loss = 0.899841
Epoch 1.67: Loss = 0.884415
Epoch 1.68: Loss = 0.886276
Epoch 1.69: Loss = 0.834412
Epoch 1.70: Loss = 0.885605
Epoch 1.71: Loss = 0.815628
Epoch 1.72: Loss = 0.801147
Epoch 1.73: Loss = 0.80043
Epoch 1.74: Loss = 0.822433
Epoch 1.75: Loss = 0.7995
Epoch 1.76: Loss = 0.810913
Epoch 1.77: Loss = 0.791367
Epoch 1.78: Loss = 0.779816
Epoch 1.79: Loss = 0.808151
Epoch 1.80: Loss = 0.748001
Epoch 1.81: Loss = 0.784286
Epoch 1.82: Loss = 0.785721
Epoch 1.83: Loss = 0.761353
Epoch 1.84: Loss = 0.788391
Epoch 1.85: Loss = 0.747955
Epoch 1.86: Loss = 0.807358
Epoch 1.87: Loss = 0.827942
Epoch 1.88: Loss = 0.70546
Epoch 1.89: Loss = 0.740036
Epoch 1.90: Loss = 0.726349
Epoch 1.91: Loss = 0.679474
Epoch 1.92: Loss = 0.693695
Epoch 1.93: Loss = 0.660858
Epoch 1.94: Loss = 0.70813
Epoch 1.95: Loss = 0.691544
Epoch 1.96: Loss = 0.664413
Epoch 1.97: Loss = 0.676193
Epoch 1.98: Loss = 0.65477
Epoch 1.99: Loss = 0.666061
Epoch 1.100: Loss = 0.677399
Epoch 1.101: Loss = 0.696899
Epoch 1.102: Loss = 0.634171
Epoch 1.103: Loss = 0.70871
Epoch 1.104: Loss = 0.644485
Epoch 1.105: Loss = 0.703766
Epoch 1.106: Loss = 0.700134
Epoch 1.107: Loss = 0.691422
Epoch 1.108: Loss = 0.707108
Epoch 1.109: Loss = 0.647385
Epoch 1.110: Loss = 0.670441
Epoch 1.111: Loss = 0.649261
Epoch 1.112: Loss = 0.601822
Epoch 1.113: Loss = 0.704346
Epoch 1.114: Loss = 0.643188
Epoch 1.115: Loss = 0.639236
Epoch 1.116: Loss = 0.617401
Epoch 1.117: Loss = 0.691528
Epoch 1.118: Loss = 0.62381
Epoch 1.119: Loss = 0.710571
Epoch 1.120: Loss = 0.599075
TRAIN LOSS = 1.13509
TRAIN ACC = 68.2449 % (40949/60000)
Loss = 0.638977
Loss = 0.648026
Loss = 0.775543
Loss = 0.71701
Loss = 0.775574
Loss = 0.65744
Loss = 0.622482
Loss = 0.783539
Loss = 0.733353
Loss = 0.698807
Loss = 0.364365
Loss = 0.539398
Loss = 0.399918
Loss = 0.571228
Loss = 0.483841
Loss = 0.495621
Loss = 0.435318
Loss = 0.264267
Loss = 0.435577
Loss = 0.679077
TEST LOSS = 0.585968
TEST ACC = 409.489 % (8315/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.594803
Epoch 2.2: Loss = 0.561874
Epoch 2.3: Loss = 0.568604
Epoch 2.4: Loss = 0.675415
Epoch 2.5: Loss = 0.604584
Epoch 2.6: Loss = 0.601898
Epoch 2.7: Loss = 0.576874
Epoch 2.8: Loss = 0.613953
Epoch 2.9: Loss = 0.615509
Epoch 2.10: Loss = 0.551132
Epoch 2.11: Loss = 0.597336
Epoch 2.12: Loss = 0.577255
Epoch 2.13: Loss = 0.598175
Epoch 2.14: Loss = 0.544815
Epoch 2.15: Loss = 0.635223
Epoch 2.16: Loss = 0.62442
Epoch 2.17: Loss = 0.531586
Epoch 2.18: Loss = 0.634613
Epoch 2.19: Loss = 0.487503
Epoch 2.20: Loss = 0.635376
Epoch 2.21: Loss = 0.5401
Epoch 2.22: Loss = 0.557648
Epoch 2.23: Loss = 0.606995
Epoch 2.24: Loss = 0.552979
Epoch 2.25: Loss = 0.585007
Epoch 2.26: Loss = 0.563644
Epoch 2.27: Loss = 0.567276
Epoch 2.28: Loss = 0.555573
Epoch 2.29: Loss = 0.516495
Epoch 2.30: Loss = 0.552124
Epoch 2.31: Loss = 0.518494
Epoch 2.32: Loss = 0.552643
Epoch 2.33: Loss = 0.532455
Epoch 2.34: Loss = 0.56337
Epoch 2.35: Loss = 0.530731
Epoch 2.36: Loss = 0.545364
Epoch 2.37: Loss = 0.56308
Epoch 2.38: Loss = 0.60791
Epoch 2.39: Loss = 0.488373
Epoch 2.40: Loss = 0.532471
Epoch 2.41: Loss = 0.544281
Epoch 2.42: Loss = 0.536819
Epoch 2.43: Loss = 0.515945
Epoch 2.44: Loss = 0.539734
Epoch 2.45: Loss = 0.556747
Epoch 2.46: Loss = 0.460098
Epoch 2.47: Loss = 0.482193
Epoch 2.48: Loss = 0.535294
Epoch 2.49: Loss = 0.513657
Epoch 2.50: Loss = 0.502457
Epoch 2.51: Loss = 0.560806
Epoch 2.52: Loss = 0.570313
Epoch 2.53: Loss = 0.551163
Epoch 2.54: Loss = 0.569641
Epoch 2.55: Loss = 0.519699
Epoch 2.56: Loss = 0.545319
Epoch 2.57: Loss = 0.438492
Epoch 2.58: Loss = 0.480881
Epoch 2.59: Loss = 0.477005
Epoch 2.60: Loss = 0.53862
Epoch 2.61: Loss = 0.456985
Epoch 2.62: Loss = 0.530594
Epoch 2.63: Loss = 0.515747
Epoch 2.64: Loss = 0.459473
Epoch 2.65: Loss = 0.504425
Epoch 2.66: Loss = 0.647293
Epoch 2.67: Loss = 0.463974
Epoch 2.68: Loss = 0.506973
Epoch 2.69: Loss = 0.490311
Epoch 2.70: Loss = 0.489426
Epoch 2.71: Loss = 0.456741
Epoch 2.72: Loss = 0.495331
Epoch 2.73: Loss = 0.502426
Epoch 2.74: Loss = 0.511185
Epoch 2.75: Loss = 0.550369
Epoch 2.76: Loss = 0.504517
Epoch 2.77: Loss = 0.519485
Epoch 2.78: Loss = 0.426682
Epoch 2.79: Loss = 0.491455
Epoch 2.80: Loss = 0.552475
Epoch 2.81: Loss = 0.48526
Epoch 2.82: Loss = 0.472412
Epoch 2.83: Loss = 0.490845
Epoch 2.84: Loss = 0.45285
Epoch 2.85: Loss = 0.472977
Epoch 2.86: Loss = 0.48291
Epoch 2.87: Loss = 0.50798
Epoch 2.88: Loss = 0.492477
Epoch 2.89: Loss = 0.514709
Epoch 2.90: Loss = 0.505325
Epoch 2.91: Loss = 0.491287
Epoch 2.92: Loss = 0.434464
Epoch 2.93: Loss = 0.462296
Epoch 2.94: Loss = 0.41272
Epoch 2.95: Loss = 0.523682
Epoch 2.96: Loss = 0.413605
Epoch 2.97: Loss = 0.442978
Epoch 2.98: Loss = 0.452118
Epoch 2.99: Loss = 0.492401
Epoch 2.100: Loss = 0.429733
Epoch 2.101: Loss = 0.412781
Epoch 2.102: Loss = 0.488739
Epoch 2.103: Loss = 0.513397
Epoch 2.104: Loss = 0.516434
Epoch 2.105: Loss = 0.492203
Epoch 2.106: Loss = 0.511337
Epoch 2.107: Loss = 0.512604
Epoch 2.108: Loss = 0.508591
Epoch 2.109: Loss = 0.475052
Epoch 2.110: Loss = 0.461731
Epoch 2.111: Loss = 0.447098
Epoch 2.112: Loss = 0.433701
Epoch 2.113: Loss = 0.522919
Epoch 2.114: Loss = 0.481598
Epoch 2.115: Loss = 0.430588
Epoch 2.116: Loss = 0.470337
Epoch 2.117: Loss = 0.481445
Epoch 2.118: Loss = 0.492874
Epoch 2.119: Loss = 0.487549
Epoch 2.120: Loss = 0.495529
TRAIN LOSS = 0.520416
TRAIN ACC = 84.4299 % (50661/60000)
Loss = 0.466705
Loss = 0.506226
Loss = 0.60878
Loss = 0.580917
Loss = 0.629395
Loss = 0.481506
Loss = 0.467468
Loss = 0.664017
Loss = 0.599991
Loss = 0.564484
Loss = 0.232498
Loss = 0.384018
Loss = 0.310822
Loss = 0.430099
Loss = 0.312119
Loss = 0.372116
Loss = 0.292877
Loss = 0.127365
Loss = 0.300446
Loss = 0.561676
TEST LOSS = 0.444676
TEST ACC = 506.609 % (8674/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.426163
Epoch 3.2: Loss = 0.475754
Epoch 3.3: Loss = 0.401932
Epoch 3.4: Loss = 0.446075
Epoch 3.5: Loss = 0.491623
Epoch 3.6: Loss = 0.491211
Epoch 3.7: Loss = 0.487564
Epoch 3.8: Loss = 0.478287
Epoch 3.9: Loss = 0.468002
Epoch 3.10: Loss = 0.459686
Epoch 3.11: Loss = 0.477692
Epoch 3.12: Loss = 0.481796
Epoch 3.13: Loss = 0.50354
Epoch 3.14: Loss = 0.479248
Epoch 3.15: Loss = 0.496689
Epoch 3.16: Loss = 0.39946
Epoch 3.17: Loss = 0.478485
Epoch 3.18: Loss = 0.440323
Epoch 3.19: Loss = 0.487015
Epoch 3.20: Loss = 0.497009
Epoch 3.21: Loss = 0.425919
Epoch 3.22: Loss = 0.451767
Epoch 3.23: Loss = 0.448364
Epoch 3.24: Loss = 0.457642
Epoch 3.25: Loss = 0.457733
Epoch 3.26: Loss = 0.420914
Epoch 3.27: Loss = 0.501709
Epoch 3.28: Loss = 0.433151
Epoch 3.29: Loss = 0.435684
Epoch 3.30: Loss = 0.491943
Epoch 3.31: Loss = 0.507141
Epoch 3.32: Loss = 0.484009
Epoch 3.33: Loss = 0.437851
Epoch 3.34: Loss = 0.477661
Epoch 3.35: Loss = 0.454498
Epoch 3.36: Loss = 0.398209
Epoch 3.37: Loss = 0.472794
Epoch 3.38: Loss = 0.487579
Epoch 3.39: Loss = 0.482437
Epoch 3.40: Loss = 0.460938
Epoch 3.41: Loss = 0.513947
Epoch 3.42: Loss = 0.474777
Epoch 3.43: Loss = 0.441498
Epoch 3.44: Loss = 0.432953
Epoch 3.45: Loss = 0.427231
Epoch 3.46: Loss = 0.421738
Epoch 3.47: Loss = 0.411819
Epoch 3.48: Loss = 0.404022
Epoch 3.49: Loss = 0.411789
Epoch 3.50: Loss = 0.424179
Epoch 3.51: Loss = 0.392273
Epoch 3.52: Loss = 0.488876
Epoch 3.53: Loss = 0.365433
Epoch 3.54: Loss = 0.441269
Epoch 3.55: Loss = 0.353973
Epoch 3.56: Loss = 0.485428
Epoch 3.57: Loss = 0.487762
Epoch 3.58: Loss = 0.45372
Epoch 3.59: Loss = 0.366058
Epoch 3.60: Loss = 0.45224
Epoch 3.61: Loss = 0.442322
Epoch 3.62: Loss = 0.430481
Epoch 3.63: Loss = 0.485107
Epoch 3.64: Loss = 0.455765
Epoch 3.65: Loss = 0.413071
Epoch 3.66: Loss = 0.386993
Epoch 3.67: Loss = 0.429626
Epoch 3.68: Loss = 0.440338
Epoch 3.69: Loss = 0.538483
Epoch 3.70: Loss = 0.442551
Epoch 3.71: Loss = 0.389038
Epoch 3.72: Loss = 0.404465
Epoch 3.73: Loss = 0.455627
Epoch 3.74: Loss = 0.45842
Epoch 3.75: Loss = 0.419998
Epoch 3.76: Loss = 0.460663
Epoch 3.77: Loss = 0.374451
Epoch 3.78: Loss = 0.475372
Epoch 3.79: Loss = 0.429764
Epoch 3.80: Loss = 0.508087
Epoch 3.81: Loss = 0.362579
Epoch 3.82: Loss = 0.391556
Epoch 3.83: Loss = 0.425446
Epoch 3.84: Loss = 0.417114
Epoch 3.85: Loss = 0.400955
Epoch 3.86: Loss = 0.430344
Epoch 3.87: Loss = 0.484146
Epoch 3.88: Loss = 0.486801
Epoch 3.89: Loss = 0.502823
Epoch 3.90: Loss = 0.516586
Epoch 3.91: Loss = 0.547867
Epoch 3.92: Loss = 0.396896
Epoch 3.93: Loss = 0.402328
Epoch 3.94: Loss = 0.428864
Epoch 3.95: Loss = 0.511887
Epoch 3.96: Loss = 0.395905
Epoch 3.97: Loss = 0.457779
Epoch 3.98: Loss = 0.407333
Epoch 3.99: Loss = 0.42836
Epoch 3.100: Loss = 0.365723
Epoch 3.101: Loss = 0.28038
Epoch 3.102: Loss = 0.447678
Epoch 3.103: Loss = 0.459305
Epoch 3.104: Loss = 0.427597
Epoch 3.105: Loss = 0.437042
Epoch 3.106: Loss = 0.426071
Epoch 3.107: Loss = 0.462799
Epoch 3.108: Loss = 0.41037
Epoch 3.109: Loss = 0.479431
Epoch 3.110: Loss = 0.490921
Epoch 3.111: Loss = 0.447464
Epoch 3.112: Loss = 0.413773
Epoch 3.113: Loss = 0.341812
Epoch 3.114: Loss = 0.426804
Epoch 3.115: Loss = 0.434784
Epoch 3.116: Loss = 0.440704
Epoch 3.117: Loss = 0.402176
Epoch 3.118: Loss = 0.350021
Epoch 3.119: Loss = 0.423096
Epoch 3.120: Loss = 0.404953
TRAIN LOSS = 0.443466
TRAIN ACC = 86.6669 % (52002/60000)
Loss = 0.401199
Loss = 0.46492
Loss = 0.568329
Loss = 0.53891
Loss = 0.569336
Loss = 0.429794
Loss = 0.407486
Loss = 0.605194
Loss = 0.538528
Loss = 0.499756
Loss = 0.206512
Loss = 0.334885
Loss = 0.295471
Loss = 0.397232
Loss = 0.26265
Loss = 0.342773
Loss = 0.256805
Loss = 0.0918274
Loss = 0.252716
Loss = 0.532883
TEST LOSS = 0.39986
TEST ACC = 520.02 % (8814/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.32814
Epoch 4.2: Loss = 0.455627
Epoch 4.3: Loss = 0.405136
Epoch 4.4: Loss = 0.429153
Epoch 4.5: Loss = 0.452545
Epoch 4.6: Loss = 0.497208
Epoch 4.7: Loss = 0.387405
Epoch 4.8: Loss = 0.382736
Epoch 4.9: Loss = 0.437317
Epoch 4.10: Loss = 0.333527
Epoch 4.11: Loss = 0.358398
Epoch 4.12: Loss = 0.444534
Epoch 4.13: Loss = 0.357285
Epoch 4.14: Loss = 0.425369
Epoch 4.15: Loss = 0.437943
Epoch 4.16: Loss = 0.433762
Epoch 4.17: Loss = 0.39299
Epoch 4.18: Loss = 0.427979
Epoch 4.19: Loss = 0.515396
Epoch 4.20: Loss = 0.395294
Epoch 4.21: Loss = 0.371429
Epoch 4.22: Loss = 0.474365
Epoch 4.23: Loss = 0.450668
Epoch 4.24: Loss = 0.434265
Epoch 4.25: Loss = 0.449738
Epoch 4.26: Loss = 0.457611
Epoch 4.27: Loss = 0.342041
Epoch 4.28: Loss = 0.410507
Epoch 4.29: Loss = 0.426147
Epoch 4.30: Loss = 0.443161
Epoch 4.31: Loss = 0.447723
Epoch 4.32: Loss = 0.430801
Epoch 4.33: Loss = 0.439606
Epoch 4.34: Loss = 0.32283
Epoch 4.35: Loss = 0.321564
Epoch 4.36: Loss = 0.420197
Epoch 4.37: Loss = 0.407944
Epoch 4.38: Loss = 0.367157
Epoch 4.39: Loss = 0.399124
Epoch 4.40: Loss = 0.442749
Epoch 4.41: Loss = 0.304214
Epoch 4.42: Loss = 0.403915
Epoch 4.43: Loss = 0.514496
Epoch 4.44: Loss = 0.539413
Epoch 4.45: Loss = 0.383789
Epoch 4.46: Loss = 0.425262
Epoch 4.47: Loss = 0.40892
Epoch 4.48: Loss = 0.415939
Epoch 4.49: Loss = 0.422501
Epoch 4.50: Loss = 0.476685
Epoch 4.51: Loss = 0.371979
Epoch 4.52: Loss = 0.356674
Epoch 4.53: Loss = 0.357697
Epoch 4.54: Loss = 0.356033
Epoch 4.55: Loss = 0.402222
Epoch 4.56: Loss = 0.469116
Epoch 4.57: Loss = 0.488663
Epoch 4.58: Loss = 0.452454
Epoch 4.59: Loss = 0.404694
Epoch 4.60: Loss = 0.414749
Epoch 4.61: Loss = 0.461929
Epoch 4.62: Loss = 0.402573
Epoch 4.63: Loss = 0.485611
Epoch 4.64: Loss = 0.400528
Epoch 4.65: Loss = 0.341125
Epoch 4.66: Loss = 0.399368
Epoch 4.67: Loss = 0.421051
Epoch 4.68: Loss = 0.344955
Epoch 4.69: Loss = 0.417572
Epoch 4.70: Loss = 0.4198
Epoch 4.71: Loss = 0.517731
Epoch 4.72: Loss = 0.378922
Epoch 4.73: Loss = 0.391418
Epoch 4.74: Loss = 0.341171
Epoch 4.75: Loss = 0.397812
Epoch 4.76: Loss = 0.470245
Epoch 4.77: Loss = 0.510071
Epoch 4.78: Loss = 0.428299
Epoch 4.79: Loss = 0.504028
Epoch 4.80: Loss = 0.439987
Epoch 4.81: Loss = 0.451675
Epoch 4.82: Loss = 0.346603
Epoch 4.83: Loss = 0.459274
Epoch 4.84: Loss = 0.399261
Epoch 4.85: Loss = 0.3349
Epoch 4.86: Loss = 0.43364
Epoch 4.87: Loss = 0.347794
Epoch 4.88: Loss = 0.331421
Epoch 4.89: Loss = 0.431213
Epoch 4.90: Loss = 0.586777
Epoch 4.91: Loss = 0.411133
Epoch 4.92: Loss = 0.399048
Epoch 4.93: Loss = 0.495132
Epoch 4.94: Loss = 0.352753
Epoch 4.95: Loss = 0.419006
Epoch 4.96: Loss = 0.424576
Epoch 4.97: Loss = 0.395676
Epoch 4.98: Loss = 0.469604
Epoch 4.99: Loss = 0.313965
Epoch 4.100: Loss = 0.348114
Epoch 4.101: Loss = 0.418396
Epoch 4.102: Loss = 0.38623
Epoch 4.103: Loss = 0.356018
Epoch 4.104: Loss = 0.424515
Epoch 4.105: Loss = 0.453979
Epoch 4.106: Loss = 0.399231
Epoch 4.107: Loss = 0.392441
Epoch 4.108: Loss = 0.445389
Epoch 4.109: Loss = 0.373993
Epoch 4.110: Loss = 0.479691
Epoch 4.111: Loss = 0.380371
Epoch 4.112: Loss = 0.439209
Epoch 4.113: Loss = 0.432281
Epoch 4.114: Loss = 0.449677
Epoch 4.115: Loss = 0.386368
Epoch 4.116: Loss = 0.440323
Epoch 4.117: Loss = 0.408066
Epoch 4.118: Loss = 0.444962
Epoch 4.119: Loss = 0.4216
Epoch 4.120: Loss = 0.361984
TRAIN LOSS = 0.415405
TRAIN ACC = 87.7319 % (52641/60000)
Loss = 0.387375
Loss = 0.458939
Loss = 0.554001
Loss = 0.539047
Loss = 0.547256
Loss = 0.396484
Loss = 0.401184
Loss = 0.609619
Loss = 0.525986
Loss = 0.480637
Loss = 0.186234
Loss = 0.310547
Loss = 0.257111
Loss = 0.384964
Loss = 0.236786
Loss = 0.320801
Loss = 0.240479
Loss = 0.0679626
Loss = 0.262939
Loss = 0.53688
TEST LOSS = 0.385261
TEST ACC = 526.408 % (8863/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.437759
Epoch 5.2: Loss = 0.384567
Epoch 5.3: Loss = 0.340515
Epoch 5.4: Loss = 0.473236
Epoch 5.5: Loss = 0.380295
Epoch 5.6: Loss = 0.349884
Epoch 5.7: Loss = 0.382767
Epoch 5.8: Loss = 0.364395
Epoch 5.9: Loss = 0.42955
Epoch 5.10: Loss = 0.296539
Epoch 5.11: Loss = 0.415131
Epoch 5.12: Loss = 0.456879
Epoch 5.13: Loss = 0.367722
Epoch 5.14: Loss = 0.406952
Epoch 5.15: Loss = 0.466019
Epoch 5.16: Loss = 0.39357
Epoch 5.17: Loss = 0.408707
Epoch 5.18: Loss = 0.466019
Epoch 5.19: Loss = 0.374344
Epoch 5.20: Loss = 0.34584
Epoch 5.21: Loss = 0.352066
Epoch 5.22: Loss = 0.355911
Epoch 5.23: Loss = 0.386566
Epoch 5.24: Loss = 0.369949
Epoch 5.25: Loss = 0.435623
Epoch 5.26: Loss = 0.415833
Epoch 5.27: Loss = 0.421768
Epoch 5.28: Loss = 0.447403
Epoch 5.29: Loss = 0.411469
Epoch 5.30: Loss = 0.44545
Epoch 5.31: Loss = 0.432709
Epoch 5.32: Loss = 0.45546
Epoch 5.33: Loss = 0.436661
Epoch 5.34: Loss = 0.394562
Epoch 5.35: Loss = 0.33345
Epoch 5.36: Loss = 0.449158
Epoch 5.37: Loss = 0.355392
Epoch 5.38: Loss = 0.287888
Epoch 5.39: Loss = 0.379044
Epoch 5.40: Loss = 0.363297
Epoch 5.41: Loss = 0.454712
Epoch 5.42: Loss = 0.377838
Epoch 5.43: Loss = 0.359055
Epoch 5.44: Loss = 0.380722
Epoch 5.45: Loss = 0.464966
Epoch 5.46: Loss = 0.392227
Epoch 5.47: Loss = 0.366425
Epoch 5.48: Loss = 0.47879
Epoch 5.49: Loss = 0.365494
Epoch 5.50: Loss = 0.469162
Epoch 5.51: Loss = 0.429947
Epoch 5.52: Loss = 0.357132
Epoch 5.53: Loss = 0.459473
Epoch 5.54: Loss = 0.416428
Epoch 5.55: Loss = 0.429504
Epoch 5.56: Loss = 0.465775
Epoch 5.57: Loss = 0.410614
Epoch 5.58: Loss = 0.348404
Epoch 5.59: Loss = 0.352646
Epoch 5.60: Loss = 0.382339
Epoch 5.61: Loss = 0.381943
Epoch 5.62: Loss = 0.459702
Epoch 5.63: Loss = 0.389648
Epoch 5.64: Loss = 0.346252
Epoch 5.65: Loss = 0.401672
Epoch 5.66: Loss = 0.379608
Epoch 5.67: Loss = 0.385635
Epoch 5.68: Loss = 0.335983
Epoch 5.69: Loss = 0.372955
Epoch 5.70: Loss = 0.397324
Epoch 5.71: Loss = 0.306488
Epoch 5.72: Loss = 0.411514
Epoch 5.73: Loss = 0.50116
Epoch 5.74: Loss = 0.371155
Epoch 5.75: Loss = 0.355682
Epoch 5.76: Loss = 0.50209
Epoch 5.77: Loss = 0.463043
Epoch 5.78: Loss = 0.383804
Epoch 5.79: Loss = 0.447769
Epoch 5.80: Loss = 0.318085
Epoch 5.81: Loss = 0.427872
Epoch 5.82: Loss = 0.42189
Epoch 5.83: Loss = 0.380432
Epoch 5.84: Loss = 0.440399
Epoch 5.85: Loss = 0.348618
Epoch 5.86: Loss = 0.38945
Epoch 5.87: Loss = 0.374344
Epoch 5.88: Loss = 0.403488
Epoch 5.89: Loss = 0.47937
Epoch 5.90: Loss = 0.395218
Epoch 5.91: Loss = 0.423294
Epoch 5.92: Loss = 0.333572
Epoch 5.93: Loss = 0.346832
Epoch 5.94: Loss = 0.357025
Epoch 5.95: Loss = 0.378967
Epoch 5.96: Loss = 0.445267
Epoch 5.97: Loss = 0.451324
Epoch 5.98: Loss = 0.398483
Epoch 5.99: Loss = 0.379562
Epoch 5.100: Loss = 0.402405
Epoch 5.101: Loss = 0.447281
Epoch 5.102: Loss = 0.449585
Epoch 5.103: Loss = 0.416168
Epoch 5.104: Loss = 0.431183
Epoch 5.105: Loss = 0.458511
Epoch 5.106: Loss = 0.392746
Epoch 5.107: Loss = 0.386429
Epoch 5.108: Loss = 0.422882
Epoch 5.109: Loss = 0.404755
Epoch 5.110: Loss = 0.432098
Epoch 5.111: Loss = 0.332916
Epoch 5.112: Loss = 0.375458
Epoch 5.113: Loss = 0.475342
Epoch 5.114: Loss = 0.379837
Epoch 5.115: Loss = 0.455276
Epoch 5.116: Loss = 0.375046
Epoch 5.117: Loss = 0.35379
Epoch 5.118: Loss = 0.315613
Epoch 5.119: Loss = 0.555481
Epoch 5.120: Loss = 0.432938
TRAIN LOSS = 0.401779
TRAIN ACC = 88.3163 % (52993/60000)
Loss = 0.370743
Loss = 0.447815
Loss = 0.558243
Loss = 0.53125
Loss = 0.540878
Loss = 0.381287
Loss = 0.386139
Loss = 0.609467
Loss = 0.515335
Loss = 0.462234
Loss = 0.189926
Loss = 0.33078
Loss = 0.238083
Loss = 0.38031
Loss = 0.214371
Loss = 0.324844
Loss = 0.222076
Loss = 0.0655518
Loss = 0.253784
Loss = 0.512543
TEST LOSS = 0.376783
TEST ACC = 529.929 % (8899/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.399109
Epoch 6.2: Loss = 0.397125
Epoch 6.3: Loss = 0.359253
Epoch 6.4: Loss = 0.337814
Epoch 6.5: Loss = 0.362854
Epoch 6.6: Loss = 0.371323
Epoch 6.7: Loss = 0.406189
Epoch 6.8: Loss = 0.388031
Epoch 6.9: Loss = 0.499435
Epoch 6.10: Loss = 0.37323
Epoch 6.11: Loss = 0.469406
Epoch 6.12: Loss = 0.396957
Epoch 6.13: Loss = 0.358841
Epoch 6.14: Loss = 0.383469
Epoch 6.15: Loss = 0.453873
Epoch 6.16: Loss = 0.374176
Epoch 6.17: Loss = 0.4496
Epoch 6.18: Loss = 0.370651
Epoch 6.19: Loss = 0.331329
Epoch 6.20: Loss = 0.385345
Epoch 6.21: Loss = 0.39566
Epoch 6.22: Loss = 0.32901
Epoch 6.23: Loss = 0.498947
Epoch 6.24: Loss = 0.35379
Epoch 6.25: Loss = 0.415283
Epoch 6.26: Loss = 0.406067
Epoch 6.27: Loss = 0.404297
Epoch 6.28: Loss = 0.342743
Epoch 6.29: Loss = 0.337006
Epoch 6.30: Loss = 0.356415
Epoch 6.31: Loss = 0.466003
Epoch 6.32: Loss = 0.402039
Epoch 6.33: Loss = 0.430389
Epoch 6.34: Loss = 0.423828
Epoch 6.35: Loss = 0.412552
Epoch 6.36: Loss = 0.417282
Epoch 6.37: Loss = 0.382462
Epoch 6.38: Loss = 0.489471
Epoch 6.39: Loss = 0.392853
Epoch 6.40: Loss = 0.408127
Epoch 6.41: Loss = 0.385376
Epoch 6.42: Loss = 0.399902
Epoch 6.43: Loss = 0.368225
Epoch 6.44: Loss = 0.481659
Epoch 6.45: Loss = 0.430542
Epoch 6.46: Loss = 0.332138
Epoch 6.47: Loss = 0.368744
Epoch 6.48: Loss = 0.342239
Epoch 6.49: Loss = 0.476013
Epoch 6.50: Loss = 0.310455
Epoch 6.51: Loss = 0.363693
Epoch 6.52: Loss = 0.410782
Epoch 6.53: Loss = 0.394089
Epoch 6.54: Loss = 0.371475
Epoch 6.55: Loss = 0.541519
Epoch 6.56: Loss = 0.40799
Epoch 6.57: Loss = 0.366058
Epoch 6.58: Loss = 0.398163
Epoch 6.59: Loss = 0.371872
Epoch 6.60: Loss = 0.428223
Epoch 6.61: Loss = 0.401703
Epoch 6.62: Loss = 0.328384
Epoch 6.63: Loss = 0.38826
Epoch 6.64: Loss = 0.524368
Epoch 6.65: Loss = 0.466125
Epoch 6.66: Loss = 0.352066
Epoch 6.67: Loss = 0.405396
Epoch 6.68: Loss = 0.268433
Epoch 6.69: Loss = 0.41925
Epoch 6.70: Loss = 0.369873
Epoch 6.71: Loss = 0.392914
Epoch 6.72: Loss = 0.369461
Epoch 6.73: Loss = 0.301376
Epoch 6.74: Loss = 0.447037
Epoch 6.75: Loss = 0.390457
Epoch 6.76: Loss = 0.421112
Epoch 6.77: Loss = 0.402771
Epoch 6.78: Loss = 0.484558
Epoch 6.79: Loss = 0.425598
Epoch 6.80: Loss = 0.439911
Epoch 6.81: Loss = 0.491776
Epoch 6.82: Loss = 0.325348
Epoch 6.83: Loss = 0.380096
Epoch 6.84: Loss = 0.456497
Epoch 6.85: Loss = 0.404343
Epoch 6.86: Loss = 0.370544
Epoch 6.87: Loss = 0.358826
Epoch 6.88: Loss = 0.402618
Epoch 6.89: Loss = 0.423645
Epoch 6.90: Loss = 0.406693
Epoch 6.91: Loss = 0.350815
Epoch 6.92: Loss = 0.349487
Epoch 6.93: Loss = 0.354401
Epoch 6.94: Loss = 0.42189
Epoch 6.95: Loss = 0.40213
Epoch 6.96: Loss = 0.324905
Epoch 6.97: Loss = 0.4095
Epoch 6.98: Loss = 0.5186
Epoch 6.99: Loss = 0.381271
Epoch 6.100: Loss = 0.35228
Epoch 6.101: Loss = 0.367691
Epoch 6.102: Loss = 0.466537
Epoch 6.103: Loss = 0.471024
Epoch 6.104: Loss = 0.443405
Epoch 6.105: Loss = 0.330978
Epoch 6.106: Loss = 0.399704
Epoch 6.107: Loss = 0.374466
Epoch 6.108: Loss = 0.393509
Epoch 6.109: Loss = 0.326797
Epoch 6.110: Loss = 0.487289
Epoch 6.111: Loss = 0.278305
Epoch 6.112: Loss = 0.322769
Epoch 6.113: Loss = 0.452362
Epoch 6.114: Loss = 0.389252
Epoch 6.115: Loss = 0.389648
Epoch 6.116: Loss = 0.372742
Epoch 6.117: Loss = 0.381439
Epoch 6.118: Loss = 0.468719
Epoch 6.119: Loss = 0.359528
Epoch 6.120: Loss = 0.376007
TRAIN LOSS = 0.39682
TRAIN ACC = 88.6169 % (53172/60000)
Loss = 0.376602
Loss = 0.439392
Loss = 0.548004
Loss = 0.526321
Loss = 0.562637
Loss = 0.38472
Loss = 0.371216
Loss = 0.62709
Loss = 0.528442
Loss = 0.454361
Loss = 0.17514
Loss = 0.339142
Loss = 0.24649
Loss = 0.359589
Loss = 0.203827
Loss = 0.2901
Loss = 0.19812
Loss = 0.0592804
Loss = 0.248688
Loss = 0.532959
TEST LOSS = 0.373606
TEST ACC = 531.718 % (8946/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.31694
Epoch 7.2: Loss = 0.457367
Epoch 7.3: Loss = 0.338974
Epoch 7.4: Loss = 0.386673
Epoch 7.5: Loss = 0.39653
Epoch 7.6: Loss = 0.43161
Epoch 7.7: Loss = 0.403458
Epoch 7.8: Loss = 0.329056
Epoch 7.9: Loss = 0.415375
Epoch 7.10: Loss = 0.321487
Epoch 7.11: Loss = 0.38298
Epoch 7.12: Loss = 0.439804
Epoch 7.13: Loss = 0.400833
Epoch 7.14: Loss = 0.36377
Epoch 7.15: Loss = 0.375854
Epoch 7.16: Loss = 0.450684
Epoch 7.17: Loss = 0.3974
Epoch 7.18: Loss = 0.356781
Epoch 7.19: Loss = 0.376495
Epoch 7.20: Loss = 0.333511
Epoch 7.21: Loss = 0.442001
Epoch 7.22: Loss = 0.339218
Epoch 7.23: Loss = 0.404816
Epoch 7.24: Loss = 0.401978
Epoch 7.25: Loss = 0.503586
Epoch 7.26: Loss = 0.434891
Epoch 7.27: Loss = 0.424576
Epoch 7.28: Loss = 0.362198
Epoch 7.29: Loss = 0.449524
Epoch 7.30: Loss = 0.440491
Epoch 7.31: Loss = 0.388916
Epoch 7.32: Loss = 0.42366
Epoch 7.33: Loss = 0.328384
Epoch 7.34: Loss = 0.347015
Epoch 7.35: Loss = 0.475204
Epoch 7.36: Loss = 0.395203
Epoch 7.37: Loss = 0.342667
Epoch 7.38: Loss = 0.3573
Epoch 7.39: Loss = 0.401932
Epoch 7.40: Loss = 0.330673
Epoch 7.41: Loss = 0.376617
Epoch 7.42: Loss = 0.448792
Epoch 7.43: Loss = 0.323303
Epoch 7.44: Loss = 0.487381
Epoch 7.45: Loss = 0.364609
Epoch 7.46: Loss = 0.472397
Epoch 7.47: Loss = 0.484406
Epoch 7.48: Loss = 0.408966
Epoch 7.49: Loss = 0.416733
Epoch 7.50: Loss = 0.445374
Epoch 7.51: Loss = 0.426605
Epoch 7.52: Loss = 0.371002
Epoch 7.53: Loss = 0.29483
Epoch 7.54: Loss = 0.40007
Epoch 7.55: Loss = 0.383606
Epoch 7.56: Loss = 0.381241
Epoch 7.57: Loss = 0.329803
Epoch 7.58: Loss = 0.333954
Epoch 7.59: Loss = 0.406219
Epoch 7.60: Loss = 0.35672
Epoch 7.61: Loss = 0.289413
Epoch 7.62: Loss = 0.412994
Epoch 7.63: Loss = 0.319061
Epoch 7.64: Loss = 0.352264
Epoch 7.65: Loss = 0.352875
Epoch 7.66: Loss = 0.40155
Epoch 7.67: Loss = 0.412048
Epoch 7.68: Loss = 0.422546
Epoch 7.69: Loss = 0.438553
Epoch 7.70: Loss = 0.322052
Epoch 7.71: Loss = 0.509399
Epoch 7.72: Loss = 0.500046
Epoch 7.73: Loss = 0.357819
Epoch 7.74: Loss = 0.293701
Epoch 7.75: Loss = 0.318085
Epoch 7.76: Loss = 0.358643
Epoch 7.77: Loss = 0.387314
Epoch 7.78: Loss = 0.345047
Epoch 7.79: Loss = 0.518784
Epoch 7.80: Loss = 0.388809
Epoch 7.81: Loss = 0.458023
Epoch 7.82: Loss = 0.490738
Epoch 7.83: Loss = 0.348969
Epoch 7.84: Loss = 0.377594
Epoch 7.85: Loss = 0.350067
Epoch 7.86: Loss = 0.32811
Epoch 7.87: Loss = 0.33905
Epoch 7.88: Loss = 0.34259
Epoch 7.89: Loss = 0.333252
Epoch 7.90: Loss = 0.385895
Epoch 7.91: Loss = 0.365967
Epoch 7.92: Loss = 0.416351
Epoch 7.93: Loss = 0.350998
Epoch 7.94: Loss = 0.386719
Epoch 7.95: Loss = 0.437119
Epoch 7.96: Loss = 0.41127
Epoch 7.97: Loss = 0.414185
Epoch 7.98: Loss = 0.341873
Epoch 7.99: Loss = 0.412781
Epoch 7.100: Loss = 0.452301
Epoch 7.101: Loss = 0.401215
Epoch 7.102: Loss = 0.416672
Epoch 7.103: Loss = 0.403061
Epoch 7.104: Loss = 0.418976
Epoch 7.105: Loss = 0.469788
Epoch 7.106: Loss = 0.392776
Epoch 7.107: Loss = 0.532272
Epoch 7.108: Loss = 0.377136
Epoch 7.109: Loss = 0.386383
Epoch 7.110: Loss = 0.438599
Epoch 7.111: Loss = 0.484741
Epoch 7.112: Loss = 0.334152
Epoch 7.113: Loss = 0.426056
Epoch 7.114: Loss = 0.364578
Epoch 7.115: Loss = 0.371048
Epoch 7.116: Loss = 0.354324
Epoch 7.117: Loss = 0.428772
Epoch 7.118: Loss = 0.389069
Epoch 7.119: Loss = 0.354553
Epoch 7.120: Loss = 0.320694
TRAIN LOSS = 0.392426
TRAIN ACC = 89.0869 % (53455/60000)
Loss = 0.364288
Loss = 0.44313
Loss = 0.550171
Loss = 0.53215
Loss = 0.554016
Loss = 0.404861
Loss = 0.375366
Loss = 0.635315
Loss = 0.531952
Loss = 0.454514
Loss = 0.171814
Loss = 0.316635
Loss = 0.229462
Loss = 0.373917
Loss = 0.200333
Loss = 0.299728
Loss = 0.20517
Loss = 0.0488434
Loss = 0.25148
Loss = 0.532425
TEST LOSS = 0.373778
TEST ACC = 534.549 % (8963/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.426575
Epoch 8.2: Loss = 0.384918
Epoch 8.3: Loss = 0.364304
Epoch 8.4: Loss = 0.441254
Epoch 8.5: Loss = 0.389709
Epoch 8.6: Loss = 0.34964
Epoch 8.7: Loss = 0.36441
Epoch 8.8: Loss = 0.42955
Epoch 8.9: Loss = 0.341064
Epoch 8.10: Loss = 0.391434
Epoch 8.11: Loss = 0.389923
Epoch 8.12: Loss = 0.42041
Epoch 8.13: Loss = 0.376083
Epoch 8.14: Loss = 0.365753
Epoch 8.15: Loss = 0.320724
Epoch 8.16: Loss = 0.35495
Epoch 8.17: Loss = 0.288834
Epoch 8.18: Loss = 0.335846
Epoch 8.19: Loss = 0.319916
Epoch 8.20: Loss = 0.324753
Epoch 8.21: Loss = 0.322891
Epoch 8.22: Loss = 0.354706
Epoch 8.23: Loss = 0.35257
Epoch 8.24: Loss = 0.43399
Epoch 8.25: Loss = 0.361694
Epoch 8.26: Loss = 0.398148
Epoch 8.27: Loss = 0.415054
Epoch 8.28: Loss = 0.468842
Epoch 8.29: Loss = 0.4384
Epoch 8.30: Loss = 0.451782
Epoch 8.31: Loss = 0.363968
Epoch 8.32: Loss = 0.51799
Epoch 8.33: Loss = 0.428726
Epoch 8.34: Loss = 0.373489
Epoch 8.35: Loss = 0.366135
Epoch 8.36: Loss = 0.549164
Epoch 8.37: Loss = 0.343796
Epoch 8.38: Loss = 0.481506
Epoch 8.39: Loss = 0.278671
Epoch 8.40: Loss = 0.410477
Epoch 8.41: Loss = 0.333405
Epoch 8.42: Loss = 0.370697
Epoch 8.43: Loss = 0.361496
Epoch 8.44: Loss = 0.402771
Epoch 8.45: Loss = 0.339447
Epoch 8.46: Loss = 0.370514
Epoch 8.47: Loss = 0.432892
Epoch 8.48: Loss = 0.406204
Epoch 8.49: Loss = 0.365051
Epoch 8.50: Loss = 0.470078
Epoch 8.51: Loss = 0.398056
Epoch 8.52: Loss = 0.359177
Epoch 8.53: Loss = 0.331207
Epoch 8.54: Loss = 0.360687
Epoch 8.55: Loss = 0.314362
Epoch 8.56: Loss = 0.376846
Epoch 8.57: Loss = 0.436584
Epoch 8.58: Loss = 0.436127
Epoch 8.59: Loss = 0.332565
Epoch 8.60: Loss = 0.463303
Epoch 8.61: Loss = 0.529968
Epoch 8.62: Loss = 0.395477
Epoch 8.63: Loss = 0.474213
Epoch 8.64: Loss = 0.276016
Epoch 8.65: Loss = 0.347229
Epoch 8.66: Loss = 0.353851
Epoch 8.67: Loss = 0.472092
Epoch 8.68: Loss = 0.417404
Epoch 8.69: Loss = 0.310364
Epoch 8.70: Loss = 0.401871
Epoch 8.71: Loss = 0.525284
Epoch 8.72: Loss = 0.31543
Epoch 8.73: Loss = 0.412552
Epoch 8.74: Loss = 0.48674
Epoch 8.75: Loss = 0.43277
Epoch 8.76: Loss = 0.429733
Epoch 8.77: Loss = 0.373474
Epoch 8.78: Loss = 0.366058
Epoch 8.79: Loss = 0.434738
Epoch 8.80: Loss = 0.439026
Epoch 8.81: Loss = 0.408218
Epoch 8.82: Loss = 0.378418
Epoch 8.83: Loss = 0.380264
Epoch 8.84: Loss = 0.386353
Epoch 8.85: Loss = 0.317825
Epoch 8.86: Loss = 0.352448
Epoch 8.87: Loss = 0.41655
Epoch 8.88: Loss = 0.395157
Epoch 8.89: Loss = 0.35347
Epoch 8.90: Loss = 0.466187
Epoch 8.91: Loss = 0.374969
Epoch 8.92: Loss = 0.376801
Epoch 8.93: Loss = 0.394287
Epoch 8.94: Loss = 0.406403
Epoch 8.95: Loss = 0.472748
Epoch 8.96: Loss = 0.400803
Epoch 8.97: Loss = 0.399414
Epoch 8.98: Loss = 0.375412
Epoch 8.99: Loss = 0.370041
Epoch 8.100: Loss = 0.428741
Epoch 8.101: Loss = 0.45256
Epoch 8.102: Loss = 0.398453
Epoch 8.103: Loss = 0.409851
Epoch 8.104: Loss = 0.367859
Epoch 8.105: Loss = 0.421814
Epoch 8.106: Loss = 0.351791
Epoch 8.107: Loss = 0.371155
Epoch 8.108: Loss = 0.453186
Epoch 8.109: Loss = 0.302673
Epoch 8.110: Loss = 0.329178
Epoch 8.111: Loss = 0.376389
Epoch 8.112: Loss = 0.347198
Epoch 8.113: Loss = 0.356812
Epoch 8.114: Loss = 0.429916
Epoch 8.115: Loss = 0.435211
Epoch 8.116: Loss = 0.396713
Epoch 8.117: Loss = 0.339813
Epoch 8.118: Loss = 0.422592
Epoch 8.119: Loss = 0.363419
Epoch 8.120: Loss = 0.42247
TRAIN LOSS = 0.391251
TRAIN ACC = 89.151 % (53493/60000)
Loss = 0.365143
Loss = 0.470078
Loss = 0.557663
Loss = 0.54393
Loss = 0.555954
Loss = 0.399551
Loss = 0.372391
Loss = 0.608444
Loss = 0.527817
Loss = 0.451447
Loss = 0.164703
Loss = 0.311707
Loss = 0.221573
Loss = 0.356354
Loss = 0.193085
Loss = 0.299408
Loss = 0.206192
Loss = 0.0471649
Loss = 0.247375
Loss = 0.528183
TEST LOSS = 0.371408
TEST ACC = 534.929 % (8966/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.371063
Epoch 9.2: Loss = 0.352081
Epoch 9.3: Loss = 0.375824
Epoch 9.4: Loss = 0.547577
Epoch 9.5: Loss = 0.55072
Epoch 9.6: Loss = 0.360764
Epoch 9.7: Loss = 0.40004
Epoch 9.8: Loss = 0.373215
Epoch 9.9: Loss = 0.414444
Epoch 9.10: Loss = 0.377686
Epoch 9.11: Loss = 0.38501
Epoch 9.12: Loss = 0.368332
Epoch 9.13: Loss = 0.445313
Epoch 9.14: Loss = 0.30542
Epoch 9.15: Loss = 0.361832
Epoch 9.16: Loss = 0.356949
Epoch 9.17: Loss = 0.403625
Epoch 9.18: Loss = 0.466614
Epoch 9.19: Loss = 0.443451
Epoch 9.20: Loss = 0.455887
Epoch 9.21: Loss = 0.358017
Epoch 9.22: Loss = 0.365997
Epoch 9.23: Loss = 0.445541
Epoch 9.24: Loss = 0.431305
Epoch 9.25: Loss = 0.429474
Epoch 9.26: Loss = 0.339722
Epoch 9.27: Loss = 0.328918
Epoch 9.28: Loss = 0.424454
Epoch 9.29: Loss = 0.500153
Epoch 9.30: Loss = 0.411087
Epoch 9.31: Loss = 0.368149
Epoch 9.32: Loss = 0.489105
Epoch 9.33: Loss = 0.284424
Epoch 9.34: Loss = 0.368317
Epoch 9.35: Loss = 0.366806
Epoch 9.36: Loss = 0.427933
Epoch 9.37: Loss = 0.46373
Epoch 9.38: Loss = 0.372925
Epoch 9.39: Loss = 0.450989
Epoch 9.40: Loss = 0.38501
Epoch 9.41: Loss = 0.555542
Epoch 9.42: Loss = 0.333267
Epoch 9.43: Loss = 0.320465
Epoch 9.44: Loss = 0.357498
Epoch 9.45: Loss = 0.407684
Epoch 9.46: Loss = 0.377289
Epoch 9.47: Loss = 0.406662
Epoch 9.48: Loss = 0.36026
Epoch 9.49: Loss = 0.30835
Epoch 9.50: Loss = 0.331543
Epoch 9.51: Loss = 0.358444
Epoch 9.52: Loss = 0.409363
Epoch 9.53: Loss = 0.331314
Epoch 9.54: Loss = 0.339706
Epoch 9.55: Loss = 0.44046
Epoch 9.56: Loss = 0.365311
Epoch 9.57: Loss = 0.362137
Epoch 9.58: Loss = 0.409302
Epoch 9.59: Loss = 0.5858
Epoch 9.60: Loss = 0.435654
Epoch 9.61: Loss = 0.304581
Epoch 9.62: Loss = 0.373276
Epoch 9.63: Loss = 0.353424
Epoch 9.64: Loss = 0.409668
Epoch 9.65: Loss = 0.41864
Epoch 9.66: Loss = 0.40062
Epoch 9.67: Loss = 0.383621
Epoch 9.68: Loss = 0.389893
Epoch 9.69: Loss = 0.297699
Epoch 9.70: Loss = 0.319656
Epoch 9.71: Loss = 0.327988
Epoch 9.72: Loss = 0.412521
Epoch 9.73: Loss = 0.373108
Epoch 9.74: Loss = 0.416412
Epoch 9.75: Loss = 0.446289
Epoch 9.76: Loss = 0.510208
Epoch 9.77: Loss = 0.332565
Epoch 9.78: Loss = 0.5121
Epoch 9.79: Loss = 0.398804
Epoch 9.80: Loss = 0.342819
Epoch 9.81: Loss = 0.395782
Epoch 9.82: Loss = 0.49382
Epoch 9.83: Loss = 0.341476
Epoch 9.84: Loss = 0.429459
Epoch 9.85: Loss = 0.337021
Epoch 9.86: Loss = 0.422058
Epoch 9.87: Loss = 0.418472
Epoch 9.88: Loss = 0.292755
Epoch 9.89: Loss = 0.361618
Epoch 9.90: Loss = 0.443878
Epoch 9.91: Loss = 0.398468
Epoch 9.92: Loss = 0.351089
Epoch 9.93: Loss = 0.434174
Epoch 9.94: Loss = 0.377335
Epoch 9.95: Loss = 0.40831
Epoch 9.96: Loss = 0.313843
Epoch 9.97: Loss = 0.409561
Epoch 9.98: Loss = 0.270447
Epoch 9.99: Loss = 0.44838
Epoch 9.100: Loss = 0.54129
Epoch 9.101: Loss = 0.33226
Epoch 9.102: Loss = 0.423218
Epoch 9.103: Loss = 0.360992
Epoch 9.104: Loss = 0.377808
Epoch 9.105: Loss = 0.430222
Epoch 9.106: Loss = 0.486038
Epoch 9.107: Loss = 0.25795
Epoch 9.108: Loss = 0.358871
Epoch 9.109: Loss = 0.465408
Epoch 9.110: Loss = 0.555786
Epoch 9.111: Loss = 0.332809
Epoch 9.112: Loss = 0.451401
Epoch 9.113: Loss = 0.290298
Epoch 9.114: Loss = 0.334518
Epoch 9.115: Loss = 0.390915
Epoch 9.116: Loss = 0.367752
Epoch 9.117: Loss = 0.327927
Epoch 9.118: Loss = 0.434052
Epoch 9.119: Loss = 0.313034
Epoch 9.120: Loss = 0.304199
TRAIN LOSS = 0.392761
TRAIN ACC = 89.3143 % (53591/60000)
Loss = 0.376831
Loss = 0.490021
Loss = 0.564575
Loss = 0.563477
Loss = 0.592499
Loss = 0.390228
Loss = 0.392868
Loss = 0.635406
Loss = 0.561615
Loss = 0.474609
Loss = 0.153564
Loss = 0.286987
Loss = 0.22406
Loss = 0.364929
Loss = 0.194763
Loss = 0.283279
Loss = 0.202255
Loss = 0.0403137
Loss = 0.227356
Loss = 0.529282
TEST LOSS = 0.377446
TEST ACC = 535.909 % (8959/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.410172
Epoch 10.2: Loss = 0.372803
Epoch 10.3: Loss = 0.433609
Epoch 10.4: Loss = 0.357605
Epoch 10.5: Loss = 0.402618
Epoch 10.6: Loss = 0.371658
Epoch 10.7: Loss = 0.453339
Epoch 10.8: Loss = 0.432007
Epoch 10.9: Loss = 0.446533
Epoch 10.10: Loss = 0.420197
Epoch 10.11: Loss = 0.400436
Epoch 10.12: Loss = 0.426086
Epoch 10.13: Loss = 0.420151
Epoch 10.14: Loss = 0.33522
Epoch 10.15: Loss = 0.448318
Epoch 10.16: Loss = 0.409012
Epoch 10.17: Loss = 0.33046
Epoch 10.18: Loss = 0.416977
Epoch 10.19: Loss = 0.588531
Epoch 10.20: Loss = 0.27652
Epoch 10.21: Loss = 0.372604
Epoch 10.22: Loss = 0.475433
Epoch 10.23: Loss = 0.381683
Epoch 10.24: Loss = 0.513458
Epoch 10.25: Loss = 0.383072
Epoch 10.26: Loss = 0.423065
Epoch 10.27: Loss = 0.425842
Epoch 10.28: Loss = 0.290268
Epoch 10.29: Loss = 0.375626
Epoch 10.30: Loss = 0.347244
Epoch 10.31: Loss = 0.379791
Epoch 10.32: Loss = 0.322021
Epoch 10.33: Loss = 0.468109
Epoch 10.34: Loss = 0.32988
Epoch 10.35: Loss = 0.429169
Epoch 10.36: Loss = 0.465759
Epoch 10.37: Loss = 0.358002
Epoch 10.38: Loss = 0.389374
Epoch 10.39: Loss = 0.45079
Epoch 10.40: Loss = 0.390137
Epoch 10.41: Loss = 0.422089
Epoch 10.42: Loss = 0.357025
Epoch 10.43: Loss = 0.47319
Epoch 10.44: Loss = 0.408432
Epoch 10.45: Loss = 0.42749
Epoch 10.46: Loss = 0.364716
Epoch 10.47: Loss = 0.399551
Epoch 10.48: Loss = 0.368195
Epoch 10.49: Loss = 0.346069
Epoch 10.50: Loss = 0.341599
Epoch 10.51: Loss = 0.363464
Epoch 10.52: Loss = 0.27356
Epoch 10.53: Loss = 0.317902
Epoch 10.54: Loss = 0.444839
Epoch 10.55: Loss = 0.396515
Epoch 10.56: Loss = 0.454895
Epoch 10.57: Loss = 0.426132
Epoch 10.58: Loss = 0.329086
Epoch 10.59: Loss = 0.396667
Epoch 10.60: Loss = 0.387238
Epoch 10.61: Loss = 0.363388
Epoch 10.62: Loss = 0.422577
Epoch 10.63: Loss = 0.422409
Epoch 10.64: Loss = 0.386337
Epoch 10.65: Loss = 0.498825
Epoch 10.66: Loss = 0.369492
Epoch 10.67: Loss = 0.395706
Epoch 10.68: Loss = 0.398407
Epoch 10.69: Loss = 0.436783
Epoch 10.70: Loss = 0.460068
Epoch 10.71: Loss = 0.503967
Epoch 10.72: Loss = 0.327499
Epoch 10.73: Loss = 0.361771
Epoch 10.74: Loss = 0.45491
Epoch 10.75: Loss = 0.360931
Epoch 10.76: Loss = 0.443939
Epoch 10.77: Loss = 0.413055
Epoch 10.78: Loss = 0.438522
Epoch 10.79: Loss = 0.360397
Epoch 10.80: Loss = 0.353622
Epoch 10.81: Loss = 0.285553
Epoch 10.82: Loss = 0.397293
Epoch 10.83: Loss = 0.35881
Epoch 10.84: Loss = 0.524567
Epoch 10.85: Loss = 0.25592
Epoch 10.86: Loss = 0.357468
Epoch 10.87: Loss = 0.431503
Epoch 10.88: Loss = 0.380646
Epoch 10.89: Loss = 0.303619
Epoch 10.90: Loss = 0.393036
Epoch 10.91: Loss = 0.519333
Epoch 10.92: Loss = 0.387741
Epoch 10.93: Loss = 0.395401
Epoch 10.94: Loss = 0.46933
Epoch 10.95: Loss = 0.387177
Epoch 10.96: Loss = 0.472382
Epoch 10.97: Loss = 0.384399
Epoch 10.98: Loss = 0.408844
Epoch 10.99: Loss = 0.44075
Epoch 10.100: Loss = 0.412109
Epoch 10.101: Loss = 0.453766
Epoch 10.102: Loss = 0.355515
Epoch 10.103: Loss = 0.407471
Epoch 10.104: Loss = 0.422836
Epoch 10.105: Loss = 0.448105
Epoch 10.106: Loss = 0.41922
Epoch 10.107: Loss = 0.483383
Epoch 10.108: Loss = 0.375427
Epoch 10.109: Loss = 0.266312
Epoch 10.110: Loss = 0.362793
Epoch 10.111: Loss = 0.337677
Epoch 10.112: Loss = 0.41185
Epoch 10.113: Loss = 0.313446
Epoch 10.114: Loss = 0.350403
Epoch 10.115: Loss = 0.336105
Epoch 10.116: Loss = 0.455338
Epoch 10.117: Loss = 0.357178
Epoch 10.118: Loss = 0.357269
Epoch 10.119: Loss = 0.304092
Epoch 10.120: Loss = 0.415909
TRAIN LOSS = 0.396622
TRAIN ACC = 89.2975 % (53581/60000)
Loss = 0.368134
Loss = 0.50647
Loss = 0.586136
Loss = 0.557953
Loss = 0.615967
Loss = 0.401642
Loss = 0.388077
Loss = 0.6353
Loss = 0.588654
Loss = 0.474274
Loss = 0.151352
Loss = 0.291656
Loss = 0.233582
Loss = 0.353806
Loss = 0.188065
Loss = 0.258057
Loss = 0.201065
Loss = 0.0402527
Loss = 0.242844
Loss = 0.575424
TEST LOSS = 0.382935
TEST ACC = 535.809 % (8964/10000)

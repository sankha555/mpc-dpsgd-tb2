Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 2000
Num Epochs: 10
Learning Rate: 0.4 to 0.4 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.49466
Epoch 1.2: Loss = 2.12341
Epoch 1.3: Loss = 1.83261
Epoch 1.4: Loss = 1.64247
Epoch 1.5: Loss = 1.49274
Epoch 1.6: Loss = 1.39928
Epoch 1.7: Loss = 1.24998
Epoch 1.8: Loss = 1.20634
Epoch 1.9: Loss = 1.14754
Epoch 1.10: Loss = 1.06998
Epoch 1.11: Loss = 1.07059
Epoch 1.12: Loss = 0.979492
Epoch 1.13: Loss = 0.939682
Epoch 1.14: Loss = 0.917419
Epoch 1.15: Loss = 0.91362
Epoch 1.16: Loss = 0.878693
Epoch 1.17: Loss = 0.866699
Epoch 1.18: Loss = 0.852264
Epoch 1.19: Loss = 0.885605
Epoch 1.20: Loss = 0.807343
Epoch 1.21: Loss = 0.79451
Epoch 1.22: Loss = 0.818176
Epoch 1.23: Loss = 0.780441
Epoch 1.24: Loss = 0.781906
Epoch 1.25: Loss = 0.780319
Epoch 1.26: Loss = 0.779327
Epoch 1.27: Loss = 0.782181
Epoch 1.28: Loss = 0.761993
Epoch 1.29: Loss = 0.730453
Epoch 1.30: Loss = 0.719009
TRAIN LOSS = 1.0833
TRAIN ACC = 63.2141 % (37930/60000)
Loss = 0.740051
Loss = 0.799591
Loss = 0.753525
Loss = 0.7612
Loss = 0.751526
TEST LOSS = 0.761178
TEST ACC = 379.3 % (7293/10000)
Reducing learning rate to 0.399994
Epoch 2.1: Loss = 0.709091
Epoch 2.2: Loss = 0.741592
Epoch 2.3: Loss = 0.726242
Epoch 2.4: Loss = 0.696472
Epoch 2.5: Loss = 0.714615
Epoch 2.6: Loss = 0.705643
Epoch 2.7: Loss = 0.703888
Epoch 2.8: Loss = 0.729645
Epoch 2.9: Loss = 0.7052
Epoch 2.10: Loss = 0.741241
Epoch 2.11: Loss = 0.698517
Epoch 2.12: Loss = 0.720261
Epoch 2.13: Loss = 0.698822
Epoch 2.14: Loss = 0.678238
Epoch 2.15: Loss = 0.68222
Epoch 2.16: Loss = 0.672943
Epoch 2.17: Loss = 0.663742
Epoch 2.18: Loss = 0.638214
Epoch 2.19: Loss = 0.63858
Epoch 2.20: Loss = 0.686768
Epoch 2.21: Loss = 0.657822
Epoch 2.22: Loss = 0.695816
Epoch 2.23: Loss = 0.688812
Epoch 2.24: Loss = 0.636871
Epoch 2.25: Loss = 0.668716
Epoch 2.26: Loss = 0.659958
Epoch 2.27: Loss = 0.66687
Epoch 2.28: Loss = 0.703171
Epoch 2.29: Loss = 0.609421
Epoch 2.30: Loss = 0.671753
TRAIN LOSS = 0.687057
TRAIN ACC = 76.5015 % (45903/60000)
Loss = 0.65126
Loss = 0.716476
Loss = 0.687698
Loss = 0.686905
Loss = 0.671158
TEST LOSS = 0.682699
TEST ACC = 459.029 % (7675/10000)
Reducing learning rate to 0.399994
Epoch 3.1: Loss = 0.676727
Epoch 3.2: Loss = 0.636581
Epoch 3.3: Loss = 0.627914
Epoch 3.4: Loss = 0.686554
Epoch 3.5: Loss = 0.626617
Epoch 3.6: Loss = 0.663757
Epoch 3.7: Loss = 0.689911
Epoch 3.8: Loss = 0.691254
Epoch 3.9: Loss = 0.631882
Epoch 3.10: Loss = 0.637115
Epoch 3.11: Loss = 0.623459
Epoch 3.12: Loss = 0.593521
Epoch 3.13: Loss = 0.619858
Epoch 3.14: Loss = 0.609711
Epoch 3.15: Loss = 0.609726
Epoch 3.16: Loss = 0.668411
Epoch 3.17: Loss = 0.599594
Epoch 3.18: Loss = 0.590363
Epoch 3.19: Loss = 0.660568
Epoch 3.20: Loss = 0.627228
Epoch 3.21: Loss = 0.656662
Epoch 3.22: Loss = 0.649673
Epoch 3.23: Loss = 0.663986
Epoch 3.24: Loss = 0.613907
Epoch 3.25: Loss = 0.641083
Epoch 3.26: Loss = 0.640945
Epoch 3.27: Loss = 0.642746
Epoch 3.28: Loss = 0.634979
Epoch 3.29: Loss = 0.673203
Epoch 3.30: Loss = 0.632324
TRAIN LOSS = 0.640686
TRAIN ACC = 78.6743 % (47206/60000)
Loss = 0.615463
Loss = 0.693008
Loss = 0.663101
Loss = 0.654755
Loss = 0.646545
TEST LOSS = 0.654574
TEST ACC = 472.06 % (7761/10000)
Reducing learning rate to 0.399994
Epoch 4.1: Loss = 0.620361
Epoch 4.2: Loss = 0.595444
Epoch 4.3: Loss = 0.621155
Epoch 4.4: Loss = 0.592285
Epoch 4.5: Loss = 0.614746
Epoch 4.6: Loss = 0.617203
Epoch 4.7: Loss = 0.61319
Epoch 4.8: Loss = 0.698807
Epoch 4.9: Loss = 0.606735
Epoch 4.10: Loss = 0.607147
Epoch 4.11: Loss = 0.612274
Epoch 4.12: Loss = 0.60553
Epoch 4.13: Loss = 0.610565
Epoch 4.14: Loss = 0.627762
Epoch 4.15: Loss = 0.593536
Epoch 4.16: Loss = 0.602371
Epoch 4.17: Loss = 0.614777
Epoch 4.18: Loss = 0.60376
Epoch 4.19: Loss = 0.599228
Epoch 4.20: Loss = 0.663254
Epoch 4.21: Loss = 0.622498
Epoch 4.22: Loss = 0.603104
Epoch 4.23: Loss = 0.595413
Epoch 4.24: Loss = 0.59494
Epoch 4.25: Loss = 0.691849
Epoch 4.26: Loss = 0.640076
Epoch 4.27: Loss = 0.707916
Epoch 4.28: Loss = 0.663437
Epoch 4.29: Loss = 0.677399
Epoch 4.30: Loss = 0.680847
TRAIN LOSS = 0.626602
TRAIN ACC = 79.6844 % (47813/60000)
Loss = 0.612793
Loss = 0.699112
Loss = 0.672012
Loss = 0.659576
Loss = 0.649399
TEST LOSS = 0.658578
TEST ACC = 478.13 % (7840/10000)
Reducing learning rate to 0.399994
Epoch 5.1: Loss = 0.659134
Epoch 5.2: Loss = 0.611603
Epoch 5.3: Loss = 0.599762
Epoch 5.4: Loss = 0.610565
Epoch 5.5: Loss = 0.562729
Epoch 5.6: Loss = 0.632446
Epoch 5.7: Loss = 0.629852
Epoch 5.8: Loss = 0.611313
Epoch 5.9: Loss = 0.62973
Epoch 5.10: Loss = 0.583862
Epoch 5.11: Loss = 0.58551
Epoch 5.12: Loss = 0.662521
Epoch 5.13: Loss = 0.638351
Epoch 5.14: Loss = 0.541443
Epoch 5.15: Loss = 0.544418
Epoch 5.16: Loss = 0.63208
Epoch 5.17: Loss = 0.613007
Epoch 5.18: Loss = 0.620026
Epoch 5.19: Loss = 0.633911
Epoch 5.20: Loss = 0.673508
Epoch 5.21: Loss = 0.643204
Epoch 5.22: Loss = 0.595886
Epoch 5.23: Loss = 0.685196
Epoch 5.24: Loss = 0.584793
Epoch 5.25: Loss = 0.582306
Epoch 5.26: Loss = 0.57196
Epoch 5.27: Loss = 0.596161
Epoch 5.28: Loss = 0.683136
Epoch 5.29: Loss = 0.615234
Epoch 5.30: Loss = 0.586182
TRAIN LOSS = 0.614014
TRAIN ACC = 80.4306 % (48260/60000)
Loss = 0.569031
Loss = 0.659805
Loss = 0.639069
Loss = 0.623581
Loss = 0.615433
TEST LOSS = 0.621383
TEST ACC = 482.599 % (7994/10000)
Reducing learning rate to 0.399994
Epoch 6.1: Loss = 0.592102
Epoch 6.2: Loss = 0.623276
Epoch 6.3: Loss = 0.646729
Epoch 6.4: Loss = 0.633835
Epoch 6.5: Loss = 0.58963
Epoch 6.6: Loss = 0.570175
Epoch 6.7: Loss = 0.593414
Epoch 6.8: Loss = 0.557327
Epoch 6.9: Loss = 0.614594
Epoch 6.10: Loss = 0.565399
Epoch 6.11: Loss = 0.584503
Epoch 6.12: Loss = 0.533417
Epoch 6.13: Loss = 0.58905
Epoch 6.14: Loss = 0.58252
Epoch 6.15: Loss = 0.566437
Epoch 6.16: Loss = 0.588104
Epoch 6.17: Loss = 0.564484
Epoch 6.18: Loss = 0.581451
Epoch 6.19: Loss = 0.623505
Epoch 6.20: Loss = 0.563736
Epoch 6.21: Loss = 0.612106
Epoch 6.22: Loss = 0.62471
Epoch 6.23: Loss = 0.55014
Epoch 6.24: Loss = 0.578232
Epoch 6.25: Loss = 0.610931
Epoch 6.26: Loss = 0.644714
Epoch 6.27: Loss = 0.573334
Epoch 6.28: Loss = 0.59906
Epoch 6.29: Loss = 0.64325
Epoch 6.30: Loss = 0.599716
TRAIN LOSS = 0.593338
TRAIN ACC = 81.1417 % (48687/60000)
Loss = 0.559204
Loss = 0.651703
Loss = 0.63707
Loss = 0.615646
Loss = 0.610825
TEST LOSS = 0.614889
TEST ACC = 486.87 % (8045/10000)
Reducing learning rate to 0.399994
Epoch 7.1: Loss = 0.512451
Epoch 7.2: Loss = 0.633804
Epoch 7.3: Loss = 0.583893
Epoch 7.4: Loss = 0.62796
Epoch 7.5: Loss = 0.590775
Epoch 7.6: Loss = 0.558014
Epoch 7.7: Loss = 0.581055
Epoch 7.8: Loss = 0.592087
Epoch 7.9: Loss = 0.59642
Epoch 7.10: Loss = 0.558853
Epoch 7.11: Loss = 0.610855
Epoch 7.12: Loss = 0.612427
Epoch 7.13: Loss = 0.597275
Epoch 7.14: Loss = 0.608566
Epoch 7.15: Loss = 0.651215
Epoch 7.16: Loss = 0.591309
Epoch 7.17: Loss = 0.548569
Epoch 7.18: Loss = 0.527145
Epoch 7.19: Loss = 0.657486
Epoch 7.20: Loss = 0.557907
Epoch 7.21: Loss = 0.521301
Epoch 7.22: Loss = 0.564392
Epoch 7.23: Loss = 0.606415
Epoch 7.24: Loss = 0.572617
Epoch 7.25: Loss = 0.591476
Epoch 7.26: Loss = 0.596405
Epoch 7.27: Loss = 0.567139
Epoch 7.28: Loss = 0.580322
Epoch 7.29: Loss = 0.639465
Epoch 7.30: Loss = 0.576172
TRAIN LOSS = 0.587128
TRAIN ACC = 81.5964 % (48960/60000)
Loss = 0.545959
Loss = 0.640579
Loss = 0.629608
Loss = 0.603821
Loss = 0.597992
TEST LOSS = 0.603592
TEST ACC = 489.6 % (8096/10000)
Reducing learning rate to 0.399994
Epoch 8.1: Loss = 0.56601
Epoch 8.2: Loss = 0.560913
Epoch 8.3: Loss = 0.604691
Epoch 8.4: Loss = 0.593994
Epoch 8.5: Loss = 0.593719
Epoch 8.6: Loss = 0.53212
Epoch 8.7: Loss = 0.613342
Epoch 8.8: Loss = 0.50441
Epoch 8.9: Loss = 0.533691
Epoch 8.10: Loss = 0.663635
Epoch 8.11: Loss = 0.577576
Epoch 8.12: Loss = 0.603653
Epoch 8.13: Loss = 0.60437
Epoch 8.14: Loss = 0.581146
Epoch 8.15: Loss = 0.567734
Epoch 8.16: Loss = 0.555984
Epoch 8.17: Loss = 0.563583
Epoch 8.18: Loss = 0.577759
Epoch 8.19: Loss = 0.545975
Epoch 8.20: Loss = 0.625305
Epoch 8.21: Loss = 0.589935
Epoch 8.22: Loss = 0.612579
Epoch 8.23: Loss = 0.587097
Epoch 8.24: Loss = 0.600525
Epoch 8.25: Loss = 0.568344
Epoch 8.26: Loss = 0.604584
Epoch 8.27: Loss = 0.593781
Epoch 8.28: Loss = 0.626007
Epoch 8.29: Loss = 0.561264
Epoch 8.30: Loss = 0.534592
TRAIN LOSS = 0.581619
TRAIN ACC = 81.9229 % (49156/60000)
Loss = 0.545197
Loss = 0.63829
Loss = 0.629364
Loss = 0.598541
Loss = 0.594437
TEST LOSS = 0.601166
TEST ACC = 491.559 % (8073/10000)
Reducing learning rate to 0.399994
Epoch 9.1: Loss = 0.580582
Epoch 9.2: Loss = 0.556931
Epoch 9.3: Loss = 0.538788
Epoch 9.4: Loss = 0.546524
Epoch 9.5: Loss = 0.565384
Epoch 9.6: Loss = 0.601395
Epoch 9.7: Loss = 0.627228
Epoch 9.8: Loss = 0.620712
Epoch 9.9: Loss = 0.540085
Epoch 9.10: Loss = 0.584961
Epoch 9.11: Loss = 0.617981
Epoch 9.12: Loss = 0.56282
Epoch 9.13: Loss = 0.579163
Epoch 9.14: Loss = 0.5681
Epoch 9.15: Loss = 0.588516
Epoch 9.16: Loss = 0.577957
Epoch 9.17: Loss = 0.594971
Epoch 9.18: Loss = 0.559601
Epoch 9.19: Loss = 0.591202
Epoch 9.20: Loss = 0.549011
Epoch 9.21: Loss = 0.575745
Epoch 9.22: Loss = 0.584198
Epoch 9.23: Loss = 0.558868
Epoch 9.24: Loss = 0.593491
Epoch 9.25: Loss = 0.543503
Epoch 9.26: Loss = 0.577621
Epoch 9.27: Loss = 0.550217
Epoch 9.28: Loss = 0.613983
Epoch 9.29: Loss = 0.515045
Epoch 9.30: Loss = 0.595596
TRAIN LOSS = 0.575348
TRAIN ACC = 82.2144 % (49331/60000)
Loss = 0.567749
Loss = 0.661438
Loss = 0.662415
Loss = 0.622131
Loss = 0.619247
TEST LOSS = 0.626596
TEST ACC = 493.309 % (8090/10000)
Reducing learning rate to 0.399994
Epoch 10.1: Loss = 0.587753
Epoch 10.2: Loss = 0.550385
Epoch 10.3: Loss = 0.57872
Epoch 10.4: Loss = 0.600983
Epoch 10.5: Loss = 0.592026
Epoch 10.6: Loss = 0.600525
Epoch 10.7: Loss = 0.56601
Epoch 10.8: Loss = 0.592529
Epoch 10.9: Loss = 0.593475
Epoch 10.10: Loss = 0.600586
Epoch 10.11: Loss = 0.547699
Epoch 10.12: Loss = 0.589172
Epoch 10.13: Loss = 0.635651
Epoch 10.14: Loss = 0.519028
Epoch 10.15: Loss = 0.608902
Epoch 10.16: Loss = 0.574066
Epoch 10.17: Loss = 0.633881
Epoch 10.18: Loss = 0.549652
Epoch 10.19: Loss = 0.583954
Epoch 10.20: Loss = 0.546112
Epoch 10.21: Loss = 0.504547
Epoch 10.22: Loss = 0.563873
Epoch 10.23: Loss = 0.569153
Epoch 10.24: Loss = 0.542114
Epoch 10.25: Loss = 0.579147
Epoch 10.26: Loss = 0.586777
Epoch 10.27: Loss = 0.612488
Epoch 10.28: Loss = 0.567139
Epoch 10.29: Loss = 0.547745
Epoch 10.30: Loss = 0.591095
TRAIN LOSS = 0.577179
TRAIN ACC = 82.4615 % (49479/60000)
Loss = 0.538315
Loss = 0.631332
Loss = 0.627762
Loss = 0.593964
Loss = 0.58725
TEST LOSS = 0.595724
TEST ACC = 494.789 % (8153/10000)

Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 600]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 10
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 3.5
***********************************************************
Epoch 1.1: Loss = 2.29773
Epoch 1.2: Loss = 2.23856
Epoch 1.3: Loss = 2.17535
Epoch 1.4: Loss = 2.12337
Epoch 1.5: Loss = 2.07556
Epoch 1.6: Loss = 2.06042
Epoch 1.7: Loss = 2.00169
Epoch 1.8: Loss = 1.96707
Epoch 1.9: Loss = 1.93477
Epoch 1.10: Loss = 1.8998
Epoch 1.11: Loss = 1.84184
Epoch 1.12: Loss = 1.81165
Epoch 1.13: Loss = 1.75378
Epoch 1.14: Loss = 1.71198
Epoch 1.15: Loss = 1.73729
Epoch 1.16: Loss = 1.6989
Epoch 1.17: Loss = 1.62323
Epoch 1.18: Loss = 1.60738
Epoch 1.19: Loss = 1.60637
Epoch 1.20: Loss = 1.55202
Epoch 1.21: Loss = 1.4856
Epoch 1.22: Loss = 1.42867
Epoch 1.23: Loss = 1.42369
Epoch 1.24: Loss = 1.40736
Epoch 1.25: Loss = 1.39825
Epoch 1.26: Loss = 1.38966
Epoch 1.27: Loss = 1.39142
Epoch 1.28: Loss = 1.37279
Epoch 1.29: Loss = 1.31325
Epoch 1.30: Loss = 1.29805
Epoch 1.31: Loss = 1.25127
Epoch 1.32: Loss = 1.27309
Epoch 1.33: Loss = 1.19572
Epoch 1.34: Loss = 1.16418
Epoch 1.35: Loss = 1.20897
Epoch 1.36: Loss = 1.16373
Epoch 1.37: Loss = 1.10251
Epoch 1.38: Loss = 1.10454
Epoch 1.39: Loss = 1.11639
Epoch 1.40: Loss = 1.06404
Epoch 1.41: Loss = 1.10735
Epoch 1.42: Loss = 1.09018
Epoch 1.43: Loss = 1.02313
Epoch 1.44: Loss = 0.9646
Epoch 1.45: Loss = 0.993042
Epoch 1.46: Loss = 0.9617
Epoch 1.47: Loss = 0.896179
Epoch 1.48: Loss = 0.99115
Epoch 1.49: Loss = 0.938263
Epoch 1.50: Loss = 0.971848
Epoch 1.51: Loss = 0.894531
Epoch 1.52: Loss = 0.887451
Epoch 1.53: Loss = 0.906067
Epoch 1.54: Loss = 0.896866
Epoch 1.55: Loss = 0.833496
Epoch 1.56: Loss = 0.862122
Epoch 1.57: Loss = 0.832886
Epoch 1.58: Loss = 0.889526
Epoch 1.59: Loss = 0.811874
Epoch 1.60: Loss = 0.777664
Epoch 1.61: Loss = 0.847885
Epoch 1.62: Loss = 0.792404
Epoch 1.63: Loss = 0.795807
Epoch 1.64: Loss = 0.834274
Epoch 1.65: Loss = 0.778168
Epoch 1.66: Loss = 0.739746
Epoch 1.67: Loss = 0.760529
Epoch 1.68: Loss = 0.774475
Epoch 1.69: Loss = 0.688065
Epoch 1.70: Loss = 0.669388
Epoch 1.71: Loss = 0.745193
Epoch 1.72: Loss = 0.761063
Epoch 1.73: Loss = 0.716782
Epoch 1.74: Loss = 0.742599
Epoch 1.75: Loss = 0.710938
Epoch 1.76: Loss = 0.717941
Epoch 1.77: Loss = 0.717072
Epoch 1.78: Loss = 0.686218
Epoch 1.79: Loss = 0.650146
Epoch 1.80: Loss = 0.638016
Epoch 1.81: Loss = 0.694519
Epoch 1.82: Loss = 0.649109
Epoch 1.83: Loss = 0.722702
Epoch 1.84: Loss = 0.690353
Epoch 1.85: Loss = 0.582474
Epoch 1.86: Loss = 0.612625
Epoch 1.87: Loss = 0.658585
Epoch 1.88: Loss = 0.652298
Epoch 1.89: Loss = 0.69458
Epoch 1.90: Loss = 0.658661
Epoch 1.91: Loss = 0.688461
Epoch 1.92: Loss = 0.661652
Epoch 1.93: Loss = 0.597427
Epoch 1.94: Loss = 0.592438
Epoch 1.95: Loss = 0.648651
Epoch 1.96: Loss = 0.614563
Epoch 1.97: Loss = 0.677948
Epoch 1.98: Loss = 0.596603
Epoch 1.99: Loss = 0.610092
Epoch 1.100: Loss = 0.673767
TRAIN LOSS = 1.09546
TRAIN ACC = 71.344 % (42808/60000)
Loss = 0.663712
Loss = 0.670578
Loss = 0.790833
Loss = 0.739136
Loss = 0.659286
Loss = 0.656586
Loss = 0.732605
Loss = 0.728943
Loss = 0.535278
Loss = 0.465408
Loss = 0.409729
Loss = 0.500366
Loss = 0.475769
Loss = 0.471909
Loss = 0.283722
Loss = 0.446381
Loss = 0.820358
TEST LOSS = 0.586629
TEST ACC = 428.079 % (8351/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.62825
Epoch 2.2: Loss = 0.668991
Epoch 2.3: Loss = 0.705902
Epoch 2.4: Loss = 0.670502
Epoch 2.5: Loss = 0.584915
Epoch 2.6: Loss = 0.550766
Epoch 2.7: Loss = 0.598267
Epoch 2.8: Loss = 0.58313
Epoch 2.9: Loss = 0.572937
Epoch 2.10: Loss = 0.56189
Epoch 2.11: Loss = 0.645416
Epoch 2.12: Loss = 0.655884
Epoch 2.13: Loss = 0.596451
Epoch 2.14: Loss = 0.589432
Epoch 2.15: Loss = 0.535645
Epoch 2.16: Loss = 0.518875
Epoch 2.17: Loss = 0.588043
Epoch 2.18: Loss = 0.568878
Epoch 2.19: Loss = 0.571503
Epoch 2.20: Loss = 0.541061
Epoch 2.21: Loss = 0.496475
Epoch 2.22: Loss = 0.489151
Epoch 2.23: Loss = 0.558578
Epoch 2.24: Loss = 0.569397
Epoch 2.25: Loss = 0.546005
Epoch 2.26: Loss = 0.544113
Epoch 2.27: Loss = 0.5746
Epoch 2.28: Loss = 0.50827
Epoch 2.29: Loss = 0.604904
Epoch 2.30: Loss = 0.532181
Epoch 2.31: Loss = 0.549271
Epoch 2.32: Loss = 0.505219
Epoch 2.33: Loss = 0.502625
Epoch 2.34: Loss = 0.546677
Epoch 2.35: Loss = 0.534653
Epoch 2.36: Loss = 0.534302
Epoch 2.37: Loss = 0.500092
Epoch 2.38: Loss = 0.541138
Epoch 2.39: Loss = 0.524841
Epoch 2.40: Loss = 0.540558
Epoch 2.41: Loss = 0.482758
Epoch 2.42: Loss = 0.502731
Epoch 2.43: Loss = 0.549011
Epoch 2.44: Loss = 0.56012
Epoch 2.45: Loss = 0.471771
Epoch 2.46: Loss = 0.466629
Epoch 2.47: Loss = 0.491684
Epoch 2.48: Loss = 0.561066
Epoch 2.49: Loss = 0.560654
Epoch 2.50: Loss = 0.49202
Epoch 2.51: Loss = 0.4888
Epoch 2.52: Loss = 0.488892
Epoch 2.53: Loss = 0.532379
Epoch 2.54: Loss = 0.515411
Epoch 2.55: Loss = 0.499969
Epoch 2.56: Loss = 0.481216
Epoch 2.57: Loss = 0.495041
Epoch 2.58: Loss = 0.467484
Epoch 2.59: Loss = 0.546204
Epoch 2.60: Loss = 0.501724
Epoch 2.61: Loss = 0.5327
Epoch 2.62: Loss = 0.451538
Epoch 2.63: Loss = 0.530411
Epoch 2.64: Loss = 0.440247
Epoch 2.65: Loss = 0.562256
Epoch 2.66: Loss = 0.459259
Epoch 2.67: Loss = 0.503143
Epoch 2.68: Loss = 0.468887
Epoch 2.69: Loss = 0.430008
Epoch 2.70: Loss = 0.463913
Epoch 2.71: Loss = 0.515472
Epoch 2.72: Loss = 0.447952
Epoch 2.73: Loss = 0.470291
Epoch 2.74: Loss = 0.517853
Epoch 2.75: Loss = 0.453293
Epoch 2.76: Loss = 0.47583
Epoch 2.77: Loss = 0.430786
Epoch 2.78: Loss = 0.460999
Epoch 2.79: Loss = 0.482376
Epoch 2.80: Loss = 0.42514
Epoch 2.81: Loss = 0.451065
Epoch 2.82: Loss = 0.472046
Epoch 2.83: Loss = 0.515366
Epoch 2.84: Loss = 0.489059
Epoch 2.85: Loss = 0.437561
Epoch 2.86: Loss = 0.440643
Epoch 2.87: Loss = 0.443497
Epoch 2.88: Loss = 0.419907
Epoch 2.89: Loss = 0.391281
Epoch 2.90: Loss = 0.44191
Epoch 2.91: Loss = 0.502609
Epoch 2.92: Loss = 0.469971
Epoch 2.93: Loss = 0.44809
Epoch 2.94: Loss = 0.459396
Epoch 2.95: Loss = 0.496902
Epoch 2.96: Loss = 0.464142
Epoch 2.97: Loss = 0.471802
Epoch 2.98: Loss = 0.459854
Epoch 2.99: Loss = 0.404221
Epoch 2.100: Loss = 0.458496
TRAIN LOSS = 0.514557
TRAIN ACC = 84.9808 % (50991/60000)
Loss = 0.487808
Loss = 0.513733
Loss = 0.65593
Loss = 0.601181
Loss = 0.482788
Loss = 0.492538
Loss = 0.595276
Loss = 0.556824
Loss = 0.379547
Loss = 0.32193
Loss = 0.301392
Loss = 0.348831
Loss = 0.304642
Loss = 0.34024
Loss = 0.14856
Loss = 0.30336
Loss = 0.671631
TEST LOSS = 0.43694
TEST ACC = 509.909 % (8703/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.479767
Epoch 3.2: Loss = 0.446014
Epoch 3.3: Loss = 0.412354
Epoch 3.4: Loss = 0.501785
Epoch 3.5: Loss = 0.460495
Epoch 3.6: Loss = 0.468521
Epoch 3.7: Loss = 0.526184
Epoch 3.8: Loss = 0.515869
Epoch 3.9: Loss = 0.503876
Epoch 3.10: Loss = 0.490814
Epoch 3.11: Loss = 0.361755
Epoch 3.12: Loss = 0.399246
Epoch 3.13: Loss = 0.466156
Epoch 3.14: Loss = 0.491028
Epoch 3.15: Loss = 0.466599
Epoch 3.16: Loss = 0.481552
Epoch 3.17: Loss = 0.432877
Epoch 3.18: Loss = 0.397705
Epoch 3.19: Loss = 0.383575
Epoch 3.20: Loss = 0.487762
Epoch 3.21: Loss = 0.345901
Epoch 3.22: Loss = 0.480331
Epoch 3.23: Loss = 0.44928
Epoch 3.24: Loss = 0.465576
Epoch 3.25: Loss = 0.451645
Epoch 3.26: Loss = 0.430206
Epoch 3.27: Loss = 0.495132
Epoch 3.28: Loss = 0.399429
Epoch 3.29: Loss = 0.385056
Epoch 3.30: Loss = 0.43251
Epoch 3.31: Loss = 0.375748
Epoch 3.32: Loss = 0.393921
Epoch 3.33: Loss = 0.46048
Epoch 3.34: Loss = 0.533707
Epoch 3.35: Loss = 0.400467
Epoch 3.36: Loss = 0.427292
Epoch 3.37: Loss = 0.445831
Epoch 3.38: Loss = 0.372803
Epoch 3.39: Loss = 0.41011
Epoch 3.40: Loss = 0.496338
Epoch 3.41: Loss = 0.462921
Epoch 3.42: Loss = 0.527512
Epoch 3.43: Loss = 0.435257
Epoch 3.44: Loss = 0.461227
Epoch 3.45: Loss = 0.39856
Epoch 3.46: Loss = 0.412582
Epoch 3.47: Loss = 0.463654
Epoch 3.48: Loss = 0.415527
Epoch 3.49: Loss = 0.400528
Epoch 3.50: Loss = 0.446396
Epoch 3.51: Loss = 0.456833
Epoch 3.52: Loss = 0.474854
Epoch 3.53: Loss = 0.46701
Epoch 3.54: Loss = 0.474747
Epoch 3.55: Loss = 0.423615
Epoch 3.56: Loss = 0.500702
Epoch 3.57: Loss = 0.402252
Epoch 3.58: Loss = 0.423767
Epoch 3.59: Loss = 0.422974
Epoch 3.60: Loss = 0.423187
Epoch 3.61: Loss = 0.46907
Epoch 3.62: Loss = 0.420609
Epoch 3.63: Loss = 0.403656
Epoch 3.64: Loss = 0.390381
Epoch 3.65: Loss = 0.46315
Epoch 3.66: Loss = 0.561325
Epoch 3.67: Loss = 0.441986
Epoch 3.68: Loss = 0.387375
Epoch 3.69: Loss = 0.402618
Epoch 3.70: Loss = 0.413208
Epoch 3.71: Loss = 0.402069
Epoch 3.72: Loss = 0.412079
Epoch 3.73: Loss = 0.495819
Epoch 3.74: Loss = 0.4021
Epoch 3.75: Loss = 0.40831
Epoch 3.76: Loss = 0.520554
Epoch 3.77: Loss = 0.368973
Epoch 3.78: Loss = 0.438782
Epoch 3.79: Loss = 0.417023
Epoch 3.80: Loss = 0.43869
Epoch 3.81: Loss = 0.414581
Epoch 3.82: Loss = 0.385422
Epoch 3.83: Loss = 0.439377
Epoch 3.84: Loss = 0.440048
Epoch 3.85: Loss = 0.463043
Epoch 3.86: Loss = 0.456192
Epoch 3.87: Loss = 0.490738
Epoch 3.88: Loss = 0.454987
Epoch 3.89: Loss = 0.447403
Epoch 3.90: Loss = 0.438141
Epoch 3.91: Loss = 0.409241
Epoch 3.92: Loss = 0.46283
Epoch 3.93: Loss = 0.403351
Epoch 3.94: Loss = 0.467926
Epoch 3.95: Loss = 0.490631
Epoch 3.96: Loss = 0.469025
Epoch 3.97: Loss = 0.416153
Epoch 3.98: Loss = 0.540771
Epoch 3.99: Loss = 0.385513
Epoch 3.100: Loss = 0.404694
TRAIN LOSS = 0.442596
TRAIN ACC = 86.6501 % (51993/60000)
Loss = 0.434799
Loss = 0.485458
Loss = 0.620239
Loss = 0.58316
Loss = 0.436722
Loss = 0.456284
Loss = 0.573135
Loss = 0.497894
Loss = 0.371994
Loss = 0.305511
Loss = 0.279861
Loss = 0.296066
Loss = 0.25444
Loss = 0.288132
Loss = 0.111313
Loss = 0.273743
Loss = 0.653366
TEST LOSS = 0.40226
TEST ACC = 519.93 % (8791/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.407257
Epoch 4.2: Loss = 0.402893
Epoch 4.3: Loss = 0.366806
Epoch 4.4: Loss = 0.425888
Epoch 4.5: Loss = 0.435471
Epoch 4.6: Loss = 0.381516
Epoch 4.7: Loss = 0.432922
Epoch 4.8: Loss = 0.424408
Epoch 4.9: Loss = 0.500381
Epoch 4.10: Loss = 0.409073
Epoch 4.11: Loss = 0.46701
Epoch 4.12: Loss = 0.485779
Epoch 4.13: Loss = 0.392105
Epoch 4.14: Loss = 0.393921
Epoch 4.15: Loss = 0.397888
Epoch 4.16: Loss = 0.353424
Epoch 4.17: Loss = 0.492661
Epoch 4.18: Loss = 0.415985
Epoch 4.19: Loss = 0.434341
Epoch 4.20: Loss = 0.462128
Epoch 4.21: Loss = 0.430115
Epoch 4.22: Loss = 0.40184
Epoch 4.23: Loss = 0.422409
Epoch 4.24: Loss = 0.457108
Epoch 4.25: Loss = 0.42041
Epoch 4.26: Loss = 0.421387
Epoch 4.27: Loss = 0.445801
Epoch 4.28: Loss = 0.43045
Epoch 4.29: Loss = 0.465179
Epoch 4.30: Loss = 0.377914
Epoch 4.31: Loss = 0.417267
Epoch 4.32: Loss = 0.430649
Epoch 4.33: Loss = 0.416275
Epoch 4.34: Loss = 0.421158
Epoch 4.35: Loss = 0.398682
Epoch 4.36: Loss = 0.333588
Epoch 4.37: Loss = 0.48468
Epoch 4.38: Loss = 0.370026
Epoch 4.39: Loss = 0.361099
Epoch 4.40: Loss = 0.433578
Epoch 4.41: Loss = 0.383728
Epoch 4.42: Loss = 0.448029
Epoch 4.43: Loss = 0.440292
Epoch 4.44: Loss = 0.391617
Epoch 4.45: Loss = 0.432236
Epoch 4.46: Loss = 0.392029
Epoch 4.47: Loss = 0.460648
Epoch 4.48: Loss = 0.402451
Epoch 4.49: Loss = 0.387253
Epoch 4.50: Loss = 0.395691
Epoch 4.51: Loss = 0.33728
Epoch 4.52: Loss = 0.394806
Epoch 4.53: Loss = 0.378174
Epoch 4.54: Loss = 0.407227
Epoch 4.55: Loss = 0.388931
Epoch 4.56: Loss = 0.418671
Epoch 4.57: Loss = 0.402802
Epoch 4.58: Loss = 0.443329
Epoch 4.59: Loss = 0.405029
Epoch 4.60: Loss = 0.406143
Epoch 4.61: Loss = 0.316559
Epoch 4.62: Loss = 0.381302
Epoch 4.63: Loss = 0.379395
Epoch 4.64: Loss = 0.407608
Epoch 4.65: Loss = 0.454773
Epoch 4.66: Loss = 0.377365
Epoch 4.67: Loss = 0.432175
Epoch 4.68: Loss = 0.419159
Epoch 4.69: Loss = 0.36142
Epoch 4.70: Loss = 0.420029
Epoch 4.71: Loss = 0.393494
Epoch 4.72: Loss = 0.437805
Epoch 4.73: Loss = 0.396484
Epoch 4.74: Loss = 0.468277
Epoch 4.75: Loss = 0.42807
Epoch 4.76: Loss = 0.421951
Epoch 4.77: Loss = 0.402542
Epoch 4.78: Loss = 0.402313
Epoch 4.79: Loss = 0.385971
Epoch 4.80: Loss = 0.457581
Epoch 4.81: Loss = 0.393448
Epoch 4.82: Loss = 0.405853
Epoch 4.83: Loss = 0.354111
Epoch 4.84: Loss = 0.383179
Epoch 4.85: Loss = 0.445862
Epoch 4.86: Loss = 0.371338
Epoch 4.87: Loss = 0.506714
Epoch 4.88: Loss = 0.439255
Epoch 4.89: Loss = 0.491959
Epoch 4.90: Loss = 0.426865
Epoch 4.91: Loss = 0.38176
Epoch 4.92: Loss = 0.423325
Epoch 4.93: Loss = 0.434418
Epoch 4.94: Loss = 0.489227
Epoch 4.95: Loss = 0.425079
Epoch 4.96: Loss = 0.359741
Epoch 4.97: Loss = 0.343414
Epoch 4.98: Loss = 0.411041
Epoch 4.99: Loss = 0.478043
Epoch 4.100: Loss = 0.315872
TRAIN LOSS = 0.413651
TRAIN ACC = 87.645 % (52590/60000)
Loss = 0.41803
Loss = 0.441071
Loss = 0.591537
Loss = 0.547424
Loss = 0.416412
Loss = 0.434418
Loss = 0.549759
Loss = 0.48143
Loss = 0.351318
Loss = 0.275726
Loss = 0.273285
Loss = 0.269897
Loss = 0.226868
Loss = 0.284088
Loss = 0.0887756
Loss = 0.245193
Loss = 0.625015
TEST LOSS = 0.378714
TEST ACC = 525.899 % (8884/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.435226
Epoch 5.2: Loss = 0.409027
Epoch 5.3: Loss = 0.409134
Epoch 5.4: Loss = 0.439331
Epoch 5.5: Loss = 0.397476
Epoch 5.6: Loss = 0.414261
Epoch 5.7: Loss = 0.474655
Epoch 5.8: Loss = 0.467117
Epoch 5.9: Loss = 0.386292
Epoch 5.10: Loss = 0.44165
Epoch 5.11: Loss = 0.462769
Epoch 5.12: Loss = 0.371109
Epoch 5.13: Loss = 0.389938
Epoch 5.14: Loss = 0.375397
Epoch 5.15: Loss = 0.381409
Epoch 5.16: Loss = 0.375061
Epoch 5.17: Loss = 0.407593
Epoch 5.18: Loss = 0.351746
Epoch 5.19: Loss = 0.384003
Epoch 5.20: Loss = 0.337265
Epoch 5.21: Loss = 0.375443
Epoch 5.22: Loss = 0.399261
Epoch 5.23: Loss = 0.442169
Epoch 5.24: Loss = 0.436783
Epoch 5.25: Loss = 0.402588
Epoch 5.26: Loss = 0.409973
Epoch 5.27: Loss = 0.366425
Epoch 5.28: Loss = 0.395828
Epoch 5.29: Loss = 0.370178
Epoch 5.30: Loss = 0.401733
Epoch 5.31: Loss = 0.346481
Epoch 5.32: Loss = 0.43132
Epoch 5.33: Loss = 0.426086
Epoch 5.34: Loss = 0.434952
Epoch 5.35: Loss = 0.406372
Epoch 5.36: Loss = 0.393356
Epoch 5.37: Loss = 0.399048
Epoch 5.38: Loss = 0.458893
Epoch 5.39: Loss = 0.395416
Epoch 5.40: Loss = 0.456558
Epoch 5.41: Loss = 0.450348
Epoch 5.42: Loss = 0.404236
Epoch 5.43: Loss = 0.388672
Epoch 5.44: Loss = 0.344116
Epoch 5.45: Loss = 0.427917
Epoch 5.46: Loss = 0.368835
Epoch 5.47: Loss = 0.3284
Epoch 5.48: Loss = 0.373199
Epoch 5.49: Loss = 0.363281
Epoch 5.50: Loss = 0.352737
Epoch 5.51: Loss = 0.323669
Epoch 5.52: Loss = 0.434021
Epoch 5.53: Loss = 0.432922
Epoch 5.54: Loss = 0.442078
Epoch 5.55: Loss = 0.417252
Epoch 5.56: Loss = 0.471252
Epoch 5.57: Loss = 0.306213
Epoch 5.58: Loss = 0.384949
Epoch 5.59: Loss = 0.326996
Epoch 5.60: Loss = 0.404907
Epoch 5.61: Loss = 0.400146
Epoch 5.62: Loss = 0.363586
Epoch 5.63: Loss = 0.347336
Epoch 5.64: Loss = 0.400604
Epoch 5.65: Loss = 0.374771
Epoch 5.66: Loss = 0.391541
Epoch 5.67: Loss = 0.384201
Epoch 5.68: Loss = 0.383133
Epoch 5.69: Loss = 0.348282
Epoch 5.70: Loss = 0.333633
Epoch 5.71: Loss = 0.386215
Epoch 5.72: Loss = 0.325775
Epoch 5.73: Loss = 0.338135
Epoch 5.74: Loss = 0.365875
Epoch 5.75: Loss = 0.452499
Epoch 5.76: Loss = 0.381226
Epoch 5.77: Loss = 0.39035
Epoch 5.78: Loss = 0.416946
Epoch 5.79: Loss = 0.428818
Epoch 5.80: Loss = 0.465317
Epoch 5.81: Loss = 0.453415
Epoch 5.82: Loss = 0.389343
Epoch 5.83: Loss = 0.434418
Epoch 5.84: Loss = 0.386658
Epoch 5.85: Loss = 0.364197
Epoch 5.86: Loss = 0.421158
Epoch 5.87: Loss = 0.367294
Epoch 5.88: Loss = 0.339218
Epoch 5.89: Loss = 0.424042
Epoch 5.90: Loss = 0.367737
Epoch 5.91: Loss = 0.381241
Epoch 5.92: Loss = 0.406387
Epoch 5.93: Loss = 0.40715
Epoch 5.94: Loss = 0.447311
Epoch 5.95: Loss = 0.475082
Epoch 5.96: Loss = 0.362564
Epoch 5.97: Loss = 0.398315
Epoch 5.98: Loss = 0.378738
Epoch 5.99: Loss = 0.381973
Epoch 5.100: Loss = 0.426712
TRAIN LOSS = 0.396973
TRAIN ACC = 88.4064 % (53046/60000)
Loss = 0.394775
Loss = 0.450714
Loss = 0.607681
Loss = 0.567856
Loss = 0.396454
Loss = 0.414505
Loss = 0.558517
Loss = 0.485062
Loss = 0.351089
Loss = 0.250107
Loss = 0.275513
Loss = 0.265991
Loss = 0.217102
Loss = 0.278397
Loss = 0.0811615
Loss = 0.233749
Loss = 0.641296
TEST LOSS = 0.375372
TEST ACC = 530.46 % (8912/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.361359
Epoch 6.2: Loss = 0.413498
Epoch 6.3: Loss = 0.475235
Epoch 6.4: Loss = 0.38736
Epoch 6.5: Loss = 0.383118
Epoch 6.6: Loss = 0.453735
Epoch 6.7: Loss = 0.41095
Epoch 6.8: Loss = 0.381271
Epoch 6.9: Loss = 0.409668
Epoch 6.10: Loss = 0.475906
Epoch 6.11: Loss = 0.403442
Epoch 6.12: Loss = 0.343002
Epoch 6.13: Loss = 0.427811
Epoch 6.14: Loss = 0.408569
Epoch 6.15: Loss = 0.361252
Epoch 6.16: Loss = 0.413971
Epoch 6.17: Loss = 0.392929
Epoch 6.18: Loss = 0.441727
Epoch 6.19: Loss = 0.405014
Epoch 6.20: Loss = 0.413177
Epoch 6.21: Loss = 0.424286
Epoch 6.22: Loss = 0.367035
Epoch 6.23: Loss = 0.424606
Epoch 6.24: Loss = 0.421661
Epoch 6.25: Loss = 0.371643
Epoch 6.26: Loss = 0.349731
Epoch 6.27: Loss = 0.417404
Epoch 6.28: Loss = 0.405991
Epoch 6.29: Loss = 0.32872
Epoch 6.30: Loss = 0.376251
Epoch 6.31: Loss = 0.354797
Epoch 6.32: Loss = 0.441147
Epoch 6.33: Loss = 0.449341
Epoch 6.34: Loss = 0.398163
Epoch 6.35: Loss = 0.370758
Epoch 6.36: Loss = 0.481995
Epoch 6.37: Loss = 0.435654
Epoch 6.38: Loss = 0.355011
Epoch 6.39: Loss = 0.395706
Epoch 6.40: Loss = 0.388077
Epoch 6.41: Loss = 0.378143
Epoch 6.42: Loss = 0.350876
Epoch 6.43: Loss = 0.423019
Epoch 6.44: Loss = 0.386108
Epoch 6.45: Loss = 0.425278
Epoch 6.46: Loss = 0.398224
Epoch 6.47: Loss = 0.312057
Epoch 6.48: Loss = 0.383331
Epoch 6.49: Loss = 0.374557
Epoch 6.50: Loss = 0.447327
Epoch 6.51: Loss = 0.360321
Epoch 6.52: Loss = 0.336746
Epoch 6.53: Loss = 0.429047
Epoch 6.54: Loss = 0.39064
Epoch 6.55: Loss = 0.372787
Epoch 6.56: Loss = 0.348999
Epoch 6.57: Loss = 0.399277
Epoch 6.58: Loss = 0.401154
Epoch 6.59: Loss = 0.316879
Epoch 6.60: Loss = 0.353912
Epoch 6.61: Loss = 0.371201
Epoch 6.62: Loss = 0.36055
Epoch 6.63: Loss = 0.403336
Epoch 6.64: Loss = 0.417267
Epoch 6.65: Loss = 0.411636
Epoch 6.66: Loss = 0.378128
Epoch 6.67: Loss = 0.330429
Epoch 6.68: Loss = 0.426285
Epoch 6.69: Loss = 0.344604
Epoch 6.70: Loss = 0.392532
Epoch 6.71: Loss = 0.355743
Epoch 6.72: Loss = 0.484009
Epoch 6.73: Loss = 0.425888
Epoch 6.74: Loss = 0.40976
Epoch 6.75: Loss = 0.397293
Epoch 6.76: Loss = 0.431107
Epoch 6.77: Loss = 0.385864
Epoch 6.78: Loss = 0.350037
Epoch 6.79: Loss = 0.362061
Epoch 6.80: Loss = 0.490265
Epoch 6.81: Loss = 0.306808
Epoch 6.82: Loss = 0.315552
Epoch 6.83: Loss = 0.339737
Epoch 6.84: Loss = 0.313446
Epoch 6.85: Loss = 0.361496
Epoch 6.86: Loss = 0.44223
Epoch 6.87: Loss = 0.424057
Epoch 6.88: Loss = 0.374039
Epoch 6.89: Loss = 0.440109
Epoch 6.90: Loss = 0.35965
Epoch 6.91: Loss = 0.300522
Epoch 6.92: Loss = 0.457123
Epoch 6.93: Loss = 0.397919
Epoch 6.94: Loss = 0.406998
Epoch 6.95: Loss = 0.374496
Epoch 6.96: Loss = 0.346741
Epoch 6.97: Loss = 0.39183
Epoch 6.98: Loss = 0.40564
Epoch 6.99: Loss = 0.454376
Epoch 6.100: Loss = 0.416962
TRAIN LOSS = 0.392685
TRAIN ACC = 88.6246 % (53177/60000)
Loss = 0.386719
Loss = 0.432358
Loss = 0.595169
Loss = 0.569946
Loss = 0.395386
Loss = 0.402756
Loss = 0.546753
Loss = 0.481155
Loss = 0.337494
Loss = 0.256027
Loss = 0.266205
Loss = 0.2659
Loss = 0.214401
Loss = 0.267227
Loss = 0.0688324
Loss = 0.236221
Loss = 0.63147
TEST LOSS = 0.368612
TEST ACC = 531.769 % (8933/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.400345
Epoch 7.2: Loss = 0.381363
Epoch 7.3: Loss = 0.315948
Epoch 7.4: Loss = 0.347534
Epoch 7.5: Loss = 0.486465
Epoch 7.6: Loss = 0.463745
Epoch 7.7: Loss = 0.393036
Epoch 7.8: Loss = 0.345947
Epoch 7.9: Loss = 0.457001
Epoch 7.10: Loss = 0.393204
Epoch 7.11: Loss = 0.350861
Epoch 7.12: Loss = 0.341766
Epoch 7.13: Loss = 0.38588
Epoch 7.14: Loss = 0.418335
Epoch 7.15: Loss = 0.35437
Epoch 7.16: Loss = 0.378006
Epoch 7.17: Loss = 0.381454
Epoch 7.18: Loss = 0.353241
Epoch 7.19: Loss = 0.340347
Epoch 7.20: Loss = 0.333237
Epoch 7.21: Loss = 0.401871
Epoch 7.22: Loss = 0.339355
Epoch 7.23: Loss = 0.499741
Epoch 7.24: Loss = 0.362335
Epoch 7.25: Loss = 0.416809
Epoch 7.26: Loss = 0.267258
Epoch 7.27: Loss = 0.41748
Epoch 7.28: Loss = 0.372345
Epoch 7.29: Loss = 0.369537
Epoch 7.30: Loss = 0.369156
Epoch 7.31: Loss = 0.446976
Epoch 7.32: Loss = 0.358185
Epoch 7.33: Loss = 0.408203
Epoch 7.34: Loss = 0.347839
Epoch 7.35: Loss = 0.373276
Epoch 7.36: Loss = 0.424271
Epoch 7.37: Loss = 0.388214
Epoch 7.38: Loss = 0.35675
Epoch 7.39: Loss = 0.383163
Epoch 7.40: Loss = 0.383087
Epoch 7.41: Loss = 0.464371
Epoch 7.42: Loss = 0.407516
Epoch 7.43: Loss = 0.391861
Epoch 7.44: Loss = 0.332901
Epoch 7.45: Loss = 0.259583
Epoch 7.46: Loss = 0.38443
Epoch 7.47: Loss = 0.393829
Epoch 7.48: Loss = 0.366287
Epoch 7.49: Loss = 0.357361
Epoch 7.50: Loss = 0.397186
Epoch 7.51: Loss = 0.428726
Epoch 7.52: Loss = 0.408829
Epoch 7.53: Loss = 0.447968
Epoch 7.54: Loss = 0.521912
Epoch 7.55: Loss = 0.278625
Epoch 7.56: Loss = 0.346634
Epoch 7.57: Loss = 0.373199
Epoch 7.58: Loss = 0.348557
Epoch 7.59: Loss = 0.511353
Epoch 7.60: Loss = 0.399536
Epoch 7.61: Loss = 0.457352
Epoch 7.62: Loss = 0.36525
Epoch 7.63: Loss = 0.333786
Epoch 7.64: Loss = 0.372849
Epoch 7.65: Loss = 0.402649
Epoch 7.66: Loss = 0.373428
Epoch 7.67: Loss = 0.34436
Epoch 7.68: Loss = 0.41806
Epoch 7.69: Loss = 0.355652
Epoch 7.70: Loss = 0.292282
Epoch 7.71: Loss = 0.429047
Epoch 7.72: Loss = 0.445526
Epoch 7.73: Loss = 0.342911
Epoch 7.74: Loss = 0.378021
Epoch 7.75: Loss = 0.439377
Epoch 7.76: Loss = 0.399948
Epoch 7.77: Loss = 0.40654
Epoch 7.78: Loss = 0.43486
Epoch 7.79: Loss = 0.506546
Epoch 7.80: Loss = 0.339584
Epoch 7.81: Loss = 0.407944
Epoch 7.82: Loss = 0.39679
Epoch 7.83: Loss = 0.40715
Epoch 7.84: Loss = 0.452576
Epoch 7.85: Loss = 0.386627
Epoch 7.86: Loss = 0.415726
Epoch 7.87: Loss = 0.372406
Epoch 7.88: Loss = 0.465988
Epoch 7.89: Loss = 0.297958
Epoch 7.90: Loss = 0.393356
Epoch 7.91: Loss = 0.359772
Epoch 7.92: Loss = 0.425705
Epoch 7.93: Loss = 0.439972
Epoch 7.94: Loss = 0.364609
Epoch 7.95: Loss = 0.31955
Epoch 7.96: Loss = 0.408691
Epoch 7.97: Loss = 0.327774
Epoch 7.98: Loss = 0.411179
Epoch 7.99: Loss = 0.377991
Epoch 7.100: Loss = 0.407669
TRAIN LOSS = 0.388062
TRAIN ACC = 88.8382 % (53306/60000)
Loss = 0.390228
Loss = 0.431213
Loss = 0.578384
Loss = 0.573349
Loss = 0.393463
Loss = 0.408096
Loss = 0.556458
Loss = 0.482635
Loss = 0.338348
Loss = 0.248688
Loss = 0.267365
Loss = 0.258133
Loss = 0.211945
Loss = 0.264984
Loss = 0.0623779
Loss = 0.231216
Loss = 0.630951
TEST LOSS = 0.367051
TEST ACC = 533.06 % (8940/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.376297
Epoch 8.2: Loss = 0.392761
Epoch 8.3: Loss = 0.381622
Epoch 8.4: Loss = 0.374146
Epoch 8.5: Loss = 0.416351
Epoch 8.6: Loss = 0.374954
Epoch 8.7: Loss = 0.337936
Epoch 8.8: Loss = 0.294235
Epoch 8.9: Loss = 0.363495
Epoch 8.10: Loss = 0.414032
Epoch 8.11: Loss = 0.401489
Epoch 8.12: Loss = 0.44194
Epoch 8.13: Loss = 0.416977
Epoch 8.14: Loss = 0.426697
Epoch 8.15: Loss = 0.338638
Epoch 8.16: Loss = 0.402908
Epoch 8.17: Loss = 0.389297
Epoch 8.18: Loss = 0.405991
Epoch 8.19: Loss = 0.321457
Epoch 8.20: Loss = 0.314789
Epoch 8.21: Loss = 0.446869
Epoch 8.22: Loss = 0.34581
Epoch 8.23: Loss = 0.418015
Epoch 8.24: Loss = 0.36055
Epoch 8.25: Loss = 0.342163
Epoch 8.26: Loss = 0.351685
Epoch 8.27: Loss = 0.416885
Epoch 8.28: Loss = 0.394653
Epoch 8.29: Loss = 0.377365
Epoch 8.30: Loss = 0.434708
Epoch 8.31: Loss = 0.404953
Epoch 8.32: Loss = 0.443085
Epoch 8.33: Loss = 0.374863
Epoch 8.34: Loss = 0.343872
Epoch 8.35: Loss = 0.305344
Epoch 8.36: Loss = 0.38063
Epoch 8.37: Loss = 0.412903
Epoch 8.38: Loss = 0.415726
Epoch 8.39: Loss = 0.424576
Epoch 8.40: Loss = 0.36058
Epoch 8.41: Loss = 0.38913
Epoch 8.42: Loss = 0.355499
Epoch 8.43: Loss = 0.395905
Epoch 8.44: Loss = 0.374298
Epoch 8.45: Loss = 0.393951
Epoch 8.46: Loss = 0.354187
Epoch 8.47: Loss = 0.390167
Epoch 8.48: Loss = 0.450043
Epoch 8.49: Loss = 0.317963
Epoch 8.50: Loss = 0.399338
Epoch 8.51: Loss = 0.331451
Epoch 8.52: Loss = 0.438675
Epoch 8.53: Loss = 0.373718
Epoch 8.54: Loss = 0.433899
Epoch 8.55: Loss = 0.405975
Epoch 8.56: Loss = 0.484329
Epoch 8.57: Loss = 0.353043
Epoch 8.58: Loss = 0.375198
Epoch 8.59: Loss = 0.407593
Epoch 8.60: Loss = 0.408249
Epoch 8.61: Loss = 0.436142
Epoch 8.62: Loss = 0.367996
Epoch 8.63: Loss = 0.304031
Epoch 8.64: Loss = 0.382294
Epoch 8.65: Loss = 0.381165
Epoch 8.66: Loss = 0.330109
Epoch 8.67: Loss = 0.490631
Epoch 8.68: Loss = 0.423279
Epoch 8.69: Loss = 0.381134
Epoch 8.70: Loss = 0.304626
Epoch 8.71: Loss = 0.449203
Epoch 8.72: Loss = 0.446136
Epoch 8.73: Loss = 0.457336
Epoch 8.74: Loss = 0.39064
Epoch 8.75: Loss = 0.415253
Epoch 8.76: Loss = 0.332611
Epoch 8.77: Loss = 0.324844
Epoch 8.78: Loss = 0.4189
Epoch 8.79: Loss = 0.413452
Epoch 8.80: Loss = 0.432739
Epoch 8.81: Loss = 0.410828
Epoch 8.82: Loss = 0.45311
Epoch 8.83: Loss = 0.392365
Epoch 8.84: Loss = 0.427643
Epoch 8.85: Loss = 0.380066
Epoch 8.86: Loss = 0.31662
Epoch 8.87: Loss = 0.397491
Epoch 8.88: Loss = 0.364044
Epoch 8.89: Loss = 0.440353
Epoch 8.90: Loss = 0.359238
Epoch 8.91: Loss = 0.366745
Epoch 8.92: Loss = 0.396561
Epoch 8.93: Loss = 0.392303
Epoch 8.94: Loss = 0.404358
Epoch 8.95: Loss = 0.481461
Epoch 8.96: Loss = 0.415802
Epoch 8.97: Loss = 0.356323
Epoch 8.98: Loss = 0.319946
Epoch 8.99: Loss = 0.350586
Epoch 8.100: Loss = 0.384384
TRAIN LOSS = 0.388672
TRAIN ACC = 88.9481 % (53371/60000)
Loss = 0.380783
Loss = 0.430679
Loss = 0.571045
Loss = 0.571335
Loss = 0.388397
Loss = 0.391129
Loss = 0.565964
Loss = 0.482117
Loss = 0.333206
Loss = 0.259567
Loss = 0.278275
Loss = 0.254807
Loss = 0.212875
Loss = 0.263977
Loss = 0.0662231
Loss = 0.231903
Loss = 0.636948
TEST LOSS = 0.366415
TEST ACC = 533.71 % (8962/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.38858
Epoch 9.2: Loss = 0.395355
Epoch 9.3: Loss = 0.322723
Epoch 9.4: Loss = 0.415268
Epoch 9.5: Loss = 0.356934
Epoch 9.6: Loss = 0.443771
Epoch 9.7: Loss = 0.403885
Epoch 9.8: Loss = 0.419617
Epoch 9.9: Loss = 0.460114
Epoch 9.10: Loss = 0.42865
Epoch 9.11: Loss = 0.386963
Epoch 9.12: Loss = 0.359085
Epoch 9.13: Loss = 0.390579
Epoch 9.14: Loss = 0.374649
Epoch 9.15: Loss = 0.426849
Epoch 9.16: Loss = 0.380966
Epoch 9.17: Loss = 0.34288
Epoch 9.18: Loss = 0.375
Epoch 9.19: Loss = 0.356674
Epoch 9.20: Loss = 0.319687
Epoch 9.21: Loss = 0.433197
Epoch 9.22: Loss = 0.367859
Epoch 9.23: Loss = 0.42395
Epoch 9.24: Loss = 0.342102
Epoch 9.25: Loss = 0.358826
Epoch 9.26: Loss = 0.377991
Epoch 9.27: Loss = 0.384521
Epoch 9.28: Loss = 0.487411
Epoch 9.29: Loss = 0.467697
Epoch 9.30: Loss = 0.373856
Epoch 9.31: Loss = 0.386612
Epoch 9.32: Loss = 0.313965
Epoch 9.33: Loss = 0.362793
Epoch 9.34: Loss = 0.361633
Epoch 9.35: Loss = 0.430695
Epoch 9.36: Loss = 0.447739
Epoch 9.37: Loss = 0.355225
Epoch 9.38: Loss = 0.397217
Epoch 9.39: Loss = 0.392334
Epoch 9.40: Loss = 0.355621
Epoch 9.41: Loss = 0.384232
Epoch 9.42: Loss = 0.347031
Epoch 9.43: Loss = 0.510986
Epoch 9.44: Loss = 0.335068
Epoch 9.45: Loss = 0.426208
Epoch 9.46: Loss = 0.347961
Epoch 9.47: Loss = 0.379227
Epoch 9.48: Loss = 0.371658
Epoch 9.49: Loss = 0.408035
Epoch 9.50: Loss = 0.421387
Epoch 9.51: Loss = 0.384552
Epoch 9.52: Loss = 0.402618
Epoch 9.53: Loss = 0.452194
Epoch 9.54: Loss = 0.361115
Epoch 9.55: Loss = 0.316635
Epoch 9.56: Loss = 0.376114
Epoch 9.57: Loss = 0.298187
Epoch 9.58: Loss = 0.462036
Epoch 9.59: Loss = 0.332214
Epoch 9.60: Loss = 0.334976
Epoch 9.61: Loss = 0.435913
Epoch 9.62: Loss = 0.380981
Epoch 9.63: Loss = 0.421051
Epoch 9.64: Loss = 0.408829
Epoch 9.65: Loss = 0.45787
Epoch 9.66: Loss = 0.34024
Epoch 9.67: Loss = 0.346542
Epoch 9.68: Loss = 0.350632
Epoch 9.69: Loss = 0.299744
Epoch 9.70: Loss = 0.35611
Epoch 9.71: Loss = 0.336624
Epoch 9.72: Loss = 0.465576
Epoch 9.73: Loss = 0.308945
Epoch 9.74: Loss = 0.350342
Epoch 9.75: Loss = 0.363892
Epoch 9.76: Loss = 0.424362
Epoch 9.77: Loss = 0.368027
Epoch 9.78: Loss = 0.42923
Epoch 9.79: Loss = 0.441879
Epoch 9.80: Loss = 0.403595
Epoch 9.81: Loss = 0.364105
Epoch 9.82: Loss = 0.428879
Epoch 9.83: Loss = 0.3367
Epoch 9.84: Loss = 0.367996
Epoch 9.85: Loss = 0.417236
Epoch 9.86: Loss = 0.374695
Epoch 9.87: Loss = 0.409134
Epoch 9.88: Loss = 0.314331
Epoch 9.89: Loss = 0.345596
Epoch 9.90: Loss = 0.438751
Epoch 9.91: Loss = 0.363327
Epoch 9.92: Loss = 0.37709
Epoch 9.93: Loss = 0.39003
Epoch 9.94: Loss = 0.42012
Epoch 9.95: Loss = 0.385864
Epoch 9.96: Loss = 0.385651
Epoch 9.97: Loss = 0.359619
Epoch 9.98: Loss = 0.327835
Epoch 9.99: Loss = 0.491119
Epoch 9.100: Loss = 0.319229
TRAIN LOSS = 0.3853
TRAIN ACC = 89.1586 % (53498/60000)
Loss = 0.374176
Loss = 0.433594
Loss = 0.579727
Loss = 0.576538
Loss = 0.387817
Loss = 0.396057
Loss = 0.562881
Loss = 0.473358
Loss = 0.342804
Loss = 0.264893
Loss = 0.278076
Loss = 0.248642
Loss = 0.200516
Loss = 0.255554
Loss = 0.0667114
Loss = 0.233398
Loss = 0.625427
TEST LOSS = 0.365502
TEST ACC = 534.979 % (8953/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.28125
Epoch 10.2: Loss = 0.334305
Epoch 10.3: Loss = 0.323303
Epoch 10.4: Loss = 0.331619
Epoch 10.5: Loss = 0.307816
Epoch 10.6: Loss = 0.352661
Epoch 10.7: Loss = 0.386459
Epoch 10.8: Loss = 0.386307
Epoch 10.9: Loss = 0.495285
Epoch 10.10: Loss = 0.368927
Epoch 10.11: Loss = 0.345642
Epoch 10.12: Loss = 0.349243
Epoch 10.13: Loss = 0.404907
Epoch 10.14: Loss = 0.450333
Epoch 10.15: Loss = 0.369202
Epoch 10.16: Loss = 0.397919
Epoch 10.17: Loss = 0.4048
Epoch 10.18: Loss = 0.307953
Epoch 10.19: Loss = 0.375366
Epoch 10.20: Loss = 0.434967
Epoch 10.21: Loss = 0.313934
Epoch 10.22: Loss = 0.369278
Epoch 10.23: Loss = 0.394638
Epoch 10.24: Loss = 0.377823
Epoch 10.25: Loss = 0.418427
Epoch 10.26: Loss = 0.329971
Epoch 10.27: Loss = 0.356918
Epoch 10.28: Loss = 0.460754
Epoch 10.29: Loss = 0.320175
Epoch 10.30: Loss = 0.401764
Epoch 10.31: Loss = 0.483124
Epoch 10.32: Loss = 0.334274
Epoch 10.33: Loss = 0.346344
Epoch 10.34: Loss = 0.460526
Epoch 10.35: Loss = 0.296112
Epoch 10.36: Loss = 0.358032
Epoch 10.37: Loss = 0.411697
Epoch 10.38: Loss = 0.3573
Epoch 10.39: Loss = 0.389908
Epoch 10.40: Loss = 0.37822
Epoch 10.41: Loss = 0.422501
Epoch 10.42: Loss = 0.354279
Epoch 10.43: Loss = 0.399567
Epoch 10.44: Loss = 0.354584
Epoch 10.45: Loss = 0.391403
Epoch 10.46: Loss = 0.425385
Epoch 10.47: Loss = 0.483902
Epoch 10.48: Loss = 0.403748
Epoch 10.49: Loss = 0.340332
Epoch 10.50: Loss = 0.302444
Epoch 10.51: Loss = 0.453064
Epoch 10.52: Loss = 0.344757
Epoch 10.53: Loss = 0.427399
Epoch 10.54: Loss = 0.308365
Epoch 10.55: Loss = 0.437119
Epoch 10.56: Loss = 0.345703
Epoch 10.57: Loss = 0.339874
Epoch 10.58: Loss = 0.358002
Epoch 10.59: Loss = 0.270172
Epoch 10.60: Loss = 0.391373
Epoch 10.61: Loss = 0.418686
Epoch 10.62: Loss = 0.437271
Epoch 10.63: Loss = 0.363312
Epoch 10.64: Loss = 0.450592
Epoch 10.65: Loss = 0.467575
Epoch 10.66: Loss = 0.503632
Epoch 10.67: Loss = 0.346024
Epoch 10.68: Loss = 0.36438
Epoch 10.69: Loss = 0.448242
Epoch 10.70: Loss = 0.36647
Epoch 10.71: Loss = 0.39949
Epoch 10.72: Loss = 0.354202
Epoch 10.73: Loss = 0.375916
Epoch 10.74: Loss = 0.44397
Epoch 10.75: Loss = 0.367859
Epoch 10.76: Loss = 0.313583
Epoch 10.77: Loss = 0.506683
Epoch 10.78: Loss = 0.408783
Epoch 10.79: Loss = 0.359039
Epoch 10.80: Loss = 0.413162
Epoch 10.81: Loss = 0.323013
Epoch 10.82: Loss = 0.439041
Epoch 10.83: Loss = 0.316711
Epoch 10.84: Loss = 0.404984
Epoch 10.85: Loss = 0.472137
Epoch 10.86: Loss = 0.405548
Epoch 10.87: Loss = 0.423431
Epoch 10.88: Loss = 0.457169
Epoch 10.89: Loss = 0.357803
Epoch 10.90: Loss = 0.348938
Epoch 10.91: Loss = 0.378799
Epoch 10.92: Loss = 0.368484
Epoch 10.93: Loss = 0.391052
Epoch 10.94: Loss = 0.393204
Epoch 10.95: Loss = 0.356796
Epoch 10.96: Loss = 0.407181
Epoch 10.97: Loss = 0.326126
Epoch 10.98: Loss = 0.360046
Epoch 10.99: Loss = 0.325607
Epoch 10.100: Loss = 0.316147
TRAIN LOSS = 0.382034
TRAIN ACC = 89.2548 % (53555/60000)
Loss = 0.374481
Loss = 0.420517
Loss = 0.566818
Loss = 0.574112
Loss = 0.380188
Loss = 0.388657
Loss = 0.553543
Loss = 0.463348
Loss = 0.339188
Loss = 0.255234
Loss = 0.289734
Loss = 0.234894
Loss = 0.189362
Loss = 0.256516
Loss = 0.0638885
Loss = 0.228973
Loss = 0.614578
TEST LOSS = 0.35935
TEST ACC = 535.548 % (8977/10000)
The following benchmarks are including preprocessing (offline phase).
Time = 45451.1 seconds 
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True

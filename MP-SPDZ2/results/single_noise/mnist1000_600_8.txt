Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 1000]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 15
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.33922
Epoch 1.2: Loss = 2.29124
Epoch 1.3: Loss = 2.23026
Epoch 1.4: Loss = 2.18669
Epoch 1.5: Loss = 2.1306
Epoch 1.6: Loss = 2.08141
Epoch 1.7: Loss = 2.05453
Epoch 1.8: Loss = 1.98859
Epoch 1.9: Loss = 1.9407
Epoch 1.10: Loss = 1.90129
Epoch 1.11: Loss = 1.88734
Epoch 1.12: Loss = 1.83618
Epoch 1.13: Loss = 1.78923
Epoch 1.14: Loss = 1.74844
Epoch 1.15: Loss = 1.70445
Epoch 1.16: Loss = 1.7153
Epoch 1.17: Loss = 1.66998
Epoch 1.18: Loss = 1.65341
Epoch 1.19: Loss = 1.60863
Epoch 1.20: Loss = 1.57599
Epoch 1.21: Loss = 1.55905
Epoch 1.22: Loss = 1.51154
Epoch 1.23: Loss = 1.45322
Epoch 1.24: Loss = 1.42331
Epoch 1.25: Loss = 1.40768
Epoch 1.26: Loss = 1.39188
Epoch 1.27: Loss = 1.31577
Epoch 1.28: Loss = 1.34799
Epoch 1.29: Loss = 1.21974
Epoch 1.30: Loss = 1.29881
Epoch 1.31: Loss = 1.25198
Epoch 1.32: Loss = 1.25601
Epoch 1.33: Loss = 1.24049
Epoch 1.34: Loss = 1.16704
Epoch 1.35: Loss = 1.17653
Epoch 1.36: Loss = 1.12932
Epoch 1.37: Loss = 1.12738
Epoch 1.38: Loss = 1.03453
Epoch 1.39: Loss = 1.09212
Epoch 1.40: Loss = 1.08678
Epoch 1.41: Loss = 1.07483
Epoch 1.42: Loss = 1.01724
Epoch 1.43: Loss = 1.01497
Epoch 1.44: Loss = 0.976349
Epoch 1.45: Loss = 0.977921
Epoch 1.46: Loss = 1.00316
Epoch 1.47: Loss = 1.0121
Epoch 1.48: Loss = 0.935776
Epoch 1.49: Loss = 0.972702
Epoch 1.50: Loss = 0.924866
Epoch 1.51: Loss = 0.94397
Epoch 1.52: Loss = 0.941055
Epoch 1.53: Loss = 0.872849
Epoch 1.54: Loss = 0.880173
Epoch 1.55: Loss = 0.905396
Epoch 1.56: Loss = 0.861954
Epoch 1.57: Loss = 0.819427
Epoch 1.58: Loss = 0.82869
Epoch 1.59: Loss = 0.824631
Epoch 1.60: Loss = 0.82605
Epoch 1.61: Loss = 0.817642
Epoch 1.62: Loss = 0.769958
Epoch 1.63: Loss = 0.826202
Epoch 1.64: Loss = 0.815643
Epoch 1.65: Loss = 0.763962
Epoch 1.66: Loss = 0.804062
Epoch 1.67: Loss = 0.755188
Epoch 1.68: Loss = 0.762009
Epoch 1.69: Loss = 0.840744
Epoch 1.70: Loss = 0.728333
Epoch 1.71: Loss = 0.757568
Epoch 1.72: Loss = 0.791275
Epoch 1.73: Loss = 0.70993
Epoch 1.74: Loss = 0.750153
Epoch 1.75: Loss = 0.704132
Epoch 1.76: Loss = 0.643661
Epoch 1.77: Loss = 0.704819
Epoch 1.78: Loss = 0.724228
Epoch 1.79: Loss = 0.740692
Epoch 1.80: Loss = 0.699509
Epoch 1.81: Loss = 0.746811
Epoch 1.82: Loss = 0.686172
Epoch 1.83: Loss = 0.648315
Epoch 1.84: Loss = 0.684235
Epoch 1.85: Loss = 0.686722
Epoch 1.86: Loss = 0.676651
Epoch 1.87: Loss = 0.672729
Epoch 1.88: Loss = 0.632599
Epoch 1.89: Loss = 0.635498
Epoch 1.90: Loss = 0.639954
Epoch 1.91: Loss = 0.639816
Epoch 1.92: Loss = 0.601196
Epoch 1.93: Loss = 0.633179
Epoch 1.94: Loss = 0.57814
Epoch 1.95: Loss = 0.592758
Epoch 1.96: Loss = 0.594116
Epoch 1.97: Loss = 0.586929
Epoch 1.98: Loss = 0.584854
Epoch 1.99: Loss = 0.611359
Epoch 1.100: Loss = 0.603165
TRAIN LOSS = 1.10286
TRAIN ACC = 71.0205 % (42614/60000)
Loss = 0.647659
Loss = 0.661041
Loss = 0.772781
Loss = 0.737793
Loss = 0.647781
Loss = 0.649765
Loss = 0.715057
Loss = 0.692612
Loss = 0.519882
Loss = 0.487137
Loss = 0.416122
Loss = 0.516754
Loss = 0.460098
Loss = 0.442825
Loss = 0.284119
Loss = 0.439896
Loss = 0.777878
TEST LOSS = 0.576594
TEST ACC = 426.14 % (8391/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.590927
Epoch 2.2: Loss = 0.587921
Epoch 2.3: Loss = 0.572113
Epoch 2.4: Loss = 0.593353
Epoch 2.5: Loss = 0.592148
Epoch 2.6: Loss = 0.618515
Epoch 2.7: Loss = 0.533905
Epoch 2.8: Loss = 0.577591
Epoch 2.9: Loss = 0.534332
Epoch 2.10: Loss = 0.57019
Epoch 2.11: Loss = 0.564713
Epoch 2.12: Loss = 0.610138
Epoch 2.13: Loss = 0.622116
Epoch 2.14: Loss = 0.537201
Epoch 2.15: Loss = 0.575989
Epoch 2.16: Loss = 0.567352
Epoch 2.17: Loss = 0.592484
Epoch 2.18: Loss = 0.589462
Epoch 2.19: Loss = 0.62764
Epoch 2.20: Loss = 0.597076
Epoch 2.21: Loss = 0.551437
Epoch 2.22: Loss = 0.53537
Epoch 2.23: Loss = 0.563522
Epoch 2.24: Loss = 0.499191
Epoch 2.25: Loss = 0.485641
Epoch 2.26: Loss = 0.558044
Epoch 2.27: Loss = 0.567215
Epoch 2.28: Loss = 0.487701
Epoch 2.29: Loss = 0.538254
Epoch 2.30: Loss = 0.550995
Epoch 2.31: Loss = 0.540161
Epoch 2.32: Loss = 0.547791
Epoch 2.33: Loss = 0.527832
Epoch 2.34: Loss = 0.546844
Epoch 2.35: Loss = 0.545868
Epoch 2.36: Loss = 0.578613
Epoch 2.37: Loss = 0.552322
Epoch 2.38: Loss = 0.547119
Epoch 2.39: Loss = 0.492722
Epoch 2.40: Loss = 0.595612
Epoch 2.41: Loss = 0.550568
Epoch 2.42: Loss = 0.515381
Epoch 2.43: Loss = 0.586777
Epoch 2.44: Loss = 0.513702
Epoch 2.45: Loss = 0.510834
Epoch 2.46: Loss = 0.566635
Epoch 2.47: Loss = 0.474014
Epoch 2.48: Loss = 0.452988
Epoch 2.49: Loss = 0.534393
Epoch 2.50: Loss = 0.490128
Epoch 2.51: Loss = 0.507767
Epoch 2.52: Loss = 0.493103
Epoch 2.53: Loss = 0.485992
Epoch 2.54: Loss = 0.490784
Epoch 2.55: Loss = 0.530746
Epoch 2.56: Loss = 0.501968
Epoch 2.57: Loss = 0.496307
Epoch 2.58: Loss = 0.505356
Epoch 2.59: Loss = 0.502762
Epoch 2.60: Loss = 0.390198
Epoch 2.61: Loss = 0.473877
Epoch 2.62: Loss = 0.481949
Epoch 2.63: Loss = 0.539017
Epoch 2.64: Loss = 0.446014
Epoch 2.65: Loss = 0.462799
Epoch 2.66: Loss = 0.456253
Epoch 2.67: Loss = 0.471085
Epoch 2.68: Loss = 0.460999
Epoch 2.69: Loss = 0.446304
Epoch 2.70: Loss = 0.423889
Epoch 2.71: Loss = 0.445541
Epoch 2.72: Loss = 0.550827
Epoch 2.73: Loss = 0.478073
Epoch 2.74: Loss = 0.44043
Epoch 2.75: Loss = 0.482788
Epoch 2.76: Loss = 0.461227
Epoch 2.77: Loss = 0.422638
Epoch 2.78: Loss = 0.481445
Epoch 2.79: Loss = 0.453964
Epoch 2.80: Loss = 0.465347
Epoch 2.81: Loss = 0.460861
Epoch 2.82: Loss = 0.44223
Epoch 2.83: Loss = 0.485413
Epoch 2.84: Loss = 0.490128
Epoch 2.85: Loss = 0.490265
Epoch 2.86: Loss = 0.457306
Epoch 2.87: Loss = 0.458801
Epoch 2.88: Loss = 0.452332
Epoch 2.89: Loss = 0.445251
Epoch 2.90: Loss = 0.499695
Epoch 2.91: Loss = 0.357758
Epoch 2.92: Loss = 0.454895
Epoch 2.93: Loss = 0.371277
Epoch 2.94: Loss = 0.504623
Epoch 2.95: Loss = 0.504639
Epoch 2.96: Loss = 0.401627
Epoch 2.97: Loss = 0.449814
Epoch 2.98: Loss = 0.459122
Epoch 2.99: Loss = 0.469254
Epoch 2.100: Loss = 0.4552
TRAIN LOSS = 0.510239
TRAIN ACC = 85.3638 % (51220/60000)
Loss = 0.474747
Loss = 0.500305
Loss = 0.616211
Loss = 0.579666
Loss = 0.4646
Loss = 0.478058
Loss = 0.580185
Loss = 0.524963
Loss = 0.377319
Loss = 0.334091
Loss = 0.308182
Loss = 0.357437
Loss = 0.292313
Loss = 0.332382
Loss = 0.150925
Loss = 0.283722
Loss = 0.628876
TEST LOSS = 0.424461
TEST ACC = 512.199 % (8797/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.468323
Epoch 3.2: Loss = 0.433624
Epoch 3.3: Loss = 0.459137
Epoch 3.4: Loss = 0.445679
Epoch 3.5: Loss = 0.431046
Epoch 3.6: Loss = 0.410599
Epoch 3.7: Loss = 0.457458
Epoch 3.8: Loss = 0.377609
Epoch 3.9: Loss = 0.416367
Epoch 3.10: Loss = 0.412933
Epoch 3.11: Loss = 0.397446
Epoch 3.12: Loss = 0.490601
Epoch 3.13: Loss = 0.404861
Epoch 3.14: Loss = 0.412537
Epoch 3.15: Loss = 0.467499
Epoch 3.16: Loss = 0.375717
Epoch 3.17: Loss = 0.422455
Epoch 3.18: Loss = 0.435333
Epoch 3.19: Loss = 0.390579
Epoch 3.20: Loss = 0.507172
Epoch 3.21: Loss = 0.442459
Epoch 3.22: Loss = 0.397675
Epoch 3.23: Loss = 0.473114
Epoch 3.24: Loss = 0.485214
Epoch 3.25: Loss = 0.421371
Epoch 3.26: Loss = 0.490494
Epoch 3.27: Loss = 0.465088
Epoch 3.28: Loss = 0.436966
Epoch 3.29: Loss = 0.506119
Epoch 3.30: Loss = 0.431686
Epoch 3.31: Loss = 0.475357
Epoch 3.32: Loss = 0.394577
Epoch 3.33: Loss = 0.421967
Epoch 3.34: Loss = 0.412628
Epoch 3.35: Loss = 0.368423
Epoch 3.36: Loss = 0.433319
Epoch 3.37: Loss = 0.471542
Epoch 3.38: Loss = 0.432816
Epoch 3.39: Loss = 0.409668
Epoch 3.40: Loss = 0.424881
Epoch 3.41: Loss = 0.408417
Epoch 3.42: Loss = 0.440903
Epoch 3.43: Loss = 0.397995
Epoch 3.44: Loss = 0.455048
Epoch 3.45: Loss = 0.406448
Epoch 3.46: Loss = 0.435547
Epoch 3.47: Loss = 0.445206
Epoch 3.48: Loss = 0.38829
Epoch 3.49: Loss = 0.455521
Epoch 3.50: Loss = 0.366196
Epoch 3.51: Loss = 0.415878
Epoch 3.52: Loss = 0.474167
Epoch 3.53: Loss = 0.421951
Epoch 3.54: Loss = 0.376083
Epoch 3.55: Loss = 0.355743
Epoch 3.56: Loss = 0.472855
Epoch 3.57: Loss = 0.424774
Epoch 3.58: Loss = 0.416473
Epoch 3.59: Loss = 0.42067
Epoch 3.60: Loss = 0.376465
Epoch 3.61: Loss = 0.451492
Epoch 3.62: Loss = 0.406586
Epoch 3.63: Loss = 0.459579
Epoch 3.64: Loss = 0.431137
Epoch 3.65: Loss = 0.443176
Epoch 3.66: Loss = 0.384872
Epoch 3.67: Loss = 0.3918
Epoch 3.68: Loss = 0.386063
Epoch 3.69: Loss = 0.458862
Epoch 3.70: Loss = 0.424576
Epoch 3.71: Loss = 0.503616
Epoch 3.72: Loss = 0.400162
Epoch 3.73: Loss = 0.423141
Epoch 3.74: Loss = 0.496124
Epoch 3.75: Loss = 0.419708
Epoch 3.76: Loss = 0.402283
Epoch 3.77: Loss = 0.417816
Epoch 3.78: Loss = 0.415192
Epoch 3.79: Loss = 0.423309
Epoch 3.80: Loss = 0.373306
Epoch 3.81: Loss = 0.437744
Epoch 3.82: Loss = 0.468903
Epoch 3.83: Loss = 0.476196
Epoch 3.84: Loss = 0.413452
Epoch 3.85: Loss = 0.428436
Epoch 3.86: Loss = 0.387589
Epoch 3.87: Loss = 0.482437
Epoch 3.88: Loss = 0.389877
Epoch 3.89: Loss = 0.394836
Epoch 3.90: Loss = 0.404556
Epoch 3.91: Loss = 0.428192
Epoch 3.92: Loss = 0.346481
Epoch 3.93: Loss = 0.373718
Epoch 3.94: Loss = 0.411682
Epoch 3.95: Loss = 0.380692
Epoch 3.96: Loss = 0.486069
Epoch 3.97: Loss = 0.436584
Epoch 3.98: Loss = 0.409805
Epoch 3.99: Loss = 0.401505
Epoch 3.100: Loss = 0.413422
TRAIN LOSS = 0.426514
TRAIN ACC = 87.5275 % (52519/60000)
Loss = 0.421936
Loss = 0.45665
Loss = 0.574615
Loss = 0.542862
Loss = 0.410339
Loss = 0.421921
Loss = 0.55246
Loss = 0.479156
Loss = 0.336975
Loss = 0.27919
Loss = 0.283661
Loss = 0.308548
Loss = 0.235855
Loss = 0.289566
Loss = 0.10524
Loss = 0.243637
Loss = 0.613342
TEST LOSS = 0.38109
TEST ACC = 525.189 % (8897/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.436066
Epoch 4.2: Loss = 0.423386
Epoch 4.3: Loss = 0.425446
Epoch 4.4: Loss = 0.371613
Epoch 4.5: Loss = 0.468872
Epoch 4.6: Loss = 0.410858
Epoch 4.7: Loss = 0.432846
Epoch 4.8: Loss = 0.429016
Epoch 4.9: Loss = 0.327484
Epoch 4.10: Loss = 0.346558
Epoch 4.11: Loss = 0.338928
Epoch 4.12: Loss = 0.445938
Epoch 4.13: Loss = 0.402496
Epoch 4.14: Loss = 0.383774
Epoch 4.15: Loss = 0.386215
Epoch 4.16: Loss = 0.427185
Epoch 4.17: Loss = 0.413803
Epoch 4.18: Loss = 0.463852
Epoch 4.19: Loss = 0.491776
Epoch 4.20: Loss = 0.433121
Epoch 4.21: Loss = 0.398163
Epoch 4.22: Loss = 0.389969
Epoch 4.23: Loss = 0.481857
Epoch 4.24: Loss = 0.349762
Epoch 4.25: Loss = 0.381989
Epoch 4.26: Loss = 0.407013
Epoch 4.27: Loss = 0.408295
Epoch 4.28: Loss = 0.396667
Epoch 4.29: Loss = 0.390686
Epoch 4.30: Loss = 0.325699
Epoch 4.31: Loss = 0.407883
Epoch 4.32: Loss = 0.36438
Epoch 4.33: Loss = 0.296021
Epoch 4.34: Loss = 0.363083
Epoch 4.35: Loss = 0.32312
Epoch 4.36: Loss = 0.389664
Epoch 4.37: Loss = 0.338593
Epoch 4.38: Loss = 0.408447
Epoch 4.39: Loss = 0.375473
Epoch 4.40: Loss = 0.390121
Epoch 4.41: Loss = 0.460785
Epoch 4.42: Loss = 0.432587
Epoch 4.43: Loss = 0.443771
Epoch 4.44: Loss = 0.3909
Epoch 4.45: Loss = 0.444855
Epoch 4.46: Loss = 0.411682
Epoch 4.47: Loss = 0.458603
Epoch 4.48: Loss = 0.416321
Epoch 4.49: Loss = 0.388702
Epoch 4.50: Loss = 0.40213
Epoch 4.51: Loss = 0.353165
Epoch 4.52: Loss = 0.418991
Epoch 4.53: Loss = 0.403427
Epoch 4.54: Loss = 0.339951
Epoch 4.55: Loss = 0.360062
Epoch 4.56: Loss = 0.380066
Epoch 4.57: Loss = 0.44458
Epoch 4.58: Loss = 0.511597
Epoch 4.59: Loss = 0.390869
Epoch 4.60: Loss = 0.358673
Epoch 4.61: Loss = 0.417587
Epoch 4.62: Loss = 0.348236
Epoch 4.63: Loss = 0.404083
Epoch 4.64: Loss = 0.351685
Epoch 4.65: Loss = 0.385162
Epoch 4.66: Loss = 0.413971
Epoch 4.67: Loss = 0.388565
Epoch 4.68: Loss = 0.396362
Epoch 4.69: Loss = 0.398087
Epoch 4.70: Loss = 0.402817
Epoch 4.71: Loss = 0.396118
Epoch 4.72: Loss = 0.429306
Epoch 4.73: Loss = 0.397812
Epoch 4.74: Loss = 0.375229
Epoch 4.75: Loss = 0.33287
Epoch 4.76: Loss = 0.364807
Epoch 4.77: Loss = 0.405624
Epoch 4.78: Loss = 0.411774
Epoch 4.79: Loss = 0.382111
Epoch 4.80: Loss = 0.373306
Epoch 4.81: Loss = 0.408554
Epoch 4.82: Loss = 0.344757
Epoch 4.83: Loss = 0.41098
Epoch 4.84: Loss = 0.377716
Epoch 4.85: Loss = 0.356018
Epoch 4.86: Loss = 0.420181
Epoch 4.87: Loss = 0.3517
Epoch 4.88: Loss = 0.449097
Epoch 4.89: Loss = 0.397125
Epoch 4.90: Loss = 0.363113
Epoch 4.91: Loss = 0.372467
Epoch 4.92: Loss = 0.423019
Epoch 4.93: Loss = 0.406281
Epoch 4.94: Loss = 0.351181
Epoch 4.95: Loss = 0.379364
Epoch 4.96: Loss = 0.385086
Epoch 4.97: Loss = 0.374268
Epoch 4.98: Loss = 0.319077
Epoch 4.99: Loss = 0.386902
Epoch 4.100: Loss = 0.36261
TRAIN LOSS = 0.39473
TRAIN ACC = 88.3911 % (53037/60000)
Loss = 0.398148
Loss = 0.433624
Loss = 0.552872
Loss = 0.530914
Loss = 0.387466
Loss = 0.398193
Loss = 0.543488
Loss = 0.462158
Loss = 0.317352
Loss = 0.260025
Loss = 0.285873
Loss = 0.285782
Loss = 0.210938
Loss = 0.276917
Loss = 0.0871429
Loss = 0.218643
Loss = 0.594528
TEST LOSS = 0.362753
TEST ACC = 530.37 % (8947/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.430862
Epoch 5.2: Loss = 0.346512
Epoch 5.3: Loss = 0.453415
Epoch 5.4: Loss = 0.450104
Epoch 5.5: Loss = 0.388016
Epoch 5.6: Loss = 0.341324
Epoch 5.7: Loss = 0.336853
Epoch 5.8: Loss = 0.359222
Epoch 5.9: Loss = 0.369995
Epoch 5.10: Loss = 0.363007
Epoch 5.11: Loss = 0.379196
Epoch 5.12: Loss = 0.453033
Epoch 5.13: Loss = 0.346649
Epoch 5.14: Loss = 0.401611
Epoch 5.15: Loss = 0.340775
Epoch 5.16: Loss = 0.426682
Epoch 5.17: Loss = 0.379013
Epoch 5.18: Loss = 0.373047
Epoch 5.19: Loss = 0.384155
Epoch 5.20: Loss = 0.389816
Epoch 5.21: Loss = 0.332413
Epoch 5.22: Loss = 0.425705
Epoch 5.23: Loss = 0.447586
Epoch 5.24: Loss = 0.364807
Epoch 5.25: Loss = 0.415665
Epoch 5.26: Loss = 0.330429
Epoch 5.27: Loss = 0.41922
Epoch 5.28: Loss = 0.397415
Epoch 5.29: Loss = 0.341858
Epoch 5.30: Loss = 0.372574
Epoch 5.31: Loss = 0.36644
Epoch 5.32: Loss = 0.414459
Epoch 5.33: Loss = 0.332901
Epoch 5.34: Loss = 0.347961
Epoch 5.35: Loss = 0.446732
Epoch 5.36: Loss = 0.45015
Epoch 5.37: Loss = 0.360733
Epoch 5.38: Loss = 0.39682
Epoch 5.39: Loss = 0.388916
Epoch 5.40: Loss = 0.310318
Epoch 5.41: Loss = 0.339142
Epoch 5.42: Loss = 0.415298
Epoch 5.43: Loss = 0.315323
Epoch 5.44: Loss = 0.432983
Epoch 5.45: Loss = 0.391968
Epoch 5.46: Loss = 0.392517
Epoch 5.47: Loss = 0.428772
Epoch 5.48: Loss = 0.369247
Epoch 5.49: Loss = 0.358948
Epoch 5.50: Loss = 0.333878
Epoch 5.51: Loss = 0.370255
Epoch 5.52: Loss = 0.3638
Epoch 5.53: Loss = 0.355881
Epoch 5.54: Loss = 0.423386
Epoch 5.55: Loss = 0.391922
Epoch 5.56: Loss = 0.294754
Epoch 5.57: Loss = 0.396118
Epoch 5.58: Loss = 0.37471
Epoch 5.59: Loss = 0.412399
Epoch 5.60: Loss = 0.339249
Epoch 5.61: Loss = 0.368301
Epoch 5.62: Loss = 0.429214
Epoch 5.63: Loss = 0.316803
Epoch 5.64: Loss = 0.367004
Epoch 5.65: Loss = 0.381058
Epoch 5.66: Loss = 0.296646
Epoch 5.67: Loss = 0.365219
Epoch 5.68: Loss = 0.406219
Epoch 5.69: Loss = 0.371506
Epoch 5.70: Loss = 0.282104
Epoch 5.71: Loss = 0.366684
Epoch 5.72: Loss = 0.370911
Epoch 5.73: Loss = 0.463486
Epoch 5.74: Loss = 0.331482
Epoch 5.75: Loss = 0.338852
Epoch 5.76: Loss = 0.393616
Epoch 5.77: Loss = 0.423676
Epoch 5.78: Loss = 0.336761
Epoch 5.79: Loss = 0.320709
Epoch 5.80: Loss = 0.362152
Epoch 5.81: Loss = 0.409332
Epoch 5.82: Loss = 0.396744
Epoch 5.83: Loss = 0.340195
Epoch 5.84: Loss = 0.418472
Epoch 5.85: Loss = 0.398544
Epoch 5.86: Loss = 0.464478
Epoch 5.87: Loss = 0.380463
Epoch 5.88: Loss = 0.416397
Epoch 5.89: Loss = 0.372726
Epoch 5.90: Loss = 0.400391
Epoch 5.91: Loss = 0.342239
Epoch 5.92: Loss = 0.377502
Epoch 5.93: Loss = 0.36853
Epoch 5.94: Loss = 0.330505
Epoch 5.95: Loss = 0.347717
Epoch 5.96: Loss = 0.394455
Epoch 5.97: Loss = 0.31929
Epoch 5.98: Loss = 0.410599
Epoch 5.99: Loss = 0.389084
Epoch 5.100: Loss = 0.419388
TRAIN LOSS = 0.378677
TRAIN ACC = 88.9145 % (53352/60000)
Loss = 0.379272
Loss = 0.417587
Loss = 0.544617
Loss = 0.519012
Loss = 0.368805
Loss = 0.376999
Loss = 0.532425
Loss = 0.444809
Loss = 0.306961
Loss = 0.247482
Loss = 0.279663
Loss = 0.268997
Loss = 0.192108
Loss = 0.268158
Loss = 0.0782471
Loss = 0.206757
Loss = 0.571381
TEST LOSS = 0.348769
TEST ACC = 533.519 % (9003/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.363876
Epoch 6.2: Loss = 0.348145
Epoch 6.3: Loss = 0.338547
Epoch 6.4: Loss = 0.356384
Epoch 6.5: Loss = 0.362701
Epoch 6.6: Loss = 0.334259
Epoch 6.7: Loss = 0.369797
Epoch 6.8: Loss = 0.359589
Epoch 6.9: Loss = 0.335602
Epoch 6.10: Loss = 0.274185
Epoch 6.11: Loss = 0.329773
Epoch 6.12: Loss = 0.475006
Epoch 6.13: Loss = 0.403397
Epoch 6.14: Loss = 0.381409
Epoch 6.15: Loss = 0.360123
Epoch 6.16: Loss = 0.339935
Epoch 6.17: Loss = 0.363953
Epoch 6.18: Loss = 0.360992
Epoch 6.19: Loss = 0.415405
Epoch 6.20: Loss = 0.368149
Epoch 6.21: Loss = 0.396744
Epoch 6.22: Loss = 0.296341
Epoch 6.23: Loss = 0.301727
Epoch 6.24: Loss = 0.442383
Epoch 6.25: Loss = 0.34671
Epoch 6.26: Loss = 0.30751
Epoch 6.27: Loss = 0.375793
Epoch 6.28: Loss = 0.345093
Epoch 6.29: Loss = 0.359222
Epoch 6.30: Loss = 0.39357
Epoch 6.31: Loss = 0.37674
Epoch 6.32: Loss = 0.403046
Epoch 6.33: Loss = 0.409561
Epoch 6.34: Loss = 0.381577
Epoch 6.35: Loss = 0.324417
Epoch 6.36: Loss = 0.319763
Epoch 6.37: Loss = 0.336243
Epoch 6.38: Loss = 0.4254
Epoch 6.39: Loss = 0.332275
Epoch 6.40: Loss = 0.380142
Epoch 6.41: Loss = 0.342239
Epoch 6.42: Loss = 0.363342
Epoch 6.43: Loss = 0.346359
Epoch 6.44: Loss = 0.463913
Epoch 6.45: Loss = 0.349777
Epoch 6.46: Loss = 0.362244
Epoch 6.47: Loss = 0.366791
Epoch 6.48: Loss = 0.330627
Epoch 6.49: Loss = 0.309265
Epoch 6.50: Loss = 0.406189
Epoch 6.51: Loss = 0.348938
Epoch 6.52: Loss = 0.374939
Epoch 6.53: Loss = 0.367111
Epoch 6.54: Loss = 0.324265
Epoch 6.55: Loss = 0.312302
Epoch 6.56: Loss = 0.354446
Epoch 6.57: Loss = 0.415253
Epoch 6.58: Loss = 0.300903
Epoch 6.59: Loss = 0.48027
Epoch 6.60: Loss = 0.345398
Epoch 6.61: Loss = 0.431458
Epoch 6.62: Loss = 0.345749
Epoch 6.63: Loss = 0.323944
Epoch 6.64: Loss = 0.347824
Epoch 6.65: Loss = 0.423538
Epoch 6.66: Loss = 0.418991
Epoch 6.67: Loss = 0.381927
Epoch 6.68: Loss = 0.351288
Epoch 6.69: Loss = 0.308533
Epoch 6.70: Loss = 0.404587
Epoch 6.71: Loss = 0.363281
Epoch 6.72: Loss = 0.455093
Epoch 6.73: Loss = 0.411896
Epoch 6.74: Loss = 0.343597
Epoch 6.75: Loss = 0.315186
Epoch 6.76: Loss = 0.342392
Epoch 6.77: Loss = 0.346497
Epoch 6.78: Loss = 0.340164
Epoch 6.79: Loss = 0.364883
Epoch 6.80: Loss = 0.406601
Epoch 6.81: Loss = 0.455414
Epoch 6.82: Loss = 0.354065
Epoch 6.83: Loss = 0.381027
Epoch 6.84: Loss = 0.368546
Epoch 6.85: Loss = 0.367508
Epoch 6.86: Loss = 0.367447
Epoch 6.87: Loss = 0.39978
Epoch 6.88: Loss = 0.266098
Epoch 6.89: Loss = 0.348419
Epoch 6.90: Loss = 0.31691
Epoch 6.91: Loss = 0.330032
Epoch 6.92: Loss = 0.325073
Epoch 6.93: Loss = 0.406174
Epoch 6.94: Loss = 0.321838
Epoch 6.95: Loss = 0.431686
Epoch 6.96: Loss = 0.364838
Epoch 6.97: Loss = 0.340027
Epoch 6.98: Loss = 0.456726
Epoch 6.99: Loss = 0.350571
Epoch 6.100: Loss = 0.455185
TRAIN LOSS = 0.365952
TRAIN ACC = 89.3875 % (53635/60000)
Loss = 0.368988
Loss = 0.411179
Loss = 0.530136
Loss = 0.514099
Loss = 0.355515
Loss = 0.368546
Loss = 0.52594
Loss = 0.437744
Loss = 0.30307
Loss = 0.239914
Loss = 0.283447
Loss = 0.251007
Loss = 0.17421
Loss = 0.269653
Loss = 0.069931
Loss = 0.202377
Loss = 0.556396
TEST LOSS = 0.340601
TEST ACC = 536.349 % (9016/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.304977
Epoch 7.2: Loss = 0.397125
Epoch 7.3: Loss = 0.326508
Epoch 7.4: Loss = 0.288696
Epoch 7.5: Loss = 0.387711
Epoch 7.6: Loss = 0.419586
Epoch 7.7: Loss = 0.308594
Epoch 7.8: Loss = 0.42601
Epoch 7.9: Loss = 0.359253
Epoch 7.10: Loss = 0.282196
Epoch 7.11: Loss = 0.343231
Epoch 7.12: Loss = 0.376236
Epoch 7.13: Loss = 0.340714
Epoch 7.14: Loss = 0.405685
Epoch 7.15: Loss = 0.347092
Epoch 7.16: Loss = 0.304092
Epoch 7.17: Loss = 0.36412
Epoch 7.18: Loss = 0.33046
Epoch 7.19: Loss = 0.3703
Epoch 7.20: Loss = 0.402985
Epoch 7.21: Loss = 0.32251
Epoch 7.22: Loss = 0.347977
Epoch 7.23: Loss = 0.305923
Epoch 7.24: Loss = 0.370392
Epoch 7.25: Loss = 0.347061
Epoch 7.26: Loss = 0.348557
Epoch 7.27: Loss = 0.387527
Epoch 7.28: Loss = 0.361557
Epoch 7.29: Loss = 0.391068
Epoch 7.30: Loss = 0.354156
Epoch 7.31: Loss = 0.408829
Epoch 7.32: Loss = 0.395081
Epoch 7.33: Loss = 0.328583
Epoch 7.34: Loss = 0.368607
Epoch 7.35: Loss = 0.369217
Epoch 7.36: Loss = 0.459061
Epoch 7.37: Loss = 0.39325
Epoch 7.38: Loss = 0.36264
Epoch 7.39: Loss = 0.436417
Epoch 7.40: Loss = 0.380264
Epoch 7.41: Loss = 0.342697
Epoch 7.42: Loss = 0.328278
Epoch 7.43: Loss = 0.338257
Epoch 7.44: Loss = 0.349869
Epoch 7.45: Loss = 0.307205
Epoch 7.46: Loss = 0.412933
Epoch 7.47: Loss = 0.360931
Epoch 7.48: Loss = 0.32135
Epoch 7.49: Loss = 0.34256
Epoch 7.50: Loss = 0.447266
Epoch 7.51: Loss = 0.41655
Epoch 7.52: Loss = 0.338531
Epoch 7.53: Loss = 0.343216
Epoch 7.54: Loss = 0.328293
Epoch 7.55: Loss = 0.345917
Epoch 7.56: Loss = 0.367844
Epoch 7.57: Loss = 0.367325
Epoch 7.58: Loss = 0.317642
Epoch 7.59: Loss = 0.331696
Epoch 7.60: Loss = 0.35112
Epoch 7.61: Loss = 0.310898
Epoch 7.62: Loss = 0.364258
Epoch 7.63: Loss = 0.378586
Epoch 7.64: Loss = 0.331543
Epoch 7.65: Loss = 0.33168
Epoch 7.66: Loss = 0.300659
Epoch 7.67: Loss = 0.339569
Epoch 7.68: Loss = 0.359161
Epoch 7.69: Loss = 0.304245
Epoch 7.70: Loss = 0.34552
Epoch 7.71: Loss = 0.31749
Epoch 7.72: Loss = 0.363922
Epoch 7.73: Loss = 0.411545
Epoch 7.74: Loss = 0.287689
Epoch 7.75: Loss = 0.467422
Epoch 7.76: Loss = 0.31366
Epoch 7.77: Loss = 0.384369
Epoch 7.78: Loss = 0.306
Epoch 7.79: Loss = 0.384995
Epoch 7.80: Loss = 0.348328
Epoch 7.81: Loss = 0.356461
Epoch 7.82: Loss = 0.333664
Epoch 7.83: Loss = 0.38562
Epoch 7.84: Loss = 0.327179
Epoch 7.85: Loss = 0.311188
Epoch 7.86: Loss = 0.300323
Epoch 7.87: Loss = 0.38298
Epoch 7.88: Loss = 0.372818
Epoch 7.89: Loss = 0.375809
Epoch 7.90: Loss = 0.470261
Epoch 7.91: Loss = 0.361053
Epoch 7.92: Loss = 0.335159
Epoch 7.93: Loss = 0.351669
Epoch 7.94: Loss = 0.366501
Epoch 7.95: Loss = 0.357956
Epoch 7.96: Loss = 0.384216
Epoch 7.97: Loss = 0.353592
Epoch 7.98: Loss = 0.349579
Epoch 7.99: Loss = 0.371445
Epoch 7.100: Loss = 0.428482
TRAIN LOSS = 0.358124
TRAIN ACC = 89.6683 % (53803/60000)
Loss = 0.362457
Loss = 0.40773
Loss = 0.527863
Loss = 0.507721
Loss = 0.348618
Loss = 0.354599
Loss = 0.522232
Loss = 0.430389
Loss = 0.294235
Loss = 0.237122
Loss = 0.281921
Loss = 0.243225
Loss = 0.169052
Loss = 0.25827
Loss = 0.0658264
Loss = 0.195938
Loss = 0.550873
TEST LOSS = 0.334467
TEST ACC = 538.029 % (9043/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.381226
Epoch 8.2: Loss = 0.325867
Epoch 8.3: Loss = 0.460831
Epoch 8.4: Loss = 0.413605
Epoch 8.5: Loss = 0.396927
Epoch 8.6: Loss = 0.486618
Epoch 8.7: Loss = 0.383804
Epoch 8.8: Loss = 0.389496
Epoch 8.9: Loss = 0.377625
Epoch 8.10: Loss = 0.370056
Epoch 8.11: Loss = 0.28862
Epoch 8.12: Loss = 0.348511
Epoch 8.13: Loss = 0.267395
Epoch 8.14: Loss = 0.413025

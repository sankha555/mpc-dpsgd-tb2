Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.3587
Epoch 1.2: Loss = 2.32391
Epoch 1.3: Loss = 2.29904
Epoch 1.4: Loss = 2.274
Epoch 1.5: Loss = 2.22939
Epoch 1.6: Loss = 2.19363
Epoch 1.7: Loss = 2.17445
Epoch 1.8: Loss = 2.14099
Epoch 1.9: Loss = 2.12773
Epoch 1.10: Loss = 2.08844
Epoch 1.11: Loss = 2.06528
Epoch 1.12: Loss = 2.05057
Epoch 1.13: Loss = 2.00706
Epoch 1.14: Loss = 1.95222
Epoch 1.15: Loss = 1.94823
Epoch 1.16: Loss = 1.92058
Epoch 1.17: Loss = 1.87714
Epoch 1.18: Loss = 1.85506
Epoch 1.19: Loss = 1.84061
Epoch 1.20: Loss = 1.79076
Epoch 1.21: Loss = 1.77228
Epoch 1.22: Loss = 1.73172
Epoch 1.23: Loss = 1.70503
Epoch 1.24: Loss = 1.68394
Epoch 1.25: Loss = 1.65239
Epoch 1.26: Loss = 1.63702
Epoch 1.27: Loss = 1.59404
Epoch 1.28: Loss = 1.58929
Epoch 1.29: Loss = 1.51305
Epoch 1.30: Loss = 1.52202
Epoch 1.31: Loss = 1.50345
Epoch 1.32: Loss = 1.44667
Epoch 1.33: Loss = 1.4308
Epoch 1.34: Loss = 1.38576
Epoch 1.35: Loss = 1.37039
Epoch 1.36: Loss = 1.35718
Epoch 1.37: Loss = 1.32228
Epoch 1.38: Loss = 1.34357
Epoch 1.39: Loss = 1.30617
Epoch 1.40: Loss = 1.25618
Epoch 1.41: Loss = 1.26816
Epoch 1.42: Loss = 1.18219
Epoch 1.43: Loss = 1.20303
Epoch 1.44: Loss = 1.20622
Epoch 1.45: Loss = 1.16832
Epoch 1.46: Loss = 1.11227
Epoch 1.47: Loss = 1.07217
Epoch 1.48: Loss = 1.08098
Epoch 1.49: Loss = 1.10094
Epoch 1.50: Loss = 1.08475
Epoch 1.51: Loss = 1.01584
Epoch 1.52: Loss = 0.969788
Epoch 1.53: Loss = 0.963394
Epoch 1.54: Loss = 1.00829
Epoch 1.55: Loss = 0.997864
Epoch 1.56: Loss = 0.969299
Epoch 1.57: Loss = 0.979492
Epoch 1.58: Loss = 0.933975
Epoch 1.59: Loss = 0.974869
Epoch 1.60: Loss = 0.953094
Epoch 1.61: Loss = 0.869843
Epoch 1.62: Loss = 0.90773
Epoch 1.63: Loss = 0.923172
Epoch 1.64: Loss = 0.835297
Epoch 1.65: Loss = 0.872238
Epoch 1.66: Loss = 0.878143
Epoch 1.67: Loss = 0.805267
Epoch 1.68: Loss = 0.864594
Epoch 1.69: Loss = 0.883423
Epoch 1.70: Loss = 0.815063
Epoch 1.71: Loss = 0.844742
Epoch 1.72: Loss = 0.788391
Epoch 1.73: Loss = 0.791824
Epoch 1.74: Loss = 0.765137
Epoch 1.75: Loss = 0.754395
Epoch 1.76: Loss = 0.763824
Epoch 1.77: Loss = 0.745056
Epoch 1.78: Loss = 0.730743
Epoch 1.79: Loss = 0.700592
Epoch 1.80: Loss = 0.727081
Epoch 1.81: Loss = 0.759537
Epoch 1.82: Loss = 0.731659
Epoch 1.83: Loss = 0.707001
Epoch 1.84: Loss = 0.66507
Epoch 1.85: Loss = 0.707352
Epoch 1.86: Loss = 0.717484
Epoch 1.87: Loss = 0.702682
Epoch 1.88: Loss = 0.675018
Epoch 1.89: Loss = 0.740891
Epoch 1.90: Loss = 0.679535
Epoch 1.91: Loss = 0.628662
Epoch 1.92: Loss = 0.678421
Epoch 1.93: Loss = 0.720139
Epoch 1.94: Loss = 0.639404
Epoch 1.95: Loss = 0.680954
Epoch 1.96: Loss = 0.733765
Epoch 1.97: Loss = 0.659668
Epoch 1.98: Loss = 0.588303
Epoch 1.99: Loss = 0.62825
Epoch 1.100: Loss = 0.681534
Epoch 1.101: Loss = 0.642746
Epoch 1.102: Loss = 0.64917
Epoch 1.103: Loss = 0.634354
Epoch 1.104: Loss = 0.629105
Epoch 1.105: Loss = 0.604385
Epoch 1.106: Loss = 0.621017
Epoch 1.107: Loss = 0.569122
Epoch 1.108: Loss = 0.591583
Epoch 1.109: Loss = 0.530304
Epoch 1.110: Loss = 0.635498
Epoch 1.111: Loss = 0.537735
Epoch 1.112: Loss = 0.60527
Epoch 1.113: Loss = 0.55571
Epoch 1.114: Loss = 0.591644
Epoch 1.115: Loss = 0.560822
Epoch 1.116: Loss = 0.677444
Epoch 1.117: Loss = 0.610519
Epoch 1.118: Loss = 0.524414
Epoch 1.119: Loss = 0.543808
Epoch 1.120: Loss = 0.59166
TRAIN LOSS = 1.12321
TRAIN ACC = 69.017 % (41412/60000)
Loss = 0.595062
Loss = 0.621307
Loss = 0.74382
Loss = 0.676331
Loss = 0.719177
Loss = 0.606369
Loss = 0.5634
Loss = 0.73555
Loss = 0.681168
Loss = 0.651749
Loss = 0.359756
Loss = 0.514481
Loss = 0.360046
Loss = 0.53804
Loss = 0.416916
Loss = 0.433731
Loss = 0.427521
Loss = 0.235046
Loss = 0.423569
Loss = 0.669922
TEST LOSS = 0.548648
TEST ACC = 414.119 % (8362/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.550827
Epoch 2.2: Loss = 0.585709
Epoch 2.3: Loss = 0.563553
Epoch 2.4: Loss = 0.51474
Epoch 2.5: Loss = 0.485962
Epoch 2.6: Loss = 0.559723
Epoch 2.7: Loss = 0.549377
Epoch 2.8: Loss = 0.58139
Epoch 2.9: Loss = 0.515991
Epoch 2.10: Loss = 0.553696
Epoch 2.11: Loss = 0.525375
Epoch 2.12: Loss = 0.550079
Epoch 2.13: Loss = 0.525146
Epoch 2.14: Loss = 0.513718
Epoch 2.15: Loss = 0.563705
Epoch 2.16: Loss = 0.515381
Epoch 2.17: Loss = 0.53125
Epoch 2.18: Loss = 0.549377
Epoch 2.19: Loss = 0.546249
Epoch 2.20: Loss = 0.578094
Epoch 2.21: Loss = 0.522202
Epoch 2.22: Loss = 0.503708
Epoch 2.23: Loss = 0.542557
Epoch 2.24: Loss = 0.442368
Epoch 2.25: Loss = 0.459854
Epoch 2.26: Loss = 0.508026
Epoch 2.27: Loss = 0.551025
Epoch 2.28: Loss = 0.542664
Epoch 2.29: Loss = 0.501831
Epoch 2.30: Loss = 0.544388
Epoch 2.31: Loss = 0.550705
Epoch 2.32: Loss = 0.480316
Epoch 2.33: Loss = 0.555405
Epoch 2.34: Loss = 0.53447
Epoch 2.35: Loss = 0.471649
Epoch 2.36: Loss = 0.542389
Epoch 2.37: Loss = 0.472412
Epoch 2.38: Loss = 0.553375
Epoch 2.39: Loss = 0.518677
Epoch 2.40: Loss = 0.468414
Epoch 2.41: Loss = 0.410751
Epoch 2.42: Loss = 0.449722
Epoch 2.43: Loss = 0.505386
Epoch 2.44: Loss = 0.517944
Epoch 2.45: Loss = 0.560272
Epoch 2.46: Loss = 0.54393
Epoch 2.47: Loss = 0.440002
Epoch 2.48: Loss = 0.496521
Epoch 2.49: Loss = 0.531265
Epoch 2.50: Loss = 0.493607
Epoch 2.51: Loss = 0.536514
Epoch 2.52: Loss = 0.554535
Epoch 2.53: Loss = 0.500214
Epoch 2.54: Loss = 0.450867
Epoch 2.55: Loss = 0.512283
Epoch 2.56: Loss = 0.470764
Epoch 2.57: Loss = 0.498337
Epoch 2.58: Loss = 0.477097
Epoch 2.59: Loss = 0.470795
Epoch 2.60: Loss = 0.480591
Epoch 2.61: Loss = 0.470596
Epoch 2.62: Loss = 0.514008
Epoch 2.63: Loss = 0.552277
Epoch 2.64: Loss = 0.581436
Epoch 2.65: Loss = 0.472397
Epoch 2.66: Loss = 0.521713
Epoch 2.67: Loss = 0.476135
Epoch 2.68: Loss = 0.375366
Epoch 2.69: Loss = 0.492081
Epoch 2.70: Loss = 0.490921
Epoch 2.71: Loss = 0.376129
Epoch 2.72: Loss = 0.422333
Epoch 2.73: Loss = 0.497726
Epoch 2.74: Loss = 0.49086
Epoch 2.75: Loss = 0.504089
Epoch 2.76: Loss = 0.460144
Epoch 2.77: Loss = 0.45903
Epoch 2.78: Loss = 0.466782
Epoch 2.79: Loss = 0.403168
Epoch 2.80: Loss = 0.431702
Epoch 2.81: Loss = 0.442001
Epoch 2.82: Loss = 0.375153
Epoch 2.83: Loss = 0.465836
Epoch 2.84: Loss = 0.443665
Epoch 2.85: Loss = 0.529129
Epoch 2.86: Loss = 0.526306
Epoch 2.87: Loss = 0.480682
Epoch 2.88: Loss = 0.467087
Epoch 2.89: Loss = 0.478546
Epoch 2.90: Loss = 0.410675
Epoch 2.91: Loss = 0.396057
Epoch 2.92: Loss = 0.419769
Epoch 2.93: Loss = 0.491974
Epoch 2.94: Loss = 0.422516
Epoch 2.95: Loss = 0.455444
Epoch 2.96: Loss = 0.513901
Epoch 2.97: Loss = 0.44931
Epoch 2.98: Loss = 0.513199
Epoch 2.99: Loss = 0.467575
Epoch 2.100: Loss = 0.532074
Epoch 2.101: Loss = 0.398453
Epoch 2.102: Loss = 0.431778
Epoch 2.103: Loss = 0.477081
Epoch 2.104: Loss = 0.395874
Epoch 2.105: Loss = 0.459991
Epoch 2.106: Loss = 0.357742
Epoch 2.107: Loss = 0.393936
Epoch 2.108: Loss = 0.428055
Epoch 2.109: Loss = 0.456924
Epoch 2.110: Loss = 0.449936
Epoch 2.111: Loss = 0.43306
Epoch 2.112: Loss = 0.417953
Epoch 2.113: Loss = 0.418976
Epoch 2.114: Loss = 0.416229
Epoch 2.115: Loss = 0.476471
Epoch 2.116: Loss = 0.513092
Epoch 2.117: Loss = 0.418991
Epoch 2.118: Loss = 0.391556
Epoch 2.119: Loss = 0.415314
Epoch 2.120: Loss = 0.371582
TRAIN LOSS = 0.485748
TRAIN ACC = 85.1654 % (51101/60000)
Loss = 0.436508
Loss = 0.507339
Loss = 0.584839
Loss = 0.554138
Loss = 0.593445
Loss = 0.435959
Loss = 0.405823
Loss = 0.606384
Loss = 0.55304
Loss = 0.515198
Loss = 0.22847
Loss = 0.378418
Loss = 0.271011
Loss = 0.400101
Loss = 0.255096
Loss = 0.321503
Loss = 0.283142
Loss = 0.101288
Loss = 0.281158
Loss = 0.556503
TEST LOSS = 0.413468
TEST ACC = 511.009 % (8769/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.394272
Epoch 3.2: Loss = 0.42952
Epoch 3.3: Loss = 0.47757
Epoch 3.4: Loss = 0.460678
Epoch 3.5: Loss = 0.491486
Epoch 3.6: Loss = 0.454819
Epoch 3.7: Loss = 0.450882
Epoch 3.8: Loss = 0.494278
Epoch 3.9: Loss = 0.398056
Epoch 3.10: Loss = 0.488388
Epoch 3.11: Loss = 0.381165
Epoch 3.12: Loss = 0.455276
Epoch 3.13: Loss = 0.565475
Epoch 3.14: Loss = 0.462387
Epoch 3.15: Loss = 0.44487
Epoch 3.16: Loss = 0.443726
Epoch 3.17: Loss = 0.437302
Epoch 3.18: Loss = 0.41629
Epoch 3.19: Loss = 0.453384
Epoch 3.20: Loss = 0.41568
Epoch 3.21: Loss = 0.42247
Epoch 3.22: Loss = 0.382996
Epoch 3.23: Loss = 0.446732
Epoch 3.24: Loss = 0.428818
Epoch 3.25: Loss = 0.43251
Epoch 3.26: Loss = 0.374176
Epoch 3.27: Loss = 0.485336
Epoch 3.28: Loss = 0.383224
Epoch 3.29: Loss = 0.428436
Epoch 3.30: Loss = 0.463791
Epoch 3.31: Loss = 0.408752
Epoch 3.32: Loss = 0.425064
Epoch 3.33: Loss = 0.447769
Epoch 3.34: Loss = 0.352554
Epoch 3.35: Loss = 0.365799
Epoch 3.36: Loss = 0.46756
Epoch 3.37: Loss = 0.376404
Epoch 3.38: Loss = 0.417511
Epoch 3.39: Loss = 0.523087
Epoch 3.40: Loss = 0.376678
Epoch 3.41: Loss = 0.373688
Epoch 3.42: Loss = 0.456772
Epoch 3.43: Loss = 0.396225
Epoch 3.44: Loss = 0.436523
Epoch 3.45: Loss = 0.415024
Epoch 3.46: Loss = 0.483093
Epoch 3.47: Loss = 0.321838
Epoch 3.48: Loss = 0.423615
Epoch 3.49: Loss = 0.378006
Epoch 3.50: Loss = 0.466949
Epoch 3.51: Loss = 0.456528
Epoch 3.52: Loss = 0.431351
Epoch 3.53: Loss = 0.471069
Epoch 3.54: Loss = 0.417007
Epoch 3.55: Loss = 0.373596
Epoch 3.56: Loss = 0.453857
Epoch 3.57: Loss = 0.500153
Epoch 3.58: Loss = 0.388351
Epoch 3.59: Loss = 0.518311
Epoch 3.60: Loss = 0.457947
Epoch 3.61: Loss = 0.404312
Epoch 3.62: Loss = 0.368713
Epoch 3.63: Loss = 0.369232
Epoch 3.64: Loss = 0.439636
Epoch 3.65: Loss = 0.349686
Epoch 3.66: Loss = 0.450165
Epoch 3.67: Loss = 0.37645
Epoch 3.68: Loss = 0.495483
Epoch 3.69: Loss = 0.452789
Epoch 3.70: Loss = 0.381317
Epoch 3.71: Loss = 0.417145
Epoch 3.72: Loss = 0.435104
Epoch 3.73: Loss = 0.375717
Epoch 3.74: Loss = 0.377472
Epoch 3.75: Loss = 0.301804
Epoch 3.76: Loss = 0.431305
Epoch 3.77: Loss = 0.446899
Epoch 3.78: Loss = 0.400925
Epoch 3.79: Loss = 0.488037
Epoch 3.80: Loss = 0.377899
Epoch 3.81: Loss = 0.384506
Epoch 3.82: Loss = 0.429932
Epoch 3.83: Loss = 0.388977
Epoch 3.84: Loss = 0.414856
Epoch 3.85: Loss = 0.435806
Epoch 3.86: Loss = 0.457642
Epoch 3.87: Loss = 0.46434
Epoch 3.88: Loss = 0.408279
Epoch 3.89: Loss = 0.355087
Epoch 3.90: Loss = 0.312668
Epoch 3.91: Loss = 0.465134
Epoch 3.92: Loss = 0.397842
Epoch 3.93: Loss = 0.473007
Epoch 3.94: Loss = 0.40097
Epoch 3.95: Loss = 0.371994
Epoch 3.96: Loss = 0.376877
Epoch 3.97: Loss = 0.419983
Epoch 3.98: Loss = 0.393234
Epoch 3.99: Loss = 0.412277
Epoch 3.100: Loss = 0.399109
Epoch 3.101: Loss = 0.418945
Epoch 3.102: Loss = 0.40535
Epoch 3.103: Loss = 0.373459
Epoch 3.104: Loss = 0.427567
Epoch 3.105: Loss = 0.323547
Epoch 3.106: Loss = 0.439499
Epoch 3.107: Loss = 0.43512
Epoch 3.108: Loss = 0.40155
Epoch 3.109: Loss = 0.373123
Epoch 3.110: Loss = 0.427811
Epoch 3.111: Loss = 0.388947
Epoch 3.112: Loss = 0.366089
Epoch 3.113: Loss = 0.424133
Epoch 3.114: Loss = 0.499756
Epoch 3.115: Loss = 0.484985
Epoch 3.116: Loss = 0.394547
Epoch 3.117: Loss = 0.370087
Epoch 3.118: Loss = 0.373978
Epoch 3.119: Loss = 0.375076
Epoch 3.120: Loss = 0.406021
TRAIN LOSS = 0.420486
TRAIN ACC = 87.4664 % (52483/60000)
Loss = 0.400314
Loss = 0.492706
Loss = 0.587677
Loss = 0.541
Loss = 0.572113
Loss = 0.393051
Loss = 0.379196
Loss = 0.613052
Loss = 0.523346
Loss = 0.515991
Loss = 0.186661
Loss = 0.353287
Loss = 0.269226
Loss = 0.375885
Loss = 0.222824
Loss = 0.287811
Loss = 0.242096
Loss = 0.0749664
Loss = 0.23909
Loss = 0.539291
TEST LOSS = 0.390479
TEST ACC = 524.829 % (8863/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.370316
Epoch 4.2: Loss = 0.340454
Epoch 4.3: Loss = 0.480484
Epoch 4.4: Loss = 0.555573
Epoch 4.5: Loss = 0.375854
Epoch 4.6: Loss = 0.407074
Epoch 4.7: Loss = 0.446838
Epoch 4.8: Loss = 0.421387
Epoch 4.9: Loss = 0.44104
Epoch 4.10: Loss = 0.476868
Epoch 4.11: Loss = 0.415588
Epoch 4.12: Loss = 0.429657
Epoch 4.13: Loss = 0.486038
Epoch 4.14: Loss = 0.43161
Epoch 4.15: Loss = 0.355057
Epoch 4.16: Loss = 0.333679
Epoch 4.17: Loss = 0.490463
Epoch 4.18: Loss = 0.4496
Epoch 4.19: Loss = 0.380219
Epoch 4.20: Loss = 0.488983
Epoch 4.21: Loss = 0.4207
Epoch 4.22: Loss = 0.430801
Epoch 4.23: Loss = 0.393509
Epoch 4.24: Loss = 0.475815
Epoch 4.25: Loss = 0.430206
Epoch 4.26: Loss = 0.432632
Epoch 4.27: Loss = 0.434982
Epoch 4.28: Loss = 0.461639
Epoch 4.29: Loss = 0.368851
Epoch 4.30: Loss = 0.491241
Epoch 4.31: Loss = 0.414261
Epoch 4.32: Loss = 0.359146
Epoch 4.33: Loss = 0.38797
Epoch 4.34: Loss = 0.422302
Epoch 4.35: Loss = 0.407257
Epoch 4.36: Loss = 0.395203
Epoch 4.37: Loss = 0.404587
Epoch 4.38: Loss = 0.385834
Epoch 4.39: Loss = 0.386292
Epoch 4.40: Loss = 0.456772
Epoch 4.41: Loss = 0.497162
Epoch 4.42: Loss = 0.32103
Epoch 4.43: Loss = 0.539902
Epoch 4.44: Loss = 0.278137
Epoch 4.45: Loss = 0.445877
Epoch 4.46: Loss = 0.494873
Epoch 4.47: Loss = 0.397842
Epoch 4.48: Loss = 0.395828
Epoch 4.49: Loss = 0.434509
Epoch 4.50: Loss = 0.406342
Epoch 4.51: Loss = 0.472656
Epoch 4.52: Loss = 0.406784
Epoch 4.53: Loss = 0.441574
Epoch 4.54: Loss = 0.4431
Epoch 4.55: Loss = 0.352386
Epoch 4.56: Loss = 0.370285
Epoch 4.57: Loss = 0.296677
Epoch 4.58: Loss = 0.384949
Epoch 4.59: Loss = 0.366531
Epoch 4.60: Loss = 0.333588
Epoch 4.61: Loss = 0.346161
Epoch 4.62: Loss = 0.455963
Epoch 4.63: Loss = 0.459457
Epoch 4.64: Loss = 0.404251
Epoch 4.65: Loss = 0.363068
Epoch 4.66: Loss = 0.458878
Epoch 4.67: Loss = 0.32959
Epoch 4.68: Loss = 0.417419
Epoch 4.69: Loss = 0.358261
Epoch 4.70: Loss = 0.499863
Epoch 4.71: Loss = 0.412018
Epoch 4.72: Loss = 0.401367
Epoch 4.73: Loss = 0.377182
Epoch 4.74: Loss = 0.374969
Epoch 4.75: Loss = 0.362625
Epoch 4.76: Loss = 0.364029
Epoch 4.77: Loss = 0.272491
Epoch 4.78: Loss = 0.443298
Epoch 4.79: Loss = 0.364838
Epoch 4.80: Loss = 0.419373
Epoch 4.81: Loss = 0.415558
Epoch 4.82: Loss = 0.36647
Epoch 4.83: Loss = 0.350876
Epoch 4.84: Loss = 0.457397
Epoch 4.85: Loss = 0.337433
Epoch 4.86: Loss = 0.386749
Epoch 4.87: Loss = 0.423737
Epoch 4.88: Loss = 0.381927
Epoch 4.89: Loss = 0.440002
Epoch 4.90: Loss = 0.371948
Epoch 4.91: Loss = 0.41188
Epoch 4.92: Loss = 0.489548
Epoch 4.93: Loss = 0.413986
Epoch 4.94: Loss = 0.385681
Epoch 4.95: Loss = 0.463699
Epoch 4.96: Loss = 0.291061
Epoch 4.97: Loss = 0.337799
Epoch 4.98: Loss = 0.439453
Epoch 4.99: Loss = 0.375793
Epoch 4.100: Loss = 0.437424
Epoch 4.101: Loss = 0.454697
Epoch 4.102: Loss = 0.398911
Epoch 4.103: Loss = 0.361374
Epoch 4.104: Loss = 0.36586
Epoch 4.105: Loss = 0.34935
Epoch 4.106: Loss = 0.385681
Epoch 4.107: Loss = 0.420517
Epoch 4.108: Loss = 0.357132
Epoch 4.109: Loss = 0.339386
Epoch 4.110: Loss = 0.33461
Epoch 4.111: Loss = 0.390869
Epoch 4.112: Loss = 0.409882
Epoch 4.113: Loss = 0.345413
Epoch 4.114: Loss = 0.378647
Epoch 4.115: Loss = 0.387421
Epoch 4.116: Loss = 0.319397
Epoch 4.117: Loss = 0.417236
Epoch 4.118: Loss = 0.347687
Epoch 4.119: Loss = 0.265793
Epoch 4.120: Loss = 0.437881
TRAIN LOSS = 0.402893
TRAIN ACC = 88.4552 % (53076/60000)
Loss = 0.369186
Loss = 0.467209
Loss = 0.549011
Loss = 0.536133
Loss = 0.567505
Loss = 0.382813
Loss = 0.357864
Loss = 0.62204
Loss = 0.516739
Loss = 0.494141
Loss = 0.180664
Loss = 0.317398
Loss = 0.287125
Loss = 0.365341
Loss = 0.19548
Loss = 0.294159
Loss = 0.233871
Loss = 0.0587463
Loss = 0.210434
Loss = 0.504883
TEST LOSS = 0.375537
TEST ACC = 530.759 % (8947/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.37384
Epoch 5.2: Loss = 0.302856
Epoch 5.3: Loss = 0.302979
Epoch 5.4: Loss = 0.378464
Epoch 5.5: Loss = 0.400543
Epoch 5.6: Loss = 0.339722
Epoch 5.7: Loss = 0.410324
Epoch 5.8: Loss = 0.359283
Epoch 5.9: Loss = 0.329239
Epoch 5.10: Loss = 0.384796
Epoch 5.11: Loss = 0.358124
Epoch 5.12: Loss = 0.380554
Epoch 5.13: Loss = 0.438339
Epoch 5.14: Loss = 0.391785
Epoch 5.15: Loss = 0.386185
Epoch 5.16: Loss = 0.425644
Epoch 5.17: Loss = 0.38591
Epoch 5.18: Loss = 0.414001
Epoch 5.19: Loss = 0.377777
Epoch 5.20: Loss = 0.47493
Epoch 5.21: Loss = 0.42189
Epoch 5.22: Loss = 0.421692
Epoch 5.23: Loss = 0.376007
Epoch 5.24: Loss = 0.51886
Epoch 5.25: Loss = 0.341263
Epoch 5.26: Loss = 0.434006
Epoch 5.27: Loss = 0.385208
Epoch 5.28: Loss = 0.416153
Epoch 5.29: Loss = 0.423538
Epoch 5.30: Loss = 0.352142
Epoch 5.31: Loss = 0.383728
Epoch 5.32: Loss = 0.337845
Epoch 5.33: Loss = 0.361526
Epoch 5.34: Loss = 0.477737
Epoch 5.35: Loss = 0.401321
Epoch 5.36: Loss = 0.315201
Epoch 5.37: Loss = 0.381683
Epoch 5.38: Loss = 0.552353
Epoch 5.39: Loss = 0.368713
Epoch 5.40: Loss = 0.420914
Epoch 5.41: Loss = 0.345474
Epoch 5.42: Loss = 0.452698
Epoch 5.43: Loss = 0.317215
Epoch 5.44: Loss = 0.365311
Epoch 5.45: Loss = 0.409729
Epoch 5.46: Loss = 0.380447
Epoch 5.47: Loss = 0.465286
Epoch 5.48: Loss = 0.31456
Epoch 5.49: Loss = 0.319702
Epoch 5.50: Loss = 0.421982
Epoch 5.51: Loss = 0.384186
Epoch 5.52: Loss = 0.417191
Epoch 5.53: Loss = 0.263962
Epoch 5.54: Loss = 0.331726
Epoch 5.55: Loss = 0.370438
Epoch 5.56: Loss = 0.404007
Epoch 5.57: Loss = 0.327621
Epoch 5.58: Loss = 0.401947
Epoch 5.59: Loss = 0.422913
Epoch 5.60: Loss = 0.449326
Epoch 5.61: Loss = 0.404449
Epoch 5.62: Loss = 0.41835
Epoch 5.63: Loss = 0.445816
Epoch 5.64: Loss = 0.316254
Epoch 5.65: Loss = 0.348297
Epoch 5.66: Loss = 0.439423
Epoch 5.67: Loss = 0.430435
Epoch 5.68: Loss = 0.50531
Epoch 5.69: Loss = 0.438721
Epoch 5.70: Loss = 0.411163
Epoch 5.71: Loss = 0.358459
Epoch 5.72: Loss = 0.445999
Epoch 5.73: Loss = 0.335403
Epoch 5.74: Loss = 0.34491
Epoch 5.75: Loss = 0.377548
Epoch 5.76: Loss = 0.370148
Epoch 5.77: Loss = 0.403503
Epoch 5.78: Loss = 0.470612
Epoch 5.79: Loss = 0.360931
Epoch 5.80: Loss = 0.402054
Epoch 5.81: Loss = 0.490585
Epoch 5.82: Loss = 0.352829
Epoch 5.83: Loss = 0.399796
Epoch 5.84: Loss = 0.469025
Epoch 5.85: Loss = 0.367538
Epoch 5.86: Loss = 0.396423
Epoch 5.87: Loss = 0.328491
Epoch 5.88: Loss = 0.438354
Epoch 5.89: Loss = 0.474899
Epoch 5.90: Loss = 0.45256
Epoch 5.91: Loss = 0.368011
Epoch 5.92: Loss = 0.339218
Epoch 5.93: Loss = 0.347076
Epoch 5.94: Loss = 0.372177
Epoch 5.95: Loss = 0.492432
Epoch 5.96: Loss = 0.456924
Epoch 5.97: Loss = 0.466583
Epoch 5.98: Loss = 0.422729
Epoch 5.99: Loss = 0.372665
Epoch 5.100: Loss = 0.435883
Epoch 5.101: Loss = 0.440414
Epoch 5.102: Loss = 0.426956
Epoch 5.103: Loss = 0.391373
Epoch 5.104: Loss = 0.412064
Epoch 5.105: Loss = 0.297516
Epoch 5.106: Loss = 0.37352
Epoch 5.107: Loss = 0.412582
Epoch 5.108: Loss = 0.338608
Epoch 5.109: Loss = 0.41214
Epoch 5.110: Loss = 0.290009
Epoch 5.111: Loss = 0.417969
Epoch 5.112: Loss = 0.423325
Epoch 5.113: Loss = 0.328064
Epoch 5.114: Loss = 0.394089
Epoch 5.115: Loss = 0.579239
Epoch 5.116: Loss = 0.343048
Epoch 5.117: Loss = 0.413284
Epoch 5.118: Loss = 0.37175
Epoch 5.119: Loss = 0.340332
Epoch 5.120: Loss = 0.310745
TRAIN LOSS = 0.393845
TRAIN ACC = 89.0747 % (53447/60000)
Loss = 0.365723
Loss = 0.495697
Loss = 0.532974
Loss = 0.544083
Loss = 0.567612
Loss = 0.366806
Loss = 0.365768
Loss = 0.599945
Loss = 0.502121
Loss = 0.468658
Loss = 0.198868
Loss = 0.348694
Loss = 0.269348
Loss = 0.336273
Loss = 0.186401
Loss = 0.284973
Loss = 0.2117
Loss = 0.0535889
Loss = 0.222336
Loss = 0.505219
TEST LOSS = 0.371339
TEST ACC = 534.47 % (8972/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.372559
Epoch 6.2: Loss = 0.354248
Epoch 6.3: Loss = 0.352127
Epoch 6.4: Loss = 0.375137
Epoch 6.5: Loss = 0.326065
Epoch 6.6: Loss = 0.450409
Epoch 6.7: Loss = 0.409729
Epoch 6.8: Loss = 0.353394
Epoch 6.9: Loss = 0.395813
Epoch 6.10: Loss = 0.446762
Epoch 6.11: Loss = 0.36937
Epoch 6.12: Loss = 0.414993
Epoch 6.13: Loss = 0.381439
Epoch 6.14: Loss = 0.375427
Epoch 6.15: Loss = 0.353424
Epoch 6.16: Loss = 0.374146
Epoch 6.17: Loss = 0.481384
Epoch 6.18: Loss = 0.452667
Epoch 6.19: Loss = 0.338913
Epoch 6.20: Loss = 0.463348
Epoch 6.21: Loss = 0.474335
Epoch 6.22: Loss = 0.46669
Epoch 6.23: Loss = 0.492462
Epoch 6.24: Loss = 0.429123
Epoch 6.25: Loss = 0.362732
Epoch 6.26: Loss = 0.529327
Epoch 6.27: Loss = 0.463974
Epoch 6.28: Loss = 0.332932
Epoch 6.29: Loss = 0.404724
Epoch 6.30: Loss = 0.386887
Epoch 6.31: Loss = 0.435883
Epoch 6.32: Loss = 0.390503
Epoch 6.33: Loss = 0.402756
Epoch 6.34: Loss = 0.369659
Epoch 6.35: Loss = 0.308472
Epoch 6.36: Loss = 0.419479
Epoch 6.37: Loss = 0.328049
Epoch 6.38: Loss = 0.37294
Epoch 6.39: Loss = 0.31189
Epoch 6.40: Loss = 0.341309
Epoch 6.41: Loss = 0.492447
Epoch 6.42: Loss = 0.311401
Epoch 6.43: Loss = 0.35141
Epoch 6.44: Loss = 0.39917
Epoch 6.45: Loss = 0.457016
Epoch 6.46: Loss = 0.447815
Epoch 6.47: Loss = 0.406769
Epoch 6.48: Loss = 0.458633
Epoch 6.49: Loss = 0.379303
Epoch 6.50: Loss = 0.38707
Epoch 6.51: Loss = 0.303436
Epoch 6.52: Loss = 0.335007
Epoch 6.53: Loss = 0.430756
Epoch 6.54: Loss = 0.355865
Epoch 6.55: Loss = 0.415695
Epoch 6.56: Loss = 0.374313
Epoch 6.57: Loss = 0.311066
Epoch 6.58: Loss = 0.280273
Epoch 6.59: Loss = 0.419235
Epoch 6.60: Loss = 0.445358
Epoch 6.61: Loss = 0.321198
Epoch 6.62: Loss = 0.408066
Epoch 6.63: Loss = 0.430527
Epoch 6.64: Loss = 0.392517
Epoch 6.65: Loss = 0.423035
Epoch 6.66: Loss = 0.310349
Epoch 6.67: Loss = 0.268066
Epoch 6.68: Loss = 0.354187
Epoch 6.69: Loss = 0.444046
Epoch 6.70: Loss = 0.419022
Epoch 6.71: Loss = 0.404068
Epoch 6.72: Loss = 0.457809
Epoch 6.73: Loss = 0.349701
Epoch 6.74: Loss = 0.414459
Epoch 6.75: Loss = 0.372253
Epoch 6.76: Loss = 0.386627
Epoch 6.77: Loss = 0.281433
Epoch 6.78: Loss = 0.480057
Epoch 6.79: Loss = 0.353699
Epoch 6.80: Loss = 0.385574
Epoch 6.81: Loss = 0.368362
Epoch 6.82: Loss = 0.32959
Epoch 6.83: Loss = 0.373886
Epoch 6.84: Loss = 0.440216
Epoch 6.85: Loss = 0.484192
Epoch 6.86: Loss = 0.44603
Epoch 6.87: Loss = 0.351852
Epoch 6.88: Loss = 0.478256
Epoch 6.89: Loss = 0.382019
Epoch 6.90: Loss = 0.372101
Epoch 6.91: Loss = 0.373611
Epoch 6.92: Loss = 0.353241
Epoch 6.93: Loss = 0.338043
Epoch 6.94: Loss = 0.417847
Epoch 6.95: Loss = 0.403442
Epoch 6.96: Loss = 0.430389
Epoch 6.97: Loss = 0.527893
Epoch 6.98: Loss = 0.381226
Epoch 6.99: Loss = 0.382767
Epoch 6.100: Loss = 0.404465
Epoch 6.101: Loss = 0.329727
Epoch 6.102: Loss = 0.433182
Epoch 6.103: Loss = 0.427628
Epoch 6.104: Loss = 0.286438
Epoch 6.105: Loss = 0.387634
Epoch 6.106: Loss = 0.369751
Epoch 6.107: Loss = 0.314133
Epoch 6.108: Loss = 0.437744
Epoch 6.109: Loss = 0.377838
Epoch 6.110: Loss = 0.394516
Epoch 6.111: Loss = 0.403793
Epoch 6.112: Loss = 0.401978
Epoch 6.113: Loss = 0.395065
Epoch 6.114: Loss = 0.368591
Epoch 6.115: Loss = 0.304352
Epoch 6.116: Loss = 0.34729
Epoch 6.117: Loss = 0.348663
Epoch 6.118: Loss = 0.496323
Epoch 6.119: Loss = 0.41861
Epoch 6.120: Loss = 0.49649
TRAIN LOSS = 0.392197
TRAIN ACC = 89.5721 % (53746/60000)
Loss = 0.371414
Loss = 0.475494
Loss = 0.546082
Loss = 0.555649
Loss = 0.559586
Loss = 0.355774
Loss = 0.358459
Loss = 0.613007
Loss = 0.522003
Loss = 0.477875
Loss = 0.223907
Loss = 0.316391
Loss = 0.325485
Loss = 0.35498
Loss = 0.179398
Loss = 0.281921
Loss = 0.212112
Loss = 0.0389709
Loss = 0.244324
Loss = 0.517776
TEST LOSS = 0.37653
TEST ACC = 537.459 % (9008/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.30748
Epoch 7.2: Loss = 0.377457
Epoch 7.3: Loss = 0.404129
Epoch 7.4: Loss = 0.351151
Epoch 7.5: Loss = 0.481186
Epoch 7.6: Loss = 0.354736
Epoch 7.7: Loss = 0.366592
Epoch 7.8: Loss = 0.349304
Epoch 7.9: Loss = 0.356857
Epoch 7.10: Loss = 0.442429
Epoch 7.11: Loss = 0.410599
Epoch 7.12: Loss = 0.376831
Epoch 7.13: Loss = 0.293198
Epoch 7.14: Loss = 0.461777
Epoch 7.15: Loss = 0.31424
Epoch 7.16: Loss = 0.389664
Epoch 7.17: Loss = 0.35994
Epoch 7.18: Loss = 0.354965
Epoch 7.19: Loss = 0.393753
Epoch 7.20: Loss = 0.322525
Epoch 7.21: Loss = 0.329483
Epoch 7.22: Loss = 0.421509
Epoch 7.23: Loss = 0.373871
Epoch 7.24: Loss = 0.409485
Epoch 7.25: Loss = 0.526062
Epoch 7.26: Loss = 0.425995
Epoch 7.27: Loss = 0.533997
Epoch 7.28: Loss = 0.345657
Epoch 7.29: Loss = 0.372604
Epoch 7.30: Loss = 0.561859
Epoch 7.31: Loss = 0.38501
Epoch 7.32: Loss = 0.54071
Epoch 7.33: Loss = 0.438629
Epoch 7.34: Loss = 0.419785
Epoch 7.35: Loss = 0.344376
Epoch 7.36: Loss = 0.387482
Epoch 7.37: Loss = 0.424149
Epoch 7.38: Loss = 0.477005
Epoch 7.39: Loss = 0.426117
Epoch 7.40: Loss = 0.421616
Epoch 7.41: Loss = 0.449661
Epoch 7.42: Loss = 0.323135
Epoch 7.43: Loss = 0.299332
Epoch 7.44: Loss = 0.295807
Epoch 7.45: Loss = 0.328171
Epoch 7.46: Loss = 0.400681
Epoch 7.47: Loss = 0.399002
Epoch 7.48: Loss = 0.298828
Epoch 7.49: Loss = 0.529022
Epoch 7.50: Loss = 0.462402
Epoch 7.51: Loss = 0.459518
Epoch 7.52: Loss = 0.459351
Epoch 7.53: Loss = 0.421249
Epoch 7.54: Loss = 0.445358
Epoch 7.55: Loss = 0.430954
Epoch 7.56: Loss = 0.412064
Epoch 7.57: Loss = 0.416122
Epoch 7.58: Loss = 0.296082
Epoch 7.59: Loss = 0.349731
Epoch 7.60: Loss = 0.269272
Epoch 7.61: Loss = 0.377167
Epoch 7.62: Loss = 0.404373
Epoch 7.63: Loss = 0.331863
Epoch 7.64: Loss = 0.339676
Epoch 7.65: Loss = 0.40271
Epoch 7.66: Loss = 0.349854
Epoch 7.67: Loss = 0.337387
Epoch 7.68: Loss = 0.455185
Epoch 7.69: Loss = 0.281128
Epoch 7.70: Loss = 0.298798
Epoch 7.71: Loss = 0.371674
Epoch 7.72: Loss = 0.448029
Epoch 7.73: Loss = 0.351425
Epoch 7.74: Loss = 0.327286
Epoch 7.75: Loss = 0.358215
Epoch 7.76: Loss = 0.431061
Epoch 7.77: Loss = 0.335098
Epoch 7.78: Loss = 0.51442
Epoch 7.79: Loss = 0.402222
Epoch 7.80: Loss = 0.569183
Epoch 7.81: Loss = 0.330933
Epoch 7.82: Loss = 0.366348
Epoch 7.83: Loss = 0.389359
Epoch 7.84: Loss = 0.488144
Epoch 7.85: Loss = 0.47995
Epoch 7.86: Loss = 0.472321
Epoch 7.87: Loss = 0.336655
Epoch 7.88: Loss = 0.471161
Epoch 7.89: Loss = 0.465164
Epoch 7.90: Loss = 0.372147
Epoch 7.91: Loss = 0.371796
Epoch 7.92: Loss = 0.398178
Epoch 7.93: Loss = 0.452988
Epoch 7.94: Loss = 0.399689
Epoch 7.95: Loss = 0.419617
Epoch 7.96: Loss = 0.475327
Epoch 7.97: Loss = 0.413422
Epoch 7.98: Loss = 0.368576
Epoch 7.99: Loss = 0.356476
Epoch 7.100: Loss = 0.467575
Epoch 7.101: Loss = 0.4552
Epoch 7.102: Loss = 0.312042
Epoch 7.103: Loss = 0.415756
Epoch 7.104: Loss = 0.45253
Epoch 7.105: Loss = 0.407822
Epoch 7.106: Loss = 0.360153
Epoch 7.107: Loss = 0.349472
Epoch 7.108: Loss = 0.40979
Epoch 7.109: Loss = 0.403778
Epoch 7.110: Loss = 0.427734
Epoch 7.111: Loss = 0.378326
Epoch 7.112: Loss = 0.406769
Epoch 7.113: Loss = 0.302246
Epoch 7.114: Loss = 0.392548
Epoch 7.115: Loss = 0.431244
Epoch 7.116: Loss = 0.350449
Epoch 7.117: Loss = 0.342926
Epoch 7.118: Loss = 0.394806
Epoch 7.119: Loss = 0.386276
Epoch 7.120: Loss = 0.343658
TRAIN LOSS = 0.395706
TRAIN ACC = 89.6683 % (53803/60000)
Loss = 0.372055
Loss = 0.475784
Loss = 0.555405
Loss = 0.570526
Loss = 0.573959
Loss = 0.362427
Loss = 0.371841
Loss = 0.604019
Loss = 0.528687
Loss = 0.484512
Loss = 0.204514
Loss = 0.320175
Loss = 0.311615
Loss = 0.360245
Loss = 0.196976
Loss = 0.278549
Loss = 0.230087
Loss = 0.0481262
Loss = 0.222397
Loss = 0.539764
TEST LOSS = 0.380583
TEST ACC = 538.029 % (9004/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.443237
Epoch 8.2: Loss = 0.352951
Epoch 8.3: Loss = 0.350616
Epoch 8.4: Loss = 0.278183
Epoch 8.5: Loss = 0.407822
Epoch 8.6: Loss = 0.388016
Epoch 8.7: Loss = 0.360367
Epoch 8.8: Loss = 0.335403
Epoch 8.9: Loss = 0.405243
Epoch 8.10: Loss = 0.341904
Epoch 8.11: Loss = 0.449341
Epoch 8.12: Loss = 0.448929
Epoch 8.13: Loss = 0.416183
Epoch 8.14: Loss = 0.420212
Epoch 8.15: Loss = 0.392731
Epoch 8.16: Loss = 0.373352
Epoch 8.17: Loss = 0.423645
Epoch 8.18: Loss = 0.33992
Epoch 8.19: Loss = 0.399857
Epoch 8.20: Loss = 0.441681
Epoch 8.21: Loss = 0.312057
Epoch 8.22: Loss = 0.432404
Epoch 8.23: Loss = 0.459457
Epoch 8.24: Loss = 0.382477
Epoch 8.25: Loss = 0.360809
Epoch 8.26: Loss = 0.476807
Epoch 8.27: Loss = 0.471283
Epoch 8.28: Loss = 0.315552
Epoch 8.29: Loss = 0.537323
Epoch 8.30: Loss = 0.468384
Epoch 8.31: Loss = 0.39003
Epoch 8.32: Loss = 0.388779
Epoch 8.33: Loss = 0.450485
Epoch 8.34: Loss = 0.420105
Epoch 8.35: Loss = 0.461273
Epoch 8.36: Loss = 0.557663
Epoch 8.37: Loss = 0.336365
Epoch 8.38: Loss = 0.438828
Epoch 8.39: Loss = 0.389175
Epoch 8.40: Loss = 0.405014
Epoch 8.41: Loss = 0.331879
Epoch 8.42: Loss = 0.452744
Epoch 8.43: Loss = 0.388229
Epoch 8.44: Loss = 0.387497
Epoch 8.45: Loss = 0.327164
Epoch 8.46: Loss = 0.374863
Epoch 8.47: Loss = 0.437576
Epoch 8.48: Loss = 0.462341
Epoch 8.49: Loss = 0.380234
Epoch 8.50: Loss = 0.395355
Epoch 8.51: Loss = 0.369736
Epoch 8.52: Loss = 0.402328
Epoch 8.53: Loss = 0.433273
Epoch 8.54: Loss = 0.352112
Epoch 8.55: Loss = 0.344376
Epoch 8.56: Loss = 0.464935
Epoch 8.57: Loss = 0.372513
Epoch 8.58: Loss = 0.322632
Epoch 8.59: Loss = 0.29538
Epoch 8.60: Loss = 0.360245
Epoch 8.61: Loss = 0.399124
Epoch 8.62: Loss = 0.372482
Epoch 8.63: Loss = 0.352341
Epoch 8.64: Loss = 0.464996
Epoch 8.65: Loss = 0.310211
Epoch 8.66: Loss = 0.378586
Epoch 8.67: Loss = 0.401077
Epoch 8.68: Loss = 0.35051
Epoch 8.69: Loss = 0.380798
Epoch 8.70: Loss = 0.378662
Epoch 8.71: Loss = 0.408234
Epoch 8.72: Loss = 0.334244
Epoch 8.73: Loss = 0.439209
Epoch 8.74: Loss = 0.483734
Epoch 8.75: Loss = 0.417114
Epoch 8.76: Loss = 0.363281
Epoch 8.77: Loss = 0.470444
Epoch 8.78: Loss = 0.357147
Epoch 8.79: Loss = 0.382095
Epoch 8.80: Loss = 0.322052
Epoch 8.81: Loss = 0.397247
Epoch 8.82: Loss = 0.496155
Epoch 8.83: Loss = 0.32756
Epoch 8.84: Loss = 0.328873
Epoch 8.85: Loss = 0.383804
Epoch 8.86: Loss = 0.351089
Epoch 8.87: Loss = 0.431381
Epoch 8.88: Loss = 0.298187
Epoch 8.89: Loss = 0.533066
Epoch 8.90: Loss = 0.502731
Epoch 8.91: Loss = 0.454102
Epoch 8.92: Loss = 0.41713
Epoch 8.93: Loss = 0.251816
Epoch 8.94: Loss = 0.286713
Epoch 8.95: Loss = 0.457809
Epoch 8.96: Loss = 0.449173
Epoch 8.97: Loss = 0.381546
Epoch 8.98: Loss = 0.455276
Epoch 8.99: Loss = 0.414948
Epoch 8.100: Loss = 0.533188
Epoch 8.101: Loss = 0.408737
Epoch 8.102: Loss = 0.450439
Epoch 8.103: Loss = 0.348404
Epoch 8.104: Loss = 0.319412
Epoch 8.105: Loss = 0.362961
Epoch 8.106: Loss = 0.436188
Epoch 8.107: Loss = 0.449585
Epoch 8.108: Loss = 0.28598
Epoch 8.109: Loss = 0.282333
Epoch 8.110: Loss = 0.538147
Epoch 8.111: Loss = 0.412277
Epoch 8.112: Loss = 0.42897
Epoch 8.113: Loss = 0.406723
Epoch 8.114: Loss = 0.425385
Epoch 8.115: Loss = 0.625702
Epoch 8.116: Loss = 0.314362
Epoch 8.117: Loss = 0.453979
Epoch 8.118: Loss = 0.428787
Epoch 8.119: Loss = 0.465378
Epoch 8.120: Loss = 0.324173
TRAIN LOSS = 0.399704
TRAIN ACC = 89.6927 % (53818/60000)
Loss = 0.396622
Loss = 0.482101
Loss = 0.563416
Loss = 0.554123
Loss = 0.605331
Loss = 0.371979
Loss = 0.38945
Loss = 0.635132
Loss = 0.523819
Loss = 0.472595
Loss = 0.170776
Loss = 0.357483
Loss = 0.322098
Loss = 0.345383
Loss = 0.204849
Loss = 0.297882
Loss = 0.225845
Loss = 0.0457916
Loss = 0.212753
Loss = 0.588821
TEST LOSS = 0.388312
TEST ACC = 538.179 % (8991/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.408356
Epoch 9.2: Loss = 0.315277
Epoch 9.3: Loss = 0.435822
Epoch 9.4: Loss = 0.382614
Epoch 9.5: Loss = 0.444748
Epoch 9.6: Loss = 0.394638
Epoch 9.7: Loss = 0.337463
Epoch 9.8: Loss = 0.379028
Epoch 9.9: Loss = 0.375122
Epoch 9.10: Loss = 0.368896
Epoch 9.11: Loss = 0.405563
Epoch 9.12: Loss = 0.370316
Epoch 9.13: Loss = 0.437637
Epoch 9.14: Loss = 0.325363
Epoch 9.15: Loss = 0.353714
Epoch 9.16: Loss = 0.327057
Epoch 9.17: Loss = 0.395493
Epoch 9.18: Loss = 0.383224
Epoch 9.19: Loss = 0.471603
Epoch 9.20: Loss = 0.393753
Epoch 9.21: Loss = 0.394638
Epoch 9.22: Loss = 0.313324
Epoch 9.23: Loss = 0.448715
Epoch 9.24: Loss = 0.43074
Epoch 9.25: Loss = 0.450836
Epoch 9.26: Loss = 0.412552
Epoch 9.27: Loss = 0.495193
Epoch 9.28: Loss = 0.381516
Epoch 9.29: Loss = 0.416397
Epoch 9.30: Loss = 0.386978
Epoch 9.31: Loss = 0.436493
Epoch 9.32: Loss = 0.436661
Epoch 9.33: Loss = 0.411057
Epoch 9.34: Loss = 0.446777
Epoch 9.35: Loss = 0.435959
Epoch 9.36: Loss = 0.471359
Epoch 9.37: Loss = 0.351898
Epoch 9.38: Loss = 0.450775
Epoch 9.39: Loss = 0.425278
Epoch 9.40: Loss = 0.380508
Epoch 9.41: Loss = 0.327469
Epoch 9.42: Loss = 0.325867
Epoch 9.43: Loss = 0.348923
Epoch 9.44: Loss = 0.402466
Epoch 9.45: Loss = 0.336487
Epoch 9.46: Loss = 0.554718
Epoch 9.47: Loss = 0.249664
Epoch 9.48: Loss = 0.351608
Epoch 9.49: Loss = 0.514404
Epoch 9.50: Loss = 0.299057
Epoch 9.51: Loss = 0.494644
Epoch 9.52: Loss = 0.292557
Epoch 9.53: Loss = 0.332718
Epoch 9.54: Loss = 0.3974
Epoch 9.55: Loss = 0.38855
Epoch 9.56: Loss = 0.331207
Epoch 9.57: Loss = 0.341812
Epoch 9.58: Loss = 0.313766
Epoch 9.59: Loss = 0.323242
Epoch 9.60: Loss = 0.397507
Epoch 9.61: Loss = 0.377884
Epoch 9.62: Loss = 0.463882
Epoch 9.63: Loss = 0.371124
Epoch 9.64: Loss = 0.40123
Epoch 9.65: Loss = 0.338348
Epoch 9.66: Loss = 0.408768
Epoch 9.67: Loss = 0.373917
Epoch 9.68: Loss = 0.471954
Epoch 9.69: Loss = 0.345001
Epoch 9.70: Loss = 0.418579
Epoch 9.71: Loss = 0.445633
Epoch 9.72: Loss = 0.397858
Epoch 9.73: Loss = 0.408737
Epoch 9.74: Loss = 0.341827
Epoch 9.75: Loss = 0.345108
Epoch 9.76: Loss = 0.447205
Epoch 9.77: Loss = 0.429245
Epoch 9.78: Loss = 0.389862
Epoch 9.79: Loss = 0.439301
Epoch 9.80: Loss = 0.528381
Epoch 9.81: Loss = 0.539063
Epoch 9.82: Loss = 0.435455
Epoch 9.83: Loss = 0.427231
Epoch 9.84: Loss = 0.352829
Epoch 9.85: Loss = 0.428162
Epoch 9.86: Loss = 0.385773
Epoch 9.87: Loss = 0.453171
Epoch 9.88: Loss = 0.383514
Epoch 9.89: Loss = 0.360916
Epoch 9.90: Loss = 0.554718
Epoch 9.91: Loss = 0.440964
Epoch 9.92: Loss = 0.374527
Epoch 9.93: Loss = 0.529861
Epoch 9.94: Loss = 0.429489
Epoch 9.95: Loss = 0.44693
Epoch 9.96: Loss = 0.425095
Epoch 9.97: Loss = 0.311508
Epoch 9.98: Loss = 0.340958
Epoch 9.99: Loss = 0.577972
Epoch 9.100: Loss = 0.409866
Epoch 9.101: Loss = 0.432327
Epoch 9.102: Loss = 0.296478
Epoch 9.103: Loss = 0.409378
Epoch 9.104: Loss = 0.401672
Epoch 9.105: Loss = 0.470154
Epoch 9.106: Loss = 0.362823
Epoch 9.107: Loss = 0.415619
Epoch 9.108: Loss = 0.380798
Epoch 9.109: Loss = 0.412567
Epoch 9.110: Loss = 0.440079
Epoch 9.111: Loss = 0.370071
Epoch 9.112: Loss = 0.360931
Epoch 9.113: Loss = 0.490143
Epoch 9.114: Loss = 0.481293
Epoch 9.115: Loss = 0.347519
Epoch 9.116: Loss = 0.390091
Epoch 9.117: Loss = 0.406525
Epoch 9.118: Loss = 0.329712
Epoch 9.119: Loss = 0.533401
Epoch 9.120: Loss = 0.532471
TRAIN LOSS = 0.403519
TRAIN ACC = 89.9124 % (53950/60000)
Loss = 0.402573
Loss = 0.473312
Loss = 0.567291
Loss = 0.59053
Loss = 0.594803
Loss = 0.365097
Loss = 0.378418
Loss = 0.63681
Loss = 0.536652
Loss = 0.490845
Loss = 0.171829
Loss = 0.3134
Loss = 0.336777
Loss = 0.339203
Loss = 0.193466
Loss = 0.275177
Loss = 0.190887
Loss = 0.0401154
Loss = 0.210358
Loss = 0.581955
TEST LOSS = 0.384475
TEST ACC = 539.499 % (9033/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.368927
Epoch 10.2: Loss = 0.386093
Epoch 10.3: Loss = 0.418472
Epoch 10.4: Loss = 0.341965
Epoch 10.5: Loss = 0.343597
Epoch 10.6: Loss = 0.355835
Epoch 10.7: Loss = 0.405869
Epoch 10.8: Loss = 0.365479
Epoch 10.9: Loss = 0.592133
Epoch 10.10: Loss = 0.379898
Epoch 10.11: Loss = 0.393387
Epoch 10.12: Loss = 0.372803
Epoch 10.13: Loss = 0.220001
Epoch 10.14: Loss = 0.466309
Epoch 10.15: Loss = 0.397018
Epoch 10.16: Loss = 0.419601
Epoch 10.17: Loss = 0.445511
Epoch 10.18: Loss = 0.574997
Epoch 10.19: Loss = 0.441696
Epoch 10.20: Loss = 0.345886
Epoch 10.21: Loss = 0.51152
Epoch 10.22: Loss = 0.403885
Epoch 10.23: Loss = 0.339462
Epoch 10.24: Loss = 0.328674
Epoch 10.25: Loss = 0.482788
Epoch 10.26: Loss = 0.57283
Epoch 10.27: Loss = 0.447144
Epoch 10.28: Loss = 0.405365
Epoch 10.29: Loss = 0.423462
Epoch 10.30: Loss = 0.334442
Epoch 10.31: Loss = 0.344513
Epoch 10.32: Loss = 0.290634
Epoch 10.33: Loss = 0.537369
Epoch 10.34: Loss = 0.452057
Epoch 10.35: Loss = 0.368011
Epoch 10.36: Loss = 0.37587
Epoch 10.37: Loss = 0.415268
Epoch 10.38: Loss = 0.48999
Epoch 10.39: Loss = 0.436478
Epoch 10.40: Loss = 0.375443
Epoch 10.41: Loss = 0.251968
Epoch 10.42: Loss = 0.510773
Epoch 10.43: Loss = 0.378418
Epoch 10.44: Loss = 0.371994
Epoch 10.45: Loss = 0.407318
Epoch 10.46: Loss = 0.304459
Epoch 10.47: Loss = 0.475662
Epoch 10.48: Loss = 0.422592
Epoch 10.49: Loss = 0.442596
Epoch 10.50: Loss = 0.423386
Epoch 10.51: Loss = 0.404083
Epoch 10.52: Loss = 0.35321
Epoch 10.53: Loss = 0.430374
Epoch 10.54: Loss = 0.281876
Epoch 10.55: Loss = 0.381851
Epoch 10.56: Loss = 0.369904
Epoch 10.57: Loss = 0.423416
Epoch 10.58: Loss = 0.476242
Epoch 10.59: Loss = 0.374924
Epoch 10.60: Loss = 0.442947
Epoch 10.61: Loss = 0.47699
Epoch 10.62: Loss = 0.426254
Epoch 10.63: Loss = 0.458466
Epoch 10.64: Loss = 0.515244
Epoch 10.65: Loss = 0.39505
Epoch 10.66: Loss = 0.349548
Epoch 10.67: Loss = 0.311249
Epoch 10.68: Loss = 0.379471
Epoch 10.69: Loss = 0.446548
Epoch 10.70: Loss = 0.486191
Epoch 10.71: Loss = 0.44075
Epoch 10.72: Loss = 0.278091
Epoch 10.73: Loss = 0.357895
Epoch 10.74: Loss = 0.434036
Epoch 10.75: Loss = 0.328278
Epoch 10.76: Loss = 0.377136
Epoch 10.77: Loss = 0.467697
Epoch 10.78: Loss = 0.414246
Epoch 10.79: Loss = 0.456253
Epoch 10.80: Loss = 0.338593
Epoch 10.81: Loss = 0.32225
Epoch 10.82: Loss = 0.417068
Epoch 10.83: Loss = 0.343292
Epoch 10.84: Loss = 0.39267
Epoch 10.85: Loss = 0.379593
Epoch 10.86: Loss = 0.322357
Epoch 10.87: Loss = 0.470886
Epoch 10.88: Loss = 0.420303
Epoch 10.89: Loss = 0.470596
Epoch 10.90: Loss = 0.40033
Epoch 10.91: Loss = 0.498657
Epoch 10.92: Loss = 0.361847
Epoch 10.93: Loss = 0.338623
Epoch 10.94: Loss = 0.415329
Epoch 10.95: Loss = 0.414963
Epoch 10.96: Loss = 0.512436
Epoch 10.97: Loss = 0.370193
Epoch 10.98: Loss = 0.592407
Epoch 10.99: Loss = 0.382004
Epoch 10.100: Loss = 0.447113
Epoch 10.101: Loss = 0.409958
Epoch 10.102: Loss = 0.322159
Epoch 10.103: Loss = 0.365875
Epoch 10.104: Loss = 0.31871
Epoch 10.105: Loss = 0.410233
Epoch 10.106: Loss = 0.434769
Epoch 10.107: Loss = 0.391449
Epoch 10.108: Loss = 0.352234
Epoch 10.109: Loss = 0.437485
Epoch 10.110: Loss = 0.458954
Epoch 10.111: Loss = 0.468124
Epoch 10.112: Loss = 0.306763
Epoch 10.113: Loss = 0.410736
Epoch 10.114: Loss = 0.436707
Epoch 10.115: Loss = 0.415756
Epoch 10.116: Loss = 0.397263
Epoch 10.117: Loss = 0.357498
Epoch 10.118: Loss = 0.343903
Epoch 10.119: Loss = 0.410538
Epoch 10.120: Loss = 0.458359
TRAIN LOSS = 0.405136
TRAIN ACC = 90.0955 % (54059/60000)
Loss = 0.393417
Loss = 0.482742
Loss = 0.573761
Loss = 0.58548
Loss = 0.582596
Loss = 0.36972
Loss = 0.398117
Loss = 0.635223
Loss = 0.554916
Loss = 0.510925
Loss = 0.174805
Loss = 0.306305
Loss = 0.276428
Loss = 0.344559
Loss = 0.180206
Loss = 0.267944
Loss = 0.207443
Loss = 0.0410461
Loss = 0.223709
Loss = 0.569824
TEST LOSS = 0.383958
TEST ACC = 540.588 % (9037/10000)

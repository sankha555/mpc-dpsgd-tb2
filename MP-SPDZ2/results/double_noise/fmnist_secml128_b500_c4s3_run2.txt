Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.39116
Epoch 1.2: Loss = 2.25546
Epoch 1.3: Loss = 2.23163
Epoch 1.4: Loss = 2.20818
Epoch 1.5: Loss = 2.14627
Epoch 1.6: Loss = 2.07468
Epoch 1.7: Loss = 2.02872
Epoch 1.8: Loss = 1.99118
Epoch 1.9: Loss = 1.9543
Epoch 1.10: Loss = 1.91086
Epoch 1.11: Loss = 1.87604
Epoch 1.12: Loss = 1.83073
Epoch 1.13: Loss = 1.82794
Epoch 1.14: Loss = 1.74039
Epoch 1.15: Loss = 1.7249
Epoch 1.16: Loss = 1.72449
Epoch 1.17: Loss = 1.68353
Epoch 1.18: Loss = 1.62276
Epoch 1.19: Loss = 1.61899
Epoch 1.20: Loss = 1.55632
Epoch 1.21: Loss = 1.56114
Epoch 1.22: Loss = 1.49432
Epoch 1.23: Loss = 1.46042
Epoch 1.24: Loss = 1.4124
Epoch 1.25: Loss = 1.41423
Epoch 1.26: Loss = 1.33281
Epoch 1.27: Loss = 1.35925
Epoch 1.28: Loss = 1.31201
Epoch 1.29: Loss = 1.37173
Epoch 1.30: Loss = 1.30742
Epoch 1.31: Loss = 1.24733
Epoch 1.32: Loss = 1.2606
Epoch 1.33: Loss = 1.2149
Epoch 1.34: Loss = 1.20523
Epoch 1.35: Loss = 1.21785
Epoch 1.36: Loss = 1.12912
Epoch 1.37: Loss = 1.11865
Epoch 1.38: Loss = 1.15039
Epoch 1.39: Loss = 1.13158
Epoch 1.40: Loss = 1.13094
Epoch 1.41: Loss = 1.06595
Epoch 1.42: Loss = 1.05527
Epoch 1.43: Loss = 1.1039
Epoch 1.44: Loss = 1.05615
Epoch 1.45: Loss = 1.05986
Epoch 1.46: Loss = 1.09293
Epoch 1.47: Loss = 1.08748
Epoch 1.48: Loss = 1.04585
Epoch 1.49: Loss = 1.00378
Epoch 1.50: Loss = 1.01779
Epoch 1.51: Loss = 0.957458
Epoch 1.52: Loss = 0.950882
Epoch 1.53: Loss = 0.934158
Epoch 1.54: Loss = 0.956253
Epoch 1.55: Loss = 0.935516
Epoch 1.56: Loss = 0.954346
Epoch 1.57: Loss = 0.971283
Epoch 1.58: Loss = 0.937241
Epoch 1.59: Loss = 0.883453
Epoch 1.60: Loss = 0.954315
Epoch 1.61: Loss = 0.957901
Epoch 1.62: Loss = 0.970245
Epoch 1.63: Loss = 0.831207
Epoch 1.64: Loss = 0.885422
Epoch 1.65: Loss = 0.876587
Epoch 1.66: Loss = 0.887741
Epoch 1.67: Loss = 0.853271
Epoch 1.68: Loss = 0.840225
Epoch 1.69: Loss = 0.847748
Epoch 1.70: Loss = 0.860809
Epoch 1.71: Loss = 0.932434
Epoch 1.72: Loss = 0.83168
Epoch 1.73: Loss = 0.895676
Epoch 1.74: Loss = 0.863647
Epoch 1.75: Loss = 0.811325
Epoch 1.76: Loss = 0.779434
Epoch 1.77: Loss = 0.923996
Epoch 1.78: Loss = 0.895203
Epoch 1.79: Loss = 0.891983
Epoch 1.80: Loss = 0.806488
Epoch 1.81: Loss = 0.80542
Epoch 1.82: Loss = 0.871338
Epoch 1.83: Loss = 0.89505
Epoch 1.84: Loss = 0.830185
Epoch 1.85: Loss = 0.83609
Epoch 1.86: Loss = 0.662827
Epoch 1.87: Loss = 0.789429
Epoch 1.88: Loss = 0.68927
Epoch 1.89: Loss = 0.764648
Epoch 1.90: Loss = 0.783966
Epoch 1.91: Loss = 0.724579
Epoch 1.92: Loss = 0.810806
Epoch 1.93: Loss = 0.8311
Epoch 1.94: Loss = 0.797592
Epoch 1.95: Loss = 0.851074
Epoch 1.96: Loss = 0.741226
Epoch 1.97: Loss = 0.75853
Epoch 1.98: Loss = 0.839188
Epoch 1.99: Loss = 0.790482
Epoch 1.100: Loss = 0.805099
Epoch 1.101: Loss = 0.742081
Epoch 1.102: Loss = 0.777008
Epoch 1.103: Loss = 0.776672
Epoch 1.104: Loss = 0.744919
Epoch 1.105: Loss = 0.72197
Epoch 1.106: Loss = 0.847794
Epoch 1.107: Loss = 0.701111
Epoch 1.108: Loss = 0.769608
Epoch 1.109: Loss = 0.706726
Epoch 1.110: Loss = 0.805801
Epoch 1.111: Loss = 0.785065
Epoch 1.112: Loss = 0.761612
Epoch 1.113: Loss = 0.785034
Epoch 1.114: Loss = 0.761871
Epoch 1.115: Loss = 0.855392
Epoch 1.116: Loss = 0.790787
Epoch 1.117: Loss = 0.662613
Epoch 1.118: Loss = 0.830826
Epoch 1.119: Loss = 0.807205
Epoch 1.120: Loss = 0.733841
TRAIN LOSS = 1.10788
TRAIN ACC = 64.9475 % (38970/60000)
Loss = 0.685013
Loss = 0.813614
Loss = 0.782639
Loss = 0.699265
Loss = 0.709641
Loss = 0.8638
Loss = 0.876831
Loss = 0.838394
Loss = 0.755325
Loss = 0.732727
Loss = 0.856277
Loss = 0.776062
Loss = 0.798309
Loss = 0.79454
Loss = 0.739807
Loss = 0.793259
Loss = 0.731354
Loss = 0.772446
Loss = 0.817902
Loss = 0.744888
TEST LOSS = 0.779104
TEST ACC = 389.699 % (7263/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.752548
Epoch 2.2: Loss = 0.756516
Epoch 2.3: Loss = 0.742172
Epoch 2.4: Loss = 0.802979
Epoch 2.5: Loss = 0.687378
Epoch 2.6: Loss = 0.747711
Epoch 2.7: Loss = 0.716385
Epoch 2.8: Loss = 0.751373
Epoch 2.9: Loss = 0.782974
Epoch 2.10: Loss = 0.636169
Epoch 2.11: Loss = 0.733795
Epoch 2.12: Loss = 0.739838
Epoch 2.13: Loss = 0.728027
Epoch 2.14: Loss = 0.658463
Epoch 2.15: Loss = 0.719147
Epoch 2.16: Loss = 0.749084
Epoch 2.17: Loss = 0.767426
Epoch 2.18: Loss = 0.759964
Epoch 2.19: Loss = 0.68541
Epoch 2.20: Loss = 0.888611
Epoch 2.21: Loss = 0.681061
Epoch 2.22: Loss = 0.694656
Epoch 2.23: Loss = 0.66185
Epoch 2.24: Loss = 0.787338
Epoch 2.25: Loss = 0.747986
Epoch 2.26: Loss = 0.82637
Epoch 2.27: Loss = 0.735809
Epoch 2.28: Loss = 0.616455
Epoch 2.29: Loss = 0.674759
Epoch 2.30: Loss = 0.717316
Epoch 2.31: Loss = 0.684067
Epoch 2.32: Loss = 0.800674
Epoch 2.33: Loss = 0.712326
Epoch 2.34: Loss = 0.63063
Epoch 2.35: Loss = 0.723801
Epoch 2.36: Loss = 0.777115
Epoch 2.37: Loss = 0.676041
Epoch 2.38: Loss = 0.633041
Epoch 2.39: Loss = 0.749924
Epoch 2.40: Loss = 0.739685
Epoch 2.41: Loss = 0.747223
Epoch 2.42: Loss = 0.639481
Epoch 2.43: Loss = 0.696915
Epoch 2.44: Loss = 0.736938
Epoch 2.45: Loss = 0.782074
Epoch 2.46: Loss = 0.708893
Epoch 2.47: Loss = 0.785461
Epoch 2.48: Loss = 0.726563
Epoch 2.49: Loss = 0.579956
Epoch 2.50: Loss = 0.751434
Epoch 2.51: Loss = 0.717316
Epoch 2.52: Loss = 0.730408
Epoch 2.53: Loss = 0.728729
Epoch 2.54: Loss = 0.765503
Epoch 2.55: Loss = 0.703308
Epoch 2.56: Loss = 0.626495
Epoch 2.57: Loss = 0.713837
Epoch 2.58: Loss = 0.672302
Epoch 2.59: Loss = 0.727921
Epoch 2.60: Loss = 0.654205
Epoch 2.61: Loss = 0.709381
Epoch 2.62: Loss = 0.771637
Epoch 2.63: Loss = 0.685455
Epoch 2.64: Loss = 0.76683
Epoch 2.65: Loss = 0.752121
Epoch 2.66: Loss = 0.687332
Epoch 2.67: Loss = 0.705048
Epoch 2.68: Loss = 0.673782
Epoch 2.69: Loss = 0.640411
Epoch 2.70: Loss = 0.755432
Epoch 2.71: Loss = 0.741531
Epoch 2.72: Loss = 0.754761
Epoch 2.73: Loss = 0.722549
Epoch 2.74: Loss = 0.690475
Epoch 2.75: Loss = 0.622955
Epoch 2.76: Loss = 0.705338
Epoch 2.77: Loss = 0.702057
Epoch 2.78: Loss = 0.768997
Epoch 2.79: Loss = 0.71994
Epoch 2.80: Loss = 0.684372
Epoch 2.81: Loss = 0.648148
Epoch 2.82: Loss = 0.685608
Epoch 2.83: Loss = 0.673294
Epoch 2.84: Loss = 0.696945
Epoch 2.85: Loss = 0.666855
Epoch 2.86: Loss = 0.718994
Epoch 2.87: Loss = 0.729385
Epoch 2.88: Loss = 0.852173
Epoch 2.89: Loss = 0.584564
Epoch 2.90: Loss = 0.740677
Epoch 2.91: Loss = 0.708008
Epoch 2.92: Loss = 0.653458
Epoch 2.93: Loss = 0.652878
Epoch 2.94: Loss = 0.75795
Epoch 2.95: Loss = 0.633713
Epoch 2.96: Loss = 0.761932
Epoch 2.97: Loss = 0.664902
Epoch 2.98: Loss = 0.717392
Epoch 2.99: Loss = 0.703827
Epoch 2.100: Loss = 0.680252
Epoch 2.101: Loss = 0.678619
Epoch 2.102: Loss = 0.717117
Epoch 2.103: Loss = 0.652954
Epoch 2.104: Loss = 0.653076
Epoch 2.105: Loss = 0.714386
Epoch 2.106: Loss = 0.71788
Epoch 2.107: Loss = 0.692459
Epoch 2.108: Loss = 0.722961
Epoch 2.109: Loss = 0.615356
Epoch 2.110: Loss = 0.749939
Epoch 2.111: Loss = 0.675034
Epoch 2.112: Loss = 0.654999
Epoch 2.113: Loss = 0.777573
Epoch 2.114: Loss = 0.630768
Epoch 2.115: Loss = 0.687241
Epoch 2.116: Loss = 0.649994
Epoch 2.117: Loss = 0.761047
Epoch 2.118: Loss = 0.628281
Epoch 2.119: Loss = 0.751617
Epoch 2.120: Loss = 0.704773
TRAIN LOSS = 0.710831
TRAIN ACC = 75.5646 % (45341/60000)
Loss = 0.60643
Loss = 0.755859
Loss = 0.685608
Loss = 0.637527
Loss = 0.659683
Loss = 0.798035
Loss = 0.815826
Loss = 0.810699
Loss = 0.690155
Loss = 0.665054
Loss = 0.804153
Loss = 0.740341
Loss = 0.710022
Loss = 0.703812
Loss = 0.690018
Loss = 0.725571
Loss = 0.667953
Loss = 0.715866
Loss = 0.761871
Loss = 0.676041
TEST LOSS = 0.716026
TEST ACC = 453.409 % (7588/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.70462
Epoch 3.2: Loss = 0.674042
Epoch 3.3: Loss = 0.701279
Epoch 3.4: Loss = 0.724106
Epoch 3.5: Loss = 0.718979
Epoch 3.6: Loss = 0.603867
Epoch 3.7: Loss = 0.697144
Epoch 3.8: Loss = 0.640732
Epoch 3.9: Loss = 0.575317
Epoch 3.10: Loss = 0.744446
Epoch 3.11: Loss = 0.624939
Epoch 3.12: Loss = 0.744705
Epoch 3.13: Loss = 0.740875
Epoch 3.14: Loss = 0.669495
Epoch 3.15: Loss = 0.697296
Epoch 3.16: Loss = 0.703888
Epoch 3.17: Loss = 0.676895
Epoch 3.18: Loss = 0.747131
Epoch 3.19: Loss = 0.699295
Epoch 3.20: Loss = 0.665848
Epoch 3.21: Loss = 0.743927
Epoch 3.22: Loss = 0.613708
Epoch 3.23: Loss = 0.697449
Epoch 3.24: Loss = 0.689209
Epoch 3.25: Loss = 0.759506
Epoch 3.26: Loss = 0.614517
Epoch 3.27: Loss = 0.670898
Epoch 3.28: Loss = 0.738556
Epoch 3.29: Loss = 0.756195
Epoch 3.30: Loss = 0.6203
Epoch 3.31: Loss = 0.619354
Epoch 3.32: Loss = 0.616852
Epoch 3.33: Loss = 0.737854
Epoch 3.34: Loss = 0.695618
Epoch 3.35: Loss = 0.641815
Epoch 3.36: Loss = 0.683258
Epoch 3.37: Loss = 0.651764
Epoch 3.38: Loss = 0.614838
Epoch 3.39: Loss = 0.766983
Epoch 3.40: Loss = 0.704346
Epoch 3.41: Loss = 0.745407
Epoch 3.42: Loss = 0.757141
Epoch 3.43: Loss = 0.774796
Epoch 3.44: Loss = 0.615036
Epoch 3.45: Loss = 0.70578
Epoch 3.46: Loss = 0.630447
Epoch 3.47: Loss = 0.70314
Epoch 3.48: Loss = 0.6409
Epoch 3.49: Loss = 0.627777
Epoch 3.50: Loss = 0.750381
Epoch 3.51: Loss = 0.706665
Epoch 3.52: Loss = 0.790054
Epoch 3.53: Loss = 0.653656
Epoch 3.54: Loss = 0.696518
Epoch 3.55: Loss = 0.616165
Epoch 3.56: Loss = 0.645065
Epoch 3.57: Loss = 0.837097
Epoch 3.58: Loss = 0.676712
Epoch 3.59: Loss = 0.714859
Epoch 3.60: Loss = 0.683167
Epoch 3.61: Loss = 0.658844
Epoch 3.62: Loss = 0.845505
Epoch 3.63: Loss = 0.641037
Epoch 3.64: Loss = 0.623917
Epoch 3.65: Loss = 0.759964
Epoch 3.66: Loss = 0.747589
Epoch 3.67: Loss = 0.613815
Epoch 3.68: Loss = 0.746033
Epoch 3.69: Loss = 0.648117
Epoch 3.70: Loss = 0.779037
Epoch 3.71: Loss = 0.719025
Epoch 3.72: Loss = 0.753754
Epoch 3.73: Loss = 0.631256
Epoch 3.74: Loss = 0.64296
Epoch 3.75: Loss = 0.665009
Epoch 3.76: Loss = 0.688065
Epoch 3.77: Loss = 0.659805
Epoch 3.78: Loss = 0.667801
Epoch 3.79: Loss = 0.642929
Epoch 3.80: Loss = 0.742172
Epoch 3.81: Loss = 0.703339
Epoch 3.82: Loss = 0.644379
Epoch 3.83: Loss = 0.704971
Epoch 3.84: Loss = 0.687531
Epoch 3.85: Loss = 0.673782
Epoch 3.86: Loss = 0.771576
Epoch 3.87: Loss = 0.700607
Epoch 3.88: Loss = 0.673904
Epoch 3.89: Loss = 0.845322
Epoch 3.90: Loss = 0.715225
Epoch 3.91: Loss = 0.760788
Epoch 3.92: Loss = 0.659515
Epoch 3.93: Loss = 0.767227
Epoch 3.94: Loss = 0.688324
Epoch 3.95: Loss = 0.700348
Epoch 3.96: Loss = 0.666412
Epoch 3.97: Loss = 0.632065
Epoch 3.98: Loss = 0.64447
Epoch 3.99: Loss = 0.71315
Epoch 3.100: Loss = 0.697174
Epoch 3.101: Loss = 0.691391
Epoch 3.102: Loss = 0.686264
Epoch 3.103: Loss = 0.643661
Epoch 3.104: Loss = 0.624222
Epoch 3.105: Loss = 0.712204
Epoch 3.106: Loss = 0.733353
Epoch 3.107: Loss = 0.763977
Epoch 3.108: Loss = 0.824722
Epoch 3.109: Loss = 0.710846
Epoch 3.110: Loss = 0.824966
Epoch 3.111: Loss = 0.591919
Epoch 3.112: Loss = 0.768066
Epoch 3.113: Loss = 0.650696
Epoch 3.114: Loss = 0.681778
Epoch 3.115: Loss = 0.745636
Epoch 3.116: Loss = 0.56842
Epoch 3.117: Loss = 0.601425
Epoch 3.118: Loss = 0.687454
Epoch 3.119: Loss = 0.665985
Epoch 3.120: Loss = 0.634003
TRAIN LOSS = 0.69252
TRAIN ACC = 77.4139 % (46450/60000)
Loss = 0.618454
Loss = 0.790878
Loss = 0.703629
Loss = 0.662567
Loss = 0.722916
Loss = 0.84288
Loss = 0.855301
Loss = 0.85704
Loss = 0.76091
Loss = 0.706848
Loss = 0.877914
Loss = 0.743866
Loss = 0.765976
Loss = 0.748154
Loss = 0.732071
Loss = 0.79126
Loss = 0.695908
Loss = 0.778549
Loss = 0.787186
Loss = 0.75322
TEST LOSS = 0.759776
TEST ACC = 464.499 % (7582/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.73735
Epoch 4.2: Loss = 0.705826
Epoch 4.3: Loss = 0.775116
Epoch 4.4: Loss = 0.678177
Epoch 4.5: Loss = 0.724503
Epoch 4.6: Loss = 0.621902
Epoch 4.7: Loss = 0.690567
Epoch 4.8: Loss = 0.632263
Epoch 4.9: Loss = 0.855057
Epoch 4.10: Loss = 0.756439
Epoch 4.11: Loss = 0.684021
Epoch 4.12: Loss = 0.59639
Epoch 4.13: Loss = 0.685287
Epoch 4.14: Loss = 0.649002
Epoch 4.15: Loss = 0.716278
Epoch 4.16: Loss = 0.827194
Epoch 4.17: Loss = 0.575272
Epoch 4.18: Loss = 0.742798
Epoch 4.19: Loss = 0.636795
Epoch 4.20: Loss = 0.684052
Epoch 4.21: Loss = 0.586533
Epoch 4.22: Loss = 0.784348
Epoch 4.23: Loss = 0.632675
Epoch 4.24: Loss = 0.688187
Epoch 4.25: Loss = 0.612183
Epoch 4.26: Loss = 0.636414
Epoch 4.27: Loss = 0.565186
Epoch 4.28: Loss = 0.702637
Epoch 4.29: Loss = 0.598022
Epoch 4.30: Loss = 0.70343
Epoch 4.31: Loss = 0.631409
Epoch 4.32: Loss = 0.666733
Epoch 4.33: Loss = 0.725647
Epoch 4.34: Loss = 0.725113
Epoch 4.35: Loss = 0.69046
Epoch 4.36: Loss = 0.682587
Epoch 4.37: Loss = 0.703827
Epoch 4.38: Loss = 0.689087
Epoch 4.39: Loss = 0.629745
Epoch 4.40: Loss = 0.759949
Epoch 4.41: Loss = 0.619293
Epoch 4.42: Loss = 0.674118
Epoch 4.43: Loss = 0.762817
Epoch 4.44: Loss = 0.681107
Epoch 4.45: Loss = 0.719147
Epoch 4.46: Loss = 0.650497
Epoch 4.47: Loss = 0.586151
Epoch 4.48: Loss = 0.674774
Epoch 4.49: Loss = 0.630661
Epoch 4.50: Loss = 0.766541
Epoch 4.51: Loss = 0.714813
Epoch 4.52: Loss = 0.775284
Epoch 4.53: Loss = 0.654404
Epoch 4.54: Loss = 0.596512
Epoch 4.55: Loss = 0.714523
Epoch 4.56: Loss = 0.74678
Epoch 4.57: Loss = 0.611282
Epoch 4.58: Loss = 0.645889
Epoch 4.59: Loss = 0.633423
Epoch 4.60: Loss = 0.72142
Epoch 4.61: Loss = 0.701141
Epoch 4.62: Loss = 0.751526
Epoch 4.63: Loss = 0.715088
Epoch 4.64: Loss = 0.766022
Epoch 4.65: Loss = 0.712753
Epoch 4.66: Loss = 0.781403
Epoch 4.67: Loss = 0.719803
Epoch 4.68: Loss = 0.762222
Epoch 4.69: Loss = 0.733719
Epoch 4.70: Loss = 0.578217
Epoch 4.71: Loss = 0.724899
Epoch 4.72: Loss = 0.754654
Epoch 4.73: Loss = 0.711395
Epoch 4.74: Loss = 0.678848
Epoch 4.75: Loss = 0.76976
Epoch 4.76: Loss = 0.639023
Epoch 4.77: Loss = 0.713577
Epoch 4.78: Loss = 0.768005
Epoch 4.79: Loss = 0.662537
Epoch 4.80: Loss = 0.652252
Epoch 4.81: Loss = 0.728363
Epoch 4.82: Loss = 0.672897
Epoch 4.83: Loss = 0.612381
Epoch 4.84: Loss = 0.695862
Epoch 4.85: Loss = 0.552551
Epoch 4.86: Loss = 0.729843
Epoch 4.87: Loss = 0.689133
Epoch 4.88: Loss = 0.702896
Epoch 4.89: Loss = 0.723953
Epoch 4.90: Loss = 0.777725
Epoch 4.91: Loss = 0.59668
Epoch 4.92: Loss = 0.654343
Epoch 4.93: Loss = 0.617325
Epoch 4.94: Loss = 0.75325
Epoch 4.95: Loss = 0.647812
Epoch 4.96: Loss = 0.5849
Epoch 4.97: Loss = 0.756134
Epoch 4.98: Loss = 0.766418
Epoch 4.99: Loss = 0.774231
Epoch 4.100: Loss = 0.709763
Epoch 4.101: Loss = 0.644119
Epoch 4.102: Loss = 0.660477
Epoch 4.103: Loss = 0.622635
Epoch 4.104: Loss = 0.689011
Epoch 4.105: Loss = 0.675262
Epoch 4.106: Loss = 0.735962
Epoch 4.107: Loss = 0.723282
Epoch 4.108: Loss = 0.720642
Epoch 4.109: Loss = 0.709549
Epoch 4.110: Loss = 0.706711
Epoch 4.111: Loss = 0.714539
Epoch 4.112: Loss = 0.771896
Epoch 4.113: Loss = 0.602234
Epoch 4.114: Loss = 0.528564
Epoch 4.115: Loss = 0.781586
Epoch 4.116: Loss = 0.744873
Epoch 4.117: Loss = 0.697632
Epoch 4.118: Loss = 0.79126
Epoch 4.119: Loss = 0.72644
Epoch 4.120: Loss = 0.756119
TRAIN LOSS = 0.691803
TRAIN ACC = 78.7125 % (47229/60000)
Loss = 0.604584
Loss = 0.736633
Loss = 0.645508
Loss = 0.623566
Loss = 0.661957
Loss = 0.790436
Loss = 0.837494
Loss = 0.803894
Loss = 0.722916
Loss = 0.665543
Loss = 0.846024
Loss = 0.76973
Loss = 0.710968
Loss = 0.707642
Loss = 0.698441
Loss = 0.721527
Loss = 0.696518
Loss = 0.714844
Loss = 0.766754
Loss = 0.703293
TEST LOSS = 0.721413
TEST ACC = 472.289 % (7826/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.632034
Epoch 5.2: Loss = 0.559189
Epoch 5.3: Loss = 0.793839
Epoch 5.4: Loss = 0.578644
Epoch 5.5: Loss = 0.74646
Epoch 5.6: Loss = 0.722321
Epoch 5.7: Loss = 0.59491
Epoch 5.8: Loss = 0.729309
Epoch 5.9: Loss = 0.671066
Epoch 5.10: Loss = 0.725967
Epoch 5.11: Loss = 0.738235
Epoch 5.12: Loss = 0.652115
Epoch 5.13: Loss = 0.656174
Epoch 5.14: Loss = 0.59816
Epoch 5.15: Loss = 0.646484
Epoch 5.16: Loss = 0.658478
Epoch 5.17: Loss = 0.663071
Epoch 5.18: Loss = 0.840637
Epoch 5.19: Loss = 0.724487
Epoch 5.20: Loss = 0.805313
Epoch 5.21: Loss = 0.618073
Epoch 5.22: Loss = 0.698303
Epoch 5.23: Loss = 0.68541
Epoch 5.24: Loss = 0.663849
Epoch 5.25: Loss = 0.711258
Epoch 5.26: Loss = 0.729721
Epoch 5.27: Loss = 0.71077
Epoch 5.28: Loss = 0.904587
Epoch 5.29: Loss = 0.590851
Epoch 5.30: Loss = 0.734589
Epoch 5.31: Loss = 0.669907
Epoch 5.32: Loss = 0.75444
Epoch 5.33: Loss = 0.84108
Epoch 5.34: Loss = 0.625671
Epoch 5.35: Loss = 0.643707
Epoch 5.36: Loss = 0.730453
Epoch 5.37: Loss = 0.703735
Epoch 5.38: Loss = 0.654099
Epoch 5.39: Loss = 0.537659
Epoch 5.40: Loss = 0.615173
Epoch 5.41: Loss = 0.701874
Epoch 5.42: Loss = 0.730835
Epoch 5.43: Loss = 0.71637
Epoch 5.44: Loss = 0.68306
Epoch 5.45: Loss = 0.734909
Epoch 5.46: Loss = 0.689194
Epoch 5.47: Loss = 0.774048
Epoch 5.48: Loss = 0.632919
Epoch 5.49: Loss = 0.743362
Epoch 5.50: Loss = 0.684677
Epoch 5.51: Loss = 0.729187
Epoch 5.52: Loss = 0.725815
Epoch 5.53: Loss = 0.727234
Epoch 5.54: Loss = 0.695343
Epoch 5.55: Loss = 0.6362
Epoch 5.56: Loss = 0.736435
Epoch 5.57: Loss = 0.596481
Epoch 5.58: Loss = 0.646255
Epoch 5.59: Loss = 0.712997
Epoch 5.60: Loss = 0.606369
Epoch 5.61: Loss = 0.628372
Epoch 5.62: Loss = 0.69043
Epoch 5.63: Loss = 0.644318
Epoch 5.64: Loss = 0.745911
Epoch 5.65: Loss = 0.824188
Epoch 5.66: Loss = 0.768143
Epoch 5.67: Loss = 0.711685
Epoch 5.68: Loss = 0.548294
Epoch 5.69: Loss = 0.777405
Epoch 5.70: Loss = 0.53064
Epoch 5.71: Loss = 0.600967
Epoch 5.72: Loss = 0.817947
Epoch 5.73: Loss = 0.65274
Epoch 5.74: Loss = 0.68428
Epoch 5.75: Loss = 0.692551
Epoch 5.76: Loss = 0.644699
Epoch 5.77: Loss = 0.761169
Epoch 5.78: Loss = 0.737335
Epoch 5.79: Loss = 0.657471
Epoch 5.80: Loss = 0.743149
Epoch 5.81: Loss = 0.685089
Epoch 5.82: Loss = 0.604889
Epoch 5.83: Loss = 0.755768
Epoch 5.84: Loss = 0.669342
Epoch 5.85: Loss = 0.608276
Epoch 5.86: Loss = 0.775253
Epoch 5.87: Loss = 0.606552
Epoch 5.88: Loss = 0.724915
Epoch 5.89: Loss = 0.799271
Epoch 5.90: Loss = 0.583466
Epoch 5.91: Loss = 0.718475
Epoch 5.92: Loss = 0.855194
Epoch 5.93: Loss = 0.656754
Epoch 5.94: Loss = 0.710526
Epoch 5.95: Loss = 0.736282
Epoch 5.96: Loss = 0.604645
Epoch 5.97: Loss = 0.676407
Epoch 5.98: Loss = 0.878525
Epoch 5.99: Loss = 0.662354
Epoch 5.100: Loss = 0.741486
Epoch 5.101: Loss = 0.605225
Epoch 5.102: Loss = 0.801056
Epoch 5.103: Loss = 0.7276
Epoch 5.104: Loss = 0.606232
Epoch 5.105: Loss = 0.708893
Epoch 5.106: Loss = 0.579407
Epoch 5.107: Loss = 0.691589
Epoch 5.108: Loss = 0.567368
Epoch 5.109: Loss = 0.67363
Epoch 5.110: Loss = 0.769623
Epoch 5.111: Loss = 0.716248
Epoch 5.112: Loss = 0.705566
Epoch 5.113: Loss = 0.688675
Epoch 5.114: Loss = 0.661621
Epoch 5.115: Loss = 0.776016
Epoch 5.116: Loss = 0.775879
Epoch 5.117: Loss = 0.605606
Epoch 5.118: Loss = 0.727966
Epoch 5.119: Loss = 0.766724
Epoch 5.120: Loss = 0.610428
TRAIN LOSS = 0.692886
TRAIN ACC = 79.5486 % (47731/60000)
Loss = 0.590927
Loss = 0.764603
Loss = 0.657532
Loss = 0.661041
Loss = 0.705032
Loss = 0.828293
Loss = 0.870758
Loss = 0.818146
Loss = 0.773239
Loss = 0.664963
Loss = 0.854492
Loss = 0.821686
Loss = 0.738678
Loss = 0.730637
Loss = 0.72995
Loss = 0.728439
Loss = 0.715942
Loss = 0.743378
Loss = 0.779816
Loss = 0.725372
TEST LOSS = 0.745146
TEST ACC = 477.309 % (7902/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.723801
Epoch 6.2: Loss = 0.66861
Epoch 6.3: Loss = 0.83223
Epoch 6.4: Loss = 0.624008
Epoch 6.5: Loss = 0.663895
Epoch 6.6: Loss = 0.640793
Epoch 6.7: Loss = 0.713165
Epoch 6.8: Loss = 0.644257
Epoch 6.9: Loss = 0.653061
Epoch 6.10: Loss = 0.590118
Epoch 6.11: Loss = 0.695602
Epoch 6.12: Loss = 0.682648
Epoch 6.13: Loss = 0.815399
Epoch 6.14: Loss = 0.601913
Epoch 6.15: Loss = 0.686371
Epoch 6.16: Loss = 0.749619
Epoch 6.17: Loss = 0.613235
Epoch 6.18: Loss = 0.810562
Epoch 6.19: Loss = 0.663559
Epoch 6.20: Loss = 0.775772
Epoch 6.21: Loss = 0.774933
Epoch 6.22: Loss = 0.689987
Epoch 6.23: Loss = 0.704605
Epoch 6.24: Loss = 0.63237
Epoch 6.25: Loss = 0.650024
Epoch 6.26: Loss = 0.602524
Epoch 6.27: Loss = 0.691788
Epoch 6.28: Loss = 0.673187
Epoch 6.29: Loss = 0.664841
Epoch 6.30: Loss = 0.713257
Epoch 6.31: Loss = 0.706604
Epoch 6.32: Loss = 0.779907
Epoch 6.33: Loss = 0.714081
Epoch 6.34: Loss = 0.683151
Epoch 6.35: Loss = 0.713303
Epoch 6.36: Loss = 0.621994
Epoch 6.37: Loss = 0.748444
Epoch 6.38: Loss = 0.641937
Epoch 6.39: Loss = 0.622009
Epoch 6.40: Loss = 0.711029
Epoch 6.41: Loss = 0.708664
Epoch 6.42: Loss = 0.767441
Epoch 6.43: Loss = 0.660019
Epoch 6.44: Loss = 0.681946
Epoch 6.45: Loss = 0.615082
Epoch 6.46: Loss = 0.722214
Epoch 6.47: Loss = 0.611404
Epoch 6.48: Loss = 0.720901
Epoch 6.49: Loss = 0.700165
Epoch 6.50: Loss = 0.62619
Epoch 6.51: Loss = 0.617783
Epoch 6.52: Loss = 0.704529
Epoch 6.53: Loss = 0.753265
Epoch 6.54: Loss = 0.735458
Epoch 6.55: Loss = 0.744949
Epoch 6.56: Loss = 0.792923
Epoch 6.57: Loss = 0.643066
Epoch 6.58: Loss = 0.689911
Epoch 6.59: Loss = 0.862762
Epoch 6.60: Loss = 0.674271
Epoch 6.61: Loss = 0.682861
Epoch 6.62: Loss = 0.638336
Epoch 6.63: Loss = 0.791809
Epoch 6.64: Loss = 0.746353
Epoch 6.65: Loss = 0.583603
Epoch 6.66: Loss = 0.655807
Epoch 6.67: Loss = 1.00017
Epoch 6.68: Loss = 0.599609
Epoch 6.69: Loss = 0.717667
Epoch 6.70: Loss = 0.727722
Epoch 6.71: Loss = 0.679214
Epoch 6.72: Loss = 0.693604
Epoch 6.73: Loss = 0.665466
Epoch 6.74: Loss = 0.73967
Epoch 6.75: Loss = 0.675262
Epoch 6.76: Loss = 0.703781
Epoch 6.77: Loss = 0.742554
Epoch 6.78: Loss = 0.754639
Epoch 6.79: Loss = 0.711975
Epoch 6.80: Loss = 0.759766
Epoch 6.81: Loss = 0.671707
Epoch 6.82: Loss = 0.683792
Epoch 6.83: Loss = 0.824707
Epoch 6.84: Loss = 0.637436
Epoch 6.85: Loss = 0.74826
Epoch 6.86: Loss = 0.777557
Epoch 6.87: Loss = 0.675934
Epoch 6.88: Loss = 0.574783
Epoch 6.89: Loss = 0.653793
Epoch 6.90: Loss = 0.821976
Epoch 6.91: Loss = 0.65918
Epoch 6.92: Loss = 0.661697
Epoch 6.93: Loss = 0.625916
Epoch 6.94: Loss = 0.633087
Epoch 6.95: Loss = 0.760529
Epoch 6.96: Loss = 0.631897
Epoch 6.97: Loss = 0.762161
Epoch 6.98: Loss = 0.825134
Epoch 6.99: Loss = 0.679428
Epoch 6.100: Loss = 0.609772
Epoch 6.101: Loss = 0.742203
Epoch 6.102: Loss = 0.657364
Epoch 6.103: Loss = 0.62001
Epoch 6.104: Loss = 0.811935
Epoch 6.105: Loss = 0.718048
Epoch 6.106: Loss = 0.676834
Epoch 6.107: Loss = 0.585953
Epoch 6.108: Loss = 0.734756
Epoch 6.109: Loss = 0.846024
Epoch 6.110: Loss = 0.735245
Epoch 6.111: Loss = 0.653442
Epoch 6.112: Loss = 0.557144
Epoch 6.113: Loss = 0.711273
Epoch 6.114: Loss = 0.803528
Epoch 6.115: Loss = 0.810974
Epoch 6.116: Loss = 0.810883
Epoch 6.117: Loss = 0.840378
Epoch 6.118: Loss = 0.661438
Epoch 6.119: Loss = 0.669754
Epoch 6.120: Loss = 0.717682
TRAIN LOSS = 0.701477
TRAIN ACC = 80.0674 % (48043/60000)
Loss = 0.604858
Loss = 0.791046
Loss = 0.693054
Loss = 0.638123
Loss = 0.722046
Loss = 0.875244
Loss = 0.925797
Loss = 0.808655
Loss = 0.761475
Loss = 0.67012
Loss = 0.88678
Loss = 0.781754
Loss = 0.769714
Loss = 0.768097
Loss = 0.748795
Loss = 0.755035
Loss = 0.733597
Loss = 0.74585
Loss = 0.803848
Loss = 0.749237
TEST LOSS = 0.761656
TEST ACC = 480.429 % (7912/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.735229
Epoch 7.2: Loss = 0.747833
Epoch 7.3: Loss = 0.652603
Epoch 7.4: Loss = 0.817612
Epoch 7.5: Loss = 0.730179
Epoch 7.6: Loss = 0.777588
Epoch 7.7: Loss = 0.738205
Epoch 7.8: Loss = 0.671799
Epoch 7.9: Loss = 0.756165
Epoch 7.10: Loss = 0.725693
Epoch 7.11: Loss = 0.659393
Epoch 7.12: Loss = 0.709656
Epoch 7.13: Loss = 0.617477
Epoch 7.14: Loss = 0.700195
Epoch 7.15: Loss = 0.745483
Epoch 7.16: Loss = 0.805634
Epoch 7.17: Loss = 0.861313
Epoch 7.18: Loss = 0.792816
Epoch 7.19: Loss = 0.715256
Epoch 7.20: Loss = 0.835449
Epoch 7.21: Loss = 0.700455
Epoch 7.22: Loss = 0.628372
Epoch 7.23: Loss = 0.649643
Epoch 7.24: Loss = 0.699478
Epoch 7.25: Loss = 0.751923
Epoch 7.26: Loss = 0.765732
Epoch 7.27: Loss = 0.640198
Epoch 7.28: Loss = 0.804886
Epoch 7.29: Loss = 0.665924
Epoch 7.30: Loss = 0.779129
Epoch 7.31: Loss = 0.619324
Epoch 7.32: Loss = 0.760269
Epoch 7.33: Loss = 0.735229
Epoch 7.34: Loss = 0.650467
Epoch 7.35: Loss = 0.762573
Epoch 7.36: Loss = 0.686356
Epoch 7.37: Loss = 0.658035
Epoch 7.38: Loss = 0.65036
Epoch 7.39: Loss = 0.74881
Epoch 7.40: Loss = 0.653793
Epoch 7.41: Loss = 0.690094
Epoch 7.42: Loss = 0.793655
Epoch 7.43: Loss = 0.728195
Epoch 7.44: Loss = 0.764587
Epoch 7.45: Loss = 0.801102
Epoch 7.46: Loss = 0.710922
Epoch 7.47: Loss = 0.776947
Epoch 7.48: Loss = 0.656387
Epoch 7.49: Loss = 0.644913
Epoch 7.50: Loss = 0.807602
Epoch 7.51: Loss = 0.80632
Epoch 7.52: Loss = 0.585037
Epoch 7.53: Loss = 0.642105
Epoch 7.54: Loss = 0.700317
Epoch 7.55: Loss = 0.603577
Epoch 7.56: Loss = 0.703629
Epoch 7.57: Loss = 0.616318
Epoch 7.58: Loss = 0.638077
Epoch 7.59: Loss = 0.647385
Epoch 7.60: Loss = 0.606369
Epoch 7.61: Loss = 0.596924
Epoch 7.62: Loss = 0.716141
Epoch 7.63: Loss = 0.679489
Epoch 7.64: Loss = 0.671432
Epoch 7.65: Loss = 0.888367
Epoch 7.66: Loss = 0.645401
Epoch 7.67: Loss = 0.721161
Epoch 7.68: Loss = 0.833054
Epoch 7.69: Loss = 0.775238
Epoch 7.70: Loss = 0.659943
Epoch 7.71: Loss = 0.803375
Epoch 7.72: Loss = 0.680908
Epoch 7.73: Loss = 0.707535
Epoch 7.74: Loss = 0.834259
Epoch 7.75: Loss = 0.769043
Epoch 7.76: Loss = 0.710632
Epoch 7.77: Loss = 0.618835
Epoch 7.78: Loss = 0.549683
Epoch 7.79: Loss = 0.661087
Epoch 7.80: Loss = 0.726044
Epoch 7.81: Loss = 0.700714
Epoch 7.82: Loss = 0.597122
Epoch 7.83: Loss = 0.675781
Epoch 7.84: Loss = 0.795471
Epoch 7.85: Loss = 0.662674
Epoch 7.86: Loss = 0.850464
Epoch 7.87: Loss = 0.573959
Epoch 7.88: Loss = 0.860825
Epoch 7.89: Loss = 0.624725
Epoch 7.90: Loss = 0.79364
Epoch 7.91: Loss = 0.74852
Epoch 7.92: Loss = 0.772247
Epoch 7.93: Loss = 0.57814
Epoch 7.94: Loss = 0.670166
Epoch 7.95: Loss = 0.837997
Epoch 7.96: Loss = 0.685883
Epoch 7.97: Loss = 0.616013
Epoch 7.98: Loss = 0.644958
Epoch 7.99: Loss = 0.694214
Epoch 7.100: Loss = 0.65506
Epoch 7.101: Loss = 0.67627
Epoch 7.102: Loss = 0.763596
Epoch 7.103: Loss = 0.794312
Epoch 7.104: Loss = 0.602219
Epoch 7.105: Loss = 0.750214
Epoch 7.106: Loss = 0.620682
Epoch 7.107: Loss = 0.779236
Epoch 7.108: Loss = 0.737213
Epoch 7.109: Loss = 0.716736
Epoch 7.110: Loss = 0.736237
Epoch 7.111: Loss = 0.773788
Epoch 7.112: Loss = 0.758057
Epoch 7.113: Loss = 0.677872
Epoch 7.114: Loss = 0.583221
Epoch 7.115: Loss = 0.727844
Epoch 7.116: Loss = 0.788498
Epoch 7.117: Loss = 0.809387
Epoch 7.118: Loss = 0.715149
Epoch 7.119: Loss = 0.692749
Epoch 7.120: Loss = 0.649948
TRAIN LOSS = 0.711426
TRAIN ACC = 80.6366 % (48384/60000)
Loss = 0.60289
Loss = 0.789383
Loss = 0.71521
Loss = 0.653336
Loss = 0.733231
Loss = 0.88591
Loss = 0.969666
Loss = 0.880478
Loss = 0.744873
Loss = 0.714539
Loss = 0.936386
Loss = 0.832779
Loss = 0.790894
Loss = 0.818161
Loss = 0.756317
Loss = 0.743164
Loss = 0.778488
Loss = 0.765503
Loss = 0.815506
Loss = 0.789215
TEST LOSS = 0.785796
TEST ACC = 483.839 % (7964/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.669312
Epoch 8.2: Loss = 0.607376
Epoch 8.3: Loss = 0.796402
Epoch 8.4: Loss = 0.50856
Epoch 8.5: Loss = 0.795944
Epoch 8.6: Loss = 0.681107
Epoch 8.7: Loss = 0.732788
Epoch 8.8: Loss = 0.812256
Epoch 8.9: Loss = 0.840561
Epoch 8.10: Loss = 0.687683
Epoch 8.11: Loss = 0.888809
Epoch 8.12: Loss = 0.835083
Epoch 8.13: Loss = 0.660263
Epoch 8.14: Loss = 0.710114
Epoch 8.15: Loss = 0.675735
Epoch 8.16: Loss = 0.625565
Epoch 8.17: Loss = 0.605652
Epoch 8.18: Loss = 0.758667
Epoch 8.19: Loss = 0.564728
Epoch 8.20: Loss = 0.676361
Epoch 8.21: Loss = 0.783325
Epoch 8.22: Loss = 0.791367
Epoch 8.23: Loss = 0.688477
Epoch 8.24: Loss = 0.718643
Epoch 8.25: Loss = 0.678604
Epoch 8.26: Loss = 0.706833
Epoch 8.27: Loss = 0.716492
Epoch 8.28: Loss = 0.695709
Epoch 8.29: Loss = 0.744431
Epoch 8.30: Loss = 0.723099
Epoch 8.31: Loss = 0.774918
Epoch 8.32: Loss = 0.864288
Epoch 8.33: Loss = 0.79454
Epoch 8.34: Loss = 0.749008
Epoch 8.35: Loss = 0.74617
Epoch 8.36: Loss = 0.784164
Epoch 8.37: Loss = 0.725632
Epoch 8.38: Loss = 0.664383
Epoch 8.39: Loss = 0.593216
Epoch 8.40: Loss = 0.685196
Epoch 8.41: Loss = 0.770676
Epoch 8.42: Loss = 0.799316
Epoch 8.43: Loss = 0.736557
Epoch 8.44: Loss = 0.630142
Epoch 8.45: Loss = 0.649521
Epoch 8.46: Loss = 0.69017
Epoch 8.47: Loss = 0.669846
Epoch 8.48: Loss = 0.743423
Epoch 8.49: Loss = 0.693542
Epoch 8.50: Loss = 0.886429
Epoch 8.51: Loss = 0.795517
Epoch 8.52: Loss = 0.687256
Epoch 8.53: Loss = 0.702682
Epoch 8.54: Loss = 0.774857
Epoch 8.55: Loss = 0.721497
Epoch 8.56: Loss = 0.701324
Epoch 8.57: Loss = 0.759949
Epoch 8.58: Loss = 0.619232
Epoch 8.59: Loss = 0.636108
Epoch 8.60: Loss = 0.873428
Epoch 8.61: Loss = 0.735962
Epoch 8.62: Loss = 0.603226
Epoch 8.63: Loss = 0.805374
Epoch 8.64: Loss = 0.880859
Epoch 8.65: Loss = 0.636093
Epoch 8.66: Loss = 0.77951
Epoch 8.67: Loss = 0.645477
Epoch 8.68: Loss = 0.702362
Epoch 8.69: Loss = 0.760147
Epoch 8.70: Loss = 0.567886
Epoch 8.71: Loss = 0.715469
Epoch 8.72: Loss = 0.76358
Epoch 8.73: Loss = 0.796066
Epoch 8.74: Loss = 0.645111
Epoch 8.75: Loss = 0.858368
Epoch 8.76: Loss = 0.950119
Epoch 8.77: Loss = 0.710602
Epoch 8.78: Loss = 0.880234
Epoch 8.79: Loss = 0.805756
Epoch 8.80: Loss = 0.673859
Epoch 8.81: Loss = 0.899551
Epoch 8.82: Loss = 0.686356
Epoch 8.83: Loss = 0.784882
Epoch 8.84: Loss = 0.906464
Epoch 8.85: Loss = 0.732178
Epoch 8.86: Loss = 0.766296
Epoch 8.87: Loss = 0.732834
Epoch 8.88: Loss = 0.73822
Epoch 8.89: Loss = 0.746155
Epoch 8.90: Loss = 0.796432
Epoch 8.91: Loss = 0.817734
Epoch 8.92: Loss = 0.721313
Epoch 8.93: Loss = 0.765457
Epoch 8.94: Loss = 0.689743
Epoch 8.95: Loss = 0.706192
Epoch 8.96: Loss = 0.777802
Epoch 8.97: Loss = 0.734879
Epoch 8.98: Loss = 0.764404
Epoch 8.99: Loss = 0.6707
Epoch 8.100: Loss = 0.796143
Epoch 8.101: Loss = 0.819473
Epoch 8.102: Loss = 0.802856
Epoch 8.103: Loss = 0.72261
Epoch 8.104: Loss = 0.65361
Epoch 8.105: Loss = 0.800491
Epoch 8.106: Loss = 0.71814
Epoch 8.107: Loss = 0.720642
Epoch 8.108: Loss = 0.730133
Epoch 8.109: Loss = 0.715652
Epoch 8.110: Loss = 0.842819
Epoch 8.111: Loss = 0.771057
Epoch 8.112: Loss = 0.740448
Epoch 8.113: Loss = 0.617279
Epoch 8.114: Loss = 0.757904
Epoch 8.115: Loss = 0.763641
Epoch 8.116: Loss = 0.745316
Epoch 8.117: Loss = 0.77652
Epoch 8.118: Loss = 0.726273
Epoch 8.119: Loss = 0.600021
Epoch 8.120: Loss = 0.695038
TRAIN LOSS = 0.734787
TRAIN ACC = 80.571 % (48345/60000)
Loss = 0.58226
Loss = 0.800232
Loss = 0.725845
Loss = 0.680038
Loss = 0.722046
Loss = 0.913696
Loss = 0.99704
Loss = 0.893158
Loss = 0.719772
Loss = 0.7146
Loss = 0.940308
Loss = 0.884552
Loss = 0.825974
Loss = 0.831497
Loss = 0.758652
Loss = 0.75148
Loss = 0.780029
Loss = 0.786133
Loss = 0.874512
Loss = 0.790237
TEST LOSS = 0.798603
TEST ACC = 483.449 % (7979/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.856705
Epoch 9.2: Loss = 0.733459
Epoch 9.3: Loss = 0.525009
Epoch 9.4: Loss = 0.681259
Epoch 9.5: Loss = 0.643951
Epoch 9.6: Loss = 0.739822
Epoch 9.7: Loss = 0.844162
Epoch 9.8: Loss = 0.765854
Epoch 9.9: Loss = 0.788101
Epoch 9.10: Loss = 0.758347
Epoch 9.11: Loss = 0.618515
Epoch 9.12: Loss = 0.797867
Epoch 9.13: Loss = 0.778244
Epoch 9.14: Loss = 0.716202
Epoch 9.15: Loss = 0.670609
Epoch 9.16: Loss = 0.63649
Epoch 9.17: Loss = 0.587616
Epoch 9.18: Loss = 0.999069
Epoch 9.19: Loss = 0.857376
Epoch 9.20: Loss = 0.693985
Epoch 9.21: Loss = 0.775986
Epoch 9.22: Loss = 0.537109
Epoch 9.23: Loss = 0.651001
Epoch 9.24: Loss = 0.638397
Epoch 9.25: Loss = 0.813065
Epoch 9.26: Loss = 0.745865
Epoch 9.27: Loss = 0.743866
Epoch 9.28: Loss = 0.742645
Epoch 9.29: Loss = 0.889496
Epoch 9.30: Loss = 0.724197
Epoch 9.31: Loss = 0.905884
Epoch 9.32: Loss = 0.631454
Epoch 9.33: Loss = 0.695404
Epoch 9.34: Loss = 0.661545
Epoch 9.35: Loss = 0.771088
Epoch 9.36: Loss = 0.757187
Epoch 9.37: Loss = 0.822449
Epoch 9.38: Loss = 0.638733
Epoch 9.39: Loss = 0.60675
Epoch 9.40: Loss = 0.750153
Epoch 9.41: Loss = 0.949158
Epoch 9.42: Loss = 0.822449
Epoch 9.43: Loss = 0.676819
Epoch 9.44: Loss = 0.793228
Epoch 9.45: Loss = 0.767151
Epoch 9.46: Loss = 0.620316
Epoch 9.47: Loss = 0.57666
Epoch 9.48: Loss = 0.686676
Epoch 9.49: Loss = 0.927322
Epoch 9.50: Loss = 0.730209
Epoch 9.51: Loss = 0.807343
Epoch 9.52: Loss = 0.826523
Epoch 9.53: Loss = 0.827286
Epoch 9.54: Loss = 0.809402
Epoch 9.55: Loss = 0.627502
Epoch 9.56: Loss = 0.648773
Epoch 9.57: Loss = 0.615662
Epoch 9.58: Loss = 0.89888
Epoch 9.59: Loss = 0.731628
Epoch 9.60: Loss = 0.848831
Epoch 9.61: Loss = 0.805038
Epoch 9.62: Loss = 0.621811
Epoch 9.63: Loss = 0.791458
Epoch 9.64: Loss = 0.623367
Epoch 9.65: Loss = 0.795715
Epoch 9.66: Loss = 0.684296
Epoch 9.67: Loss = 0.815979
Epoch 9.68: Loss = 0.844208
Epoch 9.69: Loss = 0.64119
Epoch 9.70: Loss = 0.755661
Epoch 9.71: Loss = 0.786453
Epoch 9.72: Loss = 0.865112
Epoch 9.73: Loss = 0.697937
Epoch 9.74: Loss = 0.805206
Epoch 9.75: Loss = 0.867447
Epoch 9.76: Loss = 0.854355
Epoch 9.77: Loss = 0.62059
Epoch 9.78: Loss = 0.806763
Epoch 9.79: Loss = 0.781281
Epoch 9.80: Loss = 0.637207
Epoch 9.81: Loss = 0.697571
Epoch 9.82: Loss = 0.985367
Epoch 9.83: Loss = 0.586548
Epoch 9.84: Loss = 0.700089
Epoch 9.85: Loss = 0.769577
Epoch 9.86: Loss = 0.835907
Epoch 9.87: Loss = 0.79039
Epoch 9.88: Loss = 0.889114
Epoch 9.89: Loss = 0.588943
Epoch 9.90: Loss = 0.921875
Epoch 9.91: Loss = 0.747391
Epoch 9.92: Loss = 0.835419
Epoch 9.93: Loss = 0.64946
Epoch 9.94: Loss = 0.801208
Epoch 9.95: Loss = 0.773712
Epoch 9.96: Loss = 0.849518
Epoch 9.97: Loss = 0.778961
Epoch 9.98: Loss = 0.728577
Epoch 9.99: Loss = 0.835175
Epoch 9.100: Loss = 0.926941
Epoch 9.101: Loss = 0.908401
Epoch 9.102: Loss = 0.69664
Epoch 9.103: Loss = 0.74147
Epoch 9.104: Loss = 0.759445
Epoch 9.105: Loss = 0.768158
Epoch 9.106: Loss = 0.755417
Epoch 9.107: Loss = 0.890686
Epoch 9.108: Loss = 0.790924
Epoch 9.109: Loss = 0.781082
Epoch 9.110: Loss = 0.784744
Epoch 9.111: Loss = 0.707611
Epoch 9.112: Loss = 0.805298
Epoch 9.113: Loss = 0.806046
Epoch 9.114: Loss = 0.816681
Epoch 9.115: Loss = 0.617157
Epoch 9.116: Loss = 0.793121
Epoch 9.117: Loss = 0.788559
Epoch 9.118: Loss = 0.70842
Epoch 9.119: Loss = 0.850769
Epoch 9.120: Loss = 0.798126
TRAIN LOSS = 0.756256
TRAIN ACC = 80.6595 % (48398/60000)
Loss = 0.657211
Loss = 0.825592
Loss = 0.785919
Loss = 0.695526
Loss = 0.738235
Loss = 0.986649
Loss = 1.08186
Loss = 0.921875
Loss = 0.764282
Loss = 0.778473
Loss = 1.01396
Loss = 0.892487
Loss = 0.841553
Loss = 0.803833
Loss = 0.7668
Loss = 0.806091
Loss = 0.802856
Loss = 0.766113
Loss = 0.877121
Loss = 0.817032
TEST LOSS = 0.831173
TEST ACC = 483.98 % (7937/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.800613
Epoch 10.2: Loss = 0.709015
Epoch 10.3: Loss = 0.767212
Epoch 10.4: Loss = 1.02209
Epoch 10.5: Loss = 0.711426
Epoch 10.6: Loss = 0.727005
Epoch 10.7: Loss = 0.815933
Epoch 10.8: Loss = 0.854004
Epoch 10.9: Loss = 0.880417
Epoch 10.10: Loss = 0.829926
Epoch 10.11: Loss = 0.820999
Epoch 10.12: Loss = 0.797104
Epoch 10.13: Loss = 0.683517
Epoch 10.14: Loss = 0.640167
Epoch 10.15: Loss = 0.887848
Epoch 10.16: Loss = 0.800049
Epoch 10.17: Loss = 0.74176
Epoch 10.18: Loss = 0.751434
Epoch 10.19: Loss = 0.874222
Epoch 10.20: Loss = 0.844315
Epoch 10.21: Loss = 0.741409
Epoch 10.22: Loss = 0.625214
Epoch 10.23: Loss = 0.662201
Epoch 10.24: Loss = 0.743561
Epoch 10.25: Loss = 0.934723
Epoch 10.26: Loss = 0.784897
Epoch 10.27: Loss = 0.81694
Epoch 10.28: Loss = 0.665695
Epoch 10.29: Loss = 0.864166
Epoch 10.30: Loss = 0.782333
Epoch 10.31: Loss = 0.765778
Epoch 10.32: Loss = 0.654648
Epoch 10.33: Loss = 0.691635
Epoch 10.34: Loss = 0.771301
Epoch 10.35: Loss = 0.698578
Epoch 10.36: Loss = 0.760788
Epoch 10.37: Loss = 0.701981
Epoch 10.38: Loss = 0.715057
Epoch 10.39: Loss = 0.742371
Epoch 10.40: Loss = 0.947433
Epoch 10.41: Loss = 0.748383
Epoch 10.42: Loss = 0.851379
Epoch 10.43: Loss = 0.84436
Epoch 10.44: Loss = 0.718323
Epoch 10.45: Loss = 0.760132
Epoch 10.46: Loss = 0.847824
Epoch 10.47: Loss = 0.788498
Epoch 10.48: Loss = 0.825821
Epoch 10.49: Loss = 0.848511
Epoch 10.50: Loss = 0.875198
Epoch 10.51: Loss = 0.746506
Epoch 10.52: Loss = 0.792542
Epoch 10.53: Loss = 0.800644
Epoch 10.54: Loss = 0.680084
Epoch 10.55: Loss = 0.799637
Epoch 10.56: Loss = 0.765503
Epoch 10.57: Loss = 0.797989
Epoch 10.58: Loss = 0.7276
Epoch 10.59: Loss = 0.781998
Epoch 10.60: Loss = 0.947983
Epoch 10.61: Loss = 0.824295
Epoch 10.62: Loss = 0.796936
Epoch 10.63: Loss = 0.840286
Epoch 10.64: Loss = 0.704285
Epoch 10.65: Loss = 0.815231
Epoch 10.66: Loss = 0.615631
Epoch 10.67: Loss = 0.797836
Epoch 10.68: Loss = 0.666992
Epoch 10.69: Loss = 0.659317
Epoch 10.70: Loss = 0.844574
Epoch 10.71: Loss = 0.698288
Epoch 10.72: Loss = 0.987305
Epoch 10.73: Loss = 0.661453
Epoch 10.74: Loss = 0.883865
Epoch 10.75: Loss = 0.674866
Epoch 10.76: Loss = 0.84584
Epoch 10.77: Loss = 0.802124
Epoch 10.78: Loss = 0.974777
Epoch 10.79: Loss = 0.670807
Epoch 10.80: Loss = 0.894684
Epoch 10.81: Loss = 0.653854
Epoch 10.82: Loss = 0.842209
Epoch 10.83: Loss = 0.798141
Epoch 10.84: Loss = 0.789749
Epoch 10.85: Loss = 0.619583
Epoch 10.86: Loss = 0.840317
Epoch 10.87: Loss = 0.716339
Epoch 10.88: Loss = 0.700394
Epoch 10.89: Loss = 0.861649
Epoch 10.90: Loss = 0.886642
Epoch 10.91: Loss = 0.631348
Epoch 10.92: Loss = 0.730362
Epoch 10.93: Loss = 0.799683
Epoch 10.94: Loss = 0.611679
Epoch 10.95: Loss = 0.87532
Epoch 10.96: Loss = 0.75795
Epoch 10.97: Loss = 0.834564
Epoch 10.98: Loss = 0.703064
Epoch 10.99: Loss = 0.679092
Epoch 10.100: Loss = 0.771286
Epoch 10.101: Loss = 0.772903
Epoch 10.102: Loss = 0.897034
Epoch 10.103: Loss = 0.799759
Epoch 10.104: Loss = 0.70433
Epoch 10.105: Loss = 0.80481
Epoch 10.106: Loss = 0.794952
Epoch 10.107: Loss = 0.732285
Epoch 10.108: Loss = 0.599487
Epoch 10.109: Loss = 0.700439
Epoch 10.110: Loss = 0.71312
Epoch 10.111: Loss = 0.860596
Epoch 10.112: Loss = 0.636215
Epoch 10.113: Loss = 0.817871
Epoch 10.114: Loss = 0.718887
Epoch 10.115: Loss = 0.717896
Epoch 10.116: Loss = 0.665054
Epoch 10.117: Loss = 0.500168
Epoch 10.118: Loss = 0.858459
Epoch 10.119: Loss = 0.748917
Epoch 10.120: Loss = 0.826843
TRAIN LOSS = 0.771774
TRAIN ACC = 80.7144 % (48431/60000)
Loss = 0.629181
Loss = 0.810547
Loss = 0.74707
Loss = 0.67569
Loss = 0.751892
Loss = 0.977173
Loss = 1.0435
Loss = 0.887711
Loss = 0.76915
Loss = 0.741592
Loss = 0.975723
Loss = 0.893829
Loss = 0.892578
Loss = 0.847427
Loss = 0.794495
Loss = 0.82103
Loss = 0.73909
Loss = 0.809891
Loss = 0.907059
Loss = 0.74202
TEST LOSS = 0.822832
TEST ACC = 484.309 % (7993/10000)

Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 512]) => Dense([60000, 1, 512]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 2000
Num Epochs: 10
Learning Rate: 0.4 to 0.4 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.3299
Epoch 1.2: Loss = 1.97563
Epoch 1.3: Loss = 1.77658
Epoch 1.4: Loss = 1.60313
Epoch 1.5: Loss = 1.44855
Epoch 1.6: Loss = 1.32251
Epoch 1.7: Loss = 1.25891
Epoch 1.8: Loss = 1.13272
Epoch 1.9: Loss = 1.13925
Epoch 1.10: Loss = 1.03084
Epoch 1.11: Loss = 1.01482
Epoch 1.12: Loss = 0.959595
Epoch 1.13: Loss = 0.975357
Epoch 1.14: Loss = 0.877701
Epoch 1.15: Loss = 0.942413
Epoch 1.16: Loss = 0.884598
Epoch 1.17: Loss = 0.918106
Epoch 1.18: Loss = 0.82225
Epoch 1.19: Loss = 0.960587
Epoch 1.20: Loss = 0.876419
Epoch 1.21: Loss = 0.878799
Epoch 1.22: Loss = 0.809769
Epoch 1.23: Loss = 0.827637
Epoch 1.24: Loss = 0.780319
Epoch 1.25: Loss = 0.813339
Epoch 1.26: Loss = 0.731018
Epoch 1.27: Loss = 0.79097
Epoch 1.28: Loss = 0.784775
Epoch 1.29: Loss = 0.780594
Epoch 1.30: Loss = 0.763428
TRAIN LOSS = 1.0737
TRAIN ACC = 64.4211 % (38655/60000)
Loss = 0.785645
Loss = 0.882172
Loss = 0.821732
Loss = 0.841827
Loss = 0.832291
TEST LOSS = 0.832733
TEST ACC = 386.549 % (7282/10000)
Reducing learning rate to 0.399994
Epoch 2.1: Loss = 0.783066
Epoch 2.2: Loss = 0.859482
Epoch 2.3: Loss = 0.798431
Epoch 2.4: Loss = 0.713638
Epoch 2.5: Loss = 0.799255
Epoch 2.6: Loss = 0.717789
Epoch 2.7: Loss = 0.796112
Epoch 2.8: Loss = 0.713821
Epoch 2.9: Loss = 0.788437
Epoch 2.10: Loss = 0.776169
Epoch 2.11: Loss = 0.835815
Epoch 2.12: Loss = 0.779938
Epoch 2.13: Loss = 0.890808
Epoch 2.14: Loss = 0.806595
Epoch 2.15: Loss = 0.767746
Epoch 2.16: Loss = 0.670303
Epoch 2.17: Loss = 0.819748
Epoch 2.18: Loss = 0.702499
Epoch 2.19: Loss = 0.82811
Epoch 2.20: Loss = 0.748199
Epoch 2.21: Loss = 0.719299
Epoch 2.22: Loss = 0.753052
Epoch 2.23: Loss = 0.736008
Epoch 2.24: Loss = 0.733597
Epoch 2.25: Loss = 0.852875
Epoch 2.26: Loss = 0.779007
Epoch 2.27: Loss = 0.883118
Epoch 2.28: Loss = 0.91304
Epoch 2.29: Loss = 0.748444
Epoch 2.30: Loss = 0.695801
TRAIN LOSS = 0.78035
TRAIN ACC = 74.3729 % (44625/60000)
Loss = 0.748032
Loss = 0.820129
Loss = 0.780457
Loss = 0.772812
Loss = 0.753967
TEST LOSS = 0.775079
TEST ACC = 446.249 % (7460/10000)
Reducing learning rate to 0.399994
Epoch 3.1: Loss = 0.786514
Epoch 3.2: Loss = 0.673187
Epoch 3.3: Loss = 0.825241
Epoch 3.4: Loss = 0.759155
Epoch 3.5: Loss = 0.692383
Epoch 3.6: Loss = 0.724503
Epoch 3.7: Loss = 0.808167
Epoch 3.8: Loss = 0.756012
Epoch 3.9: Loss = 0.794067
Epoch 3.10: Loss = 0.710693
Epoch 3.11: Loss = 0.771759
Epoch 3.12: Loss = 0.749359
Epoch 3.13: Loss = 0.819
Epoch 3.14: Loss = 0.721832
Epoch 3.15: Loss = 0.756729
Epoch 3.16: Loss = 0.667328
Epoch 3.17: Loss = 0.794083
Epoch 3.18: Loss = 0.65361
Epoch 3.19: Loss = 0.729996
Epoch 3.20: Loss = 0.708069
Epoch 3.21: Loss = 0.81041
Epoch 3.22: Loss = 0.705872
Epoch 3.23: Loss = 0.716476
Epoch 3.24: Loss = 0.679962
Epoch 3.25: Loss = 0.730484
Epoch 3.26: Loss = 0.627563
Epoch 3.27: Loss = 0.623383
Epoch 3.28: Loss = 0.696457
Epoch 3.29: Loss = 0.783691
Epoch 3.30: Loss = 0.739639
TRAIN LOSS = 0.733871
TRAIN ACC = 76.7883 % (46075/60000)
Loss = 0.75058
Loss = 0.846008
Loss = 0.799606
Loss = 0.786667
Loss = 0.780853
TEST LOSS = 0.792743
TEST ACC = 460.75 % (7565/10000)
Reducing learning rate to 0.399994
Epoch 4.1: Loss = 0.770401
Epoch 4.2: Loss = 0.670029
Epoch 4.3: Loss = 0.70224
Epoch 4.4: Loss = 0.794708
Epoch 4.5: Loss = 0.839157
Epoch 4.6: Loss = 0.712585
Epoch 4.7: Loss = 0.888657
Epoch 4.8: Loss = 0.753525
Epoch 4.9: Loss = 0.671906
Epoch 4.10: Loss = 0.668839
Epoch 4.11: Loss = 0.766541
Epoch 4.12: Loss = 0.729202
Epoch 4.13: Loss = 0.70546
Epoch 4.14: Loss = 0.751099
Epoch 4.15: Loss = 0.73645
Epoch 4.16: Loss = 0.735245
Epoch 4.17: Loss = 0.753922
Epoch 4.18: Loss = 0.754547
Epoch 4.19: Loss = 0.906158
Epoch 4.20: Loss = 0.776794
Epoch 4.21: Loss = 0.750778
Epoch 4.22: Loss = 0.756271
Epoch 4.23: Loss = 0.831573
Epoch 4.24: Loss = 0.760437
Epoch 4.25: Loss = 0.746719
Epoch 4.26: Loss = 0.756775
Epoch 4.27: Loss = 0.855545
Epoch 4.28: Loss = 0.837646
Epoch 4.29: Loss = 0.714371
Epoch 4.30: Loss = 0.64418
TRAIN LOSS = 0.758072
TRAIN ACC = 77.597 % (46560/60000)
Loss = 0.694824
Loss = 0.80278
Loss = 0.771561
Loss = 0.761261
Loss = 0.75325
TEST LOSS = 0.756735
TEST ACC = 465.599 % (7770/10000)
Reducing learning rate to 0.399994
Epoch 5.1: Loss = 0.720947
Epoch 5.2: Loss = 0.730469
Epoch 5.3: Loss = 0.752762
Epoch 5.4: Loss = 0.723053
Epoch 5.5: Loss = 0.685089
Epoch 5.6: Loss = 0.660172
Epoch 5.7: Loss = 0.755173
Epoch 5.8: Loss = 0.739029
Epoch 5.9: Loss = 0.713074
Epoch 5.10: Loss = 0.622452
Epoch 5.11: Loss = 0.694382
Epoch 5.12: Loss = 0.664124
Epoch 5.13: Loss = 0.742035
Epoch 5.14: Loss = 0.836472
Epoch 5.15: Loss = 0.806839
Epoch 5.16: Loss = 0.917038
Epoch 5.17: Loss = 0.737488
Epoch 5.18: Loss = 0.740784
Epoch 5.19: Loss = 0.740616
Epoch 5.20: Loss = 0.723328
Epoch 5.21: Loss = 0.786728
Epoch 5.22: Loss = 0.763626
Epoch 5.23: Loss = 0.776077
Epoch 5.24: Loss = 0.771698
Epoch 5.25: Loss = 0.653687
Epoch 5.26: Loss = 0.817093
Epoch 5.27: Loss = 0.635193
Epoch 5.28: Loss = 0.673248
Epoch 5.29: Loss = 0.688782
Epoch 5.30: Loss = 0.705231
TRAIN LOSS = 0.732574
TRAIN ACC = 78.775 % (47267/60000)
Loss = 0.653366
Loss = 0.763
Loss = 0.754517
Loss = 0.707062
Loss = 0.72113
TEST LOSS = 0.719815
TEST ACC = 472.668 % (7913/10000)
Reducing learning rate to 0.399994
Epoch 6.1: Loss = 0.712051
Epoch 6.2: Loss = 0.651611
Epoch 6.3: Loss = 0.733124
Epoch 6.4: Loss = 0.736679
Epoch 6.5: Loss = 0.711517
Epoch 6.6: Loss = 0.812119
Epoch 6.7: Loss = 0.746201
Epoch 6.8: Loss = 0.908188
Epoch 6.9: Loss = 0.664764
Epoch 6.10: Loss = 0.817932
Epoch 6.11: Loss = 0.7341
Epoch 6.12: Loss = 0.853745
Epoch 6.13: Loss = 0.646866
Epoch 6.14: Loss = 0.724213
Epoch 6.15: Loss = 0.716446
Epoch 6.16: Loss = 0.712708
Epoch 6.17: Loss = 0.736832
Epoch 6.18: Loss = 0.777161
Epoch 6.19: Loss = 0.776306
Epoch 6.20: Loss = 0.749344
Epoch 6.21: Loss = 0.678009
Epoch 6.22: Loss = 0.723358
Epoch 6.23: Loss = 0.661697
Epoch 6.24: Loss = 0.716171
Epoch 6.25: Loss = 0.75174
Epoch 6.26: Loss = 0.669861
Epoch 6.27: Loss = 0.705322
Epoch 6.28: Loss = 0.751587
Epoch 6.29: Loss = 0.733276
Epoch 6.30: Loss = 0.678482
TRAIN LOSS = 0.733063
TRAIN ACC = 79.5303 % (47720/60000)
Loss = 0.60907
Loss = 0.724976
Loss = 0.711533
Loss = 0.659271
Loss = 0.679092
TEST LOSS = 0.676788
TEST ACC = 477.199 % (8013/10000)
Reducing learning rate to 0.399994
Epoch 7.1: Loss = 0.62616
Epoch 7.2: Loss = 0.582428
Epoch 7.3: Loss = 0.755005
Epoch 7.4: Loss = 0.758255
Epoch 7.5: Loss = 0.637726
Epoch 7.6: Loss = 0.682373
Epoch 7.7: Loss = 0.801437
Epoch 7.8: Loss = 0.928528
Epoch 7.9: Loss = 0.720078
Epoch 7.10: Loss = 0.623886
Epoch 7.11: Loss = 0.6521
Epoch 7.12: Loss = 0.723557
Epoch 7.13: Loss = 1.02531
Epoch 7.14: Loss = 0.786346
Epoch 7.15: Loss = 0.653214
Epoch 7.16: Loss = 0.579941
Epoch 7.17: Loss = 0.699768
Epoch 7.18: Loss = 0.760605
Epoch 7.19: Loss = 0.717392
Epoch 7.20: Loss = 0.698761
Epoch 7.21: Loss = 0.755112
Epoch 7.22: Loss = 0.713089
Epoch 7.23: Loss = 0.710007
Epoch 7.24: Loss = 0.742523
Epoch 7.25: Loss = 0.781769
Epoch 7.26: Loss = 0.707565
Epoch 7.27: Loss = 0.977631
Epoch 7.28: Loss = 0.771423
Epoch 7.29: Loss = 0.735397
Epoch 7.30: Loss = 0.736313
TRAIN LOSS = 0.734802
TRAIN ACC = 79.5944 % (47759/60000)
Loss = 0.713333
Loss = 0.826172
Loss = 0.833893
Loss = 0.779709
Loss = 0.777847
TEST LOSS = 0.786191
TEST ACC = 477.589 % (7899/10000)
Reducing learning rate to 0.399994
Epoch 8.1: Loss = 0.8367
Epoch 8.2: Loss = 0.731079
Epoch 8.3: Loss = 0.702713
Epoch 8.4: Loss = 0.694427
Epoch 8.5: Loss = 0.641998
Epoch 8.6: Loss = 0.657974
Epoch 8.7: Loss = 0.613159
Epoch 8.8: Loss = 0.723114
Epoch 8.9: Loss = 0.711487
Epoch 8.10: Loss = 0.849884
Epoch 8.11: Loss = 0.723923
Epoch 8.12: Loss = 0.706024
Epoch 8.13: Loss = 0.660004
Epoch 8.14: Loss = 0.651154
Epoch 8.15: Loss = 0.64534
Epoch 8.16: Loss = 0.623947
Epoch 8.17: Loss = 0.657425
Epoch 8.18: Loss = 0.681152
Epoch 8.19: Loss = 0.830292
Epoch 8.20: Loss = 0.96373
Epoch 8.21: Loss = 0.630264
Epoch 8.22: Loss = 0.645752
Epoch 8.23: Loss = 0.564819
Epoch 8.24: Loss = 0.771606
Epoch 8.25: Loss = 0.658646
Epoch 8.26: Loss = 0.696396
Epoch 8.27: Loss = 0.668549
Epoch 8.28: Loss = 0.62648
Epoch 8.29: Loss = 0.574524
Epoch 8.30: Loss = 0.812531
TRAIN LOSS = 0.698517
TRAIN ACC = 80.7587 % (48458/60000)
Loss = 0.65741
Loss = 0.747147
Loss = 0.72316
Loss = 0.696396
Loss = 0.683777
TEST LOSS = 0.701578
TEST ACC = 484.579 % (7932/10000)
Reducing learning rate to 0.399994
Epoch 9.1: Loss = 0.68988
Epoch 9.2: Loss = 0.848663
Epoch 9.3: Loss = 0.62471
Epoch 9.4: Loss = 0.760223
Epoch 9.5: Loss = 0.65274
Epoch 9.6: Loss = 0.753479
Epoch 9.7: Loss = 0.795181
Epoch 9.8: Loss = 0.745651
Epoch 9.9: Loss = 0.700562
Epoch 9.10: Loss = 0.814575
Epoch 9.11: Loss = 0.826889
Epoch 9.12: Loss = 0.702271
Epoch 9.13: Loss = 0.681015
Epoch 9.14: Loss = 0.761566
Epoch 9.15: Loss = 0.781128
Epoch 9.16: Loss = 0.82193
Epoch 9.17: Loss = 0.662979
Epoch 9.18: Loss = 0.787704
Epoch 9.19: Loss = 0.671494
Epoch 9.20: Loss = 0.677505
Epoch 9.21: Loss = 0.686646
Epoch 9.22: Loss = 0.718048
Epoch 9.23: Loss = 0.705582
Epoch 9.24: Loss = 0.682541
Epoch 9.25: Loss = 0.663986
Epoch 9.26: Loss = 0.693863
Epoch 9.27: Loss = 0.701508
Epoch 9.28: Loss = 0.736832
Epoch 9.29: Loss = 0.778946
Epoch 9.30: Loss = 0.70015
TRAIN LOSS = 0.727631
TRAIN ACC = 80.6168 % (48372/60000)
Loss = 0.813171
Loss = 0.953812
Loss = 0.936752
Loss = 0.937485
Loss = 0.923782
TEST LOSS = 0.913
TEST ACC = 483.719 % (7952/10000)
Reducing learning rate to 0.399994
Epoch 10.1: Loss = 0.966034
Epoch 10.2: Loss = 0.668854
Epoch 10.3: Loss = 0.808975
Epoch 10.4: Loss = 0.717697
Epoch 10.5: Loss = 0.902557
Epoch 10.6: Loss = 0.880753
Epoch 10.7: Loss = 0.790054
Epoch 10.8: Loss = 0.771271
Epoch 10.9: Loss = 0.658646
Epoch 10.10: Loss = 0.63382
Epoch 10.11: Loss = 0.651459
Epoch 10.12: Loss = 0.575348
Epoch 10.13: Loss = 0.727631
Epoch 10.14: Loss = 0.714401
Epoch 10.15: Loss = 0.789825
Epoch 10.16: Loss = 0.68425
Epoch 10.17: Loss = 0.764603
Epoch 10.18: Loss = 0.663864
Epoch 10.19: Loss = 0.755692
Epoch 10.20: Loss = 0.828369
Epoch 10.21: Loss = 0.736816
Epoch 10.22: Loss = 0.756531
Epoch 10.23: Loss = 0.718842
Epoch 10.24: Loss = 0.795761
Epoch 10.25: Loss = 0.804016
Epoch 10.26: Loss = 0.805817
Epoch 10.27: Loss = 0.808884
Epoch 10.28: Loss = 0.669479
Epoch 10.29: Loss = 0.958923
Epoch 10.30: Loss = 0.605637
TRAIN LOSS = 0.753845
TRAIN ACC = 80.6381 % (48385/60000)
Loss = 0.716553
Loss = 0.855591
Loss = 0.844589
Loss = 0.828659
Loss = 0.820389
TEST LOSS = 0.813156
TEST ACC = 483.849 % (8082/10000)

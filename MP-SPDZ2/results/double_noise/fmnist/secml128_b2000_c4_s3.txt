Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 2000
Num Epochs: 10
Learning Rate: 0.4 to 0.4 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.4149
Epoch 1.2: Loss = 2.18524
Epoch 1.3: Loss = 1.98865
Epoch 1.4: Loss = 1.86069
Epoch 1.5: Loss = 1.69704
Epoch 1.6: Loss = 1.61179
Epoch 1.7: Loss = 1.47971
Epoch 1.8: Loss = 1.38417
Epoch 1.9: Loss = 1.2803
Epoch 1.10: Loss = 1.20714
Epoch 1.11: Loss = 1.16576
Epoch 1.12: Loss = 1.12495
Epoch 1.13: Loss = 1.09483
Epoch 1.14: Loss = 1.09549
Epoch 1.15: Loss = 1.05835
Epoch 1.16: Loss = 1.04657
Epoch 1.17: Loss = 1.04846
Epoch 1.18: Loss = 1.01375
Epoch 1.19: Loss = 1.0305
Epoch 1.20: Loss = 0.994446
Epoch 1.21: Loss = 0.941788
Epoch 1.22: Loss = 0.919312
Epoch 1.23: Loss = 0.912415
Epoch 1.24: Loss = 0.827606
Epoch 1.25: Loss = 0.859116
Epoch 1.26: Loss = 0.807098
Epoch 1.27: Loss = 0.858505
Epoch 1.28: Loss = 0.755234
Epoch 1.29: Loss = 0.888412
Epoch 1.30: Loss = 0.837738
TRAIN LOSS = 1.21301
TRAIN ACC = 56.8741 % (34126/60000)
Loss = 0.897842
Loss = 0.94873
Loss = 0.896912
Loss = 0.90303
Loss = 0.911728
TEST LOSS = 0.911648
TEST ACC = 341.26 % (6737/10000)
Reducing learning rate to 0.399994
Epoch 2.1: Loss = 0.870712
Epoch 2.2: Loss = 0.80925
Epoch 2.3: Loss = 0.847412
Epoch 2.4: Loss = 0.882614
Epoch 2.5: Loss = 0.865753
Epoch 2.6: Loss = 0.852249
Epoch 2.7: Loss = 0.825027
Epoch 2.8: Loss = 0.776825
Epoch 2.9: Loss = 0.78125
Epoch 2.10: Loss = 0.891815
Epoch 2.11: Loss = 0.763275
Epoch 2.12: Loss = 0.74649
Epoch 2.13: Loss = 0.720932
Epoch 2.14: Loss = 0.797348
Epoch 2.15: Loss = 0.703217
Epoch 2.16: Loss = 0.757675
Epoch 2.17: Loss = 0.690735
Epoch 2.18: Loss = 0.736862
Epoch 2.19: Loss = 0.645508
Epoch 2.20: Loss = 0.829056
Epoch 2.21: Loss = 0.768097
Epoch 2.22: Loss = 0.759201
Epoch 2.23: Loss = 0.715622
Epoch 2.24: Loss = 0.712219
Epoch 2.25: Loss = 0.702011
Epoch 2.26: Loss = 0.753235
Epoch 2.27: Loss = 0.69072
Epoch 2.28: Loss = 0.721268
Epoch 2.29: Loss = 0.73584
Epoch 2.30: Loss = 0.736618
TRAIN LOSS = 0.769638
TRAIN ACC = 73.3063 % (43985/60000)
Loss = 0.686737
Loss = 0.783875
Loss = 0.736298
Loss = 0.738571
Loss = 0.73938
TEST LOSS = 0.736972
TEST ACC = 439.85 % (7489/10000)
Reducing learning rate to 0.399994
Epoch 3.1: Loss = 0.720032
Epoch 3.2: Loss = 0.790955
Epoch 3.3: Loss = 0.80304
Epoch 3.4: Loss = 0.803497
Epoch 3.5: Loss = 0.726044
Epoch 3.6: Loss = 0.780258
Epoch 3.7: Loss = 0.727264
Epoch 3.8: Loss = 0.746857
Epoch 3.9: Loss = 0.73053
Epoch 3.10: Loss = 0.722366
Epoch 3.11: Loss = 0.653397
Epoch 3.12: Loss = 0.778656
Epoch 3.13: Loss = 0.740173
Epoch 3.14: Loss = 0.822083
Epoch 3.15: Loss = 0.907028
Epoch 3.16: Loss = 0.729401
Epoch 3.17: Loss = 0.748245
Epoch 3.18: Loss = 0.804977
Epoch 3.19: Loss = 0.824081
Epoch 3.20: Loss = 0.822708
Epoch 3.21: Loss = 0.756851
Epoch 3.22: Loss = 0.725494
Epoch 3.23: Loss = 0.756866
Epoch 3.24: Loss = 0.867035
Epoch 3.25: Loss = 0.800385
Epoch 3.26: Loss = 0.775986
Epoch 3.27: Loss = 0.689056
Epoch 3.28: Loss = 0.722031
Epoch 3.29: Loss = 0.703827
Epoch 3.30: Loss = 0.738251
TRAIN LOSS = 0.763931
TRAIN ACC = 74.8947 % (44939/60000)
Loss = 0.680679
Loss = 0.768845
Loss = 0.739197
Loss = 0.724533
Loss = 0.722229
TEST LOSS = 0.727096
TEST ACC = 449.39 % (7624/10000)
Reducing learning rate to 0.399994
Epoch 4.1: Loss = 0.704437
Epoch 4.2: Loss = 0.674774
Epoch 4.3: Loss = 0.632294
Epoch 4.4: Loss = 0.710876
Epoch 4.5: Loss = 0.629288
Epoch 4.6: Loss = 0.680099
Epoch 4.7: Loss = 0.676407
Epoch 4.8: Loss = 0.696259
Epoch 4.9: Loss = 0.67601
Epoch 4.10: Loss = 0.726547
Epoch 4.11: Loss = 0.717072
Epoch 4.12: Loss = 0.669434
Epoch 4.13: Loss = 0.720947
Epoch 4.14: Loss = 0.638275
Epoch 4.15: Loss = 0.716385
Epoch 4.16: Loss = 0.706772
Epoch 4.17: Loss = 0.652939
Epoch 4.18: Loss = 0.732773
Epoch 4.19: Loss = 0.659546
Epoch 4.20: Loss = 0.618042
Epoch 4.21: Loss = 0.685181
Epoch 4.22: Loss = 0.704086
Epoch 4.23: Loss = 0.648102
Epoch 4.24: Loss = 0.72934
Epoch 4.25: Loss = 0.644806
Epoch 4.26: Loss = 0.624619
Epoch 4.27: Loss = 0.695267
Epoch 4.28: Loss = 0.734818
Epoch 4.29: Loss = 0.790466
Epoch 4.30: Loss = 0.641998
TRAIN LOSS = 0.684616
TRAIN ACC = 78.1631 % (46900/60000)
Loss = 0.677383
Loss = 0.786148
Loss = 0.751099
Loss = 0.760208
Loss = 0.741028
TEST LOSS = 0.743173
TEST ACC = 468.999 % (7744/10000)
Reducing learning rate to 0.399994
Epoch 5.1: Loss = 0.660568
Epoch 5.2: Loss = 0.73143
Epoch 5.3: Loss = 0.751328
Epoch 5.4: Loss = 0.784607
Epoch 5.5: Loss = 0.736679
Epoch 5.6: Loss = 0.758682
Epoch 5.7: Loss = 0.732895
Epoch 5.8: Loss = 0.717804
Epoch 5.9: Loss = 0.670166
Epoch 5.10: Loss = 0.657608
Epoch 5.11: Loss = 0.710114
Epoch 5.12: Loss = 0.69104
Epoch 5.13: Loss = 0.663132
Epoch 5.14: Loss = 0.645691
Epoch 5.15: Loss = 0.683716
Epoch 5.16: Loss = 0.687759
Epoch 5.17: Loss = 0.706604
Epoch 5.18: Loss = 0.748123
Epoch 5.19: Loss = 0.743622
Epoch 5.20: Loss = 0.695007
Epoch 5.21: Loss = 0.661499
Epoch 5.22: Loss = 0.66597
Epoch 5.23: Loss = 0.704971
Epoch 5.24: Loss = 0.639084
Epoch 5.25: Loss = 0.72821
Epoch 5.26: Loss = 0.687561
Epoch 5.27: Loss = 0.651443
Epoch 5.28: Loss = 0.655411
Epoch 5.29: Loss = 0.61348
Epoch 5.30: Loss = 0.703415
TRAIN LOSS = 0.696274
TRAIN ACC = 78.5355 % (47123/60000)
Loss = 0.721939
Loss = 0.809448
Loss = 0.797165
Loss = 0.762268
Loss = 0.757523
TEST LOSS = 0.769668
TEST ACC = 471.23 % (7654/10000)
Reducing learning rate to 0.399994
Epoch 6.1: Loss = 0.723328
Epoch 6.2: Loss = 0.772934
Epoch 6.3: Loss = 0.726669
Epoch 6.4: Loss = 0.701752
Epoch 6.5: Loss = 0.671097
Epoch 6.6: Loss = 0.759155
Epoch 6.7: Loss = 0.746307
Epoch 6.8: Loss = 0.697968
Epoch 6.9: Loss = 0.747742
Epoch 6.10: Loss = 0.714371
Epoch 6.11: Loss = 0.706924
Epoch 6.12: Loss = 0.734436
Epoch 6.13: Loss = 0.696548
Epoch 6.14: Loss = 0.734558
Epoch 6.15: Loss = 0.720108
Epoch 6.16: Loss = 0.73378
Epoch 6.17: Loss = 0.726654
Epoch 6.18: Loss = 0.70311
Epoch 6.19: Loss = 0.722275
Epoch 6.20: Loss = 0.740891
Epoch 6.21: Loss = 0.72142
Epoch 6.22: Loss = 0.715958
Epoch 6.23: Loss = 0.706894
Epoch 6.24: Loss = 0.684204
Epoch 6.25: Loss = 0.650589
Epoch 6.26: Loss = 0.760468
Epoch 6.27: Loss = 0.656006
Epoch 6.28: Loss = 0.66861
Epoch 6.29: Loss = 0.611786
Epoch 6.30: Loss = 0.6297
TRAIN LOSS = 0.709549
TRAIN ACC = 79.2313 % (47541/60000)
Loss = 0.653198
Loss = 0.753815
Loss = 0.727524
Loss = 0.706833
Loss = 0.702179
TEST LOSS = 0.70871
TEST ACC = 475.409 % (7836/10000)
Reducing learning rate to 0.399994
Epoch 7.1: Loss = 0.625916
Epoch 7.2: Loss = 0.67749
Epoch 7.3: Loss = 0.722214
Epoch 7.4: Loss = 0.685608
Epoch 7.5: Loss = 0.696091
Epoch 7.6: Loss = 0.689682
Epoch 7.7: Loss = 0.646637
Epoch 7.8: Loss = 0.753342
Epoch 7.9: Loss = 0.696243
Epoch 7.10: Loss = 0.701263
Epoch 7.11: Loss = 0.723328
Epoch 7.12: Loss = 0.758789
Epoch 7.13: Loss = 0.648285
Epoch 7.14: Loss = 0.666153
Epoch 7.15: Loss = 0.625168
Epoch 7.16: Loss = 0.731949
Epoch 7.17: Loss = 0.638306
Epoch 7.18: Loss = 0.689056
Epoch 7.19: Loss = 0.649033
Epoch 7.20: Loss = 0.801285
Epoch 7.21: Loss = 0.59491
Epoch 7.22: Loss = 0.737976
Epoch 7.23: Loss = 0.667206
Epoch 7.24: Loss = 0.885788
Epoch 7.25: Loss = 0.672028
Epoch 7.26: Loss = 0.665543
Epoch 7.27: Loss = 0.740845
Epoch 7.28: Loss = 0.724976
Epoch 7.29: Loss = 0.686417
Epoch 7.30: Loss = 0.666153
TRAIN LOSS = 0.695602
TRAIN ACC = 79.8798 % (47930/60000)
Loss = 0.643585
Loss = 0.741028
Loss = 0.729828
Loss = 0.674255
Loss = 0.688309
TEST LOSS = 0.695401
TEST ACC = 479.3 % (7882/10000)
Reducing learning rate to 0.399994
Epoch 8.1: Loss = 0.612473
Epoch 8.2: Loss = 0.754593
Epoch 8.3: Loss = 0.654617
Epoch 8.4: Loss = 0.673981
Epoch 8.5: Loss = 0.727829
Epoch 8.6: Loss = 0.660828
Epoch 8.7: Loss = 0.728836
Epoch 8.8: Loss = 0.72261
Epoch 8.9: Loss = 0.669189
Epoch 8.10: Loss = 0.694504
Epoch 8.11: Loss = 0.684891
Epoch 8.12: Loss = 0.662491
Epoch 8.13: Loss = 0.6754
Epoch 8.14: Loss = 0.683258
Epoch 8.15: Loss = 0.751602
Epoch 8.16: Loss = 0.689682
Epoch 8.17: Loss = 0.653336
Epoch 8.18: Loss = 0.655411
Epoch 8.19: Loss = 0.688309
Epoch 8.20: Loss = 0.778366
Epoch 8.21: Loss = 0.664886
Epoch 8.22: Loss = 0.624741
Epoch 8.23: Loss = 0.64711
Epoch 8.24: Loss = 0.662598
Epoch 8.25: Loss = 0.660049
Epoch 8.26: Loss = 0.725067
Epoch 8.27: Loss = 0.608307
Epoch 8.28: Loss = 0.799973
Epoch 8.29: Loss = 0.692276
Epoch 8.30: Loss = 0.716705
TRAIN LOSS = 0.687485
TRAIN ACC = 80.2673 % (48163/60000)
Loss = 0.620163
Loss = 0.720383
Loss = 0.708908
Loss = 0.664459
Loss = 0.662018
TEST LOSS = 0.675186
TEST ACC = 481.63 % (7973/10000)
Reducing learning rate to 0.399994
Epoch 9.1: Loss = 0.662033
Epoch 9.2: Loss = 0.799789
Epoch 9.3: Loss = 0.695435
Epoch 9.4: Loss = 0.825485
Epoch 9.5: Loss = 0.623795
Epoch 9.6: Loss = 0.690292
Epoch 9.7: Loss = 0.594025
Epoch 9.8: Loss = 0.700241
Epoch 9.9: Loss = 0.620743
Epoch 9.10: Loss = 0.663513
Epoch 9.11: Loss = 0.63324
Epoch 9.12: Loss = 0.644211
Epoch 9.13: Loss = 0.609787
Epoch 9.14: Loss = 0.746017
Epoch 9.15: Loss = 0.697327
Epoch 9.16: Loss = 0.676086
Epoch 9.17: Loss = 0.715088
Epoch 9.18: Loss = 0.718033
Epoch 9.19: Loss = 0.743027
Epoch 9.20: Loss = 0.766113
Epoch 9.21: Loss = 0.702255
Epoch 9.22: Loss = 0.690155
Epoch 9.23: Loss = 0.675751
Epoch 9.24: Loss = 0.672516
Epoch 9.25: Loss = 0.767105
Epoch 9.26: Loss = 0.727386
Epoch 9.27: Loss = 0.675003
Epoch 9.28: Loss = 0.671234
Epoch 9.29: Loss = 0.658737
Epoch 9.30: Loss = 0.736908
TRAIN LOSS = 0.69339
TRAIN ACC = 80.5618 % (48340/60000)
Loss = 0.626007
Loss = 0.72998
Loss = 0.723373
Loss = 0.680054
Loss = 0.675171
TEST LOSS = 0.686917
TEST ACC = 483.398 % (8004/10000)
Reducing learning rate to 0.399994
Epoch 10.1: Loss = 0.688141
Epoch 10.2: Loss = 0.636627
Epoch 10.3: Loss = 0.62738
Epoch 10.4: Loss = 0.68689
Epoch 10.5: Loss = 0.676361
Epoch 10.6: Loss = 0.656815
Epoch 10.7: Loss = 0.664917
Epoch 10.8: Loss = 0.758469
Epoch 10.9: Loss = 0.729294
Epoch 10.10: Loss = 0.760696
Epoch 10.11: Loss = 0.698257
Epoch 10.12: Loss = 0.619537
Epoch 10.13: Loss = 0.669434
Epoch 10.14: Loss = 0.696732
Epoch 10.15: Loss = 0.69989
Epoch 10.16: Loss = 0.625565
Epoch 10.17: Loss = 0.616592
Epoch 10.18: Loss = 0.677231
Epoch 10.19: Loss = 0.649704
Epoch 10.20: Loss = 0.672577
Epoch 10.21: Loss = 0.642975
Epoch 10.22: Loss = 0.767761
Epoch 10.23: Loss = 0.599258
Epoch 10.24: Loss = 0.692215
Epoch 10.25: Loss = 0.623672
Epoch 10.26: Loss = 0.713715
Epoch 10.27: Loss = 0.653366
Epoch 10.28: Loss = 0.758179
Epoch 10.29: Loss = 0.614517
Epoch 10.30: Loss = 0.721985
TRAIN LOSS = 0.676636
TRAIN ACC = 81.34 % (48806/60000)
Loss = 0.647461
Loss = 0.748169
Loss = 0.758057
Loss = 0.691864
Loss = 0.692383
TEST LOSS = 0.707586
TEST ACC = 488.058 % (7993/10000)

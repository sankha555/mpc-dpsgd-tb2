Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.3954
Epoch 1.2: Loss = 2.33432
Epoch 1.3: Loss = 2.31799
Epoch 1.4: Loss = 2.24522
Epoch 1.5: Loss = 2.23038
Epoch 1.6: Loss = 2.18919
Epoch 1.7: Loss = 2.13681
Epoch 1.8: Loss = 2.13295
Epoch 1.9: Loss = 2.08806
Epoch 1.10: Loss = 2.05165
Epoch 1.11: Loss = 2.03897
Epoch 1.12: Loss = 1.96294
Epoch 1.13: Loss = 1.94949
Epoch 1.14: Loss = 1.90112
Epoch 1.15: Loss = 1.89734
Epoch 1.16: Loss = 1.89539
Epoch 1.17: Loss = 1.83228
Epoch 1.18: Loss = 1.79987
Epoch 1.19: Loss = 1.73192
Epoch 1.20: Loss = 1.76756
Epoch 1.21: Loss = 1.69962
Epoch 1.22: Loss = 1.68532
Epoch 1.23: Loss = 1.62767
Epoch 1.24: Loss = 1.58788
Epoch 1.25: Loss = 1.55418
Epoch 1.26: Loss = 1.51523
Epoch 1.27: Loss = 1.49706
Epoch 1.28: Loss = 1.50844
Epoch 1.29: Loss = 1.44016
Epoch 1.30: Loss = 1.43137
Epoch 1.31: Loss = 1.39302
Epoch 1.32: Loss = 1.39227
Epoch 1.33: Loss = 1.37103
Epoch 1.34: Loss = 1.34076
Epoch 1.35: Loss = 1.2991
Epoch 1.36: Loss = 1.34396
Epoch 1.37: Loss = 1.32524
Epoch 1.38: Loss = 1.30075
Epoch 1.39: Loss = 1.19966
Epoch 1.40: Loss = 1.20177
Epoch 1.41: Loss = 1.12564
Epoch 1.42: Loss = 1.1651
Epoch 1.43: Loss = 1.17081
Epoch 1.44: Loss = 1.19943
Epoch 1.45: Loss = 1.12944
Epoch 1.46: Loss = 1.15042
Epoch 1.47: Loss = 1.13531
Epoch 1.48: Loss = 1.0894
Epoch 1.49: Loss = 1.11949
Epoch 1.50: Loss = 1.09872
Epoch 1.51: Loss = 1.10321
Epoch 1.52: Loss = 1.03215
Epoch 1.53: Loss = 1.02902
Epoch 1.54: Loss = 0.970337
Epoch 1.55: Loss = 1.02838
Epoch 1.56: Loss = 0.99147
Epoch 1.57: Loss = 1.00986
Epoch 1.58: Loss = 1.01378
Epoch 1.59: Loss = 0.981674
Epoch 1.60: Loss = 0.973831
Epoch 1.61: Loss = 0.951996
Epoch 1.62: Loss = 0.90834
Epoch 1.63: Loss = 0.886398
Epoch 1.64: Loss = 0.940063
Epoch 1.65: Loss = 0.902435
Epoch 1.66: Loss = 0.850784
Epoch 1.67: Loss = 0.862061
Epoch 1.68: Loss = 0.879639
Epoch 1.69: Loss = 0.762405
Epoch 1.70: Loss = 0.832199
Epoch 1.71: Loss = 0.822144
Epoch 1.72: Loss = 0.806183
Epoch 1.73: Loss = 0.866501
Epoch 1.74: Loss = 0.818344
Epoch 1.75: Loss = 0.80069
Epoch 1.76: Loss = 0.866547
Epoch 1.77: Loss = 0.777634
Epoch 1.78: Loss = 0.770386
Epoch 1.79: Loss = 0.834534
Epoch 1.80: Loss = 0.784042
Epoch 1.81: Loss = 0.773148
Epoch 1.82: Loss = 0.816437
Epoch 1.83: Loss = 0.792847
Epoch 1.84: Loss = 0.777222
Epoch 1.85: Loss = 0.729034
Epoch 1.86: Loss = 0.799789
Epoch 1.87: Loss = 0.759735
Epoch 1.88: Loss = 0.744095
Epoch 1.89: Loss = 0.720428
Epoch 1.90: Loss = 0.746536
Epoch 1.91: Loss = 0.777481
Epoch 1.92: Loss = 0.721252
Epoch 1.93: Loss = 0.704224
Epoch 1.94: Loss = 0.731979
Epoch 1.95: Loss = 0.725021
Epoch 1.96: Loss = 0.760025
Epoch 1.97: Loss = 0.676544
Epoch 1.98: Loss = 0.676498
Epoch 1.99: Loss = 0.69281
Epoch 1.100: Loss = 0.668839
Epoch 1.101: Loss = 0.700104
Epoch 1.102: Loss = 0.70636
Epoch 1.103: Loss = 0.635803
Epoch 1.104: Loss = 0.663589
Epoch 1.105: Loss = 0.587967
Epoch 1.106: Loss = 0.696976
Epoch 1.107: Loss = 0.702591
Epoch 1.108: Loss = 0.678711
Epoch 1.109: Loss = 0.666351
Epoch 1.110: Loss = 0.653
Epoch 1.111: Loss = 0.651627
Epoch 1.112: Loss = 0.627441
Epoch 1.113: Loss = 0.673508
Epoch 1.114: Loss = 0.649323
Epoch 1.115: Loss = 0.657883
Epoch 1.116: Loss = 0.580338
Epoch 1.117: Loss = 0.713272
Epoch 1.118: Loss = 0.573608
Epoch 1.119: Loss = 0.562592
Epoch 1.120: Loss = 0.607422
TRAIN LOSS = 1.13199
TRAIN ACC = 68.9896 % (41395/60000)
Loss = 0.662994
Loss = 0.683456
Loss = 0.819992
Loss = 0.748596
Loss = 0.778992
Loss = 0.665573
Loss = 0.6203
Loss = 0.79335
Loss = 0.767609
Loss = 0.747498
Loss = 0.371841
Loss = 0.580917
Loss = 0.38504
Loss = 0.620071
Loss = 0.50943
Loss = 0.497879
Loss = 0.478241
Loss = 0.283463
Loss = 0.450546
Loss = 0.730652
TEST LOSS = 0.609822
TEST ACC = 413.95 % (8237/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.623123
Epoch 2.2: Loss = 0.659439
Epoch 2.3: Loss = 0.699554
Epoch 2.4: Loss = 0.605392
Epoch 2.5: Loss = 0.590073
Epoch 2.6: Loss = 0.65387
Epoch 2.7: Loss = 0.58844
Epoch 2.8: Loss = 0.588409
Epoch 2.9: Loss = 0.610199
Epoch 2.10: Loss = 0.631027
Epoch 2.11: Loss = 0.636292
Epoch 2.12: Loss = 0.587585
Epoch 2.13: Loss = 0.611923
Epoch 2.14: Loss = 0.624771
Epoch 2.15: Loss = 0.625168
Epoch 2.16: Loss = 0.631744
Epoch 2.17: Loss = 0.639587
Epoch 2.18: Loss = 0.539719
Epoch 2.19: Loss = 0.565414
Epoch 2.20: Loss = 0.69574
Epoch 2.21: Loss = 0.610168
Epoch 2.22: Loss = 0.554565
Epoch 2.23: Loss = 0.616791
Epoch 2.24: Loss = 0.654083
Epoch 2.25: Loss = 0.569321
Epoch 2.26: Loss = 0.614594
Epoch 2.27: Loss = 0.520798
Epoch 2.28: Loss = 0.590576
Epoch 2.29: Loss = 0.588074
Epoch 2.30: Loss = 0.598099
Epoch 2.31: Loss = 0.591446
Epoch 2.32: Loss = 0.626617
Epoch 2.33: Loss = 0.547165
Epoch 2.34: Loss = 0.5979
Epoch 2.35: Loss = 0.52037
Epoch 2.36: Loss = 0.610519
Epoch 2.37: Loss = 0.600098
Epoch 2.38: Loss = 0.649429
Epoch 2.39: Loss = 0.627472
Epoch 2.40: Loss = 0.521072
Epoch 2.41: Loss = 0.560715
Epoch 2.42: Loss = 0.49472
Epoch 2.43: Loss = 0.659454
Epoch 2.44: Loss = 0.628479
Epoch 2.45: Loss = 0.653244
Epoch 2.46: Loss = 0.632889
Epoch 2.47: Loss = 0.522522
Epoch 2.48: Loss = 0.584366
Epoch 2.49: Loss = 0.564362
Epoch 2.50: Loss = 0.595215
Epoch 2.51: Loss = 0.542007
Epoch 2.52: Loss = 0.577423
Epoch 2.53: Loss = 0.557312
Epoch 2.54: Loss = 0.586655
Epoch 2.55: Loss = 0.615845
Epoch 2.56: Loss = 0.467911
Epoch 2.57: Loss = 0.561737
Epoch 2.58: Loss = 0.559875
Epoch 2.59: Loss = 0.556976
Epoch 2.60: Loss = 0.602859
Epoch 2.61: Loss = 0.524811
Epoch 2.62: Loss = 0.585434
Epoch 2.63: Loss = 0.517578
Epoch 2.64: Loss = 0.592163
Epoch 2.65: Loss = 0.533066
Epoch 2.66: Loss = 0.589767
Epoch 2.67: Loss = 0.525711
Epoch 2.68: Loss = 0.529999
Epoch 2.69: Loss = 0.581543
Epoch 2.70: Loss = 0.498474
Epoch 2.71: Loss = 0.550232
Epoch 2.72: Loss = 0.561935
Epoch 2.73: Loss = 0.55304
Epoch 2.74: Loss = 0.439682
Epoch 2.75: Loss = 0.614792
Epoch 2.76: Loss = 0.580765
Epoch 2.77: Loss = 0.524139
Epoch 2.78: Loss = 0.520416
Epoch 2.79: Loss = 0.552734
Epoch 2.80: Loss = 0.50975
Epoch 2.81: Loss = 0.541885
Epoch 2.82: Loss = 0.575119
Epoch 2.83: Loss = 0.464584
Epoch 2.84: Loss = 0.522034
Epoch 2.85: Loss = 0.592911
Epoch 2.86: Loss = 0.53743
Epoch 2.87: Loss = 0.48851
Epoch 2.88: Loss = 0.484009
Epoch 2.89: Loss = 0.503448
Epoch 2.90: Loss = 0.51532
Epoch 2.91: Loss = 0.481827
Epoch 2.92: Loss = 0.576614
Epoch 2.93: Loss = 0.570114
Epoch 2.94: Loss = 0.564758
Epoch 2.95: Loss = 0.446762
Epoch 2.96: Loss = 0.467896
Epoch 2.97: Loss = 0.521896
Epoch 2.98: Loss = 0.524994
Epoch 2.99: Loss = 0.63623
Epoch 2.100: Loss = 0.515259
Epoch 2.101: Loss = 0.528824
Epoch 2.102: Loss = 0.538483
Epoch 2.103: Loss = 0.472153
Epoch 2.104: Loss = 0.457962
Epoch 2.105: Loss = 0.503082
Epoch 2.106: Loss = 0.578842
Epoch 2.107: Loss = 0.441605
Epoch 2.108: Loss = 0.578491
Epoch 2.109: Loss = 0.529099
Epoch 2.110: Loss = 0.512009
Epoch 2.111: Loss = 0.488647
Epoch 2.112: Loss = 0.468262
Epoch 2.113: Loss = 0.461685
Epoch 2.114: Loss = 0.444809
Epoch 2.115: Loss = 0.509308
Epoch 2.116: Loss = 0.531235
Epoch 2.117: Loss = 0.4617
Epoch 2.118: Loss = 0.556564
Epoch 2.119: Loss = 0.518982
Epoch 2.120: Loss = 0.477478
TRAIN LOSS = 0.560165
TRAIN ACC = 83.2413 % (49947/60000)
Loss = 0.527832
Loss = 0.572952
Loss = 0.680725
Loss = 0.652939
Loss = 0.697876
Loss = 0.544434
Loss = 0.491684
Loss = 0.709473
Loss = 0.673035
Loss = 0.653473
Loss = 0.26825
Loss = 0.458511
Loss = 0.289932
Loss = 0.51004
Loss = 0.351089
Loss = 0.403793
Loss = 0.330246
Loss = 0.162354
Loss = 0.332138
Loss = 0.676025
TEST LOSS = 0.49934
TEST ACC = 499.469 % (8481/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.465912
Epoch 3.2: Loss = 0.505417
Epoch 3.3: Loss = 0.500443
Epoch 3.4: Loss = 0.442993
Epoch 3.5: Loss = 0.521927
Epoch 3.6: Loss = 0.479767
Epoch 3.7: Loss = 0.480927
Epoch 3.8: Loss = 0.474365
Epoch 3.9: Loss = 0.523438
Epoch 3.10: Loss = 0.550308
Epoch 3.11: Loss = 0.50032
Epoch 3.12: Loss = 0.51207
Epoch 3.13: Loss = 0.467102
Epoch 3.14: Loss = 0.564255
Epoch 3.15: Loss = 0.549805
Epoch 3.16: Loss = 0.52951
Epoch 3.17: Loss = 0.498917
Epoch 3.18: Loss = 0.457672
Epoch 3.19: Loss = 0.478851
Epoch 3.20: Loss = 0.541626
Epoch 3.21: Loss = 0.601288
Epoch 3.22: Loss = 0.594635
Epoch 3.23: Loss = 0.53244
Epoch 3.24: Loss = 0.449371
Epoch 3.25: Loss = 0.475815
Epoch 3.26: Loss = 0.470032
Epoch 3.27: Loss = 0.54541
Epoch 3.28: Loss = 0.459564
Epoch 3.29: Loss = 0.468658
Epoch 3.30: Loss = 0.454269
Epoch 3.31: Loss = 0.526825
Epoch 3.32: Loss = 0.52298
Epoch 3.33: Loss = 0.531631
Epoch 3.34: Loss = 0.519073
Epoch 3.35: Loss = 0.587021
Epoch 3.36: Loss = 0.497314
Epoch 3.37: Loss = 0.471619
Epoch 3.38: Loss = 0.442398
Epoch 3.39: Loss = 0.574142
Epoch 3.40: Loss = 0.457565
Epoch 3.41: Loss = 0.468903
Epoch 3.42: Loss = 0.516296
Epoch 3.43: Loss = 0.557602
Epoch 3.44: Loss = 0.483673
Epoch 3.45: Loss = 0.563995
Epoch 3.46: Loss = 0.493759
Epoch 3.47: Loss = 0.525803
Epoch 3.48: Loss = 0.471024
Epoch 3.49: Loss = 0.439774
Epoch 3.50: Loss = 0.522522
Epoch 3.51: Loss = 0.49501
Epoch 3.52: Loss = 0.48381
Epoch 3.53: Loss = 0.434631
Epoch 3.54: Loss = 0.544739
Epoch 3.55: Loss = 0.478867
Epoch 3.56: Loss = 0.478607
Epoch 3.57: Loss = 0.376312
Epoch 3.58: Loss = 0.402878
Epoch 3.59: Loss = 0.448441
Epoch 3.60: Loss = 0.487366
Epoch 3.61: Loss = 0.508621
Epoch 3.62: Loss = 0.45578
Epoch 3.63: Loss = 0.535629
Epoch 3.64: Loss = 0.557251
Epoch 3.65: Loss = 0.548767
Epoch 3.66: Loss = 0.583191
Epoch 3.67: Loss = 0.586884
Epoch 3.68: Loss = 0.470428
Epoch 3.69: Loss = 0.487778
Epoch 3.70: Loss = 0.521408
Epoch 3.71: Loss = 0.533035
Epoch 3.72: Loss = 0.576874
Epoch 3.73: Loss = 0.401703
Epoch 3.74: Loss = 0.54837
Epoch 3.75: Loss = 0.513916
Epoch 3.76: Loss = 0.47377
Epoch 3.77: Loss = 0.506378
Epoch 3.78: Loss = 0.509979
Epoch 3.79: Loss = 0.534805
Epoch 3.80: Loss = 0.528748
Epoch 3.81: Loss = 0.500824
Epoch 3.82: Loss = 0.582626
Epoch 3.83: Loss = 0.541336
Epoch 3.84: Loss = 0.484711
Epoch 3.85: Loss = 0.543091
Epoch 3.86: Loss = 0.446335
Epoch 3.87: Loss = 0.452759
Epoch 3.88: Loss = 0.512985
Epoch 3.89: Loss = 0.586777
Epoch 3.90: Loss = 0.494736
Epoch 3.91: Loss = 0.459503
Epoch 3.92: Loss = 0.477859
Epoch 3.93: Loss = 0.473099
Epoch 3.94: Loss = 0.47052
Epoch 3.95: Loss = 0.444717
Epoch 3.96: Loss = 0.515869
Epoch 3.97: Loss = 0.429047
Epoch 3.98: Loss = 0.541336
Epoch 3.99: Loss = 0.552673
Epoch 3.100: Loss = 0.538864
Epoch 3.101: Loss = 0.523666
Epoch 3.102: Loss = 0.451065
Epoch 3.103: Loss = 0.455811
Epoch 3.104: Loss = 0.530212
Epoch 3.105: Loss = 0.63028
Epoch 3.106: Loss = 0.484543
Epoch 3.107: Loss = 0.52269
Epoch 3.108: Loss = 0.432816
Epoch 3.109: Loss = 0.467133
Epoch 3.110: Loss = 0.465652
Epoch 3.111: Loss = 0.547897
Epoch 3.112: Loss = 0.487671
Epoch 3.113: Loss = 0.527878
Epoch 3.114: Loss = 0.486313
Epoch 3.115: Loss = 0.576965
Epoch 3.116: Loss = 0.514832
Epoch 3.117: Loss = 0.474335
Epoch 3.118: Loss = 0.442093
Epoch 3.119: Loss = 0.449997
Epoch 3.120: Loss = 0.454971
TRAIN LOSS = 0.502197
TRAIN ACC = 84.7839 % (50873/60000)
Loss = 0.462021
Loss = 0.577637
Loss = 0.612671
Loss = 0.629837
Loss = 0.673492
Loss = 0.500427
Loss = 0.462036
Loss = 0.635025
Loss = 0.605179
Loss = 0.590607
Loss = 0.261292
Loss = 0.445923
Loss = 0.281265
Loss = 0.472214
Loss = 0.298935
Loss = 0.320007
Loss = 0.304611
Loss = 0.121521
Loss = 0.325378
Loss = 0.670151
TEST LOSS = 0.462511
TEST ACC = 508.73 % (8585/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.417068
Epoch 4.2: Loss = 0.487961
Epoch 4.3: Loss = 0.46933
Epoch 4.4: Loss = 0.492599
Epoch 4.5: Loss = 0.380676
Epoch 4.6: Loss = 0.394272
Epoch 4.7: Loss = 0.439377
Epoch 4.8: Loss = 0.54364
Epoch 4.9: Loss = 0.424057
Epoch 4.10: Loss = 0.558899
Epoch 4.11: Loss = 0.438644
Epoch 4.12: Loss = 0.432281
Epoch 4.13: Loss = 0.46106
Epoch 4.14: Loss = 0.442551
Epoch 4.15: Loss = 0.395294
Epoch 4.16: Loss = 0.496704
Epoch 4.17: Loss = 0.553497
Epoch 4.18: Loss = 0.481735
Epoch 4.19: Loss = 0.485107
Epoch 4.20: Loss = 0.461395
Epoch 4.21: Loss = 0.510971
Epoch 4.22: Loss = 0.481812
Epoch 4.23: Loss = 0.454834
Epoch 4.24: Loss = 0.529526
Epoch 4.25: Loss = 0.506683
Epoch 4.26: Loss = 0.565216
Epoch 4.27: Loss = 0.498032
Epoch 4.28: Loss = 0.39769
Epoch 4.29: Loss = 0.428146
Epoch 4.30: Loss = 0.467606
Epoch 4.31: Loss = 0.449341
Epoch 4.32: Loss = 0.542557
Epoch 4.33: Loss = 0.410126
Epoch 4.34: Loss = 0.469528
Epoch 4.35: Loss = 0.427673
Epoch 4.36: Loss = 0.442886
Epoch 4.37: Loss = 0.412292
Epoch 4.38: Loss = 0.501678
Epoch 4.39: Loss = 0.567123
Epoch 4.40: Loss = 0.413132
Epoch 4.41: Loss = 0.52298
Epoch 4.42: Loss = 0.463959
Epoch 4.43: Loss = 0.504318
Epoch 4.44: Loss = 0.548172
Epoch 4.45: Loss = 0.406403
Epoch 4.46: Loss = 0.441864
Epoch 4.47: Loss = 0.463287
Epoch 4.48: Loss = 0.525528
Epoch 4.49: Loss = 0.591049
Epoch 4.50: Loss = 0.415359
Epoch 4.51: Loss = 0.462631
Epoch 4.52: Loss = 0.526901
Epoch 4.53: Loss = 0.612839
Epoch 4.54: Loss = 0.522263
Epoch 4.55: Loss = 0.519958
Epoch 4.56: Loss = 0.460114
Epoch 4.57: Loss = 0.454361
Epoch 4.58: Loss = 0.46965
Epoch 4.59: Loss = 0.492996
Epoch 4.60: Loss = 0.456512
Epoch 4.61: Loss = 0.554855
Epoch 4.62: Loss = 0.526871
Epoch 4.63: Loss = 0.488434
Epoch 4.64: Loss = 0.49469
Epoch 4.65: Loss = 0.388535
Epoch 4.66: Loss = 0.509064
Epoch 4.67: Loss = 0.514938
Epoch 4.68: Loss = 0.483017
Epoch 4.69: Loss = 0.514145
Epoch 4.70: Loss = 0.489517
Epoch 4.71: Loss = 0.477798
Epoch 4.72: Loss = 0.494171
Epoch 4.73: Loss = 0.552307
Epoch 4.74: Loss = 0.496872
Epoch 4.75: Loss = 0.465042
Epoch 4.76: Loss = 0.437805
Epoch 4.77: Loss = 0.575394
Epoch 4.78: Loss = 0.545975
Epoch 4.79: Loss = 0.555969
Epoch 4.80: Loss = 0.53212
Epoch 4.81: Loss = 0.528412
Epoch 4.82: Loss = 0.475433
Epoch 4.83: Loss = 0.551956
Epoch 4.84: Loss = 0.543808
Epoch 4.85: Loss = 0.564682
Epoch 4.86: Loss = 0.544388
Epoch 4.87: Loss = 0.455688
Epoch 4.88: Loss = 0.463882
Epoch 4.89: Loss = 0.437103
Epoch 4.90: Loss = 0.475937
Epoch 4.91: Loss = 0.528381
Epoch 4.92: Loss = 0.4655
Epoch 4.93: Loss = 0.436447
Epoch 4.94: Loss = 0.520554
Epoch 4.95: Loss = 0.49791
Epoch 4.96: Loss = 0.469238
Epoch 4.97: Loss = 0.468811
Epoch 4.98: Loss = 0.565109
Epoch 4.99: Loss = 0.461243
Epoch 4.100: Loss = 0.49765
Epoch 4.101: Loss = 0.444672
Epoch 4.102: Loss = 0.460434
Epoch 4.103: Loss = 0.526825
Epoch 4.104: Loss = 0.489975
Epoch 4.105: Loss = 0.558472
Epoch 4.106: Loss = 0.407913
Epoch 4.107: Loss = 0.427917
Epoch 4.108: Loss = 0.507904
Epoch 4.109: Loss = 0.53186
Epoch 4.110: Loss = 0.537354
Epoch 4.111: Loss = 0.514694
Epoch 4.112: Loss = 0.576416
Epoch 4.113: Loss = 0.462189
Epoch 4.114: Loss = 0.538452
Epoch 4.115: Loss = 0.447617
Epoch 4.116: Loss = 0.487076
Epoch 4.117: Loss = 0.442093
Epoch 4.118: Loss = 0.595383
Epoch 4.119: Loss = 0.484375
Epoch 4.120: Loss = 0.514923
TRAIN LOSS = 0.488342
TRAIN ACC = 85.5469 % (51330/60000)
Loss = 0.461166
Loss = 0.566513
Loss = 0.619308
Loss = 0.649643
Loss = 0.656189
Loss = 0.463318
Loss = 0.444733
Loss = 0.681076
Loss = 0.578308
Loss = 0.617584
Loss = 0.247086
Loss = 0.411545
Loss = 0.259842
Loss = 0.463928
Loss = 0.289001
Loss = 0.328094
Loss = 0.301559
Loss = 0.106644
Loss = 0.310196
Loss = 0.687744
TEST LOSS = 0.457174
TEST ACC = 513.3 % (8638/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.568985
Epoch 5.2: Loss = 0.512939
Epoch 5.3: Loss = 0.575638
Epoch 5.4: Loss = 0.473083
Epoch 5.5: Loss = 0.509033
Epoch 5.6: Loss = 0.515961
Epoch 5.7: Loss = 0.4254
Epoch 5.8: Loss = 0.441055
Epoch 5.9: Loss = 0.457077
Epoch 5.10: Loss = 0.470047
Epoch 5.11: Loss = 0.439941
Epoch 5.12: Loss = 0.448502
Epoch 5.13: Loss = 0.402786
Epoch 5.14: Loss = 0.483765
Epoch 5.15: Loss = 0.466507
Epoch 5.16: Loss = 0.390106
Epoch 5.17: Loss = 0.449585
Epoch 5.18: Loss = 0.421936
Epoch 5.19: Loss = 0.47197
Epoch 5.20: Loss = 0.396225
Epoch 5.21: Loss = 0.51387
Epoch 5.22: Loss = 0.506882
Epoch 5.23: Loss = 0.517212
Epoch 5.24: Loss = 0.484268
Epoch 5.25: Loss = 0.42305
Epoch 5.26: Loss = 0.52774
Epoch 5.27: Loss = 0.420776
Epoch 5.28: Loss = 0.502258
Epoch 5.29: Loss = 0.484634
Epoch 5.30: Loss = 0.498993
Epoch 5.31: Loss = 0.513611
Epoch 5.32: Loss = 0.424408
Epoch 5.33: Loss = 0.517563
Epoch 5.34: Loss = 0.519272
Epoch 5.35: Loss = 0.456253
Epoch 5.36: Loss = 0.473587
Epoch 5.37: Loss = 0.470673
Epoch 5.38: Loss = 0.40596
Epoch 5.39: Loss = 0.506256
Epoch 5.40: Loss = 0.52182
Epoch 5.41: Loss = 0.510498
Epoch 5.42: Loss = 0.461456
Epoch 5.43: Loss = 0.40387
Epoch 5.44: Loss = 0.410278
Epoch 5.45: Loss = 0.503708
Epoch 5.46: Loss = 0.53595
Epoch 5.47: Loss = 0.54158
Epoch 5.48: Loss = 0.478699
Epoch 5.49: Loss = 0.424438
Epoch 5.50: Loss = 0.446564
Epoch 5.51: Loss = 0.49649
Epoch 5.52: Loss = 0.402573
Epoch 5.53: Loss = 0.485062
Epoch 5.54: Loss = 0.454102
Epoch 5.55: Loss = 0.487839
Epoch 5.56: Loss = 0.435486
Epoch 5.57: Loss = 0.56073
Epoch 5.58: Loss = 0.532959
Epoch 5.59: Loss = 0.486557
Epoch 5.60: Loss = 0.467484
Epoch 5.61: Loss = 0.502029
Epoch 5.62: Loss = 0.520966
Epoch 5.63: Loss = 0.512421
Epoch 5.64: Loss = 0.445572
Epoch 5.65: Loss = 0.403305
Epoch 5.66: Loss = 0.457169
Epoch 5.67: Loss = 0.497528
Epoch 5.68: Loss = 0.492828
Epoch 5.69: Loss = 0.503952
Epoch 5.70: Loss = 0.519302
Epoch 5.71: Loss = 0.398743
Epoch 5.72: Loss = 0.448792
Epoch 5.73: Loss = 0.478119
Epoch 5.74: Loss = 0.439484
Epoch 5.75: Loss = 0.477875
Epoch 5.76: Loss = 0.496872
Epoch 5.77: Loss = 0.471832
Epoch 5.78: Loss = 0.448685
Epoch 5.79: Loss = 0.507309
Epoch 5.80: Loss = 0.553055
Epoch 5.81: Loss = 0.478928
Epoch 5.82: Loss = 0.54422
Epoch 5.83: Loss = 0.448624
Epoch 5.84: Loss = 0.4254
Epoch 5.85: Loss = 0.479797
Epoch 5.86: Loss = 0.465424
Epoch 5.87: Loss = 0.399292
Epoch 5.88: Loss = 0.438782
Epoch 5.89: Loss = 0.539078
Epoch 5.90: Loss = 0.47438
Epoch 5.91: Loss = 0.413986
Epoch 5.92: Loss = 0.566284
Epoch 5.93: Loss = 0.424881
Epoch 5.94: Loss = 0.453049
Epoch 5.95: Loss = 0.537354
Epoch 5.96: Loss = 0.461243
Epoch 5.97: Loss = 0.501312
Epoch 5.98: Loss = 0.418137
Epoch 5.99: Loss = 0.459961
Epoch 5.100: Loss = 0.523758
Epoch 5.101: Loss = 0.438995
Epoch 5.102: Loss = 0.415222
Epoch 5.103: Loss = 0.49733
Epoch 5.104: Loss = 0.490295
Epoch 5.105: Loss = 0.576843
Epoch 5.106: Loss = 0.503235
Epoch 5.107: Loss = 0.548538
Epoch 5.108: Loss = 0.435089
Epoch 5.109: Loss = 0.546906
Epoch 5.110: Loss = 0.499039
Epoch 5.111: Loss = 0.522476
Epoch 5.112: Loss = 0.439545
Epoch 5.113: Loss = 0.470734
Epoch 5.114: Loss = 0.413498
Epoch 5.115: Loss = 0.632217
Epoch 5.116: Loss = 0.472595
Epoch 5.117: Loss = 0.548813
Epoch 5.118: Loss = 0.458893
Epoch 5.119: Loss = 0.571243
Epoch 5.120: Loss = 0.435638
TRAIN LOSS = 0.479034
TRAIN ACC = 85.9467 % (51571/60000)
Loss = 0.451691
Loss = 0.535629
Loss = 0.650085
Loss = 0.627716
Loss = 0.657486
Loss = 0.480484
Loss = 0.456207
Loss = 0.717163
Loss = 0.573135
Loss = 0.574097
Loss = 0.234497
Loss = 0.438995
Loss = 0.263382
Loss = 0.444717
Loss = 0.25502
Loss = 0.315781
Loss = 0.270828
Loss = 0.0960846
Loss = 0.289673
Loss = 0.662415
TEST LOSS = 0.449754
TEST ACC = 515.709 % (8730/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.438568
Epoch 6.2: Loss = 0.504044
Epoch 6.3: Loss = 0.522507
Epoch 6.4: Loss = 0.411972
Epoch 6.5: Loss = 0.398849
Epoch 6.6: Loss = 0.485901
Epoch 6.7: Loss = 0.45018
Epoch 6.8: Loss = 0.527298
Epoch 6.9: Loss = 0.431854
Epoch 6.10: Loss = 0.438385
Epoch 6.11: Loss = 0.535645
Epoch 6.12: Loss = 0.448898
Epoch 6.13: Loss = 0.464615
Epoch 6.14: Loss = 0.444641
Epoch 6.15: Loss = 0.374023
Epoch 6.16: Loss = 0.438095
Epoch 6.17: Loss = 0.504593
Epoch 6.18: Loss = 0.522278
Epoch 6.19: Loss = 0.423965
Epoch 6.20: Loss = 0.520828
Epoch 6.21: Loss = 0.347641
Epoch 6.22: Loss = 0.475433
Epoch 6.23: Loss = 0.537079
Epoch 6.24: Loss = 0.477463
Epoch 6.25: Loss = 0.443817
Epoch 6.26: Loss = 0.489029
Epoch 6.27: Loss = 0.498856
Epoch 6.28: Loss = 0.53981
Epoch 6.29: Loss = 0.522766
Epoch 6.30: Loss = 0.439941
Epoch 6.31: Loss = 0.484802
Epoch 6.32: Loss = 0.517944
Epoch 6.33: Loss = 0.517548
Epoch 6.34: Loss = 0.481461
Epoch 6.35: Loss = 0.372025
Epoch 6.36: Loss = 0.611389
Epoch 6.37: Loss = 0.470474
Epoch 6.38: Loss = 0.473358
Epoch 6.39: Loss = 0.47374
Epoch 6.40: Loss = 0.578232
Epoch 6.41: Loss = 0.351303
Epoch 6.42: Loss = 0.49942
Epoch 6.43: Loss = 0.56871
Epoch 6.44: Loss = 0.454529
Epoch 6.45: Loss = 0.461151
Epoch 6.46: Loss = 0.42514
Epoch 6.47: Loss = 0.552078
Epoch 6.48: Loss = 0.338272
Epoch 6.49: Loss = 0.452072
Epoch 6.50: Loss = 0.534073
Epoch 6.51: Loss = 0.453735
Epoch 6.52: Loss = 0.453415
Epoch 6.53: Loss = 0.423355
Epoch 6.54: Loss = 0.626358
Epoch 6.55: Loss = 0.466858
Epoch 6.56: Loss = 0.519485
Epoch 6.57: Loss = 0.494339
Epoch 6.58: Loss = 0.416122
Epoch 6.59: Loss = 0.425598
Epoch 6.60: Loss = 0.608948
Epoch 6.61: Loss = 0.520294
Epoch 6.62: Loss = 0.429031
Epoch 6.63: Loss = 0.417267
Epoch 6.64: Loss = 0.445404
Epoch 6.65: Loss = 0.522629
Epoch 6.66: Loss = 0.542801
Epoch 6.67: Loss = 0.51619
Epoch 6.68: Loss = 0.460922
Epoch 6.69: Loss = 0.398361
Epoch 6.70: Loss = 0.53241
Epoch 6.71: Loss = 0.646561
Epoch 6.72: Loss = 0.520859
Epoch 6.73: Loss = 0.551544
Epoch 6.74: Loss = 0.529449
Epoch 6.75: Loss = 0.469986
Epoch 6.76: Loss = 0.490433
Epoch 6.77: Loss = 0.487274
Epoch 6.78: Loss = 0.535828
Epoch 6.79: Loss = 0.464188
Epoch 6.80: Loss = 0.557297
Epoch 6.81: Loss = 0.519928
Epoch 6.82: Loss = 0.554291
Epoch 6.83: Loss = 0.475906
Epoch 6.84: Loss = 0.400894
Epoch 6.85: Loss = 0.47963
Epoch 6.86: Loss = 0.55191
Epoch 6.87: Loss = 0.685699
Epoch 6.88: Loss = 0.4328
Epoch 6.89: Loss = 0.442947
Epoch 6.90: Loss = 0.437897
Epoch 6.91: Loss = 0.514114
Epoch 6.92: Loss = 0.540329
Epoch 6.93: Loss = 0.515381
Epoch 6.94: Loss = 0.454376
Epoch 6.95: Loss = 0.449646
Epoch 6.96: Loss = 0.537277
Epoch 6.97: Loss = 0.421844
Epoch 6.98: Loss = 0.411316
Epoch 6.99: Loss = 0.572632
Epoch 6.100: Loss = 0.493011
Epoch 6.101: Loss = 0.509232
Epoch 6.102: Loss = 0.508301
Epoch 6.103: Loss = 0.402466
Epoch 6.104: Loss = 0.442886
Epoch 6.105: Loss = 0.44664
Epoch 6.106: Loss = 0.423874
Epoch 6.107: Loss = 0.515762
Epoch 6.108: Loss = 0.603592
Epoch 6.109: Loss = 0.470612
Epoch 6.110: Loss = 0.518143
Epoch 6.111: Loss = 0.439636
Epoch 6.112: Loss = 0.552933
Epoch 6.113: Loss = 0.425964
Epoch 6.114: Loss = 0.505539
Epoch 6.115: Loss = 0.437302
Epoch 6.116: Loss = 0.345657
Epoch 6.117: Loss = 0.447861
Epoch 6.118: Loss = 0.427567
Epoch 6.119: Loss = 0.422119
Epoch 6.120: Loss = 0.553085
TRAIN LOSS = 0.482452
TRAIN ACC = 86.4929 % (51898/60000)
Loss = 0.449615
Loss = 0.568878
Loss = 0.631393
Loss = 0.647507
Loss = 0.643265
Loss = 0.466888
Loss = 0.440964
Loss = 0.738647
Loss = 0.602676
Loss = 0.602127
Loss = 0.2285
Loss = 0.442947
Loss = 0.249817
Loss = 0.477646
Loss = 0.233398
Loss = 0.317993
Loss = 0.291458
Loss = 0.0717773
Loss = 0.305496
Loss = 0.715729
TEST LOSS = 0.456336
TEST ACC = 518.979 % (8747/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.538788
Epoch 7.2: Loss = 0.5634
Epoch 7.3: Loss = 0.44223
Epoch 7.4: Loss = 0.526627
Epoch 7.5: Loss = 0.544907
Epoch 7.6: Loss = 0.521057
Epoch 7.7: Loss = 0.52005
Epoch 7.8: Loss = 0.440079
Epoch 7.9: Loss = 0.516754
Epoch 7.10: Loss = 0.516891
Epoch 7.11: Loss = 0.433426
Epoch 7.12: Loss = 0.567993
Epoch 7.13: Loss = 0.502609
Epoch 7.14: Loss = 0.550385
Epoch 7.15: Loss = 0.469986
Epoch 7.16: Loss = 0.493225
Epoch 7.17: Loss = 0.495178
Epoch 7.18: Loss = 0.416031
Epoch 7.19: Loss = 0.516724
Epoch 7.20: Loss = 0.525635
Epoch 7.21: Loss = 0.519043
Epoch 7.22: Loss = 0.467346
Epoch 7.23: Loss = 0.436386
Epoch 7.24: Loss = 0.452759
Epoch 7.25: Loss = 0.489853
Epoch 7.26: Loss = 0.496109
Epoch 7.27: Loss = 0.480072
Epoch 7.28: Loss = 0.46817
Epoch 7.29: Loss = 0.407928
Epoch 7.30: Loss = 0.420639
Epoch 7.31: Loss = 0.395599
Epoch 7.32: Loss = 0.492706
Epoch 7.33: Loss = 0.477112
Epoch 7.34: Loss = 0.442688
Epoch 7.35: Loss = 0.513306
Epoch 7.36: Loss = 0.503006
Epoch 7.37: Loss = 0.387054
Epoch 7.38: Loss = 0.427338
Epoch 7.39: Loss = 0.553116
Epoch 7.40: Loss = 0.491959
Epoch 7.41: Loss = 0.586807
Epoch 7.42: Loss = 0.50209
Epoch 7.43: Loss = 0.594009
Epoch 7.44: Loss = 0.470734
Epoch 7.45: Loss = 0.677414
Epoch 7.46: Loss = 0.584045
Epoch 7.47: Loss = 0.395233
Epoch 7.48: Loss = 0.55748
Epoch 7.49: Loss = 0.514832
Epoch 7.50: Loss = 0.455292
Epoch 7.51: Loss = 0.524963
Epoch 7.52: Loss = 0.420013
Epoch 7.53: Loss = 0.495712
Epoch 7.54: Loss = 0.551163
Epoch 7.55: Loss = 0.527161
Epoch 7.56: Loss = 0.531219
Epoch 7.57: Loss = 0.458817
Epoch 7.58: Loss = 0.511154
Epoch 7.59: Loss = 0.489258
Epoch 7.60: Loss = 0.499878
Epoch 7.61: Loss = 0.493073
Epoch 7.62: Loss = 0.462433
Epoch 7.63: Loss = 0.596298
Epoch 7.64: Loss = 0.499893
Epoch 7.65: Loss = 0.423767
Epoch 7.66: Loss = 0.379211
Epoch 7.67: Loss = 0.418198
Epoch 7.68: Loss = 0.509628
Epoch 7.69: Loss = 0.470947
Epoch 7.70: Loss = 0.416931
Epoch 7.71: Loss = 0.490158
Epoch 7.72: Loss = 0.50853
Epoch 7.73: Loss = 0.622833
Epoch 7.74: Loss = 0.586975
Epoch 7.75: Loss = 0.409302
Epoch 7.76: Loss = 0.498001
Epoch 7.77: Loss = 0.520248
Epoch 7.78: Loss = 0.539474
Epoch 7.79: Loss = 0.488968
Epoch 7.80: Loss = 0.551208
Epoch 7.81: Loss = 0.513184
Epoch 7.82: Loss = 0.351776
Epoch 7.83: Loss = 0.429092
Epoch 7.84: Loss = 0.562225
Epoch 7.85: Loss = 0.602173
Epoch 7.86: Loss = 0.631104
Epoch 7.87: Loss = 0.476944
Epoch 7.88: Loss = 0.648224
Epoch 7.89: Loss = 0.474472
Epoch 7.90: Loss = 0.518082
Epoch 7.91: Loss = 0.584961
Epoch 7.92: Loss = 0.437759
Epoch 7.93: Loss = 0.519135
Epoch 7.94: Loss = 0.507126
Epoch 7.95: Loss = 0.45047
Epoch 7.96: Loss = 0.451096
Epoch 7.97: Loss = 0.461197
Epoch 7.98: Loss = 0.441666
Epoch 7.99: Loss = 0.404648
Epoch 7.100: Loss = 0.542023
Epoch 7.101: Loss = 0.428024
Epoch 7.102: Loss = 0.402206
Epoch 7.103: Loss = 0.597977
Epoch 7.104: Loss = 0.555786
Epoch 7.105: Loss = 0.467575
Epoch 7.106: Loss = 0.524933
Epoch 7.107: Loss = 0.450363
Epoch 7.108: Loss = 0.611755
Epoch 7.109: Loss = 0.505127
Epoch 7.110: Loss = 0.563416
Epoch 7.111: Loss = 0.635269
Epoch 7.112: Loss = 0.567947
Epoch 7.113: Loss = 0.62645
Epoch 7.114: Loss = 0.396179
Epoch 7.115: Loss = 0.485443
Epoch 7.116: Loss = 0.569794
Epoch 7.117: Loss = 0.384369
Epoch 7.118: Loss = 0.486572
Epoch 7.119: Loss = 0.567734
Epoch 7.120: Loss = 0.530365
TRAIN LOSS = 0.500198
TRAIN ACC = 86.5082 % (51907/60000)
Loss = 0.472214
Loss = 0.609894
Loss = 0.655548
Loss = 0.665771
Loss = 0.700943
Loss = 0.503647
Loss = 0.453751
Loss = 0.809418
Loss = 0.621262
Loss = 0.628143
Loss = 0.238174
Loss = 0.463425
Loss = 0.296829
Loss = 0.486115
Loss = 0.243958
Loss = 0.340424
Loss = 0.323288
Loss = 0.0904999
Loss = 0.314575
Loss = 0.707748
TEST LOSS = 0.481281
TEST ACC = 519.069 % (8776/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.516525
Epoch 8.2: Loss = 0.567795
Epoch 8.3: Loss = 0.435684
Epoch 8.4: Loss = 0.528381
Epoch 8.5: Loss = 0.471237
Epoch 8.6: Loss = 0.496918
Epoch 8.7: Loss = 0.487183
Epoch 8.8: Loss = 0.42514
Epoch 8.9: Loss = 0.480698
Epoch 8.10: Loss = 0.533447
Epoch 8.11: Loss = 0.484421
Epoch 8.12: Loss = 0.534805
Epoch 8.13: Loss = 0.474503
Epoch 8.14: Loss = 0.528778
Epoch 8.15: Loss = 0.438004
Epoch 8.16: Loss = 0.439438
Epoch 8.17: Loss = 0.51506
Epoch 8.18: Loss = 0.405258
Epoch 8.19: Loss = 0.47818
Epoch 8.20: Loss = 0.56781
Epoch 8.21: Loss = 0.48317
Epoch 8.22: Loss = 0.551605
Epoch 8.23: Loss = 0.442947
Epoch 8.24: Loss = 0.46521
Epoch 8.25: Loss = 0.508026
Epoch 8.26: Loss = 0.522339
Epoch 8.27: Loss = 0.546219
Epoch 8.28: Loss = 0.451019
Epoch 8.29: Loss = 0.504013
Epoch 8.30: Loss = 0.545853
Epoch 8.31: Loss = 0.487442
Epoch 8.32: Loss = 0.479019
Epoch 8.33: Loss = 0.475189
Epoch 8.34: Loss = 0.470322
Epoch 8.35: Loss = 0.435165
Epoch 8.36: Loss = 0.507111
Epoch 8.37: Loss = 0.523254
Epoch 8.38: Loss = 0.604538
Epoch 8.39: Loss = 0.542267
Epoch 8.40: Loss = 0.478683
Epoch 8.41: Loss = 0.408188
Epoch 8.42: Loss = 0.528549
Epoch 8.43: Loss = 0.688644
Epoch 8.44: Loss = 0.601288
Epoch 8.45: Loss = 0.481506
Epoch 8.46: Loss = 0.552933
Epoch 8.47: Loss = 0.455826
Epoch 8.48: Loss = 0.419235
Epoch 8.49: Loss = 0.5354
Epoch 8.50: Loss = 0.506927
Epoch 8.51: Loss = 0.543381
Epoch 8.52: Loss = 0.621063
Epoch 8.53: Loss = 0.563156
Epoch 8.54: Loss = 0.606033
Epoch 8.55: Loss = 0.437897
Epoch 8.56: Loss = 0.539871
Epoch 8.57: Loss = 0.494293
Epoch 8.58: Loss = 0.636719
Epoch 8.59: Loss = 0.540802
Epoch 8.60: Loss = 0.564468
Epoch 8.61: Loss = 0.350311
Epoch 8.62: Loss = 0.541092
Epoch 8.63: Loss = 0.435242
Epoch 8.64: Loss = 0.464508
Epoch 8.65: Loss = 0.510147
Epoch 8.66: Loss = 0.525162
Epoch 8.67: Loss = 0.458069
Epoch 8.68: Loss = 0.544632
Epoch 8.69: Loss = 0.577362
Epoch 8.70: Loss = 0.571228
Epoch 8.71: Loss = 0.495056
Epoch 8.72: Loss = 0.721466
Epoch 8.73: Loss = 0.720734
Epoch 8.74: Loss = 0.472137
Epoch 8.75: Loss = 0.46759
Epoch 8.76: Loss = 0.612061
Epoch 8.77: Loss = 0.433655
Epoch 8.78: Loss = 0.581467
Epoch 8.79: Loss = 0.545166
Epoch 8.80: Loss = 0.532333
Epoch 8.81: Loss = 0.610962
Epoch 8.82: Loss = 0.608963
Epoch 8.83: Loss = 0.566376
Epoch 8.84: Loss = 0.457306
Epoch 8.85: Loss = 0.488724
Epoch 8.86: Loss = 0.597626
Epoch 8.87: Loss = 0.647171
Epoch 8.88: Loss = 0.591934
Epoch 8.89: Loss = 0.638214
Epoch 8.90: Loss = 0.624817
Epoch 8.91: Loss = 0.624893
Epoch 8.92: Loss = 0.549362
Epoch 8.93: Loss = 0.595886
Epoch 8.94: Loss = 0.459518
Epoch 8.95: Loss = 0.392578
Epoch 8.96: Loss = 0.524292
Epoch 8.97: Loss = 0.633316
Epoch 8.98: Loss = 0.518036
Epoch 8.99: Loss = 0.55307
Epoch 8.100: Loss = 0.573257
Epoch 8.101: Loss = 0.532791
Epoch 8.102: Loss = 0.462204
Epoch 8.103: Loss = 0.468201
Epoch 8.104: Loss = 0.436142
Epoch 8.105: Loss = 0.557892
Epoch 8.106: Loss = 0.593079
Epoch 8.107: Loss = 0.510101
Epoch 8.108: Loss = 0.550949
Epoch 8.109: Loss = 0.630768
Epoch 8.110: Loss = 0.447662
Epoch 8.111: Loss = 0.60495
Epoch 8.112: Loss = 0.369507
Epoch 8.113: Loss = 0.49762
Epoch 8.114: Loss = 0.657288
Epoch 8.115: Loss = 0.586899
Epoch 8.116: Loss = 0.493057
Epoch 8.117: Loss = 0.418228
Epoch 8.118: Loss = 0.508148
Epoch 8.119: Loss = 0.5681
Epoch 8.120: Loss = 0.486252
TRAIN LOSS = 0.522964
TRAIN ACC = 86.3754 % (51828/60000)
Loss = 0.507492
Loss = 0.665268
Loss = 0.681305
Loss = 0.731064
Loss = 0.748459
Loss = 0.516037
Loss = 0.493988
Loss = 0.830856
Loss = 0.648438
Loss = 0.645081
Loss = 0.224808
Loss = 0.489624
Loss = 0.333023
Loss = 0.555374
Loss = 0.254364
Loss = 0.384705
Loss = 0.312927
Loss = 0.0879059
Loss = 0.338486
Loss = 0.74025
TEST LOSS = 0.509473
TEST ACC = 518.279 % (8736/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.487244
Epoch 9.2: Loss = 0.575302
Epoch 9.3: Loss = 0.424805
Epoch 9.4: Loss = 0.485184
Epoch 9.5: Loss = 0.483505
Epoch 9.6: Loss = 0.525955
Epoch 9.7: Loss = 0.514069
Epoch 9.8: Loss = 0.54097
Epoch 9.9: Loss = 0.613724
Epoch 9.10: Loss = 0.521957
Epoch 9.11: Loss = 0.440292
Epoch 9.12: Loss = 0.500061
Epoch 9.13: Loss = 0.528061
Epoch 9.14: Loss = 0.550781
Epoch 9.15: Loss = 0.589355
Epoch 9.16: Loss = 0.586273
Epoch 9.17: Loss = 0.555161
Epoch 9.18: Loss = 0.533173
Epoch 9.19: Loss = 0.561646
Epoch 9.20: Loss = 0.515244
Epoch 9.21: Loss = 0.493896
Epoch 9.22: Loss = 0.516602
Epoch 9.23: Loss = 0.601578
Epoch 9.24: Loss = 0.457672
Epoch 9.25: Loss = 0.521851
Epoch 9.26: Loss = 0.585449
Epoch 9.27: Loss = 0.597061
Epoch 9.28: Loss = 0.671692
Epoch 9.29: Loss = 0.498367
Epoch 9.30: Loss = 0.63652
Epoch 9.31: Loss = 0.467545
Epoch 9.32: Loss = 0.514328
Epoch 9.33: Loss = 0.414871
Epoch 9.34: Loss = 0.554626
Epoch 9.35: Loss = 0.597473
Epoch 9.36: Loss = 0.571182
Epoch 9.37: Loss = 0.611725
Epoch 9.38: Loss = 0.596222
Epoch 9.39: Loss = 0.332779
Epoch 9.40: Loss = 0.521774
Epoch 9.41: Loss = 0.485703
Epoch 9.42: Loss = 0.601425
Epoch 9.43: Loss = 0.57251
Epoch 9.44: Loss = 0.50209
Epoch 9.45: Loss = 0.457016
Epoch 9.46: Loss = 0.607407
Epoch 9.47: Loss = 0.605026
Epoch 9.48: Loss = 0.585098
Epoch 9.49: Loss = 0.522079
Epoch 9.50: Loss = 0.535339
Epoch 9.51: Loss = 0.522461
Epoch 9.52: Loss = 0.492889
Epoch 9.53: Loss = 0.564529
Epoch 9.54: Loss = 0.445023
Epoch 9.55: Loss = 0.544464
Epoch 9.56: Loss = 0.494797
Epoch 9.57: Loss = 0.519913
Epoch 9.58: Loss = 0.503387
Epoch 9.59: Loss = 0.635437
Epoch 9.60: Loss = 0.608475
Epoch 9.61: Loss = 0.565567
Epoch 9.62: Loss = 0.516479
Epoch 9.63: Loss = 0.461304
Epoch 9.64: Loss = 0.573532
Epoch 9.65: Loss = 0.392197
Epoch 9.66: Loss = 0.595779
Epoch 9.67: Loss = 0.474014
Epoch 9.68: Loss = 0.506958
Epoch 9.69: Loss = 0.689972
Epoch 9.70: Loss = 0.537399
Epoch 9.71: Loss = 0.615601
Epoch 9.72: Loss = 0.587799
Epoch 9.73: Loss = 0.528564
Epoch 9.74: Loss = 0.631165
Epoch 9.75: Loss = 0.468399
Epoch 9.76: Loss = 0.46228
Epoch 9.77: Loss = 0.510498
Epoch 9.78: Loss = 0.523483
Epoch 9.79: Loss = 0.445084
Epoch 9.80: Loss = 0.630127
Epoch 9.81: Loss = 0.532501
Epoch 9.82: Loss = 0.64801
Epoch 9.83: Loss = 0.474716
Epoch 9.84: Loss = 0.539108
Epoch 9.85: Loss = 0.52803
Epoch 9.86: Loss = 0.590408
Epoch 9.87: Loss = 0.532562
Epoch 9.88: Loss = 0.556839
Epoch 9.89: Loss = 0.507904
Epoch 9.90: Loss = 0.459885
Epoch 9.91: Loss = 0.51474
Epoch 9.92: Loss = 0.663437
Epoch 9.93: Loss = 0.536499
Epoch 9.94: Loss = 0.484299
Epoch 9.95: Loss = 0.576355
Epoch 9.96: Loss = 0.520767
Epoch 9.97: Loss = 0.505539
Epoch 9.98: Loss = 0.81076
Epoch 9.99: Loss = 0.538925
Epoch 9.100: Loss = 0.614258
Epoch 9.101: Loss = 0.414368
Epoch 9.102: Loss = 0.508987
Epoch 9.103: Loss = 0.644577
Epoch 9.104: Loss = 0.511292
Epoch 9.105: Loss = 0.640076
Epoch 9.106: Loss = 0.543716
Epoch 9.107: Loss = 0.650452
Epoch 9.108: Loss = 0.531265
Epoch 9.109: Loss = 0.532364
Epoch 9.110: Loss = 0.595505
Epoch 9.111: Loss = 0.501678
Epoch 9.112: Loss = 0.617065
Epoch 9.113: Loss = 0.49469
Epoch 9.114: Loss = 0.602097
Epoch 9.115: Loss = 0.498123
Epoch 9.116: Loss = 0.504929
Epoch 9.117: Loss = 0.597534
Epoch 9.118: Loss = 0.688782
Epoch 9.119: Loss = 0.684875
Epoch 9.120: Loss = 0.523392
TRAIN LOSS = 0.543671
TRAIN ACC = 86.1984 % (51721/60000)
Loss = 0.528687
Loss = 0.621674
Loss = 0.636749
Loss = 0.731125
Loss = 0.751663
Loss = 0.51239
Loss = 0.513184
Loss = 0.878616
Loss = 0.6521
Loss = 0.655838
Loss = 0.206909
Loss = 0.54628
Loss = 0.384705
Loss = 0.561157
Loss = 0.260559
Loss = 0.349091
Loss = 0.311722
Loss = 0.0719757
Loss = 0.296402
Loss = 0.742554
TEST LOSS = 0.510669
TEST ACC = 517.209 % (8736/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.631073
Epoch 10.2: Loss = 0.479614
Epoch 10.3: Loss = 0.603867
Epoch 10.4: Loss = 0.458466
Epoch 10.5: Loss = 0.525574
Epoch 10.6: Loss = 0.548004
Epoch 10.7: Loss = 0.481033
Epoch 10.8: Loss = 0.597336
Epoch 10.9: Loss = 0.652588
Epoch 10.10: Loss = 0.557922
Epoch 10.11: Loss = 0.500748
Epoch 10.12: Loss = 0.570053
Epoch 10.13: Loss = 0.496277
Epoch 10.14: Loss = 0.586334
Epoch 10.15: Loss = 0.547668
Epoch 10.16: Loss = 0.508224
Epoch 10.17: Loss = 0.568115
Epoch 10.18: Loss = 0.586533
Epoch 10.19: Loss = 0.496567
Epoch 10.20: Loss = 0.524292
Epoch 10.21: Loss = 0.603882
Epoch 10.22: Loss = 0.607208
Epoch 10.23: Loss = 0.561813
Epoch 10.24: Loss = 0.620941
Epoch 10.25: Loss = 0.551376
Epoch 10.26: Loss = 0.408569
Epoch 10.27: Loss = 0.507263
Epoch 10.28: Loss = 0.558334
Epoch 10.29: Loss = 0.461319
Epoch 10.30: Loss = 0.597031
Epoch 10.31: Loss = 0.479431
Epoch 10.32: Loss = 0.522339
Epoch 10.33: Loss = 0.447464
Epoch 10.34: Loss = 0.49556
Epoch 10.35: Loss = 0.647797
Epoch 10.36: Loss = 0.513184
Epoch 10.37: Loss = 0.597626
Epoch 10.38: Loss = 0.564194
Epoch 10.39: Loss = 0.588226
Epoch 10.40: Loss = 0.476196
Epoch 10.41: Loss = 0.69751
Epoch 10.42: Loss = 0.472656
Epoch 10.43: Loss = 0.4935
Epoch 10.44: Loss = 0.539063
Epoch 10.45: Loss = 0.495346
Epoch 10.46: Loss = 0.525925
Epoch 10.47: Loss = 0.620819
Epoch 10.48: Loss = 0.541412
Epoch 10.49: Loss = 0.577164
Epoch 10.50: Loss = 0.413177
Epoch 10.51: Loss = 0.623032
Epoch 10.52: Loss = 0.442032
Epoch 10.53: Loss = 0.451904
Epoch 10.54: Loss = 0.782242
Epoch 10.55: Loss = 0.466476
Epoch 10.56: Loss = 0.562195
Epoch 10.57: Loss = 0.570343
Epoch 10.58: Loss = 0.560791
Epoch 10.59: Loss = 0.518707
Epoch 10.60: Loss = 0.513
Epoch 10.61: Loss = 0.522598
Epoch 10.62: Loss = 0.624954
Epoch 10.63: Loss = 0.58963
Epoch 10.64: Loss = 0.556
Epoch 10.65: Loss = 0.36676
Epoch 10.66: Loss = 0.622208
Epoch 10.67: Loss = 0.474564
Epoch 10.68: Loss = 0.532043
Epoch 10.69: Loss = 0.547012
Epoch 10.70: Loss = 0.619232
Epoch 10.71: Loss = 0.546371
Epoch 10.72: Loss = 0.722366
Epoch 10.73: Loss = 0.585693
Epoch 10.74: Loss = 0.517639
Epoch 10.75: Loss = 0.627701
Epoch 10.76: Loss = 0.481827
Epoch 10.77: Loss = 0.452789
Epoch 10.78: Loss = 0.523895
Epoch 10.79: Loss = 0.846252
Epoch 10.80: Loss = 0.692047
Epoch 10.81: Loss = 0.660049
Epoch 10.82: Loss = 0.548904
Epoch 10.83: Loss = 0.550613
Epoch 10.84: Loss = 0.63089
Epoch 10.85: Loss = 0.664246
Epoch 10.86: Loss = 0.603836
Epoch 10.87: Loss = 0.522583
Epoch 10.88: Loss = 0.528595
Epoch 10.89: Loss = 0.64946
Epoch 10.90: Loss = 0.612106
Epoch 10.91: Loss = 0.53392
Epoch 10.92: Loss = 0.545441
Epoch 10.93: Loss = 0.597473
Epoch 10.94: Loss = 0.644272
Epoch 10.95: Loss = 0.483154
Epoch 10.96: Loss = 0.470825
Epoch 10.97: Loss = 0.620468
Epoch 10.98: Loss = 0.470627
Epoch 10.99: Loss = 0.560135
Epoch 10.100: Loss = 0.535843
Epoch 10.101: Loss = 0.741028
Epoch 10.102: Loss = 0.570892
Epoch 10.103: Loss = 0.552841
Epoch 10.104: Loss = 0.604568
Epoch 10.105: Loss = 0.526123
Epoch 10.106: Loss = 0.604263
Epoch 10.107: Loss = 0.553207
Epoch 10.108: Loss = 0.497147
Epoch 10.109: Loss = 0.442581
Epoch 10.110: Loss = 0.457565
Epoch 10.111: Loss = 0.581238
Epoch 10.112: Loss = 0.612045
Epoch 10.113: Loss = 0.470856
Epoch 10.114: Loss = 0.555328
Epoch 10.115: Loss = 0.553818
Epoch 10.116: Loss = 0.521881
Epoch 10.117: Loss = 0.600616
Epoch 10.118: Loss = 0.421997
Epoch 10.119: Loss = 0.716705
Epoch 10.120: Loss = 0.562271
TRAIN LOSS = 0.555267
TRAIN ACC = 86.1771 % (51709/60000)
Loss = 0.542618
Loss = 0.655853
Loss = 0.717972
Loss = 0.759445
Loss = 0.759369
Loss = 0.509109
Loss = 0.534897
Loss = 0.913574
Loss = 0.69812
Loss = 0.71991
Loss = 0.225464
Loss = 0.495438
Loss = 0.341553
Loss = 0.568024
Loss = 0.259064
Loss = 0.360138
Loss = 0.324921
Loss = 0.062851
Loss = 0.327087
Loss = 0.778595
TEST LOSS = 0.5277
TEST ACC = 517.09 % (8774/10000)

Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.37279
Epoch 1.2: Loss = 2.32623
Epoch 1.3: Loss = 2.30165
Epoch 1.4: Loss = 2.28477
Epoch 1.5: Loss = 2.2402
Epoch 1.6: Loss = 2.20691
Epoch 1.7: Loss = 2.19995
Epoch 1.8: Loss = 2.17836
Epoch 1.9: Loss = 2.18298
Epoch 1.10: Loss = 2.12622
Epoch 1.11: Loss = 2.10536
Epoch 1.12: Loss = 2.05345
Epoch 1.13: Loss = 2.03813
Epoch 1.14: Loss = 2.04108
Epoch 1.15: Loss = 1.97266
Epoch 1.16: Loss = 1.94635
Epoch 1.17: Loss = 1.91862
Epoch 1.18: Loss = 1.89816
Epoch 1.19: Loss = 1.87985
Epoch 1.20: Loss = 1.84688
Epoch 1.21: Loss = 1.79131
Epoch 1.22: Loss = 1.7966
Epoch 1.23: Loss = 1.75313
Epoch 1.24: Loss = 1.7518
Epoch 1.25: Loss = 1.69083
Epoch 1.26: Loss = 1.631
Epoch 1.27: Loss = 1.61617
Epoch 1.28: Loss = 1.60788
Epoch 1.29: Loss = 1.60077
Epoch 1.30: Loss = 1.53922
Epoch 1.31: Loss = 1.53114
Epoch 1.32: Loss = 1.47992
Epoch 1.33: Loss = 1.47556
Epoch 1.34: Loss = 1.4774
Epoch 1.35: Loss = 1.42471
Epoch 1.36: Loss = 1.38068
Epoch 1.37: Loss = 1.36681
Epoch 1.38: Loss = 1.32529
Epoch 1.39: Loss = 1.34827
Epoch 1.40: Loss = 1.31259
Epoch 1.41: Loss = 1.26645
Epoch 1.42: Loss = 1.22226
Epoch 1.43: Loss = 1.33211
Epoch 1.44: Loss = 1.21323
Epoch 1.45: Loss = 1.18225
Epoch 1.46: Loss = 1.16293
Epoch 1.47: Loss = 1.16544
Epoch 1.48: Loss = 1.08604
Epoch 1.49: Loss = 1.11967
Epoch 1.50: Loss = 1.08499
Epoch 1.51: Loss = 1.04971
Epoch 1.52: Loss = 1.05898
Epoch 1.53: Loss = 1.00801
Epoch 1.54: Loss = 1.03976
Epoch 1.55: Loss = 0.983887
Epoch 1.56: Loss = 1.01552
Epoch 1.57: Loss = 1.01222
Epoch 1.58: Loss = 1.01485
Epoch 1.59: Loss = 0.965454
Epoch 1.60: Loss = 0.951233
Epoch 1.61: Loss = 0.979187
Epoch 1.62: Loss = 0.921326
Epoch 1.63: Loss = 0.895798
Epoch 1.64: Loss = 0.876099
Epoch 1.65: Loss = 0.928802
Epoch 1.66: Loss = 0.927124
Epoch 1.67: Loss = 0.87175
Epoch 1.68: Loss = 0.85524
Epoch 1.69: Loss = 0.855957
Epoch 1.70: Loss = 0.839233
Epoch 1.71: Loss = 0.850403
Epoch 1.72: Loss = 0.809448
Epoch 1.73: Loss = 0.849594
Epoch 1.74: Loss = 0.860825
Epoch 1.75: Loss = 0.78743
Epoch 1.76: Loss = 0.856705
Epoch 1.77: Loss = 0.741211
Epoch 1.78: Loss = 0.807724
Epoch 1.79: Loss = 0.776382
Epoch 1.80: Loss = 0.749344
Epoch 1.81: Loss = 0.714264
Epoch 1.82: Loss = 0.776779
Epoch 1.83: Loss = 0.801834
Epoch 1.84: Loss = 0.687943
Epoch 1.85: Loss = 0.751678
Epoch 1.86: Loss = 0.818253
Epoch 1.87: Loss = 0.782928
Epoch 1.88: Loss = 0.707916
Epoch 1.89: Loss = 0.751846
Epoch 1.90: Loss = 0.735809
Epoch 1.91: Loss = 0.765579
Epoch 1.92: Loss = 0.753326
Epoch 1.93: Loss = 0.612839
Epoch 1.94: Loss = 0.62944
Epoch 1.95: Loss = 0.641785
Epoch 1.96: Loss = 0.666397
Epoch 1.97: Loss = 0.704895
Epoch 1.98: Loss = 0.638092
Epoch 1.99: Loss = 0.687714
Epoch 1.100: Loss = 0.61293
Epoch 1.101: Loss = 0.647202
Epoch 1.102: Loss = 0.702835
Epoch 1.103: Loss = 0.720535
Epoch 1.104: Loss = 0.612076
Epoch 1.105: Loss = 0.611481
Epoch 1.106: Loss = 0.636734
Epoch 1.107: Loss = 0.672531
Epoch 1.108: Loss = 0.665405
Epoch 1.109: Loss = 0.641846
Epoch 1.110: Loss = 0.641678
Epoch 1.111: Loss = 0.573334
Epoch 1.112: Loss = 0.635605
Epoch 1.113: Loss = 0.645676
Epoch 1.114: Loss = 0.639236
Epoch 1.115: Loss = 0.582291
Epoch 1.116: Loss = 0.646683
Epoch 1.117: Loss = 0.544373
Epoch 1.118: Loss = 0.533112
Epoch 1.119: Loss = 0.70369
Epoch 1.120: Loss = 0.582352
TRAIN LOSS = 1.15733
TRAIN ACC = 66.6489 % (39991/60000)
Loss = 0.661072
Loss = 0.653198
Loss = 0.780258
Loss = 0.752411
Loss = 0.776321
Loss = 0.685837
Loss = 0.632828
Loss = 0.806076
Loss = 0.773056
Loss = 0.708771
Loss = 0.399933
Loss = 0.541061
Loss = 0.36055
Loss = 0.622406
Loss = 0.446716
Loss = 0.461365
Loss = 0.425461
Loss = 0.261185
Loss = 0.447281
Loss = 0.740585
TEST LOSS = 0.596818
TEST ACC = 399.91 % (8050/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.618774
Epoch 2.2: Loss = 0.636795
Epoch 2.3: Loss = 0.598907
Epoch 2.4: Loss = 0.567383
Epoch 2.5: Loss = 0.681839
Epoch 2.6: Loss = 0.682053
Epoch 2.7: Loss = 0.540924
Epoch 2.8: Loss = 0.605225
Epoch 2.9: Loss = 0.591568
Epoch 2.10: Loss = 0.592224
Epoch 2.11: Loss = 0.634949
Epoch 2.12: Loss = 0.63356
Epoch 2.13: Loss = 0.618866
Epoch 2.14: Loss = 0.511719
Epoch 2.15: Loss = 0.562973
Epoch 2.16: Loss = 0.606384
Epoch 2.17: Loss = 0.561081
Epoch 2.18: Loss = 0.585831
Epoch 2.19: Loss = 0.57872
Epoch 2.20: Loss = 0.607986
Epoch 2.21: Loss = 0.621857
Epoch 2.22: Loss = 0.586166
Epoch 2.23: Loss = 0.584503
Epoch 2.24: Loss = 0.561005
Epoch 2.25: Loss = 0.515137
Epoch 2.26: Loss = 0.508011
Epoch 2.27: Loss = 0.591339
Epoch 2.28: Loss = 0.571701
Epoch 2.29: Loss = 0.585526
Epoch 2.30: Loss = 0.529968
Epoch 2.31: Loss = 0.521942
Epoch 2.32: Loss = 0.557098
Epoch 2.33: Loss = 0.47226
Epoch 2.34: Loss = 0.577866
Epoch 2.35: Loss = 0.626907
Epoch 2.36: Loss = 0.496292
Epoch 2.37: Loss = 0.583588
Epoch 2.38: Loss = 0.498108
Epoch 2.39: Loss = 0.594482
Epoch 2.40: Loss = 0.619492
Epoch 2.41: Loss = 0.512451
Epoch 2.42: Loss = 0.560776
Epoch 2.43: Loss = 0.510086
Epoch 2.44: Loss = 0.446594
Epoch 2.45: Loss = 0.573196
Epoch 2.46: Loss = 0.583771
Epoch 2.47: Loss = 0.517532
Epoch 2.48: Loss = 0.572784
Epoch 2.49: Loss = 0.515762
Epoch 2.50: Loss = 0.598328
Epoch 2.51: Loss = 0.531616
Epoch 2.52: Loss = 0.540604
Epoch 2.53: Loss = 0.559006
Epoch 2.54: Loss = 0.470139
Epoch 2.55: Loss = 0.605942
Epoch 2.56: Loss = 0.491196
Epoch 2.57: Loss = 0.558533
Epoch 2.58: Loss = 0.544861
Epoch 2.59: Loss = 0.458755
Epoch 2.60: Loss = 0.514648
Epoch 2.61: Loss = 0.539581
Epoch 2.62: Loss = 0.594116
Epoch 2.63: Loss = 0.466629
Epoch 2.64: Loss = 0.54393
Epoch 2.65: Loss = 0.529266
Epoch 2.66: Loss = 0.493851
Epoch 2.67: Loss = 0.517578
Epoch 2.68: Loss = 0.518494
Epoch 2.69: Loss = 0.556213
Epoch 2.70: Loss = 0.537857
Epoch 2.71: Loss = 0.488922
Epoch 2.72: Loss = 0.479919
Epoch 2.73: Loss = 0.569138
Epoch 2.74: Loss = 0.532837
Epoch 2.75: Loss = 0.555435
Epoch 2.76: Loss = 0.553223
Epoch 2.77: Loss = 0.506454
Epoch 2.78: Loss = 0.610107
Epoch 2.79: Loss = 0.504974
Epoch 2.80: Loss = 0.556915
Epoch 2.81: Loss = 0.536133
Epoch 2.82: Loss = 0.475952
Epoch 2.83: Loss = 0.50238
Epoch 2.84: Loss = 0.449173
Epoch 2.85: Loss = 0.63121
Epoch 2.86: Loss = 0.504288
Epoch 2.87: Loss = 0.427002
Epoch 2.88: Loss = 0.523911
Epoch 2.89: Loss = 0.520096
Epoch 2.90: Loss = 0.514114
Epoch 2.91: Loss = 0.541107
Epoch 2.92: Loss = 0.528824
Epoch 2.93: Loss = 0.534988
Epoch 2.94: Loss = 0.454758
Epoch 2.95: Loss = 0.522934
Epoch 2.96: Loss = 0.469955
Epoch 2.97: Loss = 0.470978
Epoch 2.98: Loss = 0.436188
Epoch 2.99: Loss = 0.484497
Epoch 2.100: Loss = 0.484177
Epoch 2.101: Loss = 0.438858
Epoch 2.102: Loss = 0.489487
Epoch 2.103: Loss = 0.451965
Epoch 2.104: Loss = 0.577667
Epoch 2.105: Loss = 0.472473
Epoch 2.106: Loss = 0.477478
Epoch 2.107: Loss = 0.503693
Epoch 2.108: Loss = 0.481705
Epoch 2.109: Loss = 0.461075
Epoch 2.110: Loss = 0.482422
Epoch 2.111: Loss = 0.446671
Epoch 2.112: Loss = 0.526901
Epoch 2.113: Loss = 0.492004
Epoch 2.114: Loss = 0.482666
Epoch 2.115: Loss = 0.498489
Epoch 2.116: Loss = 0.559143
Epoch 2.117: Loss = 0.492981
Epoch 2.118: Loss = 0.466644
Epoch 2.119: Loss = 0.537704
Epoch 2.120: Loss = 0.562088
TRAIN LOSS = 0.53717
TRAIN ACC = 83.2123 % (49929/60000)
Loss = 0.510132
Loss = 0.562668
Loss = 0.653427
Loss = 0.695236
Loss = 0.701508
Loss = 0.503326
Loss = 0.51297
Loss = 0.725983
Loss = 0.671463
Loss = 0.613907
Loss = 0.262726
Loss = 0.425156
Loss = 0.289093
Loss = 0.51329
Loss = 0.286423
Loss = 0.333786
Loss = 0.315567
Loss = 0.129333
Loss = 0.296371
Loss = 0.604599
TEST LOSS = 0.480348
TEST ACC = 499.289 % (8510/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.402435
Epoch 3.2: Loss = 0.527634
Epoch 3.3: Loss = 0.472504
Epoch 3.4: Loss = 0.496796
Epoch 3.5: Loss = 0.506516
Epoch 3.6: Loss = 0.458511
Epoch 3.7: Loss = 0.5298
Epoch 3.8: Loss = 0.516754
Epoch 3.9: Loss = 0.606125
Epoch 3.10: Loss = 0.481628
Epoch 3.11: Loss = 0.43248
Epoch 3.12: Loss = 0.536728
Epoch 3.13: Loss = 0.52388
Epoch 3.14: Loss = 0.410751
Epoch 3.15: Loss = 0.437134
Epoch 3.16: Loss = 0.48822
Epoch 3.17: Loss = 0.451675
Epoch 3.18: Loss = 0.48407
Epoch 3.19: Loss = 0.560867
Epoch 3.20: Loss = 0.444275
Epoch 3.21: Loss = 0.454926
Epoch 3.22: Loss = 0.420319
Epoch 3.23: Loss = 0.603683
Epoch 3.24: Loss = 0.583649
Epoch 3.25: Loss = 0.523483
Epoch 3.26: Loss = 0.450211
Epoch 3.27: Loss = 0.460281
Epoch 3.28: Loss = 0.466705
Epoch 3.29: Loss = 0.51561
Epoch 3.30: Loss = 0.513107
Epoch 3.31: Loss = 0.487991
Epoch 3.32: Loss = 0.453781
Epoch 3.33: Loss = 0.549454
Epoch 3.34: Loss = 0.457275
Epoch 3.35: Loss = 0.467041
Epoch 3.36: Loss = 0.494324
Epoch 3.37: Loss = 0.488617
Epoch 3.38: Loss = 0.623749
Epoch 3.39: Loss = 0.43129
Epoch 3.40: Loss = 0.479813
Epoch 3.41: Loss = 0.44838
Epoch 3.42: Loss = 0.483643
Epoch 3.43: Loss = 0.50061
Epoch 3.44: Loss = 0.469452
Epoch 3.45: Loss = 0.362411
Epoch 3.46: Loss = 0.468353
Epoch 3.47: Loss = 0.494858
Epoch 3.48: Loss = 0.465302
Epoch 3.49: Loss = 0.47142
Epoch 3.50: Loss = 0.479874
Epoch 3.51: Loss = 0.557938
Epoch 3.52: Loss = 0.44577
Epoch 3.53: Loss = 0.472183
Epoch 3.54: Loss = 0.569382
Epoch 3.55: Loss = 0.371735
Epoch 3.56: Loss = 0.426346
Epoch 3.57: Loss = 0.435715
Epoch 3.58: Loss = 0.46521
Epoch 3.59: Loss = 0.44252
Epoch 3.60: Loss = 0.591095
Epoch 3.61: Loss = 0.486099
Epoch 3.62: Loss = 0.505905
Epoch 3.63: Loss = 0.503784
Epoch 3.64: Loss = 0.507553
Epoch 3.65: Loss = 0.438004
Epoch 3.66: Loss = 0.430069
Epoch 3.67: Loss = 0.511139
Epoch 3.68: Loss = 0.512375
Epoch 3.69: Loss = 0.554626
Epoch 3.70: Loss = 0.465622
Epoch 3.71: Loss = 0.445923
Epoch 3.72: Loss = 0.578201
Epoch 3.73: Loss = 0.527924
Epoch 3.74: Loss = 0.461456
Epoch 3.75: Loss = 0.614365
Epoch 3.76: Loss = 0.408478
Epoch 3.77: Loss = 0.503204
Epoch 3.78: Loss = 0.515182
Epoch 3.79: Loss = 0.50441
Epoch 3.80: Loss = 0.496521
Epoch 3.81: Loss = 0.508087
Epoch 3.82: Loss = 0.499695
Epoch 3.83: Loss = 0.416565
Epoch 3.84: Loss = 0.517731
Epoch 3.85: Loss = 0.644821
Epoch 3.86: Loss = 0.506638
Epoch 3.87: Loss = 0.457001
Epoch 3.88: Loss = 0.503998
Epoch 3.89: Loss = 0.411804
Epoch 3.90: Loss = 0.420868
Epoch 3.91: Loss = 0.358353
Epoch 3.92: Loss = 0.523865
Epoch 3.93: Loss = 0.542114
Epoch 3.94: Loss = 0.387558
Epoch 3.95: Loss = 0.469803
Epoch 3.96: Loss = 0.504715
Epoch 3.97: Loss = 0.469879
Epoch 3.98: Loss = 0.515274
Epoch 3.99: Loss = 0.502411
Epoch 3.100: Loss = 0.635162
Epoch 3.101: Loss = 0.417221
Epoch 3.102: Loss = 0.410553
Epoch 3.103: Loss = 0.512131
Epoch 3.104: Loss = 0.473724
Epoch 3.105: Loss = 0.489899
Epoch 3.106: Loss = 0.511566
Epoch 3.107: Loss = 0.471451
Epoch 3.108: Loss = 0.531433
Epoch 3.109: Loss = 0.416138
Epoch 3.110: Loss = 0.456635
Epoch 3.111: Loss = 0.536896
Epoch 3.112: Loss = 0.482162
Epoch 3.113: Loss = 0.488022
Epoch 3.114: Loss = 0.628418
Epoch 3.115: Loss = 0.56633
Epoch 3.116: Loss = 0.587097
Epoch 3.117: Loss = 0.503448
Epoch 3.118: Loss = 0.526596
Epoch 3.119: Loss = 0.471954
Epoch 3.120: Loss = 0.454483
TRAIN LOSS = 0.490173
TRAIN ACC = 85.5011 % (51303/60000)
Loss = 0.477371
Loss = 0.566132
Loss = 0.588593
Loss = 0.672684
Loss = 0.625
Loss = 0.460098
Loss = 0.467026
Loss = 0.685501
Loss = 0.627228
Loss = 0.563293
Loss = 0.247375
Loss = 0.379623
Loss = 0.283249
Loss = 0.46933
Loss = 0.26207
Loss = 0.340591
Loss = 0.295502
Loss = 0.0939484
Loss = 0.268341
Loss = 0.607681
TEST LOSS = 0.449032
TEST ACC = 513.029 % (8692/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.466904
Epoch 4.2: Loss = 0.416306
Epoch 4.3: Loss = 0.508774
Epoch 4.4: Loss = 0.503922
Epoch 4.5: Loss = 0.439728
Epoch 4.6: Loss = 0.414642
Epoch 4.7: Loss = 0.507858
Epoch 4.8: Loss = 0.528214
Epoch 4.9: Loss = 0.519073
Epoch 4.10: Loss = 0.481995
Epoch 4.11: Loss = 0.529373
Epoch 4.12: Loss = 0.529236
Epoch 4.13: Loss = 0.473343
Epoch 4.14: Loss = 0.520096
Epoch 4.15: Loss = 0.487259
Epoch 4.16: Loss = 0.438324
Epoch 4.17: Loss = 0.509705
Epoch 4.18: Loss = 0.44635
Epoch 4.19: Loss = 0.540222
Epoch 4.20: Loss = 0.480072
Epoch 4.21: Loss = 0.510269
Epoch 4.22: Loss = 0.483582
Epoch 4.23: Loss = 0.531067
Epoch 4.24: Loss = 0.567169
Epoch 4.25: Loss = 0.41716
Epoch 4.26: Loss = 0.579971
Epoch 4.27: Loss = 0.40271
Epoch 4.28: Loss = 0.528946
Epoch 4.29: Loss = 0.490784
Epoch 4.30: Loss = 0.470642
Epoch 4.31: Loss = 0.411972
Epoch 4.32: Loss = 0.53862
Epoch 4.33: Loss = 0.415131
Epoch 4.34: Loss = 0.581375
Epoch 4.35: Loss = 0.524063
Epoch 4.36: Loss = 0.522781
Epoch 4.37: Loss = 0.408676
Epoch 4.38: Loss = 0.42981
Epoch 4.39: Loss = 0.531967
Epoch 4.40: Loss = 0.50589
Epoch 4.41: Loss = 0.576645
Epoch 4.42: Loss = 0.471634
Epoch 4.43: Loss = 0.468689
Epoch 4.44: Loss = 0.527039
Epoch 4.45: Loss = 0.445007
Epoch 4.46: Loss = 0.464218
Epoch 4.47: Loss = 0.529373
Epoch 4.48: Loss = 0.416733
Epoch 4.49: Loss = 0.516449
Epoch 4.50: Loss = 0.608292
Epoch 4.51: Loss = 0.487946
Epoch 4.52: Loss = 0.387146
Epoch 4.53: Loss = 0.590744
Epoch 4.54: Loss = 0.55661
Epoch 4.55: Loss = 0.509689
Epoch 4.56: Loss = 0.556656
Epoch 4.57: Loss = 0.481506
Epoch 4.58: Loss = 0.513901
Epoch 4.59: Loss = 0.583832
Epoch 4.60: Loss = 0.546829
Epoch 4.61: Loss = 0.525421
Epoch 4.62: Loss = 0.545212
Epoch 4.63: Loss = 0.511292
Epoch 4.64: Loss = 0.566849
Epoch 4.65: Loss = 0.492569
Epoch 4.66: Loss = 0.422409
Epoch 4.67: Loss = 0.489929
Epoch 4.68: Loss = 0.554749
Epoch 4.69: Loss = 0.411682
Epoch 4.70: Loss = 0.374588
Epoch 4.71: Loss = 0.483215
Epoch 4.72: Loss = 0.585861
Epoch 4.73: Loss = 0.496857
Epoch 4.74: Loss = 0.480545
Epoch 4.75: Loss = 0.378006
Epoch 4.76: Loss = 0.52829
Epoch 4.77: Loss = 0.541367
Epoch 4.78: Loss = 0.506897
Epoch 4.79: Loss = 0.485718
Epoch 4.80: Loss = 0.496674
Epoch 4.81: Loss = 0.480988
Epoch 4.82: Loss = 0.513016
Epoch 4.83: Loss = 0.421463
Epoch 4.84: Loss = 0.477814
Epoch 4.85: Loss = 0.57457
Epoch 4.86: Loss = 0.385178
Epoch 4.87: Loss = 0.623535
Epoch 4.88: Loss = 0.442841
Epoch 4.89: Loss = 0.369858
Epoch 4.90: Loss = 0.588577
Epoch 4.91: Loss = 0.50058
Epoch 4.92: Loss = 0.514145
Epoch 4.93: Loss = 0.463318
Epoch 4.94: Loss = 0.494537
Epoch 4.95: Loss = 0.505783
Epoch 4.96: Loss = 0.503479
Epoch 4.97: Loss = 0.512665
Epoch 4.98: Loss = 0.478516
Epoch 4.99: Loss = 0.404114
Epoch 4.100: Loss = 0.492157
Epoch 4.101: Loss = 0.513199
Epoch 4.102: Loss = 0.408722
Epoch 4.103: Loss = 0.574112
Epoch 4.104: Loss = 0.471161
Epoch 4.105: Loss = 0.496979
Epoch 4.106: Loss = 0.520721
Epoch 4.107: Loss = 0.439606
Epoch 4.108: Loss = 0.493088
Epoch 4.109: Loss = 0.480576
Epoch 4.110: Loss = 0.501755
Epoch 4.111: Loss = 0.549454
Epoch 4.112: Loss = 0.301727
Epoch 4.113: Loss = 0.420013
Epoch 4.114: Loss = 0.489517
Epoch 4.115: Loss = 0.512009
Epoch 4.116: Loss = 0.53949
Epoch 4.117: Loss = 0.570526
Epoch 4.118: Loss = 0.594833
Epoch 4.119: Loss = 0.552429
Epoch 4.120: Loss = 0.532516
TRAIN LOSS = 0.495178
TRAIN ACC = 85.7819 % (51471/60000)
Loss = 0.530609
Loss = 0.5867
Loss = 0.595276
Loss = 0.6931
Loss = 0.599136
Loss = 0.505722
Loss = 0.494431
Loss = 0.714142
Loss = 0.600235
Loss = 0.575211
Loss = 0.275269
Loss = 0.381317
Loss = 0.318695
Loss = 0.509354
Loss = 0.250275
Loss = 0.378967
Loss = 0.264786
Loss = 0.0983429
Loss = 0.289963
Loss = 0.630142
TEST LOSS = 0.464583
TEST ACC = 514.709 % (8685/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.473389
Epoch 5.2: Loss = 0.432129
Epoch 5.3: Loss = 0.546005
Epoch 5.4: Loss = 0.469711
Epoch 5.5: Loss = 0.451813
Epoch 5.6: Loss = 0.532883
Epoch 5.7: Loss = 0.556458
Epoch 5.8: Loss = 0.63562
Epoch 5.9: Loss = 0.478134
Epoch 5.10: Loss = 0.441376
Epoch 5.11: Loss = 0.472641
Epoch 5.12: Loss = 0.542206
Epoch 5.13: Loss = 0.452042
Epoch 5.14: Loss = 0.53801
Epoch 5.15: Loss = 0.45816
Epoch 5.16: Loss = 0.610855
Epoch 5.17: Loss = 0.520996
Epoch 5.18: Loss = 0.452545
Epoch 5.19: Loss = 0.439011
Epoch 5.20: Loss = 0.422791
Epoch 5.21: Loss = 0.358597
Epoch 5.22: Loss = 0.568192
Epoch 5.23: Loss = 0.448837
Epoch 5.24: Loss = 0.378082
Epoch 5.25: Loss = 0.506607
Epoch 5.26: Loss = 0.466064
Epoch 5.27: Loss = 0.496567
Epoch 5.28: Loss = 0.556503
Epoch 5.29: Loss = 0.582047
Epoch 5.30: Loss = 0.533035
Epoch 5.31: Loss = 0.475784
Epoch 5.32: Loss = 0.542557
Epoch 5.33: Loss = 0.569366
Epoch 5.34: Loss = 0.448196
Epoch 5.35: Loss = 0.441116
Epoch 5.36: Loss = 0.452347
Epoch 5.37: Loss = 0.585098
Epoch 5.38: Loss = 0.452179
Epoch 5.39: Loss = 0.489334
Epoch 5.40: Loss = 0.580902
Epoch 5.41: Loss = 0.524445
Epoch 5.42: Loss = 0.519684
Epoch 5.43: Loss = 0.441772
Epoch 5.44: Loss = 0.561844
Epoch 5.45: Loss = 0.521683
Epoch 5.46: Loss = 0.405899
Epoch 5.47: Loss = 0.427765
Epoch 5.48: Loss = 0.542221
Epoch 5.49: Loss = 0.50827
Epoch 5.50: Loss = 0.511566
Epoch 5.51: Loss = 0.377335
Epoch 5.52: Loss = 0.474533
Epoch 5.53: Loss = 0.532974
Epoch 5.54: Loss = 0.464661
Epoch 5.55: Loss = 0.556198
Epoch 5.56: Loss = 0.444138
Epoch 5.57: Loss = 0.547577
Epoch 5.58: Loss = 0.488663
Epoch 5.59: Loss = 0.425507
Epoch 5.60: Loss = 0.561584
Epoch 5.61: Loss = 0.623962
Epoch 5.62: Loss = 0.503235
Epoch 5.63: Loss = 0.383591
Epoch 5.64: Loss = 0.515823
Epoch 5.65: Loss = 0.507675
Epoch 5.66: Loss = 0.441284
Epoch 5.67: Loss = 0.484116
Epoch 5.68: Loss = 0.564819
Epoch 5.69: Loss = 0.52034
Epoch 5.70: Loss = 0.49678
Epoch 5.71: Loss = 0.449509
Epoch 5.72: Loss = 0.457077
Epoch 5.73: Loss = 0.546021
Epoch 5.74: Loss = 0.513199
Epoch 5.75: Loss = 0.421921
Epoch 5.76: Loss = 0.472336
Epoch 5.77: Loss = 0.660843
Epoch 5.78: Loss = 0.447861
Epoch 5.79: Loss = 0.40741
Epoch 5.80: Loss = 0.571838
Epoch 5.81: Loss = 0.558212
Epoch 5.82: Loss = 0.504364
Epoch 5.83: Loss = 0.602325
Epoch 5.84: Loss = 0.491394
Epoch 5.85: Loss = 0.516632
Epoch 5.86: Loss = 0.481827
Epoch 5.87: Loss = 0.467453
Epoch 5.88: Loss = 0.392609
Epoch 5.89: Loss = 0.450775
Epoch 5.90: Loss = 0.533218
Epoch 5.91: Loss = 0.526291
Epoch 5.92: Loss = 0.53894
Epoch 5.93: Loss = 0.589874
Epoch 5.94: Loss = 0.56485
Epoch 5.95: Loss = 0.503342
Epoch 5.96: Loss = 0.46199
Epoch 5.97: Loss = 0.493393
Epoch 5.98: Loss = 0.549362
Epoch 5.99: Loss = 0.501892
Epoch 5.100: Loss = 0.536621
Epoch 5.101: Loss = 0.537094
Epoch 5.102: Loss = 0.46904
Epoch 5.103: Loss = 0.447525
Epoch 5.104: Loss = 0.418472
Epoch 5.105: Loss = 0.637146
Epoch 5.106: Loss = 0.542389
Epoch 5.107: Loss = 0.453873
Epoch 5.108: Loss = 0.496048
Epoch 5.109: Loss = 0.565292
Epoch 5.110: Loss = 0.520676
Epoch 5.111: Loss = 0.492172
Epoch 5.112: Loss = 0.472687
Epoch 5.113: Loss = 0.500702
Epoch 5.114: Loss = 0.431091
Epoch 5.115: Loss = 0.493851
Epoch 5.116: Loss = 0.553192
Epoch 5.117: Loss = 0.420029
Epoch 5.118: Loss = 0.549728
Epoch 5.119: Loss = 0.418594
Epoch 5.120: Loss = 0.524124
TRAIN LOSS = 0.49913
TRAIN ACC = 86.4548 % (51875/60000)
Loss = 0.554214
Loss = 0.538712
Loss = 0.595978
Loss = 0.652512
Loss = 0.652939
Loss = 0.562775
Loss = 0.449402
Loss = 0.706848
Loss = 0.644516
Loss = 0.588379
Loss = 0.320862
Loss = 0.372467
Loss = 0.321899
Loss = 0.557404
Loss = 0.236725
Loss = 0.483307
Loss = 0.263123
Loss = 0.083252
Loss = 0.306427
Loss = 0.686279
TEST LOSS = 0.478901
TEST ACC = 518.748 % (8710/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.382111
Epoch 6.2: Loss = 0.432205
Epoch 6.3: Loss = 0.588211
Epoch 6.4: Loss = 0.432007
Epoch 6.5: Loss = 0.465302
Epoch 6.6: Loss = 0.471954
Epoch 6.7: Loss = 0.439362
Epoch 6.8: Loss = 0.582047
Epoch 6.9: Loss = 0.644272
Epoch 6.10: Loss = 0.512573
Epoch 6.11: Loss = 0.530106
Epoch 6.12: Loss = 0.522476
Epoch 6.13: Loss = 0.611923
Epoch 6.14: Loss = 0.483475
Epoch 6.15: Loss = 0.639648
Epoch 6.16: Loss = 0.602936
Epoch 6.17: Loss = 0.508987
Epoch 6.18: Loss = 0.612656
Epoch 6.19: Loss = 0.565796
Epoch 6.20: Loss = 0.502426
Epoch 6.21: Loss = 0.552338
Epoch 6.22: Loss = 0.722305
Epoch 6.23: Loss = 0.62381
Epoch 6.24: Loss = 0.403839
Epoch 6.25: Loss = 0.49646
Epoch 6.26: Loss = 0.511536
Epoch 6.27: Loss = 0.568146
Epoch 6.28: Loss = 0.398788
Epoch 6.29: Loss = 0.465378
Epoch 6.30: Loss = 0.472153
Epoch 6.31: Loss = 0.649887
Epoch 6.32: Loss = 0.47139
Epoch 6.33: Loss = 0.479691
Epoch 6.34: Loss = 0.471634
Epoch 6.35: Loss = 0.515732
Epoch 6.36: Loss = 0.482239
Epoch 6.37: Loss = 0.452484
Epoch 6.38: Loss = 0.510452
Epoch 6.39: Loss = 0.597977
Epoch 6.40: Loss = 0.53096
Epoch 6.41: Loss = 0.500092
Epoch 6.42: Loss = 0.500839
Epoch 6.43: Loss = 0.465836
Epoch 6.44: Loss = 0.615692
Epoch 6.45: Loss = 0.633163
Epoch 6.46: Loss = 0.389603
Epoch 6.47: Loss = 0.40509
Epoch 6.48: Loss = 0.425766
Epoch 6.49: Loss = 0.514938
Epoch 6.50: Loss = 0.488953
Epoch 6.51: Loss = 0.536514
Epoch 6.52: Loss = 0.541992
Epoch 6.53: Loss = 0.625061
Epoch 6.54: Loss = 0.506607
Epoch 6.55: Loss = 0.64357
Epoch 6.56: Loss = 0.542297
Epoch 6.57: Loss = 0.509521
Epoch 6.58: Loss = 0.488007
Epoch 6.59: Loss = 0.501129
Epoch 6.60: Loss = 0.580307
Epoch 6.61: Loss = 0.412628
Epoch 6.62: Loss = 0.457001
Epoch 6.63: Loss = 0.447906
Epoch 6.64: Loss = 0.534637
Epoch 6.65: Loss = 0.541656
Epoch 6.66: Loss = 0.522644
Epoch 6.67: Loss = 0.468979
Epoch 6.68: Loss = 0.716553
Epoch 6.69: Loss = 0.430664
Epoch 6.70: Loss = 0.500977
Epoch 6.71: Loss = 0.424225
Epoch 6.72: Loss = 0.485001
Epoch 6.73: Loss = 0.446579
Epoch 6.74: Loss = 0.420959
Epoch 6.75: Loss = 0.502762
Epoch 6.76: Loss = 0.536118
Epoch 6.77: Loss = 0.529022
Epoch 6.78: Loss = 0.478867
Epoch 6.79: Loss = 0.485001
Epoch 6.80: Loss = 0.314362
Epoch 6.81: Loss = 0.424423
Epoch 6.82: Loss = 0.485519
Epoch 6.83: Loss = 0.435623
Epoch 6.84: Loss = 0.565277
Epoch 6.85: Loss = 0.419556
Epoch 6.86: Loss = 0.476868
Epoch 6.87: Loss = 0.606506
Epoch 6.88: Loss = 0.467712
Epoch 6.89: Loss = 0.595459
Epoch 6.90: Loss = 0.517136
Epoch 6.91: Loss = 0.485016
Epoch 6.92: Loss = 0.577759
Epoch 6.93: Loss = 0.491959
Epoch 6.94: Loss = 0.438446
Epoch 6.95: Loss = 0.488403
Epoch 6.96: Loss = 0.537155
Epoch 6.97: Loss = 0.606125
Epoch 6.98: Loss = 0.600174
Epoch 6.99: Loss = 0.482117
Epoch 6.100: Loss = 0.651276
Epoch 6.101: Loss = 0.455429
Epoch 6.102: Loss = 0.586014
Epoch 6.103: Loss = 0.580902
Epoch 6.104: Loss = 0.578888
Epoch 6.105: Loss = 0.514801
Epoch 6.106: Loss = 0.528961
Epoch 6.107: Loss = 0.468033
Epoch 6.108: Loss = 0.53244
Epoch 6.109: Loss = 0.483673
Epoch 6.110: Loss = 0.525726
Epoch 6.111: Loss = 0.481476
Epoch 6.112: Loss = 0.423218
Epoch 6.113: Loss = 0.502274
Epoch 6.114: Loss = 0.481842
Epoch 6.115: Loss = 0.592667
Epoch 6.116: Loss = 0.545273
Epoch 6.117: Loss = 0.554199
Epoch 6.118: Loss = 0.541
Epoch 6.119: Loss = 0.564331
Epoch 6.120: Loss = 0.619354
TRAIN LOSS = 0.515778
TRAIN ACC = 86.6669 % (52002/60000)
Loss = 0.591736
Loss = 0.554886
Loss = 0.564713
Loss = 0.698914
Loss = 0.671814
Loss = 0.543335
Loss = 0.484451
Loss = 0.765884
Loss = 0.666077
Loss = 0.633987
Loss = 0.307541
Loss = 0.361618
Loss = 0.408005
Loss = 0.557129
Loss = 0.253098
Loss = 0.437317
Loss = 0.26004
Loss = 0.0910034
Loss = 0.296844
Loss = 0.677597
TEST LOSS = 0.491299
TEST ACC = 520.02 % (8723/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.516846
Epoch 7.2: Loss = 0.512497
Epoch 7.3: Loss = 0.596344
Epoch 7.4: Loss = 0.465652
Epoch 7.5: Loss = 0.556259
Epoch 7.6: Loss = 0.714935
Epoch 7.7: Loss = 0.519165
Epoch 7.8: Loss = 0.497818
Epoch 7.9: Loss = 0.602112
Epoch 7.10: Loss = 0.455765
Epoch 7.11: Loss = 0.483124
Epoch 7.12: Loss = 0.506393
Epoch 7.13: Loss = 0.49852
Epoch 7.14: Loss = 0.512878
Epoch 7.15: Loss = 0.608185
Epoch 7.16: Loss = 0.548157
Epoch 7.17: Loss = 0.582474
Epoch 7.18: Loss = 0.517929
Epoch 7.19: Loss = 0.435501
Epoch 7.20: Loss = 0.519073
Epoch 7.21: Loss = 0.536636
Epoch 7.22: Loss = 0.508789
Epoch 7.23: Loss = 0.534653
Epoch 7.24: Loss = 0.556732
Epoch 7.25: Loss = 0.620148
Epoch 7.26: Loss = 0.517883
Epoch 7.27: Loss = 0.639297
Epoch 7.28: Loss = 0.518234
Epoch 7.29: Loss = 0.619019
Epoch 7.30: Loss = 0.571198
Epoch 7.31: Loss = 0.474686
Epoch 7.32: Loss = 0.438156
Epoch 7.33: Loss = 0.584183
Epoch 7.34: Loss = 0.658829
Epoch 7.35: Loss = 0.46637
Epoch 7.36: Loss = 0.653564
Epoch 7.37: Loss = 0.633774
Epoch 7.38: Loss = 0.606232
Epoch 7.39: Loss = 0.486465
Epoch 7.40: Loss = 0.537933
Epoch 7.41: Loss = 0.586609
Epoch 7.42: Loss = 0.466827
Epoch 7.43: Loss = 0.470505
Epoch 7.44: Loss = 0.528763
Epoch 7.45: Loss = 0.477234
Epoch 7.46: Loss = 0.539688
Epoch 7.47: Loss = 0.513626
Epoch 7.48: Loss = 0.514893
Epoch 7.49: Loss = 0.551056
Epoch 7.50: Loss = 0.38028
Epoch 7.51: Loss = 0.464386
Epoch 7.52: Loss = 0.509186
Epoch 7.53: Loss = 0.468826
Epoch 7.54: Loss = 0.610733
Epoch 7.55: Loss = 0.609909
Epoch 7.56: Loss = 0.578186
Epoch 7.57: Loss = 0.575699
Epoch 7.58: Loss = 0.765167
Epoch 7.59: Loss = 0.544861
Epoch 7.60: Loss = 0.57431
Epoch 7.61: Loss = 0.435425
Epoch 7.62: Loss = 0.546722
Epoch 7.63: Loss = 0.533981
Epoch 7.64: Loss = 0.53949
Epoch 7.65: Loss = 0.560638
Epoch 7.66: Loss = 0.484833
Epoch 7.67: Loss = 0.612808
Epoch 7.68: Loss = 0.571991
Epoch 7.69: Loss = 0.500381
Epoch 7.70: Loss = 0.525299
Epoch 7.71: Loss = 0.643204
Epoch 7.72: Loss = 0.452209
Epoch 7.73: Loss = 0.533463
Epoch 7.74: Loss = 0.585175
Epoch 7.75: Loss = 0.491501
Epoch 7.76: Loss = 0.530899
Epoch 7.77: Loss = 0.481384
Epoch 7.78: Loss = 0.47438
Epoch 7.79: Loss = 0.360519
Epoch 7.80: Loss = 0.584808
Epoch 7.81: Loss = 0.615646
Epoch 7.82: Loss = 0.397629
Epoch 7.83: Loss = 0.479385
Epoch 7.84: Loss = 0.48439
Epoch 7.85: Loss = 0.516266
Epoch 7.86: Loss = 0.595642
Epoch 7.87: Loss = 0.66922
Epoch 7.88: Loss = 0.550674
Epoch 7.89: Loss = 0.523605
Epoch 7.90: Loss = 0.539017
Epoch 7.91: Loss = 0.67392
Epoch 7.92: Loss = 0.61937
Epoch 7.93: Loss = 0.578278
Epoch 7.94: Loss = 0.530502
Epoch 7.95: Loss = 0.480728
Epoch 7.96: Loss = 0.620087
Epoch 7.97: Loss = 0.701904
Epoch 7.98: Loss = 0.576416
Epoch 7.99: Loss = 0.49559
Epoch 7.100: Loss = 0.559738
Epoch 7.101: Loss = 0.494156
Epoch 7.102: Loss = 0.540558
Epoch 7.103: Loss = 0.42337
Epoch 7.104: Loss = 0.478104
Epoch 7.105: Loss = 0.445724
Epoch 7.106: Loss = 0.681961
Epoch 7.107: Loss = 0.499252
Epoch 7.108: Loss = 0.456253
Epoch 7.109: Loss = 0.507416
Epoch 7.110: Loss = 0.759033
Epoch 7.111: Loss = 0.557877
Epoch 7.112: Loss = 0.382248
Epoch 7.113: Loss = 0.475601
Epoch 7.114: Loss = 0.67157
Epoch 7.115: Loss = 0.546188
Epoch 7.116: Loss = 0.466293
Epoch 7.117: Loss = 0.471466
Epoch 7.118: Loss = 0.460938
Epoch 7.119: Loss = 0.564026
Epoch 7.120: Loss = 0.664261
TRAIN LOSS = 0.539581
TRAIN ACC = 86.5829 % (51952/60000)
Loss = 0.616852
Loss = 0.632751
Loss = 0.637054
Loss = 0.722778
Loss = 0.646484
Loss = 0.558014
Loss = 0.533035
Loss = 0.805801
Loss = 0.659729
Loss = 0.6465
Loss = 0.279709
Loss = 0.421494
Loss = 0.406235
Loss = 0.530014
Loss = 0.262009
Loss = 0.47583
Loss = 0.275177
Loss = 0.111359
Loss = 0.304581
Loss = 0.683243
TEST LOSS = 0.510432
TEST ACC = 519.519 % (8706/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.48056
Epoch 8.2: Loss = 0.525787
Epoch 8.3: Loss = 0.630081
Epoch 8.4: Loss = 0.609833
Epoch 8.5: Loss = 0.544632
Epoch 8.6: Loss = 0.61351
Epoch 8.7: Loss = 0.629089
Epoch 8.8: Loss = 0.446762
Epoch 8.9: Loss = 0.463684
Epoch 8.10: Loss = 0.482971
Epoch 8.11: Loss = 0.527664
Epoch 8.12: Loss = 0.535324
Epoch 8.13: Loss = 0.444
Epoch 8.14: Loss = 0.493515
Epoch 8.15: Loss = 0.5914
Epoch 8.16: Loss = 0.50444
Epoch 8.17: Loss = 0.695618
Epoch 8.18: Loss = 0.524384
Epoch 8.19: Loss = 0.569351
Epoch 8.20: Loss = 0.368774
Epoch 8.21: Loss = 0.478439
Epoch 8.22: Loss = 0.577133
Epoch 8.23: Loss = 0.613312
Epoch 8.24: Loss = 0.592682
Epoch 8.25: Loss = 0.656601
Epoch 8.26: Loss = 0.735321
Epoch 8.27: Loss = 0.530945
Epoch 8.28: Loss = 0.573227
Epoch 8.29: Loss = 0.606979
Epoch 8.30: Loss = 0.515335
Epoch 8.31: Loss = 0.428574
Epoch 8.32: Loss = 0.569473
Epoch 8.33: Loss = 0.485748
Epoch 8.34: Loss = 0.495514
Epoch 8.35: Loss = 0.481812
Epoch 8.36: Loss = 0.482986
Epoch 8.37: Loss = 0.507996
Epoch 8.38: Loss = 0.641357
Epoch 8.39: Loss = 0.596725
Epoch 8.40: Loss = 0.708984
Epoch 8.41: Loss = 0.426208
Epoch 8.42: Loss = 0.551666
Epoch 8.43: Loss = 0.503342
Epoch 8.44: Loss = 0.696365
Epoch 8.45: Loss = 0.51178
Epoch 8.46: Loss = 0.553391
Epoch 8.47: Loss = 0.524536
Epoch 8.48: Loss = 0.463257
Epoch 8.49: Loss = 0.56459
Epoch 8.50: Loss = 0.629898
Epoch 8.51: Loss = 0.692062
Epoch 8.52: Loss = 0.64212
Epoch 8.53: Loss = 0.557907
Epoch 8.54: Loss = 0.500534
Epoch 8.55: Loss = 0.621582
Epoch 8.56: Loss = 0.566956
Epoch 8.57: Loss = 0.677963
Epoch 8.58: Loss = 0.636841
Epoch 8.59: Loss = 0.602951
Epoch 8.60: Loss = 0.495148
Epoch 8.61: Loss = 0.487396
Epoch 8.62: Loss = 0.603165
Epoch 8.63: Loss = 0.725937
Epoch 8.64: Loss = 0.518112
Epoch 8.65: Loss = 0.603073
Epoch 8.66: Loss = 0.555695
Epoch 8.67: Loss = 0.513412
Epoch 8.68: Loss = 0.412277
Epoch 8.69: Loss = 0.498657
Epoch 8.70: Loss = 0.817535
Epoch 8.71: Loss = 0.481674
Epoch 8.72: Loss = 0.50209
Epoch 8.73: Loss = 0.573395
Epoch 8.74: Loss = 0.633301
Epoch 8.75: Loss = 0.656845
Epoch 8.76: Loss = 0.380859
Epoch 8.77: Loss = 0.526199
Epoch 8.78: Loss = 0.522873
Epoch 8.79: Loss = 0.504242
Epoch 8.80: Loss = 0.564774
Epoch 8.81: Loss = 0.545456
Epoch 8.82: Loss = 0.522629
Epoch 8.83: Loss = 0.62207
Epoch 8.84: Loss = 0.577209
Epoch 8.85: Loss = 0.437775
Epoch 8.86: Loss = 0.626114
Epoch 8.87: Loss = 0.483643
Epoch 8.88: Loss = 0.550735
Epoch 8.89: Loss = 0.357513
Epoch 8.90: Loss = 0.560364
Epoch 8.91: Loss = 0.592209
Epoch 8.92: Loss = 0.550034
Epoch 8.93: Loss = 0.53389
Epoch 8.94: Loss = 0.608231
Epoch 8.95: Loss = 0.562271
Epoch 8.96: Loss = 0.61467
Epoch 8.97: Loss = 0.546555
Epoch 8.98: Loss = 0.707031
Epoch 8.99: Loss = 0.654037
Epoch 8.100: Loss = 0.593323
Epoch 8.101: Loss = 0.463837
Epoch 8.102: Loss = 0.571701
Epoch 8.103: Loss = 0.45108
Epoch 8.104: Loss = 0.470932
Epoch 8.105: Loss = 0.581894
Epoch 8.106: Loss = 0.553329
Epoch 8.107: Loss = 0.613129
Epoch 8.108: Loss = 0.591629
Epoch 8.109: Loss = 0.771286
Epoch 8.110: Loss = 0.680359
Epoch 8.111: Loss = 0.540741
Epoch 8.112: Loss = 0.491196
Epoch 8.113: Loss = 0.562454
Epoch 8.114: Loss = 0.520721
Epoch 8.115: Loss = 0.581177
Epoch 8.116: Loss = 0.49913
Epoch 8.117: Loss = 0.611938
Epoch 8.118: Loss = 0.536728
Epoch 8.119: Loss = 0.431458
Epoch 8.120: Loss = 0.685608
TRAIN LOSS = 0.557663
TRAIN ACC = 86.8561 % (52116/60000)
Loss = 0.604691
Loss = 0.624481
Loss = 0.657623
Loss = 0.746826
Loss = 0.693207
Loss = 0.621964
Loss = 0.502426
Loss = 0.794327
Loss = 0.641769
Loss = 0.666656
Loss = 0.322021
Loss = 0.494263
Loss = 0.424194
Loss = 0.564362
Loss = 0.273178
Loss = 0.555298
Loss = 0.301422
Loss = 0.083847
Loss = 0.313416
Loss = 0.686188
TEST LOSS = 0.528608
TEST ACC = 521.159 % (8764/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.491821
Epoch 9.2: Loss = 0.597885
Epoch 9.3: Loss = 0.481979
Epoch 9.4: Loss = 0.547806
Epoch 9.5: Loss = 0.496475
Epoch 9.6: Loss = 0.660645
Epoch 9.7: Loss = 0.559692
Epoch 9.8: Loss = 0.521683
Epoch 9.9: Loss = 0.634232
Epoch 9.10: Loss = 0.489288
Epoch 9.11: Loss = 0.566513
Epoch 9.12: Loss = 0.588486
Epoch 9.13: Loss = 0.637207
Epoch 9.14: Loss = 0.65683
Epoch 9.15: Loss = 0.492981
Epoch 9.16: Loss = 0.563858
Epoch 9.17: Loss = 0.490067
Epoch 9.18: Loss = 0.670517
Epoch 9.19: Loss = 0.670563
Epoch 9.20: Loss = 0.681915
Epoch 9.21: Loss = 0.64473
Epoch 9.22: Loss = 0.615631
Epoch 9.23: Loss = 0.593597
Epoch 9.24: Loss = 0.546097
Epoch 9.25: Loss = 0.550827
Epoch 9.26: Loss = 0.480606
Epoch 9.27: Loss = 0.531479
Epoch 9.28: Loss = 0.652985
Epoch 9.29: Loss = 0.595093
Epoch 9.30: Loss = 0.582611
Epoch 9.31: Loss = 0.587265
Epoch 9.32: Loss = 0.535385
Epoch 9.33: Loss = 0.696991
Epoch 9.34: Loss = 0.564117
Epoch 9.35: Loss = 0.543091
Epoch 9.36: Loss = 0.491791
Epoch 9.37: Loss = 0.487137
Epoch 9.38: Loss = 0.510254
Epoch 9.39: Loss = 0.670242
Epoch 9.40: Loss = 0.654633
Epoch 9.41: Loss = 0.520416
Epoch 9.42: Loss = 0.63588
Epoch 9.43: Loss = 0.587036
Epoch 9.44: Loss = 0.500854
Epoch 9.45: Loss = 0.630905
Epoch 9.46: Loss = 0.579117
Epoch 9.47: Loss = 0.635132
Epoch 9.48: Loss = 0.426895
Epoch 9.49: Loss = 0.569153
Epoch 9.50: Loss = 0.543152
Epoch 9.51: Loss = 0.526382
Epoch 9.52: Loss = 0.576569
Epoch 9.53: Loss = 0.649826
Epoch 9.54: Loss = 0.352249
Epoch 9.55: Loss = 0.641937
Epoch 9.56: Loss = 0.725006
Epoch 9.57: Loss = 0.497177
Epoch 9.58: Loss = 0.544479
Epoch 9.59: Loss = 0.674316
Epoch 9.60: Loss = 0.588943
Epoch 9.61: Loss = 0.548721
Epoch 9.62: Loss = 0.639587
Epoch 9.63: Loss = 0.638672
Epoch 9.64: Loss = 0.617889
Epoch 9.65: Loss = 0.698044
Epoch 9.66: Loss = 0.559494
Epoch 9.67: Loss = 0.498169
Epoch 9.68: Loss = 0.595016
Epoch 9.69: Loss = 0.468048
Epoch 9.70: Loss = 0.648911
Epoch 9.71: Loss = 0.635727
Epoch 9.72: Loss = 0.486511
Epoch 9.73: Loss = 0.683334
Epoch 9.74: Loss = 0.797256
Epoch 9.75: Loss = 0.571442
Epoch 9.76: Loss = 0.532776
Epoch 9.77: Loss = 0.6194
Epoch 9.78: Loss = 0.653519
Epoch 9.79: Loss = 0.580643
Epoch 9.80: Loss = 0.502853
Epoch 9.81: Loss = 0.689209
Epoch 9.82: Loss = 0.576157
Epoch 9.83: Loss = 0.557083
Epoch 9.84: Loss = 0.509125
Epoch 9.85: Loss = 0.512314
Epoch 9.86: Loss = 0.584824
Epoch 9.87: Loss = 0.447815
Epoch 9.88: Loss = 0.600967
Epoch 9.89: Loss = 0.626633
Epoch 9.90: Loss = 0.671936
Epoch 9.91: Loss = 0.709503
Epoch 9.92: Loss = 0.589264
Epoch 9.93: Loss = 0.822144
Epoch 9.94: Loss = 0.596176
Epoch 9.95: Loss = 0.53952
Epoch 9.96: Loss = 0.655182
Epoch 9.97: Loss = 0.546707
Epoch 9.98: Loss = 0.622452
Epoch 9.99: Loss = 0.567261
Epoch 9.100: Loss = 0.752533
Epoch 9.101: Loss = 0.581192
Epoch 9.102: Loss = 0.592621
Epoch 9.103: Loss = 0.756119
Epoch 9.104: Loss = 0.550018
Epoch 9.105: Loss = 0.627792
Epoch 9.106: Loss = 0.510315
Epoch 9.107: Loss = 0.616455
Epoch 9.108: Loss = 0.488358
Epoch 9.109: Loss = 0.733856
Epoch 9.110: Loss = 0.533905
Epoch 9.111: Loss = 0.621567
Epoch 9.112: Loss = 0.63559
Epoch 9.113: Loss = 0.648193
Epoch 9.114: Loss = 0.456558
Epoch 9.115: Loss = 0.541931
Epoch 9.116: Loss = 0.707108
Epoch 9.117: Loss = 0.576965
Epoch 9.118: Loss = 0.585464
Epoch 9.119: Loss = 0.534134
Epoch 9.120: Loss = 0.755371
TRAIN LOSS = 0.58873
TRAIN ACC = 86.8286 % (52100/60000)
Loss = 0.592651
Loss = 0.650711
Loss = 0.731369
Loss = 0.785812
Loss = 0.751541
Loss = 0.675919
Loss = 0.494415
Loss = 0.786163
Loss = 0.661987
Loss = 0.651855
Loss = 0.320969
Loss = 0.419479
Loss = 0.497192
Loss = 0.599716
Loss = 0.283768
Loss = 0.590179
Loss = 0.303802
Loss = 0.0909882
Loss = 0.416718
Loss = 0.790512
TEST LOSS = 0.554787
TEST ACC = 520.999 % (8748/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.645599
Epoch 10.2: Loss = 0.605377
Epoch 10.3: Loss = 0.507904
Epoch 10.4: Loss = 0.581924
Epoch 10.5: Loss = 0.448944
Epoch 10.6: Loss = 0.744934
Epoch 10.7: Loss = 0.510056
Epoch 10.8: Loss = 0.560837
Epoch 10.9: Loss = 0.580292
Epoch 10.10: Loss = 0.56395
Epoch 10.11: Loss = 0.617737
Epoch 10.12: Loss = 0.802841
Epoch 10.13: Loss = 0.65863
Epoch 10.14: Loss = 0.516495
Epoch 10.15: Loss = 0.4944
Epoch 10.16: Loss = 0.798615
Epoch 10.17: Loss = 0.473801
Epoch 10.18: Loss = 0.642532
Epoch 10.19: Loss = 0.620331
Epoch 10.20: Loss = 0.617218
Epoch 10.21: Loss = 0.730133
Epoch 10.22: Loss = 0.546066
Epoch 10.23: Loss = 0.64679
Epoch 10.24: Loss = 0.561295
Epoch 10.25: Loss = 0.65892
Epoch 10.26: Loss = 0.549988
Epoch 10.27: Loss = 0.534882
Epoch 10.28: Loss = 0.647964
Epoch 10.29: Loss = 0.499908
Epoch 10.30: Loss = 0.63205
Epoch 10.31: Loss = 0.641586
Epoch 10.32: Loss = 0.534027
Epoch 10.33: Loss = 0.633331
Epoch 10.34: Loss = 0.551651
Epoch 10.35: Loss = 0.647858
Epoch 10.36: Loss = 0.566513
Epoch 10.37: Loss = 0.719315
Epoch 10.38: Loss = 0.666321
Epoch 10.39: Loss = 0.51181
Epoch 10.40: Loss = 0.572449
Epoch 10.41: Loss = 0.674881
Epoch 10.42: Loss = 0.423828
Epoch 10.43: Loss = 0.674301
Epoch 10.44: Loss = 0.749557
Epoch 10.45: Loss = 0.615173
Epoch 10.46: Loss = 0.590195
Epoch 10.47: Loss = 0.6185
Epoch 10.48: Loss = 0.427216
Epoch 10.49: Loss = 0.590057
Epoch 10.50: Loss = 0.452072
Epoch 10.51: Loss = 0.551254
Epoch 10.52: Loss = 0.7827
Epoch 10.53: Loss = 0.520569
Epoch 10.54: Loss = 0.733521
Epoch 10.55: Loss = 0.676636
Epoch 10.56: Loss = 0.446625
Epoch 10.57: Loss = 0.587463
Epoch 10.58: Loss = 0.570465
Epoch 10.59: Loss = 0.55545
Epoch 10.60: Loss = 0.700073
Epoch 10.61: Loss = 0.527512
Epoch 10.62: Loss = 0.588974
Epoch 10.63: Loss = 0.608536
Epoch 10.64: Loss = 0.565735
Epoch 10.65: Loss = 0.651062
Epoch 10.66: Loss = 0.484329
Epoch 10.67: Loss = 0.631729
Epoch 10.68: Loss = 0.618118
Epoch 10.69: Loss = 0.608246
Epoch 10.70: Loss = 0.653931
Epoch 10.71: Loss = 0.727737
Epoch 10.72: Loss = 0.63974
Epoch 10.73: Loss = 0.569336
Epoch 10.74: Loss = 0.693298
Epoch 10.75: Loss = 0.582672
Epoch 10.76: Loss = 0.658325
Epoch 10.77: Loss = 0.642868
Epoch 10.78: Loss = 0.687988
Epoch 10.79: Loss = 0.784363
Epoch 10.80: Loss = 0.493484
Epoch 10.81: Loss = 0.578995
Epoch 10.82: Loss = 0.594223
Epoch 10.83: Loss = 0.671265
Epoch 10.84: Loss = 0.653656
Epoch 10.85: Loss = 0.684021
Epoch 10.86: Loss = 0.671173
Epoch 10.87: Loss = 0.607071
Epoch 10.88: Loss = 0.673233
Epoch 10.89: Loss = 0.734192
Epoch 10.90: Loss = 0.609589
Epoch 10.91: Loss = 0.652451
Epoch 10.92: Loss = 0.861465
Epoch 10.93: Loss = 0.508194
Epoch 10.94: Loss = 0.586639
Epoch 10.95: Loss = 0.697708
Epoch 10.96: Loss = 0.695038
Epoch 10.97: Loss = 0.603439
Epoch 10.98: Loss = 0.660934
Epoch 10.99: Loss = 0.785202
Epoch 10.100: Loss = 0.558533
Epoch 10.101: Loss = 0.567413
Epoch 10.102: Loss = 0.418884
Epoch 10.103: Loss = 0.641937
Epoch 10.104: Loss = 0.750992
Epoch 10.105: Loss = 0.596558
Epoch 10.106: Loss = 0.65361
Epoch 10.107: Loss = 0.625626
Epoch 10.108: Loss = 0.553329
Epoch 10.109: Loss = 0.647156
Epoch 10.110: Loss = 0.690536
Epoch 10.111: Loss = 0.609451
Epoch 10.112: Loss = 0.632538
Epoch 10.113: Loss = 0.598862
Epoch 10.114: Loss = 0.660583
Epoch 10.115: Loss = 0.640915
Epoch 10.116: Loss = 0.593613
Epoch 10.117: Loss = 0.673706
Epoch 10.118: Loss = 0.551712
Epoch 10.119: Loss = 0.639587
Epoch 10.120: Loss = 0.641327
TRAIN LOSS = 0.615677
TRAIN ACC = 86.7691 % (52064/60000)
Loss = 0.713882
Loss = 0.725159
Loss = 0.836533
Loss = 0.814331
Loss = 0.799438
Loss = 0.702835
Loss = 0.543167
Loss = 0.884369
Loss = 0.801636
Loss = 0.734924
Loss = 0.313766
Loss = 0.471069
Loss = 0.543686
Loss = 0.631714
Loss = 0.244965
Loss = 0.555511
Loss = 0.288437
Loss = 0.106018
Loss = 0.512756
Loss = 0.816772
TEST LOSS = 0.602048
TEST ACC = 520.639 % (8697/10000)

Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.35606
Epoch 1.2: Loss = 2.30717
Epoch 1.3: Loss = 2.29259
Epoch 1.4: Loss = 2.28004
Epoch 1.5: Loss = 2.25578
Epoch 1.6: Loss = 2.2258
Epoch 1.7: Loss = 2.19667
Epoch 1.8: Loss = 2.17448
Epoch 1.9: Loss = 2.16154
Epoch 1.10: Loss = 2.10674
Epoch 1.11: Loss = 2.09839
Epoch 1.12: Loss = 2.0815
Epoch 1.13: Loss = 2.06436
Epoch 1.14: Loss = 2.03181
Epoch 1.15: Loss = 2.00108
Epoch 1.16: Loss = 1.98453
Epoch 1.17: Loss = 1.98178
Epoch 1.18: Loss = 1.95772
Epoch 1.19: Loss = 1.90746
Epoch 1.20: Loss = 1.87672
Epoch 1.21: Loss = 1.87717
Epoch 1.22: Loss = 1.80867
Epoch 1.23: Loss = 1.83545
Epoch 1.24: Loss = 1.79161
Epoch 1.25: Loss = 1.75186
Epoch 1.26: Loss = 1.74994
Epoch 1.27: Loss = 1.66447
Epoch 1.28: Loss = 1.63173
Epoch 1.29: Loss = 1.59743
Epoch 1.30: Loss = 1.59193
Epoch 1.31: Loss = 1.54706
Epoch 1.32: Loss = 1.58806
Epoch 1.33: Loss = 1.51212
Epoch 1.34: Loss = 1.53397
Epoch 1.35: Loss = 1.4472
Epoch 1.36: Loss = 1.48695
Epoch 1.37: Loss = 1.42436
Epoch 1.38: Loss = 1.47263
Epoch 1.39: Loss = 1.38408
Epoch 1.40: Loss = 1.383
Epoch 1.41: Loss = 1.34438
Epoch 1.42: Loss = 1.32504
Epoch 1.43: Loss = 1.29439
Epoch 1.44: Loss = 1.28464
Epoch 1.45: Loss = 1.27571
Epoch 1.46: Loss = 1.28311
Epoch 1.47: Loss = 1.22401
Epoch 1.48: Loss = 1.19176
Epoch 1.49: Loss = 1.25203
Epoch 1.50: Loss = 1.13789
Epoch 1.51: Loss = 1.1111
Epoch 1.52: Loss = 1.10939
Epoch 1.53: Loss = 1.13689
Epoch 1.54: Loss = 1.08852
Epoch 1.55: Loss = 1.13406
Epoch 1.56: Loss = 1.12192
Epoch 1.57: Loss = 1.0598
Epoch 1.58: Loss = 1.03221
Epoch 1.59: Loss = 1.05502
Epoch 1.60: Loss = 1.03331
Epoch 1.61: Loss = 1.05151
Epoch 1.62: Loss = 1.00648
Epoch 1.63: Loss = 1.03625
Epoch 1.64: Loss = 1.00043
Epoch 1.65: Loss = 0.953201
Epoch 1.66: Loss = 0.992188
Epoch 1.67: Loss = 0.982224
Epoch 1.68: Loss = 0.951263
Epoch 1.69: Loss = 0.925507
Epoch 1.70: Loss = 0.92749
Epoch 1.71: Loss = 0.911545
Epoch 1.72: Loss = 0.9478
Epoch 1.73: Loss = 0.850464
Epoch 1.74: Loss = 0.954849
Epoch 1.75: Loss = 0.861252
Epoch 1.76: Loss = 0.810379
Epoch 1.77: Loss = 0.896866
Epoch 1.78: Loss = 0.827133
Epoch 1.79: Loss = 0.860931
Epoch 1.80: Loss = 0.901901
Epoch 1.81: Loss = 0.790955
Epoch 1.82: Loss = 0.839722
Epoch 1.83: Loss = 0.79567
Epoch 1.84: Loss = 0.828766
Epoch 1.85: Loss = 0.785187
Epoch 1.86: Loss = 0.813919
Epoch 1.87: Loss = 0.82222
Epoch 1.88: Loss = 0.774704
Epoch 1.89: Loss = 0.814499
Epoch 1.90: Loss = 0.737091
Epoch 1.91: Loss = 0.782242
Epoch 1.92: Loss = 0.751343
Epoch 1.93: Loss = 0.765335
Epoch 1.94: Loss = 0.716644
Epoch 1.95: Loss = 0.707993
Epoch 1.96: Loss = 0.651215
Epoch 1.97: Loss = 0.681274
Epoch 1.98: Loss = 0.722992
Epoch 1.99: Loss = 0.718597
Epoch 1.100: Loss = 0.66301
Epoch 1.101: Loss = 0.699875
Epoch 1.102: Loss = 0.69696
Epoch 1.103: Loss = 0.72348
Epoch 1.104: Loss = 0.635056
Epoch 1.105: Loss = 0.737656
Epoch 1.106: Loss = 0.63208
Epoch 1.107: Loss = 0.65239
Epoch 1.108: Loss = 0.642639
Epoch 1.109: Loss = 0.623566
Epoch 1.110: Loss = 0.68161
Epoch 1.111: Loss = 0.645508
Epoch 1.112: Loss = 0.621704
Epoch 1.113: Loss = 0.655045
Epoch 1.114: Loss = 0.647827
Epoch 1.115: Loss = 0.633942
Epoch 1.116: Loss = 0.677216
Epoch 1.117: Loss = 0.584534
Epoch 1.118: Loss = 0.591827
Epoch 1.119: Loss = 0.612076
Epoch 1.120: Loss = 0.582642
TRAIN LOSS = 1.2059
TRAIN ACC = 65.8691 % (39523/60000)
Loss = 0.651672
Loss = 0.667206
Loss = 0.801361
Loss = 0.755341
Loss = 0.773987
Loss = 0.695023
Loss = 0.634888
Loss = 0.830566
Loss = 0.788803
Loss = 0.720535
Loss = 0.388275
Loss = 0.515244
Loss = 0.38002
Loss = 0.571609
Loss = 0.503891
Loss = 0.480591
Loss = 0.451157
Loss = 0.271683
Loss = 0.465485
Loss = 0.71347
TEST LOSS = 0.60304
TEST ACC = 395.229 % (8155/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.605743
Epoch 2.2: Loss = 0.595001
Epoch 2.3: Loss = 0.620926
Epoch 2.4: Loss = 0.668411
Epoch 2.5: Loss = 0.68129
Epoch 2.6: Loss = 0.601639
Epoch 2.7: Loss = 0.636765
Epoch 2.8: Loss = 0.711792
Epoch 2.9: Loss = 0.571426
Epoch 2.10: Loss = 0.586685
Epoch 2.11: Loss = 0.606201
Epoch 2.12: Loss = 0.548386
Epoch 2.13: Loss = 0.574615
Epoch 2.14: Loss = 0.558868
Epoch 2.15: Loss = 0.572433
Epoch 2.16: Loss = 0.589783
Epoch 2.17: Loss = 0.548447
Epoch 2.18: Loss = 0.555527
Epoch 2.19: Loss = 0.549896
Epoch 2.20: Loss = 0.596603
Epoch 2.21: Loss = 0.530273
Epoch 2.22: Loss = 0.596222
Epoch 2.23: Loss = 0.554291
Epoch 2.24: Loss = 0.628143
Epoch 2.25: Loss = 0.639877
Epoch 2.26: Loss = 0.592697
Epoch 2.27: Loss = 0.515778
Epoch 2.28: Loss = 0.542267
Epoch 2.29: Loss = 0.561249
Epoch 2.30: Loss = 0.530807
Epoch 2.31: Loss = 0.598846
Epoch 2.32: Loss = 0.58316
Epoch 2.33: Loss = 0.557907
Epoch 2.34: Loss = 0.576843
Epoch 2.35: Loss = 0.593231
Epoch 2.36: Loss = 0.556168
Epoch 2.37: Loss = 0.530289
Epoch 2.38: Loss = 0.555313
Epoch 2.39: Loss = 0.550262
Epoch 2.40: Loss = 0.583679
Epoch 2.41: Loss = 0.609207
Epoch 2.42: Loss = 0.537109
Epoch 2.43: Loss = 0.548294
Epoch 2.44: Loss = 0.571289
Epoch 2.45: Loss = 0.522858
Epoch 2.46: Loss = 0.554413
Epoch 2.47: Loss = 0.618942
Epoch 2.48: Loss = 0.548553
Epoch 2.49: Loss = 0.584366
Epoch 2.50: Loss = 0.530609
Epoch 2.51: Loss = 0.480637
Epoch 2.52: Loss = 0.631821
Epoch 2.53: Loss = 0.568314
Epoch 2.54: Loss = 0.465057
Epoch 2.55: Loss = 0.504807
Epoch 2.56: Loss = 0.557465
Epoch 2.57: Loss = 0.551865
Epoch 2.58: Loss = 0.603027
Epoch 2.59: Loss = 0.545151
Epoch 2.60: Loss = 0.541
Epoch 2.61: Loss = 0.50119
Epoch 2.62: Loss = 0.506866
Epoch 2.63: Loss = 0.4673
Epoch 2.64: Loss = 0.559769
Epoch 2.65: Loss = 0.545135
Epoch 2.66: Loss = 0.496887
Epoch 2.67: Loss = 0.475677
Epoch 2.68: Loss = 0.495544
Epoch 2.69: Loss = 0.505539
Epoch 2.70: Loss = 0.509003
Epoch 2.71: Loss = 0.479858
Epoch 2.72: Loss = 0.550293
Epoch 2.73: Loss = 0.470688
Epoch 2.74: Loss = 0.484726
Epoch 2.75: Loss = 0.456833
Epoch 2.76: Loss = 0.440292
Epoch 2.77: Loss = 0.586044
Epoch 2.78: Loss = 0.524628
Epoch 2.79: Loss = 0.504196
Epoch 2.80: Loss = 0.489334
Epoch 2.81: Loss = 0.57196
Epoch 2.82: Loss = 0.500992
Epoch 2.83: Loss = 0.523544
Epoch 2.84: Loss = 0.514694
Epoch 2.85: Loss = 0.547379
Epoch 2.86: Loss = 0.548569
Epoch 2.87: Loss = 0.568161
Epoch 2.88: Loss = 0.54187
Epoch 2.89: Loss = 0.509109
Epoch 2.90: Loss = 0.447525
Epoch 2.91: Loss = 0.540558
Epoch 2.92: Loss = 0.453995
Epoch 2.93: Loss = 0.574631
Epoch 2.94: Loss = 0.56427
Epoch 2.95: Loss = 0.546082
Epoch 2.96: Loss = 0.513367
Epoch 2.97: Loss = 0.509506
Epoch 2.98: Loss = 0.435791
Epoch 2.99: Loss = 0.597107
Epoch 2.100: Loss = 0.527466
Epoch 2.101: Loss = 0.488297
Epoch 2.102: Loss = 0.52594
Epoch 2.103: Loss = 0.530441
Epoch 2.104: Loss = 0.506775
Epoch 2.105: Loss = 0.567413
Epoch 2.106: Loss = 0.475098
Epoch 2.107: Loss = 0.45134
Epoch 2.108: Loss = 0.525406
Epoch 2.109: Loss = 0.590317
Epoch 2.110: Loss = 0.515839
Epoch 2.111: Loss = 0.44458
Epoch 2.112: Loss = 0.475067
Epoch 2.113: Loss = 0.536163
Epoch 2.114: Loss = 0.493362
Epoch 2.115: Loss = 0.490204
Epoch 2.116: Loss = 0.491425
Epoch 2.117: Loss = 0.545197
Epoch 2.118: Loss = 0.490204
Epoch 2.119: Loss = 0.538223
Epoch 2.120: Loss = 0.508453
TRAIN LOSS = 0.543427
TRAIN ACC = 83.2138 % (49930/60000)
Loss = 0.48175
Loss = 0.565628
Loss = 0.664261
Loss = 0.643082
Loss = 0.665726
Loss = 0.529617
Loss = 0.481247
Loss = 0.772171
Loss = 0.664993
Loss = 0.602402
Loss = 0.25
Loss = 0.41629
Loss = 0.2883
Loss = 0.410797
Loss = 0.30191
Loss = 0.37056
Loss = 0.276718
Loss = 0.122131
Loss = 0.338348
Loss = 0.593597
TEST LOSS = 0.471976
TEST ACC = 499.3 % (8558/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.492905
Epoch 3.2: Loss = 0.478897
Epoch 3.3: Loss = 0.603378
Epoch 3.4: Loss = 0.466293
Epoch 3.5: Loss = 0.512619
Epoch 3.6: Loss = 0.515793
Epoch 3.7: Loss = 0.468384
Epoch 3.8: Loss = 0.471237
Epoch 3.9: Loss = 0.387955
Epoch 3.10: Loss = 0.521271
Epoch 3.11: Loss = 0.463135
Epoch 3.12: Loss = 0.603027
Epoch 3.13: Loss = 0.598648
Epoch 3.14: Loss = 0.530624
Epoch 3.15: Loss = 0.467285
Epoch 3.16: Loss = 0.583115
Epoch 3.17: Loss = 0.427094
Epoch 3.18: Loss = 0.516953
Epoch 3.19: Loss = 0.554199
Epoch 3.20: Loss = 0.412735
Epoch 3.21: Loss = 0.533356
Epoch 3.22: Loss = 0.559998
Epoch 3.23: Loss = 0.496445
Epoch 3.24: Loss = 0.491791
Epoch 3.25: Loss = 0.522446
Epoch 3.26: Loss = 0.482346
Epoch 3.27: Loss = 0.49939
Epoch 3.28: Loss = 0.465378
Epoch 3.29: Loss = 0.541153
Epoch 3.30: Loss = 0.493286
Epoch 3.31: Loss = 0.440216
Epoch 3.32: Loss = 0.39537
Epoch 3.33: Loss = 0.470612
Epoch 3.34: Loss = 0.478729
Epoch 3.35: Loss = 0.503357
Epoch 3.36: Loss = 0.705353
Epoch 3.37: Loss = 0.415222
Epoch 3.38: Loss = 0.595718
Epoch 3.39: Loss = 0.537628
Epoch 3.40: Loss = 0.492676
Epoch 3.41: Loss = 0.511688
Epoch 3.42: Loss = 0.578018
Epoch 3.43: Loss = 0.462067
Epoch 3.44: Loss = 0.553253
Epoch 3.45: Loss = 0.497543
Epoch 3.46: Loss = 0.370789
Epoch 3.47: Loss = 0.468658
Epoch 3.48: Loss = 0.373672
Epoch 3.49: Loss = 0.516312
Epoch 3.50: Loss = 0.570816
Epoch 3.51: Loss = 0.599625
Epoch 3.52: Loss = 0.48761
Epoch 3.53: Loss = 0.513382
Epoch 3.54: Loss = 0.4263
Epoch 3.55: Loss = 0.448547
Epoch 3.56: Loss = 0.487595
Epoch 3.57: Loss = 0.535904
Epoch 3.58: Loss = 0.453018
Epoch 3.59: Loss = 0.557205
Epoch 3.60: Loss = 0.514359
Epoch 3.61: Loss = 0.465012
Epoch 3.62: Loss = 0.569016
Epoch 3.63: Loss = 0.531174
Epoch 3.64: Loss = 0.535156
Epoch 3.65: Loss = 0.464432
Epoch 3.66: Loss = 0.47998
Epoch 3.67: Loss = 0.452515
Epoch 3.68: Loss = 0.512802
Epoch 3.69: Loss = 0.361969
Epoch 3.70: Loss = 0.513809
Epoch 3.71: Loss = 0.485489
Epoch 3.72: Loss = 0.544815
Epoch 3.73: Loss = 0.536819
Epoch 3.74: Loss = 0.520386
Epoch 3.75: Loss = 0.520126
Epoch 3.76: Loss = 0.499054
Epoch 3.77: Loss = 0.475281
Epoch 3.78: Loss = 0.548615
Epoch 3.79: Loss = 0.418793
Epoch 3.80: Loss = 0.46283
Epoch 3.81: Loss = 0.568466
Epoch 3.82: Loss = 0.549316
Epoch 3.83: Loss = 0.445801
Epoch 3.84: Loss = 0.512802
Epoch 3.85: Loss = 0.492935
Epoch 3.86: Loss = 0.54538
Epoch 3.87: Loss = 0.556061
Epoch 3.88: Loss = 0.438431
Epoch 3.89: Loss = 0.47438
Epoch 3.90: Loss = 0.486496
Epoch 3.91: Loss = 0.45285
Epoch 3.92: Loss = 0.494568
Epoch 3.93: Loss = 0.485184
Epoch 3.94: Loss = 0.426529
Epoch 3.95: Loss = 0.400192
Epoch 3.96: Loss = 0.452042
Epoch 3.97: Loss = 0.546555
Epoch 3.98: Loss = 0.512253
Epoch 3.99: Loss = 0.578995
Epoch 3.100: Loss = 0.549988
Epoch 3.101: Loss = 0.437897
Epoch 3.102: Loss = 0.512955
Epoch 3.103: Loss = 0.467224
Epoch 3.104: Loss = 0.545471
Epoch 3.105: Loss = 0.428345
Epoch 3.106: Loss = 0.45401
Epoch 3.107: Loss = 0.493835
Epoch 3.108: Loss = 0.504211
Epoch 3.109: Loss = 0.479553
Epoch 3.110: Loss = 0.576584
Epoch 3.111: Loss = 0.528183
Epoch 3.112: Loss = 0.490509
Epoch 3.113: Loss = 0.484924
Epoch 3.114: Loss = 0.414673
Epoch 3.115: Loss = 0.492859
Epoch 3.116: Loss = 0.518982
Epoch 3.117: Loss = 0.46347
Epoch 3.118: Loss = 0.471313
Epoch 3.119: Loss = 0.531158
Epoch 3.120: Loss = 0.485367
TRAIN LOSS = 0.498093
TRAIN ACC = 85.0937 % (51059/60000)
Loss = 0.482513
Loss = 0.576675
Loss = 0.657333
Loss = 0.627747
Loss = 0.702164
Loss = 0.533752
Loss = 0.486069
Loss = 0.759827
Loss = 0.683578
Loss = 0.612671
Loss = 0.227631
Loss = 0.44249
Loss = 0.354645
Loss = 0.420471
Loss = 0.25914
Loss = 0.412552
Loss = 0.278931
Loss = 0.0889893
Loss = 0.324646
Loss = 0.604095
TEST LOSS = 0.476796
TEST ACC = 510.59 % (8601/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.499817
Epoch 4.2: Loss = 0.512955
Epoch 4.3: Loss = 0.40831
Epoch 4.4: Loss = 0.503418
Epoch 4.5: Loss = 0.498215
Epoch 4.6: Loss = 0.472992
Epoch 4.7: Loss = 0.46077
Epoch 4.8: Loss = 0.462494
Epoch 4.9: Loss = 0.511292
Epoch 4.10: Loss = 0.594925
Epoch 4.11: Loss = 0.499359
Epoch 4.12: Loss = 0.571777
Epoch 4.13: Loss = 0.478027
Epoch 4.14: Loss = 0.466904
Epoch 4.15: Loss = 0.652664
Epoch 4.16: Loss = 0.525696
Epoch 4.17: Loss = 0.424042
Epoch 4.18: Loss = 0.418213
Epoch 4.19: Loss = 0.540558
Epoch 4.20: Loss = 0.500443
Epoch 4.21: Loss = 0.435989
Epoch 4.22: Loss = 0.366135
Epoch 4.23: Loss = 0.441406
Epoch 4.24: Loss = 0.59967
Epoch 4.25: Loss = 0.673065
Epoch 4.26: Loss = 0.422485
Epoch 4.27: Loss = 0.348785
Epoch 4.28: Loss = 0.426025
Epoch 4.29: Loss = 0.511307
Epoch 4.30: Loss = 0.46843
Epoch 4.31: Loss = 0.409622
Epoch 4.32: Loss = 0.477722
Epoch 4.33: Loss = 0.465057
Epoch 4.34: Loss = 0.57843
Epoch 4.35: Loss = 0.466873
Epoch 4.36: Loss = 0.572571
Epoch 4.37: Loss = 0.491302
Epoch 4.38: Loss = 0.448456
Epoch 4.39: Loss = 0.538498
Epoch 4.40: Loss = 0.540924
Epoch 4.41: Loss = 0.563293
Epoch 4.42: Loss = 0.386429
Epoch 4.43: Loss = 0.424301
Epoch 4.44: Loss = 0.518768
Epoch 4.45: Loss = 0.495926
Epoch 4.46: Loss = 0.539124
Epoch 4.47: Loss = 0.38739
Epoch 4.48: Loss = 0.527252
Epoch 4.49: Loss = 0.524979
Epoch 4.50: Loss = 0.498322
Epoch 4.51: Loss = 0.554001
Epoch 4.52: Loss = 0.482529
Epoch 4.53: Loss = 0.474045
Epoch 4.54: Loss = 0.458542
Epoch 4.55: Loss = 0.432846
Epoch 4.56: Loss = 0.486237
Epoch 4.57: Loss = 0.548019
Epoch 4.58: Loss = 0.375183
Epoch 4.59: Loss = 0.551224
Epoch 4.60: Loss = 0.517242
Epoch 4.61: Loss = 0.538834
Epoch 4.62: Loss = 0.567398
Epoch 4.63: Loss = 0.436768
Epoch 4.64: Loss = 0.577957
Epoch 4.65: Loss = 0.461411
Epoch 4.66: Loss = 0.490936
Epoch 4.67: Loss = 0.426407
Epoch 4.68: Loss = 0.576385
Epoch 4.69: Loss = 0.565018
Epoch 4.70: Loss = 0.571121
Epoch 4.71: Loss = 0.485489
Epoch 4.72: Loss = 0.586273
Epoch 4.73: Loss = 0.582962
Epoch 4.74: Loss = 0.513687
Epoch 4.75: Loss = 0.445084
Epoch 4.76: Loss = 0.449402
Epoch 4.77: Loss = 0.540756
Epoch 4.78: Loss = 0.542389
Epoch 4.79: Loss = 0.526184
Epoch 4.80: Loss = 0.4478
Epoch 4.81: Loss = 0.533722
Epoch 4.82: Loss = 0.453262
Epoch 4.83: Loss = 0.423157
Epoch 4.84: Loss = 0.580032
Epoch 4.85: Loss = 0.407669
Epoch 4.86: Loss = 0.512161
Epoch 4.87: Loss = 0.493393
Epoch 4.88: Loss = 0.389923
Epoch 4.89: Loss = 0.461365
Epoch 4.90: Loss = 0.586777
Epoch 4.91: Loss = 0.478943
Epoch 4.92: Loss = 0.539719
Epoch 4.93: Loss = 0.529221
Epoch 4.94: Loss = 0.459091
Epoch 4.95: Loss = 0.557831
Epoch 4.96: Loss = 0.434418
Epoch 4.97: Loss = 0.4375
Epoch 4.98: Loss = 0.558655
Epoch 4.99: Loss = 0.579483
Epoch 4.100: Loss = 0.57251
Epoch 4.101: Loss = 0.595001
Epoch 4.102: Loss = 0.384369
Epoch 4.103: Loss = 0.50238
Epoch 4.104: Loss = 0.487427
Epoch 4.105: Loss = 0.543503
Epoch 4.106: Loss = 0.526672
Epoch 4.107: Loss = 0.515671
Epoch 4.108: Loss = 0.446655
Epoch 4.109: Loss = 0.642715
Epoch 4.110: Loss = 0.482712
Epoch 4.111: Loss = 0.497513
Epoch 4.112: Loss = 0.583359
Epoch 4.113: Loss = 0.551514
Epoch 4.114: Loss = 0.67247
Epoch 4.115: Loss = 0.463623
Epoch 4.116: Loss = 0.469086
Epoch 4.117: Loss = 0.638367
Epoch 4.118: Loss = 0.502151
Epoch 4.119: Loss = 0.685913
Epoch 4.120: Loss = 0.517975
TRAIN LOSS = 0.50386
TRAIN ACC = 85.9924 % (51597/60000)
Loss = 0.487717
Loss = 0.534027
Loss = 0.679138
Loss = 0.646088
Loss = 0.697952
Loss = 0.530212
Loss = 0.469925
Loss = 0.799545
Loss = 0.682465
Loss = 0.61377
Loss = 0.254715
Loss = 0.412781
Loss = 0.343857
Loss = 0.47403
Loss = 0.268631
Loss = 0.393646
Loss = 0.278183
Loss = 0.0942841
Loss = 0.264374
Loss = 0.658249
TEST LOSS = 0.479179
TEST ACC = 515.97 % (8709/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.554932
Epoch 5.2: Loss = 0.402939
Epoch 5.3: Loss = 0.623276
Epoch 5.4: Loss = 0.52739
Epoch 5.5: Loss = 0.432007
Epoch 5.6: Loss = 0.57402
Epoch 5.7: Loss = 0.775162
Epoch 5.8: Loss = 0.438126
Epoch 5.9: Loss = 0.369659
Epoch 5.10: Loss = 0.413635
Epoch 5.11: Loss = 0.584274
Epoch 5.12: Loss = 0.570404
Epoch 5.13: Loss = 0.530228
Epoch 5.14: Loss = 0.399673
Epoch 5.15: Loss = 0.547897
Epoch 5.16: Loss = 0.485764
Epoch 5.17: Loss = 0.527939
Epoch 5.18: Loss = 0.503815
Epoch 5.19: Loss = 0.566269
Epoch 5.20: Loss = 0.473709
Epoch 5.21: Loss = 0.430573
Epoch 5.22: Loss = 0.460358
Epoch 5.23: Loss = 0.444473
Epoch 5.24: Loss = 0.486053
Epoch 5.25: Loss = 0.644806
Epoch 5.26: Loss = 0.495422
Epoch 5.27: Loss = 0.338516
Epoch 5.28: Loss = 0.413834
Epoch 5.29: Loss = 0.483109
Epoch 5.30: Loss = 0.507507
Epoch 5.31: Loss = 0.518219
Epoch 5.32: Loss = 0.564911
Epoch 5.33: Loss = 0.610733
Epoch 5.34: Loss = 0.651932
Epoch 5.35: Loss = 0.548508
Epoch 5.36: Loss = 0.50293
Epoch 5.37: Loss = 0.363083
Epoch 5.38: Loss = 0.611557
Epoch 5.39: Loss = 0.559662
Epoch 5.40: Loss = 0.431992
Epoch 5.41: Loss = 0.554031
Epoch 5.42: Loss = 0.392654
Epoch 5.43: Loss = 0.505936
Epoch 5.44: Loss = 0.451202
Epoch 5.45: Loss = 0.519608
Epoch 5.46: Loss = 0.577972
Epoch 5.47: Loss = 0.565369
Epoch 5.48: Loss = 0.6026
Epoch 5.49: Loss = 0.472946
Epoch 5.50: Loss = 0.626053
Epoch 5.51: Loss = 0.535934
Epoch 5.52: Loss = 0.412521
Epoch 5.53: Loss = 0.473663
Epoch 5.54: Loss = 0.432968
Epoch 5.55: Loss = 0.61853
Epoch 5.56: Loss = 0.511292
Epoch 5.57: Loss = 0.500763
Epoch 5.58: Loss = 0.517975
Epoch 5.59: Loss = 0.593674
Epoch 5.60: Loss = 0.48848
Epoch 5.61: Loss = 0.536591
Epoch 5.62: Loss = 0.536102
Epoch 5.63: Loss = 0.594452
Epoch 5.64: Loss = 0.461121
Epoch 5.65: Loss = 0.515289
Epoch 5.66: Loss = 0.541718
Epoch 5.67: Loss = 0.535004
Epoch 5.68: Loss = 0.631302
Epoch 5.69: Loss = 0.506836
Epoch 5.70: Loss = 0.618393
Epoch 5.71: Loss = 0.583481
Epoch 5.72: Loss = 0.508942
Epoch 5.73: Loss = 0.56076
Epoch 5.74: Loss = 0.591293
Epoch 5.75: Loss = 0.569855
Epoch 5.76: Loss = 0.525208
Epoch 5.77: Loss = 0.495621
Epoch 5.78: Loss = 0.574966
Epoch 5.79: Loss = 0.529373
Epoch 5.80: Loss = 0.476196
Epoch 5.81: Loss = 0.603104
Epoch 5.82: Loss = 0.534119
Epoch 5.83: Loss = 0.557709
Epoch 5.84: Loss = 0.59964
Epoch 5.85: Loss = 0.454819
Epoch 5.86: Loss = 0.537064
Epoch 5.87: Loss = 0.649979
Epoch 5.88: Loss = 0.507919
Epoch 5.89: Loss = 0.530136
Epoch 5.90: Loss = 0.496933
Epoch 5.91: Loss = 0.535416
Epoch 5.92: Loss = 0.596024
Epoch 5.93: Loss = 0.523407
Epoch 5.94: Loss = 0.557007
Epoch 5.95: Loss = 0.495483
Epoch 5.96: Loss = 0.386627
Epoch 5.97: Loss = 0.583252
Epoch 5.98: Loss = 0.519501
Epoch 5.99: Loss = 0.428223
Epoch 5.100: Loss = 0.45549
Epoch 5.101: Loss = 0.552719
Epoch 5.102: Loss = 0.472458
Epoch 5.103: Loss = 0.653397
Epoch 5.104: Loss = 0.335602
Epoch 5.105: Loss = 0.492661
Epoch 5.106: Loss = 0.57695
Epoch 5.107: Loss = 0.610504
Epoch 5.108: Loss = 0.376694
Epoch 5.109: Loss = 0.509201
Epoch 5.110: Loss = 0.542313
Epoch 5.111: Loss = 0.52034
Epoch 5.112: Loss = 0.619141
Epoch 5.113: Loss = 0.502472
Epoch 5.114: Loss = 0.601532
Epoch 5.115: Loss = 0.442795
Epoch 5.116: Loss = 0.487473
Epoch 5.117: Loss = 0.402527
Epoch 5.118: Loss = 0.558578
Epoch 5.119: Loss = 0.619568
Epoch 5.120: Loss = 0.422516
TRAIN LOSS = 0.520569
TRAIN ACC = 86.3571 % (51817/60000)
Loss = 0.530029
Loss = 0.563049
Loss = 0.695816
Loss = 0.722153
Loss = 0.725006
Loss = 0.568146
Loss = 0.467728
Loss = 0.841232
Loss = 0.677536
Loss = 0.691727
Loss = 0.282211
Loss = 0.447983
Loss = 0.326187
Loss = 0.482681
Loss = 0.311249
Loss = 0.472595
Loss = 0.311539
Loss = 0.09021
Loss = 0.29364
Loss = 0.730927
TEST LOSS = 0.511582
TEST ACC = 518.169 % (8684/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.451126
Epoch 6.2: Loss = 0.502838
Epoch 6.3: Loss = 0.571808
Epoch 6.4: Loss = 0.611298
Epoch 6.5: Loss = 0.569229
Epoch 6.6: Loss = 0.397324
Epoch 6.7: Loss = 0.564224
Epoch 6.8: Loss = 0.548447
Epoch 6.9: Loss = 0.482376
Epoch 6.10: Loss = 0.550888
Epoch 6.11: Loss = 0.583099
Epoch 6.12: Loss = 0.45816
Epoch 6.13: Loss = 0.596451
Epoch 6.14: Loss = 0.635925
Epoch 6.15: Loss = 0.605942
Epoch 6.16: Loss = 0.657501
Epoch 6.17: Loss = 0.339508
Epoch 6.18: Loss = 0.469788
Epoch 6.19: Loss = 0.563095
Epoch 6.20: Loss = 0.457703
Epoch 6.21: Loss = 0.570129
Epoch 6.22: Loss = 0.582764
Epoch 6.23: Loss = 0.489197
Epoch 6.24: Loss = 0.548462
Epoch 6.25: Loss = 0.538803
Epoch 6.26: Loss = 0.586761
Epoch 6.27: Loss = 0.752762
Epoch 6.28: Loss = 0.569046
Epoch 6.29: Loss = 0.525909
Epoch 6.30: Loss = 0.525131
Epoch 6.31: Loss = 0.511734
Epoch 6.32: Loss = 0.555435
Epoch 6.33: Loss = 0.491028
Epoch 6.34: Loss = 0.535049
Epoch 6.35: Loss = 0.467407
Epoch 6.36: Loss = 0.508987
Epoch 6.37: Loss = 0.445328
Epoch 6.38: Loss = 0.621796
Epoch 6.39: Loss = 0.597092
Epoch 6.40: Loss = 0.623672
Epoch 6.41: Loss = 0.438843
Epoch 6.42: Loss = 0.549255
Epoch 6.43: Loss = 0.447952
Epoch 6.44: Loss = 0.48822
Epoch 6.45: Loss = 0.530106
Epoch 6.46: Loss = 0.679535
Epoch 6.47: Loss = 0.464005
Epoch 6.48: Loss = 0.488052
Epoch 6.49: Loss = 0.580093
Epoch 6.50: Loss = 0.639709
Epoch 6.51: Loss = 0.574783
Epoch 6.52: Loss = 0.449753
Epoch 6.53: Loss = 0.595306
Epoch 6.54: Loss = 0.55864
Epoch 6.55: Loss = 0.504898
Epoch 6.56: Loss = 0.466995
Epoch 6.57: Loss = 0.699677
Epoch 6.58: Loss = 0.647522
Epoch 6.59: Loss = 0.583542
Epoch 6.60: Loss = 0.528458
Epoch 6.61: Loss = 0.521271
Epoch 6.62: Loss = 0.537399
Epoch 6.63: Loss = 0.633865
Epoch 6.64: Loss = 0.553848
Epoch 6.65: Loss = 0.606354
Epoch 6.66: Loss = 0.624084
Epoch 6.67: Loss = 0.597977
Epoch 6.68: Loss = 0.607346
Epoch 6.69: Loss = 0.599228
Epoch 6.70: Loss = 0.499649
Epoch 6.71: Loss = 0.571915
Epoch 6.72: Loss = 0.567017
Epoch 6.73: Loss = 0.493149
Epoch 6.74: Loss = 0.506332
Epoch 6.75: Loss = 0.449554
Epoch 6.76: Loss = 0.563354
Epoch 6.77: Loss = 0.453247
Epoch 6.78: Loss = 0.592148
Epoch 6.79: Loss = 0.602127
Epoch 6.80: Loss = 0.516693
Epoch 6.81: Loss = 0.558075
Epoch 6.82: Loss = 0.538269
Epoch 6.83: Loss = 0.508347
Epoch 6.84: Loss = 0.506927
Epoch 6.85: Loss = 0.469086
Epoch 6.86: Loss = 0.582932
Epoch 6.87: Loss = 0.499191
Epoch 6.88: Loss = 0.484573
Epoch 6.89: Loss = 0.626343
Epoch 6.90: Loss = 0.432175
Epoch 6.91: Loss = 0.579361
Epoch 6.92: Loss = 0.691299
Epoch 6.93: Loss = 0.592926
Epoch 6.94: Loss = 0.548431
Epoch 6.95: Loss = 0.56842
Epoch 6.96: Loss = 0.59935
Epoch 6.97: Loss = 0.552505
Epoch 6.98: Loss = 0.576645
Epoch 6.99: Loss = 0.447205
Epoch 6.100: Loss = 0.585327
Epoch 6.101: Loss = 0.516296
Epoch 6.102: Loss = 0.473969
Epoch 6.103: Loss = 0.510559
Epoch 6.104: Loss = 0.504013
Epoch 6.105: Loss = 0.475342
Epoch 6.106: Loss = 0.629044
Epoch 6.107: Loss = 0.479416
Epoch 6.108: Loss = 0.537399
Epoch 6.109: Loss = 0.432861
Epoch 6.110: Loss = 0.540817
Epoch 6.111: Loss = 0.562805
Epoch 6.112: Loss = 0.471451
Epoch 6.113: Loss = 0.607285
Epoch 6.114: Loss = 0.524155
Epoch 6.115: Loss = 0.541855
Epoch 6.116: Loss = 0.660431
Epoch 6.117: Loss = 0.486008
Epoch 6.118: Loss = 0.62117
Epoch 6.119: Loss = 0.50592
Epoch 6.120: Loss = 0.515503
TRAIN LOSS = 0.543503
TRAIN ACC = 86.1969 % (51720/60000)
Loss = 0.551208
Loss = 0.544785
Loss = 0.766022
Loss = 0.741714
Loss = 0.764832
Loss = 0.550537
Loss = 0.494339
Loss = 0.824524
Loss = 0.651077
Loss = 0.723206
Loss = 0.250183
Loss = 0.45433
Loss = 0.349121
Loss = 0.484299
Loss = 0.323593
Loss = 0.381821
Loss = 0.239655
Loss = 0.0591278
Loss = 0.284424
Loss = 0.73259
TEST LOSS = 0.508569
TEST ACC = 517.2 % (8736/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.519379
Epoch 7.2: Loss = 0.541504
Epoch 7.3: Loss = 0.52478
Epoch 7.4: Loss = 0.782639
Epoch 7.5: Loss = 0.513687
Epoch 7.6: Loss = 0.565842
Epoch 7.7: Loss = 0.63327
Epoch 7.8: Loss = 0.449097
Epoch 7.9: Loss = 0.60347
Epoch 7.10: Loss = 0.544998
Epoch 7.11: Loss = 0.481583
Epoch 7.12: Loss = 0.426743
Epoch 7.13: Loss = 0.545624
Epoch 7.14: Loss = 0.42308
Epoch 7.15: Loss = 0.535263
Epoch 7.16: Loss = 0.515488
Epoch 7.17: Loss = 0.484085
Epoch 7.18: Loss = 0.615295
Epoch 7.19: Loss = 0.69519
Epoch 7.20: Loss = 0.468857
Epoch 7.21: Loss = 0.516357
Epoch 7.22: Loss = 0.677689
Epoch 7.23: Loss = 0.600052
Epoch 7.24: Loss = 0.66806
Epoch 7.25: Loss = 0.492538
Epoch 7.26: Loss = 0.537277
Epoch 7.27: Loss = 0.590302
Epoch 7.28: Loss = 0.489349
Epoch 7.29: Loss = 0.571823
Epoch 7.30: Loss = 0.423859
Epoch 7.31: Loss = 0.72316
Epoch 7.32: Loss = 0.615662
Epoch 7.33: Loss = 0.7164
Epoch 7.34: Loss = 0.562531
Epoch 7.35: Loss = 0.632248
Epoch 7.36: Loss = 0.494385
Epoch 7.37: Loss = 0.563263
Epoch 7.38: Loss = 0.642136
Epoch 7.39: Loss = 0.572449
Epoch 7.40: Loss = 0.601959
Epoch 7.41: Loss = 0.519226
Epoch 7.42: Loss = 0.660736
Epoch 7.43: Loss = 0.524551
Epoch 7.44: Loss = 0.697571
Epoch 7.45: Loss = 0.469971
Epoch 7.46: Loss = 0.562164
Epoch 7.47: Loss = 0.636063
Epoch 7.48: Loss = 0.535126
Epoch 7.49: Loss = 0.599136
Epoch 7.50: Loss = 0.485474
Epoch 7.51: Loss = 0.516449
Epoch 7.52: Loss = 0.493851
Epoch 7.53: Loss = 0.529907
Epoch 7.54: Loss = 0.622971
Epoch 7.55: Loss = 0.57283
Epoch 7.56: Loss = 0.554916
Epoch 7.57: Loss = 0.605804
Epoch 7.58: Loss = 0.597702
Epoch 7.59: Loss = 0.604141
Epoch 7.60: Loss = 0.649872
Epoch 7.61: Loss = 0.568253
Epoch 7.62: Loss = 0.512436
Epoch 7.63: Loss = 0.610565
Epoch 7.64: Loss = 0.6436
Epoch 7.65: Loss = 0.703903
Epoch 7.66: Loss = 0.467896
Epoch 7.67: Loss = 0.556076
Epoch 7.68: Loss = 0.548981
Epoch 7.69: Loss = 0.507385
Epoch 7.70: Loss = 0.625641
Epoch 7.71: Loss = 0.542862
Epoch 7.72: Loss = 0.741653
Epoch 7.73: Loss = 0.647171
Epoch 7.74: Loss = 0.567398
Epoch 7.75: Loss = 0.548706
Epoch 7.76: Loss = 0.500839
Epoch 7.77: Loss = 0.671356
Epoch 7.78: Loss = 0.635101
Epoch 7.79: Loss = 0.480118
Epoch 7.80: Loss = 0.753601
Epoch 7.81: Loss = 0.445145
Epoch 7.82: Loss = 0.521301
Epoch 7.83: Loss = 0.740417
Epoch 7.84: Loss = 0.496735
Epoch 7.85: Loss = 0.603531
Epoch 7.86: Loss = 0.614105
Epoch 7.87: Loss = 0.517731
Epoch 7.88: Loss = 0.604263
Epoch 7.89: Loss = 0.571457
Epoch 7.90: Loss = 0.666489
Epoch 7.91: Loss = 0.623123
Epoch 7.92: Loss = 0.522003
Epoch 7.93: Loss = 0.566971
Epoch 7.94: Loss = 0.574387
Epoch 7.95: Loss = 0.545807
Epoch 7.96: Loss = 0.603653
Epoch 7.97: Loss = 0.623428
Epoch 7.98: Loss = 0.667252
Epoch 7.99: Loss = 0.637741
Epoch 7.100: Loss = 0.512924
Epoch 7.101: Loss = 0.554993
Epoch 7.102: Loss = 0.573105
Epoch 7.103: Loss = 0.559769
Epoch 7.104: Loss = 0.477112
Epoch 7.105: Loss = 0.605499
Epoch 7.106: Loss = 0.52124
Epoch 7.107: Loss = 0.538879
Epoch 7.108: Loss = 0.663284
Epoch 7.109: Loss = 0.537766
Epoch 7.110: Loss = 0.452698
Epoch 7.111: Loss = 0.585129
Epoch 7.112: Loss = 0.488922
Epoch 7.113: Loss = 0.679077
Epoch 7.114: Loss = 0.605209
Epoch 7.115: Loss = 0.572525
Epoch 7.116: Loss = 0.627625
Epoch 7.117: Loss = 0.580902
Epoch 7.118: Loss = 0.484528
Epoch 7.119: Loss = 0.572021
Epoch 7.120: Loss = 0.526535
TRAIN LOSS = 0.572495
TRAIN ACC = 86.1984 % (51721/60000)
Loss = 0.590576
Loss = 0.668991
Loss = 0.794434
Loss = 0.800415
Loss = 0.791504
Loss = 0.602753
Loss = 0.553802
Loss = 0.922516
Loss = 0.666565
Loss = 0.768219
Loss = 0.328888
Loss = 0.518982
Loss = 0.413452
Loss = 0.520111
Loss = 0.343582
Loss = 0.391678
Loss = 0.281342
Loss = 0.0704803
Loss = 0.339294
Loss = 0.841385
TEST LOSS = 0.560448
TEST ACC = 517.209 % (8638/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.515533
Epoch 8.2: Loss = 0.547745
Epoch 8.3: Loss = 0.634689
Epoch 8.4: Loss = 0.594711
Epoch 8.5: Loss = 0.613724
Epoch 8.6: Loss = 0.552185
Epoch 8.7: Loss = 0.69162
Epoch 8.8: Loss = 0.601913
Epoch 8.9: Loss = 0.623611
Epoch 8.10: Loss = 0.450775
Epoch 8.11: Loss = 0.797974
Epoch 8.12: Loss = 0.516434
Epoch 8.13: Loss = 0.469025
Epoch 8.14: Loss = 0.513885
Epoch 8.15: Loss = 0.553345
Epoch 8.16: Loss = 0.560272
Epoch 8.17: Loss = 0.573532
Epoch 8.18: Loss = 0.515579
Epoch 8.19: Loss = 0.57196
Epoch 8.20: Loss = 0.457932
Epoch 8.21: Loss = 0.689957
Epoch 8.22: Loss = 0.614746
Epoch 8.23: Loss = 0.636932
Epoch 8.24: Loss = 0.557602
Epoch 8.25: Loss = 0.532898
Epoch 8.26: Loss = 0.546097
Epoch 8.27: Loss = 0.573929
Epoch 8.28: Loss = 0.470596
Epoch 8.29: Loss = 0.667801
Epoch 8.30: Loss = 0.509766
Epoch 8.31: Loss = 0.669922
Epoch 8.32: Loss = 0.625519
Epoch 8.33: Loss = 0.602707
Epoch 8.34: Loss = 0.604843
Epoch 8.35: Loss = 0.656052
Epoch 8.36: Loss = 0.85939
Epoch 8.37: Loss = 0.583115
Epoch 8.38: Loss = 0.630493
Epoch 8.39: Loss = 0.656418
Epoch 8.40: Loss = 0.577042
Epoch 8.41: Loss = 0.527115
Epoch 8.42: Loss = 0.644836
Epoch 8.43: Loss = 0.637451
Epoch 8.44: Loss = 0.537567
Epoch 8.45: Loss = 0.613571
Epoch 8.46: Loss = 0.528427
Epoch 8.47: Loss = 0.535446
Epoch 8.48: Loss = 0.5905
Epoch 8.49: Loss = 0.519577
Epoch 8.50: Loss = 0.552887
Epoch 8.51: Loss = 0.580551
Epoch 8.52: Loss = 0.815125
Epoch 8.53: Loss = 0.685577
Epoch 8.54: Loss = 0.676025
Epoch 8.55: Loss = 0.649384
Epoch 8.56: Loss = 0.70845
Epoch 8.57: Loss = 0.594269
Epoch 8.58: Loss = 0.55983
Epoch 8.59: Loss = 0.533371
Epoch 8.60: Loss = 0.666138
Epoch 8.61: Loss = 0.592468
Epoch 8.62: Loss = 0.658203
Epoch 8.63: Loss = 0.688705
Epoch 8.64: Loss = 0.590469
Epoch 8.65: Loss = 0.583115
Epoch 8.66: Loss = 0.562881
Epoch 8.67: Loss = 0.656204
Epoch 8.68: Loss = 0.597519
Epoch 8.69: Loss = 0.654221
Epoch 8.70: Loss = 0.679413
Epoch 8.71: Loss = 0.477524
Epoch 8.72: Loss = 0.573181
Epoch 8.73: Loss = 0.683365
Epoch 8.74: Loss = 0.606628
Epoch 8.75: Loss = 0.562546
Epoch 8.76: Loss = 0.612595
Epoch 8.77: Loss = 0.620483
Epoch 8.78: Loss = 0.68306
Epoch 8.79: Loss = 0.654266
Epoch 8.80: Loss = 0.569183
Epoch 8.81: Loss = 0.655365
Epoch 8.82: Loss = 0.513702
Epoch 8.83: Loss = 0.644852
Epoch 8.84: Loss = 0.662033
Epoch 8.85: Loss = 0.576752
Epoch 8.86: Loss = 0.537964
Epoch 8.87: Loss = 0.619064
Epoch 8.88: Loss = 0.467331
Epoch 8.89: Loss = 0.716873
Epoch 8.90: Loss = 0.579346
Epoch 8.91: Loss = 0.552444
Epoch 8.92: Loss = 0.538422
Epoch 8.93: Loss = 0.538284
Epoch 8.94: Loss = 0.595963
Epoch 8.95: Loss = 0.669235
Epoch 8.96: Loss = 0.599762
Epoch 8.97: Loss = 0.527573
Epoch 8.98: Loss = 0.512955
Epoch 8.99: Loss = 0.602585
Epoch 8.100: Loss = 0.498138
Epoch 8.101: Loss = 0.63475
Epoch 8.102: Loss = 0.668335
Epoch 8.103: Loss = 0.585434
Epoch 8.104: Loss = 0.522644
Epoch 8.105: Loss = 0.552917
Epoch 8.106: Loss = 0.58078
Epoch 8.107: Loss = 0.437515
Epoch 8.108: Loss = 0.613602
Epoch 8.109: Loss = 0.487167
Epoch 8.110: Loss = 0.724777
Epoch 8.111: Loss = 0.464417
Epoch 8.112: Loss = 0.587021
Epoch 8.113: Loss = 0.573853
Epoch 8.114: Loss = 0.517212
Epoch 8.115: Loss = 0.612946
Epoch 8.116: Loss = 0.575592
Epoch 8.117: Loss = 0.524612
Epoch 8.118: Loss = 0.834991
Epoch 8.119: Loss = 0.49791
Epoch 8.120: Loss = 0.688904
TRAIN LOSS = 0.594849
TRAIN ACC = 85.9512 % (51573/60000)
Loss = 0.618698
Loss = 0.695679
Loss = 0.844528
Loss = 0.831039
Loss = 0.821518
Loss = 0.623642
Loss = 0.585724
Loss = 1.01962
Loss = 0.75032
Loss = 0.820648
Loss = 0.284515
Loss = 0.557358
Loss = 0.414352
Loss = 0.506439
Loss = 0.299194
Loss = 0.406113
Loss = 0.274902
Loss = 0.0794678
Loss = 0.331573
Loss = 0.792206
TEST LOSS = 0.577877
TEST ACC = 515.729 % (8664/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.486877
Epoch 9.2: Loss = 0.3759
Epoch 9.3: Loss = 0.582794
Epoch 9.4: Loss = 0.559235
Epoch 9.5: Loss = 0.685959
Epoch 9.6: Loss = 0.48761
Epoch 9.7: Loss = 0.66095
Epoch 9.8: Loss = 0.471039
Epoch 9.9: Loss = 0.494263
Epoch 9.10: Loss = 0.709579
Epoch 9.11: Loss = 0.61937
Epoch 9.12: Loss = 0.547607
Epoch 9.13: Loss = 0.548782
Epoch 9.14: Loss = 0.588684
Epoch 9.15: Loss = 0.527039
Epoch 9.16: Loss = 0.562714
Epoch 9.17: Loss = 0.556702
Epoch 9.18: Loss = 0.483902
Epoch 9.19: Loss = 0.723709
Epoch 9.20: Loss = 0.668335
Epoch 9.21: Loss = 0.582062
Epoch 9.22: Loss = 0.629013
Epoch 9.23: Loss = 0.695023
Epoch 9.24: Loss = 0.579163
Epoch 9.25: Loss = 0.511795
Epoch 9.26: Loss = 0.71199
Epoch 9.27: Loss = 0.7052
Epoch 9.28: Loss = 0.821442
Epoch 9.29: Loss = 0.664459
Epoch 9.30: Loss = 0.604843
Epoch 9.31: Loss = 0.651245
Epoch 9.32: Loss = 0.773026
Epoch 9.33: Loss = 0.583511
Epoch 9.34: Loss = 0.556412
Epoch 9.35: Loss = 0.484238
Epoch 9.36: Loss = 0.607193
Epoch 9.37: Loss = 0.64183
Epoch 9.38: Loss = 0.592697
Epoch 9.39: Loss = 0.539398
Epoch 9.40: Loss = 0.49028
Epoch 9.41: Loss = 0.563812
Epoch 9.42: Loss = 0.504822
Epoch 9.43: Loss = 0.673447
Epoch 9.44: Loss = 0.678635
Epoch 9.45: Loss = 0.45462
Epoch 9.46: Loss = 0.501968
Epoch 9.47: Loss = 0.494766
Epoch 9.48: Loss = 0.742523
Epoch 9.49: Loss = 0.542542
Epoch 9.50: Loss = 0.49028
Epoch 9.51: Loss = 0.621002
Epoch 9.52: Loss = 0.572739
Epoch 9.53: Loss = 0.794647
Epoch 9.54: Loss = 0.655945
Epoch 9.55: Loss = 0.530716
Epoch 9.56: Loss = 0.592682
Epoch 9.57: Loss = 0.56485
Epoch 9.58: Loss = 0.664749
Epoch 9.59: Loss = 0.645691
Epoch 9.60: Loss = 0.613403
Epoch 9.61: Loss = 0.598846
Epoch 9.62: Loss = 0.460495
Epoch 9.63: Loss = 0.567673
Epoch 9.64: Loss = 0.776352
Epoch 9.65: Loss = 0.663986
Epoch 9.66: Loss = 0.433472
Epoch 9.67: Loss = 0.755539
Epoch 9.68: Loss = 0.620743
Epoch 9.69: Loss = 0.722839
Epoch 9.70: Loss = 0.593811
Epoch 9.71: Loss = 0.597427
Epoch 9.72: Loss = 0.627762
Epoch 9.73: Loss = 0.594254
Epoch 9.74: Loss = 0.868546
Epoch 9.75: Loss = 0.547897
Epoch 9.76: Loss = 0.565216
Epoch 9.77: Loss = 0.666504
Epoch 9.78: Loss = 0.484329
Epoch 9.79: Loss = 0.55394
Epoch 9.80: Loss = 0.532333
Epoch 9.81: Loss = 0.633606
Epoch 9.82: Loss = 0.560043
Epoch 9.83: Loss = 0.724548
Epoch 9.84: Loss = 0.447861
Epoch 9.85: Loss = 0.592041
Epoch 9.86: Loss = 0.629868
Epoch 9.87: Loss = 0.720886
Epoch 9.88: Loss = 0.620132
Epoch 9.89: Loss = 0.514038
Epoch 9.90: Loss = 0.516235
Epoch 9.91: Loss = 0.490433
Epoch 9.92: Loss = 0.680283
Epoch 9.93: Loss = 0.634079
Epoch 9.94: Loss = 0.556152
Epoch 9.95: Loss = 0.492737
Epoch 9.96: Loss = 0.432907
Epoch 9.97: Loss = 0.50296
Epoch 9.98: Loss = 0.594086
Epoch 9.99: Loss = 0.539124
Epoch 9.100: Loss = 0.455124
Epoch 9.101: Loss = 0.739944
Epoch 9.102: Loss = 0.569443
Epoch 9.103: Loss = 0.582153
Epoch 9.104: Loss = 0.570343
Epoch 9.105: Loss = 0.625626
Epoch 9.106: Loss = 0.67717
Epoch 9.107: Loss = 0.618851
Epoch 9.108: Loss = 0.646225
Epoch 9.109: Loss = 0.607239
Epoch 9.110: Loss = 0.623734
Epoch 9.111: Loss = 0.453247
Epoch 9.112: Loss = 0.623947
Epoch 9.113: Loss = 0.543411
Epoch 9.114: Loss = 0.584381
Epoch 9.115: Loss = 0.508453
Epoch 9.116: Loss = 0.54068
Epoch 9.117: Loss = 0.641541
Epoch 9.118: Loss = 0.657379
Epoch 9.119: Loss = 0.66391
Epoch 9.120: Loss = 0.72551
TRAIN LOSS = 0.595367
TRAIN ACC = 86.4822 % (51892/60000)
Loss = 0.586456
Loss = 0.599564
Loss = 0.83403
Loss = 0.797531
Loss = 0.821793
Loss = 0.577301
Loss = 0.559906
Loss = 0.931427
Loss = 0.697937
Loss = 0.83522
Loss = 0.315475
Loss = 0.603058
Loss = 0.49501
Loss = 0.490082
Loss = 0.271149
Loss = 0.360519
Loss = 0.304596
Loss = 0.0971832
Loss = 0.291565
Loss = 0.820908
TEST LOSS = 0.564535
TEST ACC = 518.919 % (8700/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.791351
Epoch 10.2: Loss = 0.625885
Epoch 10.3: Loss = 0.451294
Epoch 10.4: Loss = 0.545822
Epoch 10.5: Loss = 0.733582
Epoch 10.6: Loss = 0.723297
Epoch 10.7: Loss = 0.58345
Epoch 10.8: Loss = 0.638596
Epoch 10.9: Loss = 0.588074
Epoch 10.10: Loss = 0.827255
Epoch 10.11: Loss = 0.664764
Epoch 10.12: Loss = 0.586441
Epoch 10.13: Loss = 0.565704
Epoch 10.14: Loss = 0.64772
Epoch 10.15: Loss = 0.627686
Epoch 10.16: Loss = 0.593506
Epoch 10.17: Loss = 0.482971
Epoch 10.18: Loss = 0.480423
Epoch 10.19: Loss = 0.50676
Epoch 10.20: Loss = 0.525482
Epoch 10.21: Loss = 0.707245
Epoch 10.22: Loss = 0.550156
Epoch 10.23: Loss = 0.731339
Epoch 10.24: Loss = 0.554001
Epoch 10.25: Loss = 0.697342
Epoch 10.26: Loss = 0.770462
Epoch 10.27: Loss = 0.565964
Epoch 10.28: Loss = 0.540115
Epoch 10.29: Loss = 0.502304
Epoch 10.30: Loss = 0.553696
Epoch 10.31: Loss = 0.529144
Epoch 10.32: Loss = 0.633438
Epoch 10.33: Loss = 0.714737
Epoch 10.34: Loss = 0.571289
Epoch 10.35: Loss = 0.633911
Epoch 10.36: Loss = 0.51915
Epoch 10.37: Loss = 0.460617
Epoch 10.38: Loss = 0.446136
Epoch 10.39: Loss = 0.713806
Epoch 10.40: Loss = 0.675949
Epoch 10.41: Loss = 0.542847
Epoch 10.42: Loss = 0.583496
Epoch 10.43: Loss = 0.571991
Epoch 10.44: Loss = 0.491577
Epoch 10.45: Loss = 0.516998
Epoch 10.46: Loss = 0.489182
Epoch 10.47: Loss = 0.628265
Epoch 10.48: Loss = 0.596024
Epoch 10.49: Loss = 0.606781
Epoch 10.50: Loss = 0.630447
Epoch 10.51: Loss = 0.640854
Epoch 10.52: Loss = 0.533554
Epoch 10.53: Loss = 0.466217
Epoch 10.54: Loss = 0.751511
Epoch 10.55: Loss = 0.644135
Epoch 10.56: Loss = 0.598755
Epoch 10.57: Loss = 0.705566
Epoch 10.58: Loss = 0.53125
Epoch 10.59: Loss = 0.664017
Epoch 10.60: Loss = 0.737732
Epoch 10.61: Loss = 0.56279
Epoch 10.62: Loss = 0.53154
Epoch 10.63: Loss = 0.642532
Epoch 10.64: Loss = 0.748199
Epoch 10.65: Loss = 0.574158
Epoch 10.66: Loss = 0.587296
Epoch 10.67: Loss = 0.471542
Epoch 10.68: Loss = 0.557327
Epoch 10.69: Loss = 0.593201
Epoch 10.70: Loss = 0.595306
Epoch 10.71: Loss = 0.780945
Epoch 10.72: Loss = 0.689926
Epoch 10.73: Loss = 0.643036
Epoch 10.74: Loss = 0.779465
Epoch 10.75: Loss = 0.553772
Epoch 10.76: Loss = 0.451004
Epoch 10.77: Loss = 0.673859
Epoch 10.78: Loss = 0.570236
Epoch 10.79: Loss = 0.64595
Epoch 10.80: Loss = 0.755219
Epoch 10.81: Loss = 0.568756
Epoch 10.82: Loss = 0.504364
Epoch 10.83: Loss = 0.713699
Epoch 10.84: Loss = 0.597778
Epoch 10.85: Loss = 0.57988
Epoch 10.86: Loss = 0.63298
Epoch 10.87: Loss = 0.676117
Epoch 10.88: Loss = 0.736679
Epoch 10.89: Loss = 0.613983
Epoch 10.90: Loss = 0.732346
Epoch 10.91: Loss = 0.653229
Epoch 10.92: Loss = 0.664261
Epoch 10.93: Loss = 0.64505
Epoch 10.94: Loss = 0.726852
Epoch 10.95: Loss = 0.745087
Epoch 10.96: Loss = 0.513519
Epoch 10.97: Loss = 0.676971
Epoch 10.98: Loss = 0.57373
Epoch 10.99: Loss = 0.486755
Epoch 10.100: Loss = 0.733765
Epoch 10.101: Loss = 0.690399
Epoch 10.102: Loss = 0.440842
Epoch 10.103: Loss = 0.694717
Epoch 10.104: Loss = 0.744751
Epoch 10.105: Loss = 0.558075
Epoch 10.106: Loss = 0.640121
Epoch 10.107: Loss = 0.612259
Epoch 10.108: Loss = 0.562988
Epoch 10.109: Loss = 0.468704
Epoch 10.110: Loss = 0.743515
Epoch 10.111: Loss = 0.802353
Epoch 10.112: Loss = 0.52272
Epoch 10.113: Loss = 0.671143
Epoch 10.114: Loss = 0.694427
Epoch 10.115: Loss = 0.791672
Epoch 10.116: Loss = 0.46907
Epoch 10.117: Loss = 0.566788
Epoch 10.118: Loss = 0.709442
Epoch 10.119: Loss = 0.567001
Epoch 10.120: Loss = 0.532455
TRAIN LOSS = 0.615509
TRAIN ACC = 86.3327 % (51802/60000)
Loss = 0.635895
Loss = 0.687637
Loss = 0.838409
Loss = 0.927917
Loss = 0.892197
Loss = 0.646835
Loss = 0.554504
Loss = 0.968002
Loss = 0.66803
Loss = 0.809402
Loss = 0.299469
Loss = 0.616333
Loss = 0.542892
Loss = 0.564636
Loss = 0.313736
Loss = 0.497925
Loss = 0.343796
Loss = 0.125458
Loss = 0.317108
Loss = 0.900223
TEST LOSS = 0.60752
TEST ACC = 518.019 % (8629/10000)

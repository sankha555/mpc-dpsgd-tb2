Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.33159
Epoch 1.2: Loss = 2.31311
Epoch 1.3: Loss = 2.2878
Epoch 1.4: Loss = 2.25261
Epoch 1.5: Loss = 2.21118
Epoch 1.6: Loss = 2.19295
Epoch 1.7: Loss = 2.16783
Epoch 1.8: Loss = 2.12085
Epoch 1.9: Loss = 2.10828
Epoch 1.10: Loss = 2.07375
Epoch 1.11: Loss = 2.05923
Epoch 1.12: Loss = 2.00916
Epoch 1.13: Loss = 2.00885
Epoch 1.14: Loss = 1.95596
Epoch 1.15: Loss = 1.91551
Epoch 1.16: Loss = 1.88707
Epoch 1.17: Loss = 1.85396
Epoch 1.18: Loss = 1.84233
Epoch 1.19: Loss = 1.78787
Epoch 1.20: Loss = 1.74117
Epoch 1.21: Loss = 1.74652
Epoch 1.22: Loss = 1.70158
Epoch 1.23: Loss = 1.67509
Epoch 1.24: Loss = 1.65616
Epoch 1.25: Loss = 1.62349
Epoch 1.26: Loss = 1.59525
Epoch 1.27: Loss = 1.57454
Epoch 1.28: Loss = 1.53175
Epoch 1.29: Loss = 1.50714
Epoch 1.30: Loss = 1.51486
Epoch 1.31: Loss = 1.52832
Epoch 1.32: Loss = 1.39751
Epoch 1.33: Loss = 1.4229
Epoch 1.34: Loss = 1.31244
Epoch 1.35: Loss = 1.38336
Epoch 1.36: Loss = 1.35147
Epoch 1.37: Loss = 1.31541
Epoch 1.38: Loss = 1.26953
Epoch 1.39: Loss = 1.24791
Epoch 1.40: Loss = 1.20532
Epoch 1.41: Loss = 1.23729
Epoch 1.42: Loss = 1.24071
Epoch 1.43: Loss = 1.21184
Epoch 1.44: Loss = 1.11339
Epoch 1.45: Loss = 1.13763
Epoch 1.46: Loss = 1.11729
Epoch 1.47: Loss = 1.0513
Epoch 1.48: Loss = 1.08208
Epoch 1.49: Loss = 1.04872
Epoch 1.50: Loss = 1.03802
Epoch 1.51: Loss = 0.98967
Epoch 1.52: Loss = 0.992203
Epoch 1.53: Loss = 0.933701
Epoch 1.54: Loss = 0.980942
Epoch 1.55: Loss = 0.992584
Epoch 1.56: Loss = 0.947144
Epoch 1.57: Loss = 0.90976
Epoch 1.58: Loss = 0.916061
Epoch 1.59: Loss = 0.960709
Epoch 1.60: Loss = 0.827164
Epoch 1.61: Loss = 0.933334
Epoch 1.62: Loss = 0.893036
Epoch 1.63: Loss = 0.86615
Epoch 1.64: Loss = 0.868454
Epoch 1.65: Loss = 0.869263
Epoch 1.66: Loss = 0.869141
Epoch 1.67: Loss = 0.849289
Epoch 1.68: Loss = 0.824402
Epoch 1.69: Loss = 0.827972
Epoch 1.70: Loss = 0.846954
Epoch 1.71: Loss = 0.756149
Epoch 1.72: Loss = 0.802704
Epoch 1.73: Loss = 0.78743
Epoch 1.74: Loss = 0.792206
Epoch 1.75: Loss = 0.738205
Epoch 1.76: Loss = 0.736435
Epoch 1.77: Loss = 0.781036
Epoch 1.78: Loss = 0.736603
Epoch 1.79: Loss = 0.725128
Epoch 1.80: Loss = 0.687363
Epoch 1.81: Loss = 0.726776
Epoch 1.82: Loss = 0.725449
Epoch 1.83: Loss = 0.766617
Epoch 1.84: Loss = 0.729111
Epoch 1.85: Loss = 0.697891
Epoch 1.86: Loss = 0.724991
Epoch 1.87: Loss = 0.770035
Epoch 1.88: Loss = 0.664886
Epoch 1.89: Loss = 0.7061
Epoch 1.90: Loss = 0.682938
Epoch 1.91: Loss = 0.740845
Epoch 1.92: Loss = 0.650696
Epoch 1.93: Loss = 0.673813
Epoch 1.94: Loss = 0.720169
Epoch 1.95: Loss = 0.637756
Epoch 1.96: Loss = 0.711365
Epoch 1.97: Loss = 0.651001
Epoch 1.98: Loss = 0.603836
Epoch 1.99: Loss = 0.649933
Epoch 1.100: Loss = 0.690338
Epoch 1.101: Loss = 0.582016
Epoch 1.102: Loss = 0.618774
Epoch 1.103: Loss = 0.599091
Epoch 1.104: Loss = 0.608749
Epoch 1.105: Loss = 0.652634
Epoch 1.106: Loss = 0.630127
Epoch 1.107: Loss = 0.58316
Epoch 1.108: Loss = 0.578247
Epoch 1.109: Loss = 0.681625
Epoch 1.110: Loss = 0.595779
Epoch 1.111: Loss = 0.550064
Epoch 1.112: Loss = 0.615967
Epoch 1.113: Loss = 0.539108
Epoch 1.114: Loss = 0.646423
Epoch 1.115: Loss = 0.577759
Epoch 1.116: Loss = 0.630936
Epoch 1.117: Loss = 0.679596
Epoch 1.118: Loss = 0.625305
Epoch 1.119: Loss = 0.667648
Epoch 1.120: Loss = 0.588303
TRAIN LOSS = 1.11482
TRAIN ACC = 69.3924 % (41638/60000)
Loss = 0.60878
Loss = 0.640335
Loss = 0.764587
Loss = 0.690079
Loss = 0.732941
Loss = 0.605942
Loss = 0.578979
Loss = 0.763535
Loss = 0.723938
Loss = 0.692657
Loss = 0.359253
Loss = 0.504654
Loss = 0.393158
Loss = 0.532852
Loss = 0.441666
Loss = 0.468353
Loss = 0.421616
Loss = 0.231796
Loss = 0.40947
Loss = 0.672195
TEST LOSS = 0.561839
TEST ACC = 416.379 % (8302/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.57312
Epoch 2.2: Loss = 0.578644
Epoch 2.3: Loss = 0.617142
Epoch 2.4: Loss = 0.545883
Epoch 2.5: Loss = 0.575943
Epoch 2.6: Loss = 0.571503
Epoch 2.7: Loss = 0.606522
Epoch 2.8: Loss = 0.591812
Epoch 2.9: Loss = 0.534515
Epoch 2.10: Loss = 0.541428
Epoch 2.11: Loss = 0.587143
Epoch 2.12: Loss = 0.612122
Epoch 2.13: Loss = 0.512466
Epoch 2.14: Loss = 0.61261
Epoch 2.15: Loss = 0.583282
Epoch 2.16: Loss = 0.588776
Epoch 2.17: Loss = 0.554398
Epoch 2.18: Loss = 0.556381
Epoch 2.19: Loss = 0.619064
Epoch 2.20: Loss = 0.598282
Epoch 2.21: Loss = 0.57077
Epoch 2.22: Loss = 0.568298
Epoch 2.23: Loss = 0.537659
Epoch 2.24: Loss = 0.602829
Epoch 2.25: Loss = 0.562653
Epoch 2.26: Loss = 0.579102
Epoch 2.27: Loss = 0.461395
Epoch 2.28: Loss = 0.553925
Epoch 2.29: Loss = 0.588638
Epoch 2.30: Loss = 0.500381
Epoch 2.31: Loss = 0.552444
Epoch 2.32: Loss = 0.533325
Epoch 2.33: Loss = 0.521637
Epoch 2.34: Loss = 0.473511
Epoch 2.35: Loss = 0.53772
Epoch 2.36: Loss = 0.490677
Epoch 2.37: Loss = 0.457901
Epoch 2.38: Loss = 0.513672
Epoch 2.39: Loss = 0.531464
Epoch 2.40: Loss = 0.527786
Epoch 2.41: Loss = 0.48259
Epoch 2.42: Loss = 0.605423
Epoch 2.43: Loss = 0.597015
Epoch 2.44: Loss = 0.50087
Epoch 2.45: Loss = 0.553528
Epoch 2.46: Loss = 0.527908
Epoch 2.47: Loss = 0.478531
Epoch 2.48: Loss = 0.524017
Epoch 2.49: Loss = 0.521973
Epoch 2.50: Loss = 0.484711
Epoch 2.51: Loss = 0.581238
Epoch 2.52: Loss = 0.50119
Epoch 2.53: Loss = 0.53093
Epoch 2.54: Loss = 0.548264
Epoch 2.55: Loss = 0.488754
Epoch 2.56: Loss = 0.508194
Epoch 2.57: Loss = 0.539261
Epoch 2.58: Loss = 0.530045
Epoch 2.59: Loss = 0.463959
Epoch 2.60: Loss = 0.526459
Epoch 2.61: Loss = 0.456085
Epoch 2.62: Loss = 0.524597
Epoch 2.63: Loss = 0.492523
Epoch 2.64: Loss = 0.524872
Epoch 2.65: Loss = 0.499023
Epoch 2.66: Loss = 0.515503
Epoch 2.67: Loss = 0.457428
Epoch 2.68: Loss = 0.443909
Epoch 2.69: Loss = 0.509155
Epoch 2.70: Loss = 0.53598
Epoch 2.71: Loss = 0.477646
Epoch 2.72: Loss = 0.527756
Epoch 2.73: Loss = 0.468979
Epoch 2.74: Loss = 0.48082
Epoch 2.75: Loss = 0.474609
Epoch 2.76: Loss = 0.506195
Epoch 2.77: Loss = 0.454224
Epoch 2.78: Loss = 0.439301
Epoch 2.79: Loss = 0.466217
Epoch 2.80: Loss = 0.437881
Epoch 2.81: Loss = 0.489777
Epoch 2.82: Loss = 0.456497
Epoch 2.83: Loss = 0.515961
Epoch 2.84: Loss = 0.462357
Epoch 2.85: Loss = 0.578339
Epoch 2.86: Loss = 0.574219
Epoch 2.87: Loss = 0.432465
Epoch 2.88: Loss = 0.459702
Epoch 2.89: Loss = 0.496658
Epoch 2.90: Loss = 0.48381
Epoch 2.91: Loss = 0.504898
Epoch 2.92: Loss = 0.456314
Epoch 2.93: Loss = 0.480026
Epoch 2.94: Loss = 0.577499
Epoch 2.95: Loss = 0.528915
Epoch 2.96: Loss = 0.49913
Epoch 2.97: Loss = 0.549606
Epoch 2.98: Loss = 0.476288
Epoch 2.99: Loss = 0.527847
Epoch 2.100: Loss = 0.47525
Epoch 2.101: Loss = 0.466705
Epoch 2.102: Loss = 0.4832
Epoch 2.103: Loss = 0.490509
Epoch 2.104: Loss = 0.477692
Epoch 2.105: Loss = 0.512558
Epoch 2.106: Loss = 0.440323
Epoch 2.107: Loss = 0.466629
Epoch 2.108: Loss = 0.491791
Epoch 2.109: Loss = 0.397583
Epoch 2.110: Loss = 0.460495
Epoch 2.111: Loss = 0.51329
Epoch 2.112: Loss = 0.414032
Epoch 2.113: Loss = 0.487061
Epoch 2.114: Loss = 0.444794
Epoch 2.115: Loss = 0.522369
Epoch 2.116: Loss = 0.477432
Epoch 2.117: Loss = 0.439255
Epoch 2.118: Loss = 0.481064
Epoch 2.119: Loss = 0.523865
Epoch 2.120: Loss = 0.507309
TRAIN LOSS = 0.516159
TRAIN ACC = 84.0927 % (50458/60000)
Loss = 0.466156
Loss = 0.536255
Loss = 0.648224
Loss = 0.574921
Loss = 0.650131
Loss = 0.465454
Loss = 0.436752
Loss = 0.65712
Loss = 0.601791
Loss = 0.593216
Loss = 0.213745
Loss = 0.40744
Loss = 0.321777
Loss = 0.414261
Loss = 0.309143
Loss = 0.325729
Loss = 0.276596
Loss = 0.113968
Loss = 0.303284
Loss = 0.603241
TEST LOSS = 0.44596
TEST ACC = 504.579 % (8651/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.44458
Epoch 3.2: Loss = 0.520309
Epoch 3.3: Loss = 0.460251
Epoch 3.4: Loss = 0.520721
Epoch 3.5: Loss = 0.441788
Epoch 3.6: Loss = 0.417053
Epoch 3.7: Loss = 0.493317
Epoch 3.8: Loss = 0.474213
Epoch 3.9: Loss = 0.510498
Epoch 3.10: Loss = 0.476425
Epoch 3.11: Loss = 0.400009
Epoch 3.12: Loss = 0.440155
Epoch 3.13: Loss = 0.426758
Epoch 3.14: Loss = 0.441101
Epoch 3.15: Loss = 0.440094
Epoch 3.16: Loss = 0.435211
Epoch 3.17: Loss = 0.441803
Epoch 3.18: Loss = 0.433609
Epoch 3.19: Loss = 0.601563
Epoch 3.20: Loss = 0.408569
Epoch 3.21: Loss = 0.438538
Epoch 3.22: Loss = 0.465927
Epoch 3.23: Loss = 0.491608
Epoch 3.24: Loss = 0.419861
Epoch 3.25: Loss = 0.471161
Epoch 3.26: Loss = 0.452789
Epoch 3.27: Loss = 0.513626
Epoch 3.28: Loss = 0.449234
Epoch 3.29: Loss = 0.392761
Epoch 3.30: Loss = 0.454773
Epoch 3.31: Loss = 0.419373
Epoch 3.32: Loss = 0.457321
Epoch 3.33: Loss = 0.412949
Epoch 3.34: Loss = 0.484207
Epoch 3.35: Loss = 0.478561
Epoch 3.36: Loss = 0.447693
Epoch 3.37: Loss = 0.457611
Epoch 3.38: Loss = 0.453293
Epoch 3.39: Loss = 0.508331
Epoch 3.40: Loss = 0.416016
Epoch 3.41: Loss = 0.466064
Epoch 3.42: Loss = 0.420685
Epoch 3.43: Loss = 0.445663
Epoch 3.44: Loss = 0.403168
Epoch 3.45: Loss = 0.532288
Epoch 3.46: Loss = 0.559662
Epoch 3.47: Loss = 0.47699
Epoch 3.48: Loss = 0.463898
Epoch 3.49: Loss = 0.450027
Epoch 3.50: Loss = 0.420914
Epoch 3.51: Loss = 0.455521
Epoch 3.52: Loss = 0.447647
Epoch 3.53: Loss = 0.457382
Epoch 3.54: Loss = 0.449585
Epoch 3.55: Loss = 0.549667
Epoch 3.56: Loss = 0.493896
Epoch 3.57: Loss = 0.388641
Epoch 3.58: Loss = 0.50119
Epoch 3.59: Loss = 0.403259
Epoch 3.60: Loss = 0.452209
Epoch 3.61: Loss = 0.455688
Epoch 3.62: Loss = 0.441696
Epoch 3.63: Loss = 0.476654
Epoch 3.64: Loss = 0.422745
Epoch 3.65: Loss = 0.426575
Epoch 3.66: Loss = 0.485275
Epoch 3.67: Loss = 0.428558
Epoch 3.68: Loss = 0.567902
Epoch 3.69: Loss = 0.503845
Epoch 3.70: Loss = 0.535416
Epoch 3.71: Loss = 0.411621
Epoch 3.72: Loss = 0.390045
Epoch 3.73: Loss = 0.456192
Epoch 3.74: Loss = 0.511032
Epoch 3.75: Loss = 0.439911
Epoch 3.76: Loss = 0.444473
Epoch 3.77: Loss = 0.492081
Epoch 3.78: Loss = 0.449127
Epoch 3.79: Loss = 0.460022
Epoch 3.80: Loss = 0.505539
Epoch 3.81: Loss = 0.510147
Epoch 3.82: Loss = 0.502045
Epoch 3.83: Loss = 0.575989
Epoch 3.84: Loss = 0.48877
Epoch 3.85: Loss = 0.571671
Epoch 3.86: Loss = 0.478256
Epoch 3.87: Loss = 0.507919
Epoch 3.88: Loss = 0.503067
Epoch 3.89: Loss = 0.445114
Epoch 3.90: Loss = 0.472672
Epoch 3.91: Loss = 0.521027
Epoch 3.92: Loss = 0.4841
Epoch 3.93: Loss = 0.361374
Epoch 3.94: Loss = 0.357224
Epoch 3.95: Loss = 0.398529
Epoch 3.96: Loss = 0.464325
Epoch 3.97: Loss = 0.473709
Epoch 3.98: Loss = 0.39183
Epoch 3.99: Loss = 0.377579
Epoch 3.100: Loss = 0.434875
Epoch 3.101: Loss = 0.427505
Epoch 3.102: Loss = 0.512466
Epoch 3.103: Loss = 0.432755
Epoch 3.104: Loss = 0.40033
Epoch 3.105: Loss = 0.442856
Epoch 3.106: Loss = 0.358139
Epoch 3.107: Loss = 0.509491
Epoch 3.108: Loss = 0.510437
Epoch 3.109: Loss = 0.432312
Epoch 3.110: Loss = 0.399399
Epoch 3.111: Loss = 0.550095
Epoch 3.112: Loss = 0.494781
Epoch 3.113: Loss = 0.3582
Epoch 3.114: Loss = 0.424866
Epoch 3.115: Loss = 0.390549
Epoch 3.116: Loss = 0.451599
Epoch 3.117: Loss = 0.512939
Epoch 3.118: Loss = 0.341583
Epoch 3.119: Loss = 0.485825
Epoch 3.120: Loss = 0.544037
TRAIN LOSS = 0.459641
TRAIN ACC = 86.4136 % (51850/60000)
Loss = 0.433365
Loss = 0.505173
Loss = 0.597321
Loss = 0.549286
Loss = 0.623764
Loss = 0.438034
Loss = 0.388931
Loss = 0.638107
Loss = 0.592026
Loss = 0.556198
Loss = 0.181076
Loss = 0.307327
Loss = 0.299118
Loss = 0.385468
Loss = 0.260818
Loss = 0.275955
Loss = 0.251007
Loss = 0.0788879
Loss = 0.250854
Loss = 0.58728
TEST LOSS = 0.41
TEST ACC = 518.5 % (8821/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.440979
Epoch 4.2: Loss = 0.488785
Epoch 4.3: Loss = 0.444244
Epoch 4.4: Loss = 0.429642
Epoch 4.5: Loss = 0.384384
Epoch 4.6: Loss = 0.4758
Epoch 4.7: Loss = 0.395905
Epoch 4.8: Loss = 0.466949
Epoch 4.9: Loss = 0.403763
Epoch 4.10: Loss = 0.395233
Epoch 4.11: Loss = 0.520233
Epoch 4.12: Loss = 0.413696
Epoch 4.13: Loss = 0.491013
Epoch 4.14: Loss = 0.434158
Epoch 4.15: Loss = 0.440491
Epoch 4.16: Loss = 0.483597
Epoch 4.17: Loss = 0.367615
Epoch 4.18: Loss = 0.395325
Epoch 4.19: Loss = 0.484222
Epoch 4.20: Loss = 0.456909
Epoch 4.21: Loss = 0.431534
Epoch 4.22: Loss = 0.499649
Epoch 4.23: Loss = 0.463074
Epoch 4.24: Loss = 0.408859
Epoch 4.25: Loss = 0.39566
Epoch 4.26: Loss = 0.378555
Epoch 4.27: Loss = 0.494385
Epoch 4.28: Loss = 0.471603
Epoch 4.29: Loss = 0.521042
Epoch 4.30: Loss = 0.487488
Epoch 4.31: Loss = 0.387833
Epoch 4.32: Loss = 0.441559
Epoch 4.33: Loss = 0.46991
Epoch 4.34: Loss = 0.462143
Epoch 4.35: Loss = 0.554672
Epoch 4.36: Loss = 0.421799
Epoch 4.37: Loss = 0.460526
Epoch 4.38: Loss = 0.414886
Epoch 4.39: Loss = 0.475998
Epoch 4.40: Loss = 0.521194
Epoch 4.41: Loss = 0.46347
Epoch 4.42: Loss = 0.359329
Epoch 4.43: Loss = 0.399521
Epoch 4.44: Loss = 0.499023
Epoch 4.45: Loss = 0.389954
Epoch 4.46: Loss = 0.548019
Epoch 4.47: Loss = 0.442764
Epoch 4.48: Loss = 0.413651
Epoch 4.49: Loss = 0.528793
Epoch 4.50: Loss = 0.459778
Epoch 4.51: Loss = 0.43074
Epoch 4.52: Loss = 0.409073
Epoch 4.53: Loss = 0.495712
Epoch 4.54: Loss = 0.469223
Epoch 4.55: Loss = 0.498718
Epoch 4.56: Loss = 0.478973
Epoch 4.57: Loss = 0.38562
Epoch 4.58: Loss = 0.41571
Epoch 4.59: Loss = 0.49707
Epoch 4.60: Loss = 0.459137
Epoch 4.61: Loss = 0.446274
Epoch 4.62: Loss = 0.436096
Epoch 4.63: Loss = 0.520645
Epoch 4.64: Loss = 0.440445
Epoch 4.65: Loss = 0.393539
Epoch 4.66: Loss = 0.397247
Epoch 4.67: Loss = 0.415771
Epoch 4.68: Loss = 0.418961
Epoch 4.69: Loss = 0.411911
Epoch 4.70: Loss = 0.496704
Epoch 4.71: Loss = 0.401703
Epoch 4.72: Loss = 0.37886
Epoch 4.73: Loss = 0.486908
Epoch 4.74: Loss = 0.411362
Epoch 4.75: Loss = 0.57637
Epoch 4.76: Loss = 0.438385
Epoch 4.77: Loss = 0.488495
Epoch 4.78: Loss = 0.448547
Epoch 4.79: Loss = 0.454407
Epoch 4.80: Loss = 0.434631
Epoch 4.81: Loss = 0.420929
Epoch 4.82: Loss = 0.357681
Epoch 4.83: Loss = 0.504623
Epoch 4.84: Loss = 0.435928
Epoch 4.85: Loss = 0.414673
Epoch 4.86: Loss = 0.318207
Epoch 4.87: Loss = 0.390991
Epoch 4.88: Loss = 0.553162
Epoch 4.89: Loss = 0.450592
Epoch 4.90: Loss = 0.436417
Epoch 4.91: Loss = 0.45784
Epoch 4.92: Loss = 0.526779
Epoch 4.93: Loss = 0.397034
Epoch 4.94: Loss = 0.396255
Epoch 4.95: Loss = 0.481491
Epoch 4.96: Loss = 0.387177
Epoch 4.97: Loss = 0.468491
Epoch 4.98: Loss = 0.471375
Epoch 4.99: Loss = 0.35228
Epoch 4.100: Loss = 0.521286
Epoch 4.101: Loss = 0.467651
Epoch 4.102: Loss = 0.540085
Epoch 4.103: Loss = 0.395691
Epoch 4.104: Loss = 0.469421
Epoch 4.105: Loss = 0.340118
Epoch 4.106: Loss = 0.555954
Epoch 4.107: Loss = 0.514725
Epoch 4.108: Loss = 0.392639
Epoch 4.109: Loss = 0.4617
Epoch 4.110: Loss = 0.427338
Epoch 4.111: Loss = 0.443924
Epoch 4.112: Loss = 0.416946
Epoch 4.113: Loss = 0.562408
Epoch 4.114: Loss = 0.507431
Epoch 4.115: Loss = 0.467911
Epoch 4.116: Loss = 0.456238
Epoch 4.117: Loss = 0.445724
Epoch 4.118: Loss = 0.395004
Epoch 4.119: Loss = 0.487061
Epoch 4.120: Loss = 0.369202
TRAIN LOSS = 0.448135
TRAIN ACC = 87.4802 % (52490/60000)
Loss = 0.44725
Loss = 0.538361
Loss = 0.643188
Loss = 0.58934
Loss = 0.642822
Loss = 0.470337
Loss = 0.413849
Loss = 0.729233
Loss = 0.651947
Loss = 0.598343
Loss = 0.169052
Loss = 0.353119
Loss = 0.324448
Loss = 0.389923
Loss = 0.237793
Loss = 0.303558
Loss = 0.232224
Loss = 0.0711975
Loss = 0.256836
Loss = 0.598053
TEST LOSS = 0.433044
TEST ACC = 524.899 % (8811/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.334579
Epoch 5.2: Loss = 0.411453
Epoch 5.3: Loss = 0.455856
Epoch 5.4: Loss = 0.417084
Epoch 5.5: Loss = 0.411789
Epoch 5.6: Loss = 0.419434
Epoch 5.7: Loss = 0.296829
Epoch 5.8: Loss = 0.584946
Epoch 5.9: Loss = 0.537476
Epoch 5.10: Loss = 0.443634
Epoch 5.11: Loss = 0.556274
Epoch 5.12: Loss = 0.477722
Epoch 5.13: Loss = 0.443192
Epoch 5.14: Loss = 0.522598
Epoch 5.15: Loss = 0.48291
Epoch 5.16: Loss = 0.395569
Epoch 5.17: Loss = 0.517456
Epoch 5.18: Loss = 0.497208
Epoch 5.19: Loss = 0.430359
Epoch 5.20: Loss = 0.533432
Epoch 5.21: Loss = 0.432983
Epoch 5.22: Loss = 0.379517
Epoch 5.23: Loss = 0.529449
Epoch 5.24: Loss = 0.454254
Epoch 5.25: Loss = 0.395416
Epoch 5.26: Loss = 0.37204
Epoch 5.27: Loss = 0.382568
Epoch 5.28: Loss = 0.481812
Epoch 5.29: Loss = 0.436554
Epoch 5.30: Loss = 0.380661
Epoch 5.31: Loss = 0.508591
Epoch 5.32: Loss = 0.481033
Epoch 5.33: Loss = 0.5047
Epoch 5.34: Loss = 0.450577
Epoch 5.35: Loss = 0.42131
Epoch 5.36: Loss = 0.386063
Epoch 5.37: Loss = 0.442841
Epoch 5.38: Loss = 0.372757
Epoch 5.39: Loss = 0.464722
Epoch 5.40: Loss = 0.440094
Epoch 5.41: Loss = 0.458511
Epoch 5.42: Loss = 0.546173
Epoch 5.43: Loss = 0.439423
Epoch 5.44: Loss = 0.425156
Epoch 5.45: Loss = 0.531052
Epoch 5.46: Loss = 0.389069
Epoch 5.47: Loss = 0.45578
Epoch 5.48: Loss = 0.527725
Epoch 5.49: Loss = 0.470123
Epoch 5.50: Loss = 0.466553
Epoch 5.51: Loss = 0.497589
Epoch 5.52: Loss = 0.491013
Epoch 5.53: Loss = 0.560196
Epoch 5.54: Loss = 0.43222
Epoch 5.55: Loss = 0.438446
Epoch 5.56: Loss = 0.52713
Epoch 5.57: Loss = 0.441299
Epoch 5.58: Loss = 0.436035
Epoch 5.59: Loss = 0.430267
Epoch 5.60: Loss = 0.525208
Epoch 5.61: Loss = 0.492416
Epoch 5.62: Loss = 0.413895
Epoch 5.63: Loss = 0.411423
Epoch 5.64: Loss = 0.507339
Epoch 5.65: Loss = 0.44989
Epoch 5.66: Loss = 0.345322
Epoch 5.67: Loss = 0.420166
Epoch 5.68: Loss = 0.44162
Epoch 5.69: Loss = 0.506836
Epoch 5.70: Loss = 0.465591
Epoch 5.71: Loss = 0.527618
Epoch 5.72: Loss = 0.587158
Epoch 5.73: Loss = 0.371201
Epoch 5.74: Loss = 0.456589
Epoch 5.75: Loss = 0.539093
Epoch 5.76: Loss = 0.343597
Epoch 5.77: Loss = 0.438385
Epoch 5.78: Loss = 0.380417
Epoch 5.79: Loss = 0.407211
Epoch 5.80: Loss = 0.335251
Epoch 5.81: Loss = 0.400894
Epoch 5.82: Loss = 0.378494
Epoch 5.83: Loss = 0.420502
Epoch 5.84: Loss = 0.347244
Epoch 5.85: Loss = 0.403564
Epoch 5.86: Loss = 0.517746
Epoch 5.87: Loss = 0.445969
Epoch 5.88: Loss = 0.425766
Epoch 5.89: Loss = 0.47641
Epoch 5.90: Loss = 0.335602
Epoch 5.91: Loss = 0.443878
Epoch 5.92: Loss = 0.399948
Epoch 5.93: Loss = 0.469635
Epoch 5.94: Loss = 0.474289
Epoch 5.95: Loss = 0.467941
Epoch 5.96: Loss = 0.513672
Epoch 5.97: Loss = 0.448395
Epoch 5.98: Loss = 0.47052
Epoch 5.99: Loss = 0.419571
Epoch 5.100: Loss = 0.373688
Epoch 5.101: Loss = 0.483383
Epoch 5.102: Loss = 0.48465
Epoch 5.103: Loss = 0.530289
Epoch 5.104: Loss = 0.42778
Epoch 5.105: Loss = 0.343018
Epoch 5.106: Loss = 0.425583
Epoch 5.107: Loss = 0.430252
Epoch 5.108: Loss = 0.515045
Epoch 5.109: Loss = 0.329285
Epoch 5.110: Loss = 0.400452
Epoch 5.111: Loss = 0.409286
Epoch 5.112: Loss = 0.541794
Epoch 5.113: Loss = 0.352051
Epoch 5.114: Loss = 0.481308
Epoch 5.115: Loss = 0.481415
Epoch 5.116: Loss = 0.38942
Epoch 5.117: Loss = 0.416367
Epoch 5.118: Loss = 0.500916
Epoch 5.119: Loss = 0.390091
Epoch 5.120: Loss = 0.562805
TRAIN LOSS = 0.447891
TRAIN ACC = 87.7731 % (52666/60000)
Loss = 0.415863
Loss = 0.509216
Loss = 0.63945
Loss = 0.590652
Loss = 0.606552
Loss = 0.457993
Loss = 0.37471
Loss = 0.690613
Loss = 0.62764
Loss = 0.579987
Loss = 0.215103
Loss = 0.363495
Loss = 0.333801
Loss = 0.362762
Loss = 0.251022
Loss = 0.313843
Loss = 0.219879
Loss = 0.0704651
Loss = 0.281357
Loss = 0.62442
TEST LOSS = 0.426441
TEST ACC = 526.659 % (8878/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.361969
Epoch 6.2: Loss = 0.373871
Epoch 6.3: Loss = 0.489914
Epoch 6.4: Loss = 0.3405
Epoch 6.5: Loss = 0.428253
Epoch 6.6: Loss = 0.41127
Epoch 6.7: Loss = 0.492264
Epoch 6.8: Loss = 0.435608
Epoch 6.9: Loss = 0.45195
Epoch 6.10: Loss = 0.462097
Epoch 6.11: Loss = 0.421265
Epoch 6.12: Loss = 0.376282
Epoch 6.13: Loss = 0.362076
Epoch 6.14: Loss = 0.404144
Epoch 6.15: Loss = 0.48877
Epoch 6.16: Loss = 0.483704
Epoch 6.17: Loss = 0.43988
Epoch 6.18: Loss = 0.492722
Epoch 6.19: Loss = 0.553558
Epoch 6.20: Loss = 0.528717
Epoch 6.21: Loss = 0.513504
Epoch 6.22: Loss = 0.501175
Epoch 6.23: Loss = 0.369598
Epoch 6.24: Loss = 0.539536
Epoch 6.25: Loss = 0.535431
Epoch 6.26: Loss = 0.374802
Epoch 6.27: Loss = 0.527512
Epoch 6.28: Loss = 0.404907
Epoch 6.29: Loss = 0.596146
Epoch 6.30: Loss = 0.403534
Epoch 6.31: Loss = 0.438446
Epoch 6.32: Loss = 0.505341
Epoch 6.33: Loss = 0.472763
Epoch 6.34: Loss = 0.455444
Epoch 6.35: Loss = 0.50032
Epoch 6.36: Loss = 0.504471
Epoch 6.37: Loss = 0.412506
Epoch 6.38: Loss = 0.54538
Epoch 6.39: Loss = 0.436996
Epoch 6.40: Loss = 0.346695
Epoch 6.41: Loss = 0.479843
Epoch 6.42: Loss = 0.434769
Epoch 6.43: Loss = 0.391098
Epoch 6.44: Loss = 0.473358
Epoch 6.45: Loss = 0.511276
Epoch 6.46: Loss = 0.447922
Epoch 6.47: Loss = 0.425705
Epoch 6.48: Loss = 0.421188
Epoch 6.49: Loss = 0.453964
Epoch 6.50: Loss = 0.518127
Epoch 6.51: Loss = 0.47673
Epoch 6.52: Loss = 0.526627
Epoch 6.53: Loss = 0.465363
Epoch 6.54: Loss = 0.531448
Epoch 6.55: Loss = 0.474121
Epoch 6.56: Loss = 0.64679
Epoch 6.57: Loss = 0.395569
Epoch 6.58: Loss = 0.460861
Epoch 6.59: Loss = 0.393677
Epoch 6.60: Loss = 0.31369
Epoch 6.61: Loss = 0.476318
Epoch 6.62: Loss = 0.460403
Epoch 6.63: Loss = 0.37294
Epoch 6.64: Loss = 0.46434
Epoch 6.65: Loss = 0.312088
Epoch 6.66: Loss = 0.495789
Epoch 6.67: Loss = 0.372574
Epoch 6.68: Loss = 0.502609
Epoch 6.69: Loss = 0.395401
Epoch 6.70: Loss = 0.45224
Epoch 6.71: Loss = 0.423004
Epoch 6.72: Loss = 0.497116
Epoch 6.73: Loss = 0.498428
Epoch 6.74: Loss = 0.475174
Epoch 6.75: Loss = 0.459885
Epoch 6.76: Loss = 0.61467
Epoch 6.77: Loss = 0.322937
Epoch 6.78: Loss = 0.47644
Epoch 6.79: Loss = 0.452957
Epoch 6.80: Loss = 0.518097
Epoch 6.81: Loss = 0.535278
Epoch 6.82: Loss = 0.354782
Epoch 6.83: Loss = 0.420212
Epoch 6.84: Loss = 0.48558
Epoch 6.85: Loss = 0.516968
Epoch 6.86: Loss = 0.461868
Epoch 6.87: Loss = 0.427063
Epoch 6.88: Loss = 0.489532
Epoch 6.89: Loss = 0.430161
Epoch 6.90: Loss = 0.334137
Epoch 6.91: Loss = 0.364182
Epoch 6.92: Loss = 0.374939
Epoch 6.93: Loss = 0.471924
Epoch 6.94: Loss = 0.422485
Epoch 6.95: Loss = 0.518372
Epoch 6.96: Loss = 0.445251
Epoch 6.97: Loss = 0.32959
Epoch 6.98: Loss = 0.406906
Epoch 6.99: Loss = 0.457672
Epoch 6.100: Loss = 0.495972
Epoch 6.101: Loss = 0.390594
Epoch 6.102: Loss = 0.502563
Epoch 6.103: Loss = 0.463211
Epoch 6.104: Loss = 0.475189
Epoch 6.105: Loss = 0.559204
Epoch 6.106: Loss = 0.351151
Epoch 6.107: Loss = 0.529297
Epoch 6.108: Loss = 0.370285
Epoch 6.109: Loss = 0.416016
Epoch 6.110: Loss = 0.404678
Epoch 6.111: Loss = 0.530838
Epoch 6.112: Loss = 0.475906
Epoch 6.113: Loss = 0.539948
Epoch 6.114: Loss = 0.548737
Epoch 6.115: Loss = 0.573456
Epoch 6.116: Loss = 0.585693
Epoch 6.117: Loss = 0.459976
Epoch 6.118: Loss = 0.411896
Epoch 6.119: Loss = 0.416397
Epoch 6.120: Loss = 0.44783
TRAIN LOSS = 0.455536
TRAIN ACC = 88.1561 % (52896/60000)
Loss = 0.408112
Loss = 0.49826
Loss = 0.64769
Loss = 0.61055
Loss = 0.607925
Loss = 0.456024
Loss = 0.356903
Loss = 0.713669
Loss = 0.671524
Loss = 0.577576
Loss = 0.204147
Loss = 0.425507
Loss = 0.317078
Loss = 0.400986
Loss = 0.240829
Loss = 0.299194
Loss = 0.210098
Loss = 0.0574341
Loss = 0.267624
Loss = 0.592239
TEST LOSS = 0.428168
TEST ACC = 528.96 % (8882/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.482544
Epoch 7.2: Loss = 0.503265
Epoch 7.3: Loss = 0.44986
Epoch 7.4: Loss = 0.526047
Epoch 7.5: Loss = 0.397232
Epoch 7.6: Loss = 0.489197
Epoch 7.7: Loss = 0.308807
Epoch 7.8: Loss = 0.386658
Epoch 7.9: Loss = 0.458099
Epoch 7.10: Loss = 0.554718
Epoch 7.11: Loss = 0.401901
Epoch 7.12: Loss = 0.436874
Epoch 7.13: Loss = 0.415848
Epoch 7.14: Loss = 0.362625
Epoch 7.15: Loss = 0.411469
Epoch 7.16: Loss = 0.407562
Epoch 7.17: Loss = 0.393478
Epoch 7.18: Loss = 0.443558
Epoch 7.19: Loss = 0.461197
Epoch 7.20: Loss = 0.392883
Epoch 7.21: Loss = 0.535645
Epoch 7.22: Loss = 0.521454
Epoch 7.23: Loss = 0.493378
Epoch 7.24: Loss = 0.483383
Epoch 7.25: Loss = 0.420395
Epoch 7.26: Loss = 0.530045
Epoch 7.27: Loss = 0.439819
Epoch 7.28: Loss = 0.491913
Epoch 7.29: Loss = 0.475967
Epoch 7.30: Loss = 0.381195
Epoch 7.31: Loss = 0.519592
Epoch 7.32: Loss = 0.370178
Epoch 7.33: Loss = 0.433289
Epoch 7.34: Loss = 0.454163
Epoch 7.35: Loss = 0.562744
Epoch 7.36: Loss = 0.532867
Epoch 7.37: Loss = 0.444153
Epoch 7.38: Loss = 0.422058
Epoch 7.39: Loss = 0.57312
Epoch 7.40: Loss = 0.468643
Epoch 7.41: Loss = 0.51828
Epoch 7.42: Loss = 0.378571
Epoch 7.43: Loss = 0.557037
Epoch 7.44: Loss = 0.438721
Epoch 7.45: Loss = 0.528381
Epoch 7.46: Loss = 0.344666
Epoch 7.47: Loss = 0.503296
Epoch 7.48: Loss = 0.468735
Epoch 7.49: Loss = 0.453339
Epoch 7.50: Loss = 0.485519
Epoch 7.51: Loss = 0.534103
Epoch 7.52: Loss = 0.522476
Epoch 7.53: Loss = 0.496063
Epoch 7.54: Loss = 0.37674
Epoch 7.55: Loss = 0.388596
Epoch 7.56: Loss = 0.577835
Epoch 7.57: Loss = 0.44191
Epoch 7.58: Loss = 0.428787
Epoch 7.59: Loss = 0.472031
Epoch 7.60: Loss = 0.474136
Epoch 7.61: Loss = 0.384659
Epoch 7.62: Loss = 0.441483
Epoch 7.63: Loss = 0.495438
Epoch 7.64: Loss = 0.446228
Epoch 7.65: Loss = 0.428101
Epoch 7.66: Loss = 0.452515
Epoch 7.67: Loss = 0.467422
Epoch 7.68: Loss = 0.427399
Epoch 7.69: Loss = 0.323715
Epoch 7.70: Loss = 0.453033
Epoch 7.71: Loss = 0.448151
Epoch 7.72: Loss = 0.458893
Epoch 7.73: Loss = 0.417221
Epoch 7.74: Loss = 0.3853
Epoch 7.75: Loss = 0.429306
Epoch 7.76: Loss = 0.459091
Epoch 7.77: Loss = 0.525681
Epoch 7.78: Loss = 0.568817
Epoch 7.79: Loss = 0.471817
Epoch 7.80: Loss = 0.589783
Epoch 7.81: Loss = 0.354965
Epoch 7.82: Loss = 0.493851
Epoch 7.83: Loss = 0.325699
Epoch 7.84: Loss = 0.454697
Epoch 7.85: Loss = 0.507645
Epoch 7.86: Loss = 0.372192
Epoch 7.87: Loss = 0.510956
Epoch 7.88: Loss = 0.548538
Epoch 7.89: Loss = 0.481674
Epoch 7.90: Loss = 0.394852
Epoch 7.91: Loss = 0.409927
Epoch 7.92: Loss = 0.564636
Epoch 7.93: Loss = 0.478241
Epoch 7.94: Loss = 0.396347
Epoch 7.95: Loss = 0.487778
Epoch 7.96: Loss = 0.476471
Epoch 7.97: Loss = 0.567276
Epoch 7.98: Loss = 0.426254
Epoch 7.99: Loss = 0.521957
Epoch 7.100: Loss = 0.552353
Epoch 7.101: Loss = 0.497498
Epoch 7.102: Loss = 0.605911
Epoch 7.103: Loss = 0.386627
Epoch 7.104: Loss = 0.461578
Epoch 7.105: Loss = 0.552795
Epoch 7.106: Loss = 0.471527
Epoch 7.107: Loss = 0.415771
Epoch 7.108: Loss = 0.421768
Epoch 7.109: Loss = 0.482834
Epoch 7.110: Loss = 0.397095
Epoch 7.111: Loss = 0.515839
Epoch 7.112: Loss = 0.55307
Epoch 7.113: Loss = 0.453598
Epoch 7.114: Loss = 0.448517
Epoch 7.115: Loss = 0.454636
Epoch 7.116: Loss = 0.50296
Epoch 7.117: Loss = 0.390121
Epoch 7.118: Loss = 0.401947
Epoch 7.119: Loss = 0.625931
Epoch 7.120: Loss = 0.379745
TRAIN LOSS = 0.462051
TRAIN ACC = 88.2797 % (52970/60000)
Loss = 0.418152
Loss = 0.512314
Loss = 0.648651
Loss = 0.602615
Loss = 0.651855
Loss = 0.465118
Loss = 0.361862
Loss = 0.730225
Loss = 0.660568
Loss = 0.548492
Loss = 0.230103
Loss = 0.434235
Loss = 0.395462
Loss = 0.415848
Loss = 0.218857
Loss = 0.376831
Loss = 0.230576
Loss = 0.0578156
Loss = 0.255127
Loss = 0.632126
TEST LOSS = 0.442341
TEST ACC = 529.7 % (8892/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.281937
Epoch 8.2: Loss = 0.458313
Epoch 8.3: Loss = 0.483047
Epoch 8.4: Loss = 0.452484
Epoch 8.5: Loss = 0.541428
Epoch 8.6: Loss = 0.454468
Epoch 8.7: Loss = 0.468765
Epoch 8.8: Loss = 0.523819
Epoch 8.9: Loss = 0.56105
Epoch 8.10: Loss = 0.552444
Epoch 8.11: Loss = 0.594498
Epoch 8.12: Loss = 0.442871
Epoch 8.13: Loss = 0.451675
Epoch 8.14: Loss = 0.452225
Epoch 8.15: Loss = 0.458435
Epoch 8.16: Loss = 0.474335
Epoch 8.17: Loss = 0.329697
Epoch 8.18: Loss = 0.440781
Epoch 8.19: Loss = 0.566086
Epoch 8.20: Loss = 0.532013
Epoch 8.21: Loss = 0.624115
Epoch 8.22: Loss = 0.467178
Epoch 8.23: Loss = 0.387955
Epoch 8.24: Loss = 0.558884
Epoch 8.25: Loss = 0.494934
Epoch 8.26: Loss = 0.450684
Epoch 8.27: Loss = 0.600418
Epoch 8.28: Loss = 0.512115
Epoch 8.29: Loss = 0.460541
Epoch 8.30: Loss = 0.462967
Epoch 8.31: Loss = 0.460999
Epoch 8.32: Loss = 0.363449
Epoch 8.33: Loss = 0.584396
Epoch 8.34: Loss = 0.434341
Epoch 8.35: Loss = 0.532837
Epoch 8.36: Loss = 0.548294
Epoch 8.37: Loss = 0.608246
Epoch 8.38: Loss = 0.466049
Epoch 8.39: Loss = 0.437805
Epoch 8.40: Loss = 0.4888
Epoch 8.41: Loss = 0.549713
Epoch 8.42: Loss = 0.512939
Epoch 8.43: Loss = 0.475586
Epoch 8.44: Loss = 0.389282
Epoch 8.45: Loss = 0.409912
Epoch 8.46: Loss = 0.500656
Epoch 8.47: Loss = 0.473312
Epoch 8.48: Loss = 0.556686
Epoch 8.49: Loss = 0.416214
Epoch 8.50: Loss = 0.609665
Epoch 8.51: Loss = 0.530579
Epoch 8.52: Loss = 0.503815
Epoch 8.53: Loss = 0.5914
Epoch 8.54: Loss = 0.541763
Epoch 8.55: Loss = 0.502563
Epoch 8.56: Loss = 0.474808
Epoch 8.57: Loss = 0.509888
Epoch 8.58: Loss = 0.450287
Epoch 8.59: Loss = 0.446869
Epoch 8.60: Loss = 0.51326
Epoch 8.61: Loss = 0.588257
Epoch 8.62: Loss = 0.424911
Epoch 8.63: Loss = 0.553741
Epoch 8.64: Loss = 0.440002
Epoch 8.65: Loss = 0.463074
Epoch 8.66: Loss = 0.483047
Epoch 8.67: Loss = 0.358109
Epoch 8.68: Loss = 0.555817
Epoch 8.69: Loss = 0.579788
Epoch 8.70: Loss = 0.422623
Epoch 8.71: Loss = 0.425766
Epoch 8.72: Loss = 0.403091
Epoch 8.73: Loss = 0.487549
Epoch 8.74: Loss = 0.518127
Epoch 8.75: Loss = 0.696106
Epoch 8.76: Loss = 0.713974
Epoch 8.77: Loss = 0.477982
Epoch 8.78: Loss = 0.504715
Epoch 8.79: Loss = 0.329819
Epoch 8.80: Loss = 0.530197
Epoch 8.81: Loss = 0.434967
Epoch 8.82: Loss = 0.343216
Epoch 8.83: Loss = 0.568268
Epoch 8.84: Loss = 0.399063
Epoch 8.85: Loss = 0.439835
Epoch 8.86: Loss = 0.554596
Epoch 8.87: Loss = 0.520264
Epoch 8.88: Loss = 0.55899
Epoch 8.89: Loss = 0.532898
Epoch 8.90: Loss = 0.418808
Epoch 8.91: Loss = 0.492401
Epoch 8.92: Loss = 0.480652
Epoch 8.93: Loss = 0.534897
Epoch 8.94: Loss = 0.472046
Epoch 8.95: Loss = 0.401489
Epoch 8.96: Loss = 0.50238
Epoch 8.97: Loss = 0.475388
Epoch 8.98: Loss = 0.440765
Epoch 8.99: Loss = 0.532318
Epoch 8.100: Loss = 0.47731
Epoch 8.101: Loss = 0.471054
Epoch 8.102: Loss = 0.464188
Epoch 8.103: Loss = 0.506104
Epoch 8.104: Loss = 0.589539
Epoch 8.105: Loss = 0.479156
Epoch 8.106: Loss = 0.396957
Epoch 8.107: Loss = 0.48259
Epoch 8.108: Loss = 0.333359
Epoch 8.109: Loss = 0.538391
Epoch 8.110: Loss = 0.472824
Epoch 8.111: Loss = 0.489822
Epoch 8.112: Loss = 0.622452
Epoch 8.113: Loss = 0.47081
Epoch 8.114: Loss = 0.533585
Epoch 8.115: Loss = 0.463791
Epoch 8.116: Loss = 0.432327
Epoch 8.117: Loss = 0.386826
Epoch 8.118: Loss = 0.38855
Epoch 8.119: Loss = 0.463089
Epoch 8.120: Loss = 0.417053
TRAIN LOSS = 0.486374
TRAIN ACC = 88.2263 % (52938/60000)
Loss = 0.441818
Loss = 0.551544
Loss = 0.687408
Loss = 0.650253
Loss = 0.680817
Loss = 0.515533
Loss = 0.383408
Loss = 0.719803
Loss = 0.665741
Loss = 0.549927
Loss = 0.248032
Loss = 0.393341
Loss = 0.42038
Loss = 0.446014
Loss = 0.219162
Loss = 0.348114
Loss = 0.253708
Loss = 0.0630646
Loss = 0.296173
Loss = 0.66066
TEST LOSS = 0.459745
TEST ACC = 529.379 % (8885/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.442673
Epoch 9.2: Loss = 0.607452
Epoch 9.3: Loss = 0.430191
Epoch 9.4: Loss = 0.472931
Epoch 9.5: Loss = 0.449646
Epoch 9.6: Loss = 0.587662
Epoch 9.7: Loss = 0.481094
Epoch 9.8: Loss = 0.438278
Epoch 9.9: Loss = 0.569092
Epoch 9.10: Loss = 0.544235
Epoch 9.11: Loss = 0.541565
Epoch 9.12: Loss = 0.406525
Epoch 9.13: Loss = 0.48172
Epoch 9.14: Loss = 0.432327
Epoch 9.15: Loss = 0.492538
Epoch 9.16: Loss = 0.517075
Epoch 9.17: Loss = 0.417068
Epoch 9.18: Loss = 0.383423
Epoch 9.19: Loss = 0.406052
Epoch 9.20: Loss = 0.572525
Epoch 9.21: Loss = 0.518173
Epoch 9.22: Loss = 0.371597
Epoch 9.23: Loss = 0.596313
Epoch 9.24: Loss = 0.458542
Epoch 9.25: Loss = 0.527374
Epoch 9.26: Loss = 0.448868
Epoch 9.27: Loss = 0.438889
Epoch 9.28: Loss = 0.416153
Epoch 9.29: Loss = 0.377823
Epoch 9.30: Loss = 0.535736
Epoch 9.31: Loss = 0.492599
Epoch 9.32: Loss = 0.429565
Epoch 9.33: Loss = 0.480377
Epoch 9.34: Loss = 0.557449
Epoch 9.35: Loss = 0.417328
Epoch 9.36: Loss = 0.431305
Epoch 9.37: Loss = 0.55043
Epoch 9.38: Loss = 0.384842
Epoch 9.39: Loss = 0.452881
Epoch 9.40: Loss = 0.582428
Epoch 9.41: Loss = 0.467041
Epoch 9.42: Loss = 0.469803
Epoch 9.43: Loss = 0.405273
Epoch 9.44: Loss = 0.456085
Epoch 9.45: Loss = 0.45871
Epoch 9.46: Loss = 0.49649
Epoch 9.47: Loss = 0.502563
Epoch 9.48: Loss = 0.602234
Epoch 9.49: Loss = 0.44516
Epoch 9.50: Loss = 0.472214
Epoch 9.51: Loss = 0.467148
Epoch 9.52: Loss = 0.416733
Epoch 9.53: Loss = 0.465408
Epoch 9.54: Loss = 0.676697
Epoch 9.55: Loss = 0.51474
Epoch 9.56: Loss = 0.542603
Epoch 9.57: Loss = 0.422089
Epoch 9.58: Loss = 0.580109
Epoch 9.59: Loss = 0.375031
Epoch 9.60: Loss = 0.483063
Epoch 9.61: Loss = 0.517227
Epoch 9.62: Loss = 0.423874
Epoch 9.63: Loss = 0.467682
Epoch 9.64: Loss = 0.589279
Epoch 9.65: Loss = 0.508545
Epoch 9.66: Loss = 0.398285
Epoch 9.67: Loss = 0.559021
Epoch 9.68: Loss = 0.456451
Epoch 9.69: Loss = 0.460068
Epoch 9.70: Loss = 0.448425
Epoch 9.71: Loss = 0.483261
Epoch 9.72: Loss = 0.498306
Epoch 9.73: Loss = 0.579269
Epoch 9.74: Loss = 0.453217
Epoch 9.75: Loss = 0.532272
Epoch 9.76: Loss = 0.401581
Epoch 9.77: Loss = 0.601364
Epoch 9.78: Loss = 0.440369
Epoch 9.79: Loss = 0.508835
Epoch 9.80: Loss = 0.674179
Epoch 9.81: Loss = 0.548035
Epoch 9.82: Loss = 0.535843
Epoch 9.83: Loss = 0.545288
Epoch 9.84: Loss = 0.464401
Epoch 9.85: Loss = 0.535858
Epoch 9.86: Loss = 0.51416
Epoch 9.87: Loss = 0.439743
Epoch 9.88: Loss = 0.373337
Epoch 9.89: Loss = 0.476151
Epoch 9.90: Loss = 0.514282
Epoch 9.91: Loss = 0.460419
Epoch 9.92: Loss = 0.528275
Epoch 9.93: Loss = 0.555359
Epoch 9.94: Loss = 0.48912
Epoch 9.95: Loss = 0.648499
Epoch 9.96: Loss = 0.459503
Epoch 9.97: Loss = 0.580673
Epoch 9.98: Loss = 0.527618
Epoch 9.99: Loss = 0.686157
Epoch 9.100: Loss = 0.448975
Epoch 9.101: Loss = 0.436661
Epoch 9.102: Loss = 0.610916
Epoch 9.103: Loss = 0.537598
Epoch 9.104: Loss = 0.48584
Epoch 9.105: Loss = 0.50769
Epoch 9.106: Loss = 0.63475
Epoch 9.107: Loss = 0.557449
Epoch 9.108: Loss = 0.521698
Epoch 9.109: Loss = 0.45105
Epoch 9.110: Loss = 0.512421
Epoch 9.111: Loss = 0.602036
Epoch 9.112: Loss = 0.565231
Epoch 9.113: Loss = 0.489853
Epoch 9.114: Loss = 0.544205
Epoch 9.115: Loss = 0.497528
Epoch 9.116: Loss = 0.521423
Epoch 9.117: Loss = 0.419769
Epoch 9.118: Loss = 0.636612
Epoch 9.119: Loss = 0.581268
Epoch 9.120: Loss = 0.496902
TRAIN LOSS = 0.498383
TRAIN ACC = 88.2294 % (52940/60000)
Loss = 0.447784
Loss = 0.626923
Loss = 0.682404
Loss = 0.6754
Loss = 0.676208
Loss = 0.499008
Loss = 0.394043
Loss = 0.740921
Loss = 0.664917
Loss = 0.598984
Loss = 0.217285
Loss = 0.398529
Loss = 0.390182
Loss = 0.469696
Loss = 0.211151
Loss = 0.328232
Loss = 0.251343
Loss = 0.0632019
Loss = 0.29245
Loss = 0.695709
TEST LOSS = 0.466218
TEST ACC = 529.399 % (8912/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.399323
Epoch 10.2: Loss = 0.582245
Epoch 10.3: Loss = 0.413055
Epoch 10.4: Loss = 0.612427
Epoch 10.5: Loss = 0.649872
Epoch 10.6: Loss = 0.515076
Epoch 10.7: Loss = 0.487427
Epoch 10.8: Loss = 0.53862
Epoch 10.9: Loss = 0.59082
Epoch 10.10: Loss = 0.398544
Epoch 10.11: Loss = 0.591904
Epoch 10.12: Loss = 0.613602
Epoch 10.13: Loss = 0.528214
Epoch 10.14: Loss = 0.490311
Epoch 10.15: Loss = 0.65509
Epoch 10.16: Loss = 0.379486
Epoch 10.17: Loss = 0.586533
Epoch 10.18: Loss = 0.41716
Epoch 10.19: Loss = 0.441635
Epoch 10.20: Loss = 0.52562
Epoch 10.21: Loss = 0.573074
Epoch 10.22: Loss = 0.663727
Epoch 10.23: Loss = 0.485245
Epoch 10.24: Loss = 0.660278
Epoch 10.25: Loss = 0.458984
Epoch 10.26: Loss = 0.518631
Epoch 10.27: Loss = 0.609024
Epoch 10.28: Loss = 0.514023
Epoch 10.29: Loss = 0.477951
Epoch 10.30: Loss = 0.404724
Epoch 10.31: Loss = 0.595093
Epoch 10.32: Loss = 0.318848
Epoch 10.33: Loss = 0.558884
Epoch 10.34: Loss = 0.510895
Epoch 10.35: Loss = 0.397095
Epoch 10.36: Loss = 0.642075
Epoch 10.37: Loss = 0.581863
Epoch 10.38: Loss = 0.49704
Epoch 10.39: Loss = 0.505814
Epoch 10.40: Loss = 0.618088
Epoch 10.41: Loss = 0.439392
Epoch 10.42: Loss = 0.441757
Epoch 10.43: Loss = 0.470993
Epoch 10.44: Loss = 0.463028
Epoch 10.45: Loss = 0.465775
Epoch 10.46: Loss = 0.471634
Epoch 10.47: Loss = 0.614792
Epoch 10.48: Loss = 0.473557
Epoch 10.49: Loss = 0.433914
Epoch 10.50: Loss = 0.446533
Epoch 10.51: Loss = 0.302399
Epoch 10.52: Loss = 0.366043
Epoch 10.53: Loss = 0.418457
Epoch 10.54: Loss = 0.489288
Epoch 10.55: Loss = 0.512787
Epoch 10.56: Loss = 0.602493
Epoch 10.57: Loss = 0.532349
Epoch 10.58: Loss = 0.604691
Epoch 10.59: Loss = 0.626404
Epoch 10.60: Loss = 0.483246
Epoch 10.61: Loss = 0.417175
Epoch 10.62: Loss = 0.463181
Epoch 10.63: Loss = 0.518906
Epoch 10.64: Loss = 0.479294
Epoch 10.65: Loss = 0.50975
Epoch 10.66: Loss = 0.510193
Epoch 10.67: Loss = 0.502075
Epoch 10.68: Loss = 0.493866
Epoch 10.69: Loss = 0.611649
Epoch 10.70: Loss = 0.551468
Epoch 10.71: Loss = 0.431198
Epoch 10.72: Loss = 0.498093
Epoch 10.73: Loss = 0.639267
Epoch 10.74: Loss = 0.539185
Epoch 10.75: Loss = 0.478378
Epoch 10.76: Loss = 0.453201
Epoch 10.77: Loss = 0.546204
Epoch 10.78: Loss = 0.352066
Epoch 10.79: Loss = 0.566986
Epoch 10.80: Loss = 0.462799
Epoch 10.81: Loss = 0.419739
Epoch 10.82: Loss = 0.441452
Epoch 10.83: Loss = 0.54274
Epoch 10.84: Loss = 0.49469
Epoch 10.85: Loss = 0.500183
Epoch 10.86: Loss = 0.554321
Epoch 10.87: Loss = 0.505219
Epoch 10.88: Loss = 0.594116
Epoch 10.89: Loss = 0.670914
Epoch 10.90: Loss = 0.47171
Epoch 10.91: Loss = 0.478668
Epoch 10.92: Loss = 0.566467
Epoch 10.93: Loss = 0.40979
Epoch 10.94: Loss = 0.702164
Epoch 10.95: Loss = 0.458435
Epoch 10.96: Loss = 0.46756
Epoch 10.97: Loss = 0.451904
Epoch 10.98: Loss = 0.531631
Epoch 10.99: Loss = 0.596985
Epoch 10.100: Loss = 0.468307
Epoch 10.101: Loss = 0.685928
Epoch 10.102: Loss = 0.281128
Epoch 10.103: Loss = 0.513779
Epoch 10.104: Loss = 0.449646
Epoch 10.105: Loss = 0.34317
Epoch 10.106: Loss = 0.520477
Epoch 10.107: Loss = 0.454926
Epoch 10.108: Loss = 0.430191
Epoch 10.109: Loss = 0.783951
Epoch 10.110: Loss = 0.585052
Epoch 10.111: Loss = 0.474869
Epoch 10.112: Loss = 0.478287
Epoch 10.113: Loss = 0.469437
Epoch 10.114: Loss = 0.656448
Epoch 10.115: Loss = 0.562958
Epoch 10.116: Loss = 0.528381
Epoch 10.117: Loss = 0.73053
Epoch 10.118: Loss = 0.473709
Epoch 10.119: Loss = 0.582458
Epoch 10.120: Loss = 0.401978
TRAIN LOSS = 0.511902
TRAIN ACC = 88.3499 % (53012/60000)
Loss = 0.473328
Loss = 0.696045
Loss = 0.738037
Loss = 0.799011
Loss = 0.746857
Loss = 0.570023
Loss = 0.419189
Loss = 0.788147
Loss = 0.70462
Loss = 0.640976
Loss = 0.272003
Loss = 0.481277
Loss = 0.448578
Loss = 0.455444
Loss = 0.229782
Loss = 0.28215
Loss = 0.286819
Loss = 0.0882416
Loss = 0.278992
Loss = 0.7715
TEST LOSS = 0.508551
TEST ACC = 530.119 % (8859/10000)

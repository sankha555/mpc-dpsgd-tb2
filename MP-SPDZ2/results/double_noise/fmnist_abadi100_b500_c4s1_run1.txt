Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.45523
Epoch 1.2: Loss = 2.35995
Epoch 1.3: Loss = 2.26221
Epoch 1.4: Loss = 2.17775
Epoch 1.5: Loss = 2.09927
Epoch 1.6: Loss = 2.06776
Epoch 1.7: Loss = 1.97545
Epoch 1.8: Loss = 1.93343
Epoch 1.9: Loss = 1.89792
Epoch 1.10: Loss = 1.82425
Epoch 1.11: Loss = 1.78595
Epoch 1.12: Loss = 1.78268
Epoch 1.13: Loss = 1.71175
Epoch 1.14: Loss = 1.71031
Epoch 1.15: Loss = 1.64362
Epoch 1.16: Loss = 1.64409
Epoch 1.17: Loss = 1.55487
Epoch 1.18: Loss = 1.53804
Epoch 1.19: Loss = 1.51132
Epoch 1.20: Loss = 1.50534
Epoch 1.21: Loss = 1.47609
Epoch 1.22: Loss = 1.42172
Epoch 1.23: Loss = 1.37325
Epoch 1.24: Loss = 1.38483
Epoch 1.25: Loss = 1.37131
Epoch 1.26: Loss = 1.36609
Epoch 1.27: Loss = 1.38153
Epoch 1.28: Loss = 1.30334
Epoch 1.29: Loss = 1.22044
Epoch 1.30: Loss = 1.23849
Epoch 1.31: Loss = 1.27386
Epoch 1.32: Loss = 1.18033
Epoch 1.33: Loss = 1.18878
Epoch 1.34: Loss = 1.23141
Epoch 1.35: Loss = 1.12877
Epoch 1.36: Loss = 1.11464
Epoch 1.37: Loss = 1.16631
Epoch 1.38: Loss = 1.10709
Epoch 1.39: Loss = 1.0927
Epoch 1.40: Loss = 1.10431
Epoch 1.41: Loss = 1.06065
Epoch 1.42: Loss = 1.10669
Epoch 1.43: Loss = 1.14058
Epoch 1.44: Loss = 1.02217
Epoch 1.45: Loss = 0.99176
Epoch 1.46: Loss = 1.03365
Epoch 1.47: Loss = 1.0313
Epoch 1.48: Loss = 1.03233
Epoch 1.49: Loss = 1.0359
Epoch 1.50: Loss = 1.02538
Epoch 1.51: Loss = 0.987534
Epoch 1.52: Loss = 0.987961
Epoch 1.53: Loss = 0.942459
Epoch 1.54: Loss = 0.982864
Epoch 1.55: Loss = 0.964188
Epoch 1.56: Loss = 0.918915
Epoch 1.57: Loss = 0.964996
Epoch 1.58: Loss = 0.884995
Epoch 1.59: Loss = 0.960922
Epoch 1.60: Loss = 0.959015
Epoch 1.61: Loss = 0.909866
Epoch 1.62: Loss = 0.874969
Epoch 1.63: Loss = 0.902924
Epoch 1.64: Loss = 0.960114
Epoch 1.65: Loss = 0.886642
Epoch 1.66: Loss = 0.944214
Epoch 1.67: Loss = 0.906815
Epoch 1.68: Loss = 0.882843
Epoch 1.69: Loss = 0.843781
Epoch 1.70: Loss = 0.949554
Epoch 1.71: Loss = 0.838043
Epoch 1.72: Loss = 0.853745
Epoch 1.73: Loss = 0.859863
Epoch 1.74: Loss = 0.875153
Epoch 1.75: Loss = 0.862106
Epoch 1.76: Loss = 0.819626
Epoch 1.77: Loss = 0.870087
Epoch 1.78: Loss = 0.800735
Epoch 1.79: Loss = 0.869934
Epoch 1.80: Loss = 0.825165
Epoch 1.81: Loss = 0.811554
Epoch 1.82: Loss = 0.783478
Epoch 1.83: Loss = 0.869095
Epoch 1.84: Loss = 0.873978
Epoch 1.85: Loss = 0.844894
Epoch 1.86: Loss = 0.785095
Epoch 1.87: Loss = 0.775665
Epoch 1.88: Loss = 0.807922
Epoch 1.89: Loss = 0.818741
Epoch 1.90: Loss = 0.740356
Epoch 1.91: Loss = 0.908905
Epoch 1.92: Loss = 0.884598
Epoch 1.93: Loss = 0.789398
Epoch 1.94: Loss = 0.786377
Epoch 1.95: Loss = 0.785278
Epoch 1.96: Loss = 0.793976
Epoch 1.97: Loss = 0.743988
Epoch 1.98: Loss = 0.825027
Epoch 1.99: Loss = 0.845016
Epoch 1.100: Loss = 0.764343
Epoch 1.101: Loss = 0.806854
Epoch 1.102: Loss = 0.765976
Epoch 1.103: Loss = 0.786255
Epoch 1.104: Loss = 0.763077
Epoch 1.105: Loss = 0.754715
Epoch 1.106: Loss = 0.80426
Epoch 1.107: Loss = 0.823425
Epoch 1.108: Loss = 0.728455
Epoch 1.109: Loss = 0.745102
Epoch 1.110: Loss = 0.762054
Epoch 1.111: Loss = 0.769485
Epoch 1.112: Loss = 0.790512
Epoch 1.113: Loss = 0.833099
Epoch 1.114: Loss = 0.734512
Epoch 1.115: Loss = 0.791489
Epoch 1.116: Loss = 0.787827
Epoch 1.117: Loss = 0.781509
Epoch 1.118: Loss = 0.755692
Epoch 1.119: Loss = 0.700272
Epoch 1.120: Loss = 0.776398
TRAIN LOSS = 1.09865
TRAIN ACC = 64.0884 % (38455/60000)
Loss = 0.714188
Loss = 0.802826
Loss = 0.805725
Loss = 0.71077
Loss = 0.707886
Loss = 0.835754
Loss = 0.854797
Loss = 0.809296
Loss = 0.745453
Loss = 0.715515
Loss = 0.823288
Loss = 0.783295
Loss = 0.760254
Loss = 0.793594
Loss = 0.749954
Loss = 0.811905
Loss = 0.735779
Loss = 0.755356
Loss = 0.813858
Loss = 0.763397
TEST LOSS = 0.774644
TEST ACC = 384.549 % (7160/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.815109
Epoch 2.2: Loss = 0.724091
Epoch 2.3: Loss = 0.825851
Epoch 2.4: Loss = 0.708847
Epoch 2.5: Loss = 0.681183
Epoch 2.6: Loss = 0.781586
Epoch 2.7: Loss = 0.743637
Epoch 2.8: Loss = 0.772614
Epoch 2.9: Loss = 0.726486
Epoch 2.10: Loss = 0.733597
Epoch 2.11: Loss = 0.829788
Epoch 2.12: Loss = 0.739487
Epoch 2.13: Loss = 0.771194
Epoch 2.14: Loss = 0.736084
Epoch 2.15: Loss = 0.81073
Epoch 2.16: Loss = 0.665543
Epoch 2.17: Loss = 0.712677
Epoch 2.18: Loss = 0.697937
Epoch 2.19: Loss = 0.6651
Epoch 2.20: Loss = 0.685867
Epoch 2.21: Loss = 0.744324
Epoch 2.22: Loss = 0.697235
Epoch 2.23: Loss = 0.722488
Epoch 2.24: Loss = 0.686234
Epoch 2.25: Loss = 0.736908
Epoch 2.26: Loss = 0.860275
Epoch 2.27: Loss = 0.764313
Epoch 2.28: Loss = 0.654327
Epoch 2.29: Loss = 0.762268
Epoch 2.30: Loss = 0.672058
Epoch 2.31: Loss = 0.742538
Epoch 2.32: Loss = 0.706421
Epoch 2.33: Loss = 0.707306
Epoch 2.34: Loss = 0.662445
Epoch 2.35: Loss = 0.758438
Epoch 2.36: Loss = 0.720139
Epoch 2.37: Loss = 0.653564
Epoch 2.38: Loss = 0.690399
Epoch 2.39: Loss = 0.750412
Epoch 2.40: Loss = 0.694016
Epoch 2.41: Loss = 0.76001
Epoch 2.42: Loss = 0.761414
Epoch 2.43: Loss = 0.67868
Epoch 2.44: Loss = 0.732773
Epoch 2.45: Loss = 0.681625
Epoch 2.46: Loss = 0.676117
Epoch 2.47: Loss = 0.702133
Epoch 2.48: Loss = 0.703613
Epoch 2.49: Loss = 0.730423
Epoch 2.50: Loss = 0.744095
Epoch 2.51: Loss = 0.758713
Epoch 2.52: Loss = 0.621277
Epoch 2.53: Loss = 0.715652
Epoch 2.54: Loss = 0.754318
Epoch 2.55: Loss = 0.669907
Epoch 2.56: Loss = 0.660492
Epoch 2.57: Loss = 0.646255
Epoch 2.58: Loss = 0.746872
Epoch 2.59: Loss = 0.616684
Epoch 2.60: Loss = 0.744904
Epoch 2.61: Loss = 0.70636
Epoch 2.62: Loss = 0.682022
Epoch 2.63: Loss = 0.614639
Epoch 2.64: Loss = 0.746475
Epoch 2.65: Loss = 0.636566
Epoch 2.66: Loss = 0.648407
Epoch 2.67: Loss = 0.756195
Epoch 2.68: Loss = 0.642944
Epoch 2.69: Loss = 0.663635
Epoch 2.70: Loss = 0.706787
Epoch 2.71: Loss = 0.600464
Epoch 2.72: Loss = 0.731186
Epoch 2.73: Loss = 0.743271
Epoch 2.74: Loss = 0.686813
Epoch 2.75: Loss = 0.64743
Epoch 2.76: Loss = 0.646088
Epoch 2.77: Loss = 0.701813
Epoch 2.78: Loss = 0.685837
Epoch 2.79: Loss = 0.638306
Epoch 2.80: Loss = 0.759155
Epoch 2.81: Loss = 0.713287
Epoch 2.82: Loss = 0.659195
Epoch 2.83: Loss = 0.664642
Epoch 2.84: Loss = 0.679596
Epoch 2.85: Loss = 0.732574
Epoch 2.86: Loss = 0.721451
Epoch 2.87: Loss = 0.730331
Epoch 2.88: Loss = 0.744553
Epoch 2.89: Loss = 0.749878
Epoch 2.90: Loss = 0.722809
Epoch 2.91: Loss = 0.727722
Epoch 2.92: Loss = 0.692184
Epoch 2.93: Loss = 0.70311
Epoch 2.94: Loss = 0.68309
Epoch 2.95: Loss = 0.741013
Epoch 2.96: Loss = 0.710281
Epoch 2.97: Loss = 0.73407
Epoch 2.98: Loss = 0.714478
Epoch 2.99: Loss = 0.730881
Epoch 2.100: Loss = 0.645966
Epoch 2.101: Loss = 0.713333
Epoch 2.102: Loss = 0.718948
Epoch 2.103: Loss = 0.695831
Epoch 2.104: Loss = 0.693146
Epoch 2.105: Loss = 0.645447
Epoch 2.106: Loss = 0.605865
Epoch 2.107: Loss = 0.772156
Epoch 2.108: Loss = 0.652176
Epoch 2.109: Loss = 0.628525
Epoch 2.110: Loss = 0.720764
Epoch 2.111: Loss = 0.720642
Epoch 2.112: Loss = 0.591583
Epoch 2.113: Loss = 0.681152
Epoch 2.114: Loss = 0.643265
Epoch 2.115: Loss = 0.642487
Epoch 2.116: Loss = 0.664261
Epoch 2.117: Loss = 0.71759
Epoch 2.118: Loss = 0.654007
Epoch 2.119: Loss = 0.785095
Epoch 2.120: Loss = 0.668335
TRAIN LOSS = 0.706512
TRAIN ACC = 75.0443 % (45028/60000)
Loss = 0.629654
Loss = 0.737137
Loss = 0.714828
Loss = 0.610123
Loss = 0.62616
Loss = 0.780136
Loss = 0.800568
Loss = 0.752243
Loss = 0.690659
Loss = 0.629257
Loss = 0.764252
Loss = 0.750732
Loss = 0.69017
Loss = 0.724808
Loss = 0.683487
Loss = 0.740448
Loss = 0.660858
Loss = 0.706772
Loss = 0.760605
Loss = 0.68248
TEST LOSS = 0.706769
TEST ACC = 450.279 % (7511/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.684601
Epoch 3.2: Loss = 0.636963
Epoch 3.3: Loss = 0.640121
Epoch 3.4: Loss = 0.658081
Epoch 3.5: Loss = 0.746277
Epoch 3.6: Loss = 0.600143
Epoch 3.7: Loss = 0.751709
Epoch 3.8: Loss = 0.651215
Epoch 3.9: Loss = 0.596985
Epoch 3.10: Loss = 0.672226
Epoch 3.11: Loss = 0.70842
Epoch 3.12: Loss = 0.645599
Epoch 3.13: Loss = 0.722458
Epoch 3.14: Loss = 0.643509
Epoch 3.15: Loss = 0.652725
Epoch 3.16: Loss = 0.614594
Epoch 3.17: Loss = 0.669449
Epoch 3.18: Loss = 0.741959
Epoch 3.19: Loss = 0.653046
Epoch 3.20: Loss = 0.690903
Epoch 3.21: Loss = 0.678406
Epoch 3.22: Loss = 0.613937
Epoch 3.23: Loss = 0.681198
Epoch 3.24: Loss = 0.632202
Epoch 3.25: Loss = 0.582489
Epoch 3.26: Loss = 0.647919
Epoch 3.27: Loss = 0.641968
Epoch 3.28: Loss = 0.645004
Epoch 3.29: Loss = 0.652298
Epoch 3.30: Loss = 0.613342
Epoch 3.31: Loss = 0.633499
Epoch 3.32: Loss = 0.601974
Epoch 3.33: Loss = 0.701492
Epoch 3.34: Loss = 0.768738
Epoch 3.35: Loss = 0.607788
Epoch 3.36: Loss = 0.548492
Epoch 3.37: Loss = 0.710434
Epoch 3.38: Loss = 0.642136
Epoch 3.39: Loss = 0.669067
Epoch 3.40: Loss = 0.727859
Epoch 3.41: Loss = 0.697998
Epoch 3.42: Loss = 0.657547
Epoch 3.43: Loss = 0.699478
Epoch 3.44: Loss = 0.649094
Epoch 3.45: Loss = 0.695541
Epoch 3.46: Loss = 0.682907
Epoch 3.47: Loss = 0.64389
Epoch 3.48: Loss = 0.769791
Epoch 3.49: Loss = 0.677872
Epoch 3.50: Loss = 0.602661
Epoch 3.51: Loss = 0.680176
Epoch 3.52: Loss = 0.682465
Epoch 3.53: Loss = 0.580338
Epoch 3.54: Loss = 0.692978
Epoch 3.55: Loss = 0.601089
Epoch 3.56: Loss = 0.572372
Epoch 3.57: Loss = 0.669907
Epoch 3.58: Loss = 0.578796
Epoch 3.59: Loss = 0.634506
Epoch 3.60: Loss = 0.650391
Epoch 3.61: Loss = 0.679657
Epoch 3.62: Loss = 0.612061
Epoch 3.63: Loss = 0.578857
Epoch 3.64: Loss = 0.609863
Epoch 3.65: Loss = 0.659485
Epoch 3.66: Loss = 0.711121
Epoch 3.67: Loss = 0.58847
Epoch 3.68: Loss = 0.617996
Epoch 3.69: Loss = 0.627502
Epoch 3.70: Loss = 0.705215
Epoch 3.71: Loss = 0.760208
Epoch 3.72: Loss = 0.698776
Epoch 3.73: Loss = 0.617935
Epoch 3.74: Loss = 0.644928
Epoch 3.75: Loss = 0.672318
Epoch 3.76: Loss = 0.630524
Epoch 3.77: Loss = 0.704422
Epoch 3.78: Loss = 0.630585
Epoch 3.79: Loss = 0.651886
Epoch 3.80: Loss = 0.621353
Epoch 3.81: Loss = 0.613617
Epoch 3.82: Loss = 0.593765
Epoch 3.83: Loss = 0.576752
Epoch 3.84: Loss = 0.650085
Epoch 3.85: Loss = 0.61293
Epoch 3.86: Loss = 0.550674
Epoch 3.87: Loss = 0.67511
Epoch 3.88: Loss = 0.657242
Epoch 3.89: Loss = 0.621017
Epoch 3.90: Loss = 0.630524
Epoch 3.91: Loss = 0.66954
Epoch 3.92: Loss = 0.674896
Epoch 3.93: Loss = 0.630951
Epoch 3.94: Loss = 0.646393
Epoch 3.95: Loss = 0.680511
Epoch 3.96: Loss = 0.627884
Epoch 3.97: Loss = 0.685867
Epoch 3.98: Loss = 0.713425
Epoch 3.99: Loss = 0.787476
Epoch 3.100: Loss = 0.621674
Epoch 3.101: Loss = 0.721634
Epoch 3.102: Loss = 0.600586
Epoch 3.103: Loss = 0.634674
Epoch 3.104: Loss = 0.607117
Epoch 3.105: Loss = 0.653641
Epoch 3.106: Loss = 0.589569
Epoch 3.107: Loss = 0.58754
Epoch 3.108: Loss = 0.630417
Epoch 3.109: Loss = 0.646088
Epoch 3.110: Loss = 0.649521
Epoch 3.111: Loss = 0.614655
Epoch 3.112: Loss = 0.710907
Epoch 3.113: Loss = 0.667603
Epoch 3.114: Loss = 0.67041
Epoch 3.115: Loss = 0.660965
Epoch 3.116: Loss = 0.714844
Epoch 3.117: Loss = 0.657028
Epoch 3.118: Loss = 0.716843
Epoch 3.119: Loss = 0.612869
Epoch 3.120: Loss = 0.640137
TRAIN LOSS = 0.65361
TRAIN ACC = 78.0838 % (46852/60000)
Loss = 0.580872
Loss = 0.703186
Loss = 0.662262
Loss = 0.549911
Loss = 0.586578
Loss = 0.741592
Loss = 0.760605
Loss = 0.715927
Loss = 0.635742
Loss = 0.595581
Loss = 0.753906
Loss = 0.715866
Loss = 0.64946
Loss = 0.671295
Loss = 0.636307
Loss = 0.696503
Loss = 0.629913
Loss = 0.667892
Loss = 0.703705
Loss = 0.647736
TEST LOSS = 0.665242
TEST ACC = 468.52 % (7738/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.629059
Epoch 4.2: Loss = 0.705795
Epoch 4.3: Loss = 0.660355
Epoch 4.4: Loss = 0.65625
Epoch 4.5: Loss = 0.701172
Epoch 4.6: Loss = 0.567932
Epoch 4.7: Loss = 0.643906
Epoch 4.8: Loss = 0.676743
Epoch 4.9: Loss = 0.66095
Epoch 4.10: Loss = 0.626053
Epoch 4.11: Loss = 0.597549
Epoch 4.12: Loss = 0.623734
Epoch 4.13: Loss = 0.592087
Epoch 4.14: Loss = 0.620331
Epoch 4.15: Loss = 0.672333
Epoch 4.16: Loss = 0.607971
Epoch 4.17: Loss = 0.674225
Epoch 4.18: Loss = 0.645508
Epoch 4.19: Loss = 0.637466
Epoch 4.20: Loss = 0.694656
Epoch 4.21: Loss = 0.67572
Epoch 4.22: Loss = 0.632248
Epoch 4.23: Loss = 0.553757
Epoch 4.24: Loss = 0.59642
Epoch 4.25: Loss = 0.575974
Epoch 4.26: Loss = 0.58522
Epoch 4.27: Loss = 0.617355
Epoch 4.28: Loss = 0.58017
Epoch 4.29: Loss = 0.678314
Epoch 4.30: Loss = 0.60675
Epoch 4.31: Loss = 0.667191
Epoch 4.32: Loss = 0.587433
Epoch 4.33: Loss = 0.560471
Epoch 4.34: Loss = 0.620102
Epoch 4.35: Loss = 0.756775
Epoch 4.36: Loss = 0.6241
Epoch 4.37: Loss = 0.603149
Epoch 4.38: Loss = 0.61937
Epoch 4.39: Loss = 0.615692
Epoch 4.40: Loss = 0.613617
Epoch 4.41: Loss = 0.573914
Epoch 4.42: Loss = 0.639725
Epoch 4.43: Loss = 0.585938
Epoch 4.44: Loss = 0.605484
Epoch 4.45: Loss = 0.72789
Epoch 4.46: Loss = 0.663788
Epoch 4.47: Loss = 0.557877
Epoch 4.48: Loss = 0.661499
Epoch 4.49: Loss = 0.570404
Epoch 4.50: Loss = 0.596863
Epoch 4.51: Loss = 0.662582
Epoch 4.52: Loss = 0.628876
Epoch 4.53: Loss = 0.614044
Epoch 4.54: Loss = 0.662354
Epoch 4.55: Loss = 0.655289
Epoch 4.56: Loss = 0.712128
Epoch 4.57: Loss = 0.573654
Epoch 4.58: Loss = 0.665833
Epoch 4.59: Loss = 0.61644
Epoch 4.60: Loss = 0.679443
Epoch 4.61: Loss = 0.543732
Epoch 4.62: Loss = 0.676788
Epoch 4.63: Loss = 0.616165
Epoch 4.64: Loss = 0.596817
Epoch 4.65: Loss = 0.647446
Epoch 4.66: Loss = 0.562592
Epoch 4.67: Loss = 0.600571
Epoch 4.68: Loss = 0.602478
Epoch 4.69: Loss = 0.605743
Epoch 4.70: Loss = 0.61557
Epoch 4.71: Loss = 0.617203
Epoch 4.72: Loss = 0.568848
Epoch 4.73: Loss = 0.644485
Epoch 4.74: Loss = 0.658524
Epoch 4.75: Loss = 0.717422
Epoch 4.76: Loss = 0.603531
Epoch 4.77: Loss = 0.595596
Epoch 4.78: Loss = 0.54631
Epoch 4.79: Loss = 0.724167
Epoch 4.80: Loss = 0.584549
Epoch 4.81: Loss = 0.65094
Epoch 4.82: Loss = 0.681442
Epoch 4.83: Loss = 0.628616
Epoch 4.84: Loss = 0.575394
Epoch 4.85: Loss = 0.653534
Epoch 4.86: Loss = 0.554108
Epoch 4.87: Loss = 0.635986
Epoch 4.88: Loss = 0.699371
Epoch 4.89: Loss = 0.655746
Epoch 4.90: Loss = 0.56424
Epoch 4.91: Loss = 0.601364
Epoch 4.92: Loss = 0.651733
Epoch 4.93: Loss = 0.696045
Epoch 4.94: Loss = 0.643524
Epoch 4.95: Loss = 0.62001
Epoch 4.96: Loss = 0.73111
Epoch 4.97: Loss = 0.595856
Epoch 4.98: Loss = 0.598663
Epoch 4.99: Loss = 0.686661
Epoch 4.100: Loss = 0.615234
Epoch 4.101: Loss = 0.671829
Epoch 4.102: Loss = 0.573593
Epoch 4.103: Loss = 0.672836
Epoch 4.104: Loss = 0.60466
Epoch 4.105: Loss = 0.582474
Epoch 4.106: Loss = 0.587799
Epoch 4.107: Loss = 0.616531
Epoch 4.108: Loss = 0.549606
Epoch 4.109: Loss = 0.658203
Epoch 4.110: Loss = 0.64563
Epoch 4.111: Loss = 0.604767
Epoch 4.112: Loss = 0.63295
Epoch 4.113: Loss = 0.648361
Epoch 4.114: Loss = 0.629105
Epoch 4.115: Loss = 0.722305
Epoch 4.116: Loss = 0.618042
Epoch 4.117: Loss = 0.615692
Epoch 4.118: Loss = 0.771301
Epoch 4.119: Loss = 0.52037
Epoch 4.120: Loss = 0.600693
TRAIN LOSS = 0.629288
TRAIN ACC = 79.4083 % (47647/60000)
Loss = 0.558899
Loss = 0.674911
Loss = 0.643814
Loss = 0.528381
Loss = 0.574158
Loss = 0.710373
Loss = 0.740356
Loss = 0.695679
Loss = 0.624802
Loss = 0.578644
Loss = 0.741882
Loss = 0.719513
Loss = 0.630829
Loss = 0.65799
Loss = 0.620056
Loss = 0.664581
Loss = 0.613937
Loss = 0.652267
Loss = 0.684692
Loss = 0.626282
TEST LOSS = 0.647102
TEST ACC = 476.469 % (7880/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.602142
Epoch 5.2: Loss = 0.600998
Epoch 5.3: Loss = 0.690842
Epoch 5.4: Loss = 0.656998
Epoch 5.5: Loss = 0.575302
Epoch 5.6: Loss = 0.680984
Epoch 5.7: Loss = 0.603851
Epoch 5.8: Loss = 0.646988
Epoch 5.9: Loss = 0.571106
Epoch 5.10: Loss = 0.692581
Epoch 5.11: Loss = 0.601563
Epoch 5.12: Loss = 0.655563
Epoch 5.13: Loss = 0.640015
Epoch 5.14: Loss = 0.614517
Epoch 5.15: Loss = 0.548599
Epoch 5.16: Loss = 0.645645
Epoch 5.17: Loss = 0.688004
Epoch 5.18: Loss = 0.536407
Epoch 5.19: Loss = 0.690994
Epoch 5.20: Loss = 0.571899
Epoch 5.21: Loss = 0.762756
Epoch 5.22: Loss = 0.703964
Epoch 5.23: Loss = 0.595474
Epoch 5.24: Loss = 0.609329
Epoch 5.25: Loss = 0.534653
Epoch 5.26: Loss = 0.531815
Epoch 5.27: Loss = 0.5858
Epoch 5.28: Loss = 0.623489
Epoch 5.29: Loss = 0.608246
Epoch 5.30: Loss = 0.590469
Epoch 5.31: Loss = 0.65094
Epoch 5.32: Loss = 0.592606
Epoch 5.33: Loss = 0.607147
Epoch 5.34: Loss = 0.543182
Epoch 5.35: Loss = 0.596329
Epoch 5.36: Loss = 0.552719
Epoch 5.37: Loss = 0.653946
Epoch 5.38: Loss = 0.639526
Epoch 5.39: Loss = 0.566635
Epoch 5.40: Loss = 0.559525
Epoch 5.41: Loss = 0.67746
Epoch 5.42: Loss = 0.649063
Epoch 5.43: Loss = 0.588303
Epoch 5.44: Loss = 0.701584
Epoch 5.45: Loss = 0.657349
Epoch 5.46: Loss = 0.598892
Epoch 5.47: Loss = 0.497391
Epoch 5.48: Loss = 0.582108
Epoch 5.49: Loss = 0.600525
Epoch 5.50: Loss = 0.611694
Epoch 5.51: Loss = 0.590897
Epoch 5.52: Loss = 0.614212
Epoch 5.53: Loss = 0.55719
Epoch 5.54: Loss = 0.656967
Epoch 5.55: Loss = 0.620483
Epoch 5.56: Loss = 0.519684
Epoch 5.57: Loss = 0.5121
Epoch 5.58: Loss = 0.642654
Epoch 5.59: Loss = 0.49295
Epoch 5.60: Loss = 0.591461
Epoch 5.61: Loss = 0.599625
Epoch 5.62: Loss = 0.606094
Epoch 5.63: Loss = 0.703934
Epoch 5.64: Loss = 0.687561
Epoch 5.65: Loss = 0.636078
Epoch 5.66: Loss = 0.611298
Epoch 5.67: Loss = 0.615631
Epoch 5.68: Loss = 0.531738
Epoch 5.69: Loss = 0.630371
Epoch 5.70: Loss = 0.513382
Epoch 5.71: Loss = 0.599792
Epoch 5.72: Loss = 0.676636
Epoch 5.73: Loss = 0.62381
Epoch 5.74: Loss = 0.572739
Epoch 5.75: Loss = 0.708206
Epoch 5.76: Loss = 0.637817
Epoch 5.77: Loss = 0.599533
Epoch 5.78: Loss = 0.581924
Epoch 5.79: Loss = 0.601974
Epoch 5.80: Loss = 0.64473
Epoch 5.81: Loss = 0.678696
Epoch 5.82: Loss = 0.585297
Epoch 5.83: Loss = 0.594452
Epoch 5.84: Loss = 0.652451
Epoch 5.85: Loss = 0.61377
Epoch 5.86: Loss = 0.61644
Epoch 5.87: Loss = 0.602722
Epoch 5.88: Loss = 0.617874
Epoch 5.89: Loss = 0.643311
Epoch 5.90: Loss = 0.597534
Epoch 5.91: Loss = 0.550156
Epoch 5.92: Loss = 0.660339
Epoch 5.93: Loss = 0.563904
Epoch 5.94: Loss = 0.719086
Epoch 5.95: Loss = 0.722824
Epoch 5.96: Loss = 0.677078
Epoch 5.97: Loss = 0.574783
Epoch 5.98: Loss = 0.600571
Epoch 5.99: Loss = 0.58078
Epoch 5.100: Loss = 0.602127
Epoch 5.101: Loss = 0.575226
Epoch 5.102: Loss = 0.660934
Epoch 5.103: Loss = 0.592819
Epoch 5.104: Loss = 0.684586
Epoch 5.105: Loss = 0.642578
Epoch 5.106: Loss = 0.682007
Epoch 5.107: Loss = 0.605423
Epoch 5.108: Loss = 0.743958
Epoch 5.109: Loss = 0.677109
Epoch 5.110: Loss = 0.633423
Epoch 5.111: Loss = 0.574356
Epoch 5.112: Loss = 0.595123
Epoch 5.113: Loss = 0.608536
Epoch 5.114: Loss = 0.572449
Epoch 5.115: Loss = 0.632782
Epoch 5.116: Loss = 0.536087
Epoch 5.117: Loss = 0.676834
Epoch 5.118: Loss = 0.609604
Epoch 5.119: Loss = 0.527847
Epoch 5.120: Loss = 0.638489
TRAIN LOSS = 0.615799
TRAIN ACC = 80.1941 % (48118/60000)
Loss = 0.538757
Loss = 0.648727
Loss = 0.61853
Loss = 0.51265
Loss = 0.562576
Loss = 0.688797
Loss = 0.731873
Loss = 0.670593
Loss = 0.604507
Loss = 0.566559
Loss = 0.728485
Loss = 0.705734
Loss = 0.60759
Loss = 0.640732
Loss = 0.603455
Loss = 0.641876
Loss = 0.595627
Loss = 0.636017
Loss = 0.652557
Loss = 0.608536
TEST LOSS = 0.628209
TEST ACC = 481.18 % (7955/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.590363
Epoch 6.2: Loss = 0.573181
Epoch 6.3: Loss = 0.65921
Epoch 6.4: Loss = 0.653503
Epoch 6.5: Loss = 0.563156
Epoch 6.6: Loss = 0.601456
Epoch 6.7: Loss = 0.610504
Epoch 6.8: Loss = 0.572968
Epoch 6.9: Loss = 0.579849
Epoch 6.10: Loss = 0.663055
Epoch 6.11: Loss = 0.662704
Epoch 6.12: Loss = 0.550659
Epoch 6.13: Loss = 0.592758
Epoch 6.14: Loss = 0.507217
Epoch 6.15: Loss = 0.524918
Epoch 6.16: Loss = 0.671112
Epoch 6.17: Loss = 0.647522
Epoch 6.18: Loss = 0.626526
Epoch 6.19: Loss = 0.571945
Epoch 6.20: Loss = 0.683167
Epoch 6.21: Loss = 0.553909
Epoch 6.22: Loss = 0.504776
Epoch 6.23: Loss = 0.544662
Epoch 6.24: Loss = 0.582458
Epoch 6.25: Loss = 0.719879
Epoch 6.26: Loss = 0.606842
Epoch 6.27: Loss = 0.630447
Epoch 6.28: Loss = 0.644501
Epoch 6.29: Loss = 0.576645
Epoch 6.30: Loss = 0.696198
Epoch 6.31: Loss = 0.522049
Epoch 6.32: Loss = 0.582916
Epoch 6.33: Loss = 0.587387
Epoch 6.34: Loss = 0.50174
Epoch 6.35: Loss = 0.693985
Epoch 6.36: Loss = 0.575012
Epoch 6.37: Loss = 0.542313
Epoch 6.38: Loss = 0.610153
Epoch 6.39: Loss = 0.568741
Epoch 6.40: Loss = 0.535324
Epoch 6.41: Loss = 0.615097
Epoch 6.42: Loss = 0.552734
Epoch 6.43: Loss = 0.634048
Epoch 6.44: Loss = 0.562622
Epoch 6.45: Loss = 0.541107
Epoch 6.46: Loss = 0.540756
Epoch 6.47: Loss = 0.696182
Epoch 6.48: Loss = 0.567047
Epoch 6.49: Loss = 0.690475
Epoch 6.50: Loss = 0.678391
Epoch 6.51: Loss = 0.577942
Epoch 6.52: Loss = 0.572937
Epoch 6.53: Loss = 0.546982
Epoch 6.54: Loss = 0.648758
Epoch 6.55: Loss = 0.635605
Epoch 6.56: Loss = 0.666595
Epoch 6.57: Loss = 0.697128
Epoch 6.58: Loss = 0.607071
Epoch 6.59: Loss = 0.635147
Epoch 6.60: Loss = 0.573303
Epoch 6.61: Loss = 0.600861
Epoch 6.62: Loss = 0.572647
Epoch 6.63: Loss = 0.636597
Epoch 6.64: Loss = 0.582993
Epoch 6.65: Loss = 0.586914
Epoch 6.66: Loss = 0.578781
Epoch 6.67: Loss = 0.531372
Epoch 6.68: Loss = 0.627304
Epoch 6.69: Loss = 0.697708
Epoch 6.70: Loss = 0.56665
Epoch 6.71: Loss = 0.627136
Epoch 6.72: Loss = 0.592926
Epoch 6.73: Loss = 0.565857
Epoch 6.74: Loss = 0.672836
Epoch 6.75: Loss = 0.577759
Epoch 6.76: Loss = 0.591873
Epoch 6.77: Loss = 0.628342
Epoch 6.78: Loss = 0.571793
Epoch 6.79: Loss = 0.617188
Epoch 6.80: Loss = 0.630493
Epoch 6.81: Loss = 0.573029
Epoch 6.82: Loss = 0.619415
Epoch 6.83: Loss = 0.585571
Epoch 6.84: Loss = 0.669296
Epoch 6.85: Loss = 0.563156
Epoch 6.86: Loss = 0.626724
Epoch 6.87: Loss = 0.613968
Epoch 6.88: Loss = 0.551376
Epoch 6.89: Loss = 0.549805
Epoch 6.90: Loss = 0.565826
Epoch 6.91: Loss = 0.565765
Epoch 6.92: Loss = 0.68924
Epoch 6.93: Loss = 0.719879
Epoch 6.94: Loss = 0.708817
Epoch 6.95: Loss = 0.584763
Epoch 6.96: Loss = 0.653442
Epoch 6.97: Loss = 0.709366
Epoch 6.98: Loss = 0.549164
Epoch 6.99: Loss = 0.554535
Epoch 6.100: Loss = 0.645294
Epoch 6.101: Loss = 0.488876
Epoch 6.102: Loss = 0.661758
Epoch 6.103: Loss = 0.657364
Epoch 6.104: Loss = 0.571457
Epoch 6.105: Loss = 0.646774
Epoch 6.106: Loss = 0.645157
Epoch 6.107: Loss = 0.695938
Epoch 6.108: Loss = 0.551727
Epoch 6.109: Loss = 0.690247
Epoch 6.110: Loss = 0.542282
Epoch 6.111: Loss = 0.580597
Epoch 6.112: Loss = 0.687103
Epoch 6.113: Loss = 0.594086
Epoch 6.114: Loss = 0.533401
Epoch 6.115: Loss = 0.719757
Epoch 6.116: Loss = 0.620911
Epoch 6.117: Loss = 0.565018
Epoch 6.118: Loss = 0.588058
Epoch 6.119: Loss = 0.563782
Epoch 6.120: Loss = 0.608704
TRAIN LOSS = 0.60582
TRAIN ACC = 80.7693 % (48464/60000)
Loss = 0.534821
Loss = 0.640594
Loss = 0.617371
Loss = 0.495682
Loss = 0.560043
Loss = 0.690582
Loss = 0.729889
Loss = 0.681198
Loss = 0.597382
Loss = 0.567581
Loss = 0.750534
Loss = 0.706741
Loss = 0.611801
Loss = 0.627167
Loss = 0.602692
Loss = 0.637894
Loss = 0.595398
Loss = 0.638733
Loss = 0.65239
Loss = 0.603348
TEST LOSS = 0.627092
TEST ACC = 484.639 % (8017/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.548569
Epoch 7.2: Loss = 0.656082
Epoch 7.3: Loss = 0.652023
Epoch 7.4: Loss = 0.57309
Epoch 7.5: Loss = 0.662781
Epoch 7.6: Loss = 0.698639
Epoch 7.7: Loss = 0.631287
Epoch 7.8: Loss = 0.646881
Epoch 7.9: Loss = 0.565414
Epoch 7.10: Loss = 0.562241
Epoch 7.11: Loss = 0.65509
Epoch 7.12: Loss = 0.701935
Epoch 7.13: Loss = 0.679001
Epoch 7.14: Loss = 0.513885
Epoch 7.15: Loss = 0.589005
Epoch 7.16: Loss = 0.645493
Epoch 7.17: Loss = 0.470871
Epoch 7.18: Loss = 0.545456
Epoch 7.19: Loss = 0.608734
Epoch 7.20: Loss = 0.700409
Epoch 7.21: Loss = 0.603699
Epoch 7.22: Loss = 0.542358
Epoch 7.23: Loss = 0.556442
Epoch 7.24: Loss = 0.561234
Epoch 7.25: Loss = 0.514404
Epoch 7.26: Loss = 0.509094
Epoch 7.27: Loss = 0.517456
Epoch 7.28: Loss = 0.66156
Epoch 7.29: Loss = 0.699554
Epoch 7.30: Loss = 0.570313
Epoch 7.31: Loss = 0.504532
Epoch 7.32: Loss = 0.545471
Epoch 7.33: Loss = 0.539017
Epoch 7.34: Loss = 0.657211
Epoch 7.35: Loss = 0.623581
Epoch 7.36: Loss = 0.606415
Epoch 7.37: Loss = 0.533691
Epoch 7.38: Loss = 0.617874
Epoch 7.39: Loss = 0.491394
Epoch 7.40: Loss = 0.5849
Epoch 7.41: Loss = 0.68985
Epoch 7.42: Loss = 0.644333
Epoch 7.43: Loss = 0.509537
Epoch 7.44: Loss = 0.676315
Epoch 7.45: Loss = 0.55278
Epoch 7.46: Loss = 0.685089
Epoch 7.47: Loss = 0.485718
Epoch 7.48: Loss = 0.581955
Epoch 7.49: Loss = 0.513077
Epoch 7.50: Loss = 0.489304
Epoch 7.51: Loss = 0.64743
Epoch 7.52: Loss = 0.624252
Epoch 7.53: Loss = 0.660065
Epoch 7.54: Loss = 0.561783
Epoch 7.55: Loss = 0.541351
Epoch 7.56: Loss = 0.5849
Epoch 7.57: Loss = 0.679428
Epoch 7.58: Loss = 0.523834
Epoch 7.59: Loss = 0.63443
Epoch 7.60: Loss = 0.681549
Epoch 7.61: Loss = 0.64238
Epoch 7.62: Loss = 0.639359
Epoch 7.63: Loss = 0.651413
Epoch 7.64: Loss = 0.564407
Epoch 7.65: Loss = 0.532486
Epoch 7.66: Loss = 0.592484
Epoch 7.67: Loss = 0.622589
Epoch 7.68: Loss = 0.632858
Epoch 7.69: Loss = 0.484329
Epoch 7.70: Loss = 0.718048
Epoch 7.71: Loss = 0.617538
Epoch 7.72: Loss = 0.607803
Epoch 7.73: Loss = 0.618439
Epoch 7.74: Loss = 0.689362
Epoch 7.75: Loss = 0.554779
Epoch 7.76: Loss = 0.626755
Epoch 7.77: Loss = 0.575653
Epoch 7.78: Loss = 0.698441
Epoch 7.79: Loss = 0.689621
Epoch 7.80: Loss = 0.546997
Epoch 7.81: Loss = 0.633377
Epoch 7.82: Loss = 0.551041
Epoch 7.83: Loss = 0.575745
Epoch 7.84: Loss = 0.685287
Epoch 7.85: Loss = 0.685547
Epoch 7.86: Loss = 0.655991
Epoch 7.87: Loss = 0.541092
Epoch 7.88: Loss = 0.52066
Epoch 7.89: Loss = 0.614197
Epoch 7.90: Loss = 0.669846
Epoch 7.91: Loss = 0.571503
Epoch 7.92: Loss = 0.541794
Epoch 7.93: Loss = 0.609741
Epoch 7.94: Loss = 0.558899
Epoch 7.95: Loss = 0.564804
Epoch 7.96: Loss = 0.60318
Epoch 7.97: Loss = 0.610306
Epoch 7.98: Loss = 0.615387
Epoch 7.99: Loss = 0.586716
Epoch 7.100: Loss = 0.65155
Epoch 7.101: Loss = 0.492111
Epoch 7.102: Loss = 0.699829
Epoch 7.103: Loss = 0.560745
Epoch 7.104: Loss = 0.513504
Epoch 7.105: Loss = 0.546234
Epoch 7.106: Loss = 0.617142
Epoch 7.107: Loss = 0.543823
Epoch 7.108: Loss = 0.661011
Epoch 7.109: Loss = 0.600906
Epoch 7.110: Loss = 0.636292
Epoch 7.111: Loss = 0.56102
Epoch 7.112: Loss = 0.522949
Epoch 7.113: Loss = 0.626648
Epoch 7.114: Loss = 0.564835
Epoch 7.115: Loss = 0.614365
Epoch 7.116: Loss = 0.690445
Epoch 7.117: Loss = 0.562714
Epoch 7.118: Loss = 0.646805
Epoch 7.119: Loss = 0.606506
Epoch 7.120: Loss = 0.669998
TRAIN LOSS = 0.600052
TRAIN ACC = 81.0944 % (48659/60000)
Loss = 0.52771
Loss = 0.630753
Loss = 0.607803
Loss = 0.491531
Loss = 0.56221
Loss = 0.676682
Loss = 0.731567
Loss = 0.661407
Loss = 0.586365
Loss = 0.565094
Loss = 0.747238
Loss = 0.713898
Loss = 0.611786
Loss = 0.630997
Loss = 0.588684
Loss = 0.630722
Loss = 0.588058
Loss = 0.634384
Loss = 0.637573
Loss = 0.603027
TEST LOSS = 0.621374
TEST ACC = 486.589 % (8031/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.525879
Epoch 8.2: Loss = 0.586304
Epoch 8.3: Loss = 0.604156
Epoch 8.4: Loss = 0.570786
Epoch 8.5: Loss = 0.545547
Epoch 8.6: Loss = 0.633148
Epoch 8.7: Loss = 0.615723
Epoch 8.8: Loss = 0.593506
Epoch 8.9: Loss = 0.535599
Epoch 8.10: Loss = 0.546295
Epoch 8.11: Loss = 0.681213
Epoch 8.12: Loss = 0.539124
Epoch 8.13: Loss = 0.579987
Epoch 8.14: Loss = 0.572128
Epoch 8.15: Loss = 0.572784
Epoch 8.16: Loss = 0.599487
Epoch 8.17: Loss = 0.536804
Epoch 8.18: Loss = 0.609879
Epoch 8.19: Loss = 0.589447
Epoch 8.20: Loss = 0.561264
Epoch 8.21: Loss = 0.659119
Epoch 8.22: Loss = 0.55864
Epoch 8.23: Loss = 0.697571
Epoch 8.24: Loss = 0.552002
Epoch 8.25: Loss = 0.563644
Epoch 8.26: Loss = 0.642319
Epoch 8.27: Loss = 0.642288
Epoch 8.28: Loss = 0.572968
Epoch 8.29: Loss = 0.635498
Epoch 8.30: Loss = 0.481033
Epoch 8.31: Loss = 0.634155
Epoch 8.32: Loss = 0.549179
Epoch 8.33: Loss = 0.601761
Epoch 8.34: Loss = 0.585968
Epoch 8.35: Loss = 0.633972
Epoch 8.36: Loss = 0.654556
Epoch 8.37: Loss = 0.546967
Epoch 8.38: Loss = 0.70079
Epoch 8.39: Loss = 0.615692
Epoch 8.40: Loss = 0.546143
Epoch 8.41: Loss = 0.589233
Epoch 8.42: Loss = 0.769394
Epoch 8.43: Loss = 0.714951
Epoch 8.44: Loss = 0.565109
Epoch 8.45: Loss = 0.564224
Epoch 8.46: Loss = 0.704666
Epoch 8.47: Loss = 0.716843
Epoch 8.48: Loss = 0.574066
Epoch 8.49: Loss = 0.547058
Epoch 8.50: Loss = 0.619431
Epoch 8.51: Loss = 0.575562
Epoch 8.52: Loss = 0.545792
Epoch 8.53: Loss = 0.690201
Epoch 8.54: Loss = 0.523987
Epoch 8.55: Loss = 0.560989
Epoch 8.56: Loss = 0.615891
Epoch 8.57: Loss = 0.555649
Epoch 8.58: Loss = 0.620102
Epoch 8.59: Loss = 0.505585
Epoch 8.60: Loss = 0.630524
Epoch 8.61: Loss = 0.605118
Epoch 8.62: Loss = 0.535507
Epoch 8.63: Loss = 0.554291
Epoch 8.64: Loss = 0.630325
Epoch 8.65: Loss = 0.629059
Epoch 8.66: Loss = 0.662689
Epoch 8.67: Loss = 0.618042
Epoch 8.68: Loss = 0.671844
Epoch 8.69: Loss = 0.513214
Epoch 8.70: Loss = 0.669937
Epoch 8.71: Loss = 0.684494
Epoch 8.72: Loss = 0.531418
Epoch 8.73: Loss = 0.607193
Epoch 8.74: Loss = 0.68187
Epoch 8.75: Loss = 0.538513
Epoch 8.76: Loss = 0.613998
Epoch 8.77: Loss = 0.615372
Epoch 8.78: Loss = 0.514282
Epoch 8.79: Loss = 0.704773
Epoch 8.80: Loss = 0.69014
Epoch 8.81: Loss = 0.585175
Epoch 8.82: Loss = 0.660355
Epoch 8.83: Loss = 0.589645
Epoch 8.84: Loss = 0.570663
Epoch 8.85: Loss = 0.555893
Epoch 8.86: Loss = 0.615417
Epoch 8.87: Loss = 0.566528
Epoch 8.88: Loss = 0.632416
Epoch 8.89: Loss = 0.633133
Epoch 8.90: Loss = 0.608109
Epoch 8.91: Loss = 0.630051
Epoch 8.92: Loss = 0.677856
Epoch 8.93: Loss = 0.663651
Epoch 8.94: Loss = 0.55867
Epoch 8.95: Loss = 0.495499
Epoch 8.96: Loss = 0.550156
Epoch 8.97: Loss = 0.586578
Epoch 8.98: Loss = 0.597198
Epoch 8.99: Loss = 0.42778
Epoch 8.100: Loss = 0.59494
Epoch 8.101: Loss = 0.599457
Epoch 8.102: Loss = 0.642395
Epoch 8.103: Loss = 0.498505
Epoch 8.104: Loss = 0.535294
Epoch 8.105: Loss = 0.547089
Epoch 8.106: Loss = 0.645294
Epoch 8.107: Loss = 0.710709
Epoch 8.108: Loss = 0.517258
Epoch 8.109: Loss = 0.67421
Epoch 8.110: Loss = 0.594879
Epoch 8.111: Loss = 0.5672
Epoch 8.112: Loss = 0.579941
Epoch 8.113: Loss = 0.470627
Epoch 8.114: Loss = 0.635513
Epoch 8.115: Loss = 0.574478
Epoch 8.116: Loss = 0.56192
Epoch 8.117: Loss = 0.549194
Epoch 8.118: Loss = 0.58284
Epoch 8.119: Loss = 0.628876
Epoch 8.120: Loss = 0.650665
TRAIN LOSS = 0.597534
TRAIN ACC = 81.456 % (48875/60000)
Loss = 0.522125
Loss = 0.629013
Loss = 0.597153
Loss = 0.491058
Loss = 0.560349
Loss = 0.6698
Loss = 0.736359
Loss = 0.659409
Loss = 0.58812
Loss = 0.567795
Loss = 0.751251
Loss = 0.709702
Loss = 0.613159
Loss = 0.634918
Loss = 0.590729
Loss = 0.626938
Loss = 0.585876
Loss = 0.634598
Loss = 0.628174
Loss = 0.601608
TEST LOSS = 0.619906
TEST ACC = 488.75 % (8056/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.512543
Epoch 9.2: Loss = 0.660446
Epoch 9.3: Loss = 0.71492
Epoch 9.4: Loss = 0.618729
Epoch 9.5: Loss = 0.632477
Epoch 9.6: Loss = 0.603271
Epoch 9.7: Loss = 0.656891
Epoch 9.8: Loss = 0.616776
Epoch 9.9: Loss = 0.581375
Epoch 9.10: Loss = 0.733978
Epoch 9.11: Loss = 0.488663
Epoch 9.12: Loss = 0.575363
Epoch 9.13: Loss = 0.578079
Epoch 9.14: Loss = 0.60112
Epoch 9.15: Loss = 0.5392
Epoch 9.16: Loss = 0.58522
Epoch 9.17: Loss = 0.623245
Epoch 9.18: Loss = 0.678665
Epoch 9.19: Loss = 0.477661
Epoch 9.20: Loss = 0.540649
Epoch 9.21: Loss = 0.518295
Epoch 9.22: Loss = 0.577393
Epoch 9.23: Loss = 0.541367
Epoch 9.24: Loss = 0.511215
Epoch 9.25: Loss = 0.640823
Epoch 9.26: Loss = 0.652344
Epoch 9.27: Loss = 0.651459
Epoch 9.28: Loss = 0.493942
Epoch 9.29: Loss = 0.691086
Epoch 9.30: Loss = 0.659973
Epoch 9.31: Loss = 0.626953
Epoch 9.32: Loss = 0.555893
Epoch 9.33: Loss = 0.656357
Epoch 9.34: Loss = 0.585358
Epoch 9.35: Loss = 0.542633
Epoch 9.36: Loss = 0.507858
Epoch 9.37: Loss = 0.671158
Epoch 9.38: Loss = 0.636826
Epoch 9.39: Loss = 0.664963
Epoch 9.40: Loss = 0.594101
Epoch 9.41: Loss = 0.595398
Epoch 9.42: Loss = 0.541473
Epoch 9.43: Loss = 0.477875
Epoch 9.44: Loss = 0.765869
Epoch 9.45: Loss = 0.461563
Epoch 9.46: Loss = 0.612976
Epoch 9.47: Loss = 0.556778
Epoch 9.48: Loss = 0.568649
Epoch 9.49: Loss = 0.581085
Epoch 9.50: Loss = 0.632828
Epoch 9.51: Loss = 0.535477
Epoch 9.52: Loss = 0.61203
Epoch 9.53: Loss = 0.608505
Epoch 9.54: Loss = 0.659637
Epoch 9.55: Loss = 0.642654
Epoch 9.56: Loss = 0.567825
Epoch 9.57: Loss = 0.533905
Epoch 9.58: Loss = 0.560669
Epoch 9.59: Loss = 0.505692
Epoch 9.60: Loss = 0.45282
Epoch 9.61: Loss = 0.589966
Epoch 9.62: Loss = 0.598129
Epoch 9.63: Loss = 0.539001
Epoch 9.64: Loss = 0.610489
Epoch 9.65: Loss = 0.623871
Epoch 9.66: Loss = 0.551117
Epoch 9.67: Loss = 0.678864
Epoch 9.68: Loss = 0.575821
Epoch 9.69: Loss = 0.5233
Epoch 9.70: Loss = 0.581787
Epoch 9.71: Loss = 0.49762
Epoch 9.72: Loss = 0.508408
Epoch 9.73: Loss = 0.571915
Epoch 9.74: Loss = 0.605011
Epoch 9.75: Loss = 0.619019
Epoch 9.76: Loss = 0.727417
Epoch 9.77: Loss = 0.617996
Epoch 9.78: Loss = 0.558167
Epoch 9.79: Loss = 0.625031
Epoch 9.80: Loss = 0.615524
Epoch 9.81: Loss = 0.604233
Epoch 9.82: Loss = 0.72316
Epoch 9.83: Loss = 0.512329
Epoch 9.84: Loss = 0.57579
Epoch 9.85: Loss = 0.617752
Epoch 9.86: Loss = 0.581604
Epoch 9.87: Loss = 0.596817
Epoch 9.88: Loss = 0.772232
Epoch 9.89: Loss = 0.576401
Epoch 9.90: Loss = 0.584061
Epoch 9.91: Loss = 0.521072
Epoch 9.92: Loss = 0.547897
Epoch 9.93: Loss = 0.597488
Epoch 9.94: Loss = 0.640823
Epoch 9.95: Loss = 0.519516
Epoch 9.96: Loss = 0.510315
Epoch 9.97: Loss = 0.512009
Epoch 9.98: Loss = 0.611526
Epoch 9.99: Loss = 0.549347
Epoch 9.100: Loss = 0.598145
Epoch 9.101: Loss = 0.574356
Epoch 9.102: Loss = 0.658691
Epoch 9.103: Loss = 0.554153
Epoch 9.104: Loss = 0.537338
Epoch 9.105: Loss = 0.553223
Epoch 9.106: Loss = 0.571899
Epoch 9.107: Loss = 0.603897
Epoch 9.108: Loss = 0.573669
Epoch 9.109: Loss = 0.475906
Epoch 9.110: Loss = 0.513214
Epoch 9.111: Loss = 0.667938
Epoch 9.112: Loss = 0.621445
Epoch 9.113: Loss = 0.659439
Epoch 9.114: Loss = 0.574783
Epoch 9.115: Loss = 0.567551
Epoch 9.116: Loss = 0.520813
Epoch 9.117: Loss = 0.67952
Epoch 9.118: Loss = 0.672928
Epoch 9.119: Loss = 0.625809
Epoch 9.120: Loss = 0.651031
TRAIN LOSS = 0.590851
TRAIN ACC = 81.9168 % (49152/60000)
Loss = 0.519135
Loss = 0.620987
Loss = 0.591568
Loss = 0.483002
Loss = 0.557648
Loss = 0.666702
Loss = 0.737747
Loss = 0.655228
Loss = 0.582306
Loss = 0.56192
Loss = 0.754974
Loss = 0.703506
Loss = 0.61969
Loss = 0.627792
Loss = 0.5867
Loss = 0.620361
Loss = 0.588547
Loss = 0.631271
Loss = 0.626114
Loss = 0.590546
TEST LOSS = 0.616287
TEST ACC = 491.519 % (8111/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.587967
Epoch 10.2: Loss = 0.48671
Epoch 10.3: Loss = 0.667694
Epoch 10.4: Loss = 0.589508
Epoch 10.5: Loss = 0.605011
Epoch 10.6: Loss = 0.635193
Epoch 10.7: Loss = 0.634003
Epoch 10.8: Loss = 0.538971
Epoch 10.9: Loss = 0.653305
Epoch 10.10: Loss = 0.556808
Epoch 10.11: Loss = 0.554413
Epoch 10.12: Loss = 0.608536
Epoch 10.13: Loss = 0.47345
Epoch 10.14: Loss = 0.512238
Epoch 10.15: Loss = 0.696976
Epoch 10.16: Loss = 0.646866
Epoch 10.17: Loss = 0.571823
Epoch 10.18: Loss = 0.505997
Epoch 10.19: Loss = 0.533249
Epoch 10.20: Loss = 0.61911
Epoch 10.21: Loss = 0.516586
Epoch 10.22: Loss = 0.502487
Epoch 10.23: Loss = 0.454315
Epoch 10.24: Loss = 0.575195
Epoch 10.25: Loss = 0.569046
Epoch 10.26: Loss = 0.612259
Epoch 10.27: Loss = 0.614075
Epoch 10.28: Loss = 0.627579
Epoch 10.29: Loss = 0.560303
Epoch 10.30: Loss = 0.620956
Epoch 10.31: Loss = 0.761154
Epoch 10.32: Loss = 0.610397
Epoch 10.33: Loss = 0.639771
Epoch 10.34: Loss = 0.595078
Epoch 10.35: Loss = 0.645233
Epoch 10.36: Loss = 0.648819
Epoch 10.37: Loss = 0.606308
Epoch 10.38: Loss = 0.580475
Epoch 10.39: Loss = 0.508499
Epoch 10.40: Loss = 0.501892
Epoch 10.41: Loss = 0.53479
Epoch 10.42: Loss = 0.665634
Epoch 10.43: Loss = 0.592514
Epoch 10.44: Loss = 0.534393
Epoch 10.45: Loss = 0.676651
Epoch 10.46: Loss = 0.719345
Epoch 10.47: Loss = 0.479523
Epoch 10.48: Loss = 0.581482
Epoch 10.49: Loss = 0.592209
Epoch 10.50: Loss = 0.601227
Epoch 10.51: Loss = 0.601303
Epoch 10.52: Loss = 0.560318
Epoch 10.53: Loss = 0.590424
Epoch 10.54: Loss = 0.590088
Epoch 10.55: Loss = 0.439087
Epoch 10.56: Loss = 0.650391
Epoch 10.57: Loss = 0.594757
Epoch 10.58: Loss = 0.600632
Epoch 10.59: Loss = 0.590012
Epoch 10.60: Loss = 0.429092
Epoch 10.61: Loss = 0.55368
Epoch 10.62: Loss = 0.484116
Epoch 10.63: Loss = 0.521179
Epoch 10.64: Loss = 0.592209
Epoch 10.65: Loss = 0.601364
Epoch 10.66: Loss = 0.471039
Epoch 10.67: Loss = 0.561234
Epoch 10.68: Loss = 0.639328
Epoch 10.69: Loss = 0.694183
Epoch 10.70: Loss = 0.519211
Epoch 10.71: Loss = 0.638306
Epoch 10.72: Loss = 0.64267
Epoch 10.73: Loss = 0.633499
Epoch 10.74: Loss = 0.651535
Epoch 10.75: Loss = 0.685135
Epoch 10.76: Loss = 0.537338
Epoch 10.77: Loss = 0.586823
Epoch 10.78: Loss = 0.519989
Epoch 10.79: Loss = 0.64975
Epoch 10.80: Loss = 0.493546
Epoch 10.81: Loss = 0.583862
Epoch 10.82: Loss = 0.637482
Epoch 10.83: Loss = 0.525421
Epoch 10.84: Loss = 0.570694
Epoch 10.85: Loss = 0.630096
Epoch 10.86: Loss = 0.566772
Epoch 10.87: Loss = 0.710129
Epoch 10.88: Loss = 0.631012
Epoch 10.89: Loss = 0.630096
Epoch 10.90: Loss = 0.442001
Epoch 10.91: Loss = 0.565704
Epoch 10.92: Loss = 0.531342
Epoch 10.93: Loss = 0.507645
Epoch 10.94: Loss = 0.593414
Epoch 10.95: Loss = 0.58728
Epoch 10.96: Loss = 0.612656
Epoch 10.97: Loss = 0.6595
Epoch 10.98: Loss = 0.693756
Epoch 10.99: Loss = 0.642075
Epoch 10.100: Loss = 0.629959
Epoch 10.101: Loss = 0.587601
Epoch 10.102: Loss = 0.714035
Epoch 10.103: Loss = 0.690659
Epoch 10.104: Loss = 0.456314
Epoch 10.105: Loss = 0.620132
Epoch 10.106: Loss = 0.591232
Epoch 10.107: Loss = 0.65213
Epoch 10.108: Loss = 0.4897
Epoch 10.109: Loss = 0.547348
Epoch 10.110: Loss = 0.52533
Epoch 10.111: Loss = 0.522888
Epoch 10.112: Loss = 0.580734
Epoch 10.113: Loss = 0.672073
Epoch 10.114: Loss = 0.600143
Epoch 10.115: Loss = 0.551956
Epoch 10.116: Loss = 0.52858
Epoch 10.117: Loss = 0.627243
Epoch 10.118: Loss = 0.514587
Epoch 10.119: Loss = 0.651169
Epoch 10.120: Loss = 0.593079
TRAIN LOSS = 0.586639
TRAIN ACC = 82.1533 % (49294/60000)
Loss = 0.520294
Loss = 0.613739
Loss = 0.59259
Loss = 0.481628
Loss = 0.56337
Loss = 0.659943
Loss = 0.731979
Loss = 0.658783
Loss = 0.587723
Loss = 0.567856
Loss = 0.76709
Loss = 0.712952
Loss = 0.623779
Loss = 0.633514
Loss = 0.593735
Loss = 0.620743
Loss = 0.586472
Loss = 0.627853
Loss = 0.630371
Loss = 0.600143
TEST LOSS = 0.618728
TEST ACC = 492.94 % (8107/10000)

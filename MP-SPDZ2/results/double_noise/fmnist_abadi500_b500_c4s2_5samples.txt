Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.49353
Epoch 1.2: Loss = 2.32251
Epoch 1.3: Loss = 2.23183
Epoch 1.4: Loss = 2.15045
Epoch 1.5: Loss = 2.06172
Epoch 1.6: Loss = 1.99852
Epoch 1.7: Loss = 1.92426
Epoch 1.8: Loss = 1.85483
Epoch 1.9: Loss = 1.85576
Epoch 1.10: Loss = 1.76273
Epoch 1.11: Loss = 1.7009
Epoch 1.12: Loss = 1.72009
Epoch 1.13: Loss = 1.61958
Epoch 1.14: Loss = 1.58928
Epoch 1.15: Loss = 1.49712
Epoch 1.16: Loss = 1.52161
Epoch 1.17: Loss = 1.50189
Epoch 1.18: Loss = 1.45061
Epoch 1.19: Loss = 1.45593
Epoch 1.20: Loss = 1.40211
Epoch 1.21: Loss = 1.42726
Epoch 1.22: Loss = 1.31404
Epoch 1.23: Loss = 1.33037
Epoch 1.24: Loss = 1.28215
Epoch 1.25: Loss = 1.27707
Epoch 1.26: Loss = 1.26633
Epoch 1.27: Loss = 1.15918
Epoch 1.28: Loss = 1.17848
Epoch 1.29: Loss = 1.22264
Epoch 1.30: Loss = 1.12383
Epoch 1.31: Loss = 1.16042
Epoch 1.32: Loss = 1.1299
Epoch 1.33: Loss = 1.08084
Epoch 1.34: Loss = 1.15253
Epoch 1.35: Loss = 1.09781
Epoch 1.36: Loss = 1.03328
Epoch 1.37: Loss = 1.07097
Epoch 1.38: Loss = 1.03227
Epoch 1.39: Loss = 1.04105
Epoch 1.40: Loss = 0.994324
Epoch 1.41: Loss = 0.993271
Epoch 1.42: Loss = 1.02031
Epoch 1.43: Loss = 1.02304
Epoch 1.44: Loss = 0.938004
Epoch 1.45: Loss = 0.937057
Epoch 1.46: Loss = 1.01978
Epoch 1.47: Loss = 1.05762
Epoch 1.48: Loss = 0.993393
Epoch 1.49: Loss = 0.986328
Epoch 1.50: Loss = 0.881729
Epoch 1.51: Loss = 0.998749
Epoch 1.52: Loss = 0.954819
Epoch 1.53: Loss = 0.936218
Epoch 1.54: Loss = 0.921341
Epoch 1.55: Loss = 0.991348
Epoch 1.56: Loss = 0.84523
Epoch 1.57: Loss = 0.888107
Epoch 1.58: Loss = 0.942368
Epoch 1.59: Loss = 0.973389
Epoch 1.60: Loss = 0.982758
Epoch 1.61: Loss = 0.91037
Epoch 1.62: Loss = 0.907959
Epoch 1.63: Loss = 0.835754
Epoch 1.64: Loss = 0.886185
Epoch 1.65: Loss = 0.905853
Epoch 1.66: Loss = 0.815659
Epoch 1.67: Loss = 0.862076
Epoch 1.68: Loss = 0.890152
Epoch 1.69: Loss = 0.810867
Epoch 1.70: Loss = 0.82106
Epoch 1.71: Loss = 0.818314
Epoch 1.72: Loss = 0.832199
Epoch 1.73: Loss = 0.845062
Epoch 1.74: Loss = 0.757843
Epoch 1.75: Loss = 0.837143
Epoch 1.76: Loss = 0.7733
Epoch 1.77: Loss = 0.799271
Epoch 1.78: Loss = 0.749008
Epoch 1.79: Loss = 0.873917
Epoch 1.80: Loss = 0.798203
Epoch 1.81: Loss = 0.808456
Epoch 1.82: Loss = 0.84259
Epoch 1.83: Loss = 0.839432
Epoch 1.84: Loss = 0.815308
Epoch 1.85: Loss = 0.756348
Epoch 1.86: Loss = 0.808701
Epoch 1.87: Loss = 0.762756
Epoch 1.88: Loss = 0.737
Epoch 1.89: Loss = 0.824249
Epoch 1.90: Loss = 0.883392
Epoch 1.91: Loss = 0.819824
Epoch 1.92: Loss = 0.758316
Epoch 1.93: Loss = 0.783432
Epoch 1.94: Loss = 0.750397
Epoch 1.95: Loss = 0.82164
Epoch 1.96: Loss = 0.756042
Epoch 1.97: Loss = 0.73175
Epoch 1.98: Loss = 0.735489
Epoch 1.99: Loss = 0.826553
Epoch 1.100: Loss = 0.851898
Epoch 1.101: Loss = 0.836395
Epoch 1.102: Loss = 0.858566
Epoch 1.103: Loss = 0.713181
Epoch 1.104: Loss = 0.787003
Epoch 1.105: Loss = 0.740387
Epoch 1.106: Loss = 0.794571
Epoch 1.107: Loss = 0.796295
Epoch 1.108: Loss = 0.781189
Epoch 1.109: Loss = 0.722809
Epoch 1.110: Loss = 0.78981
Epoch 1.111: Loss = 0.786957
Epoch 1.112: Loss = 0.756622
Epoch 1.113: Loss = 0.748383
Epoch 1.114: Loss = 0.767349
Epoch 1.115: Loss = 0.822098
Epoch 1.116: Loss = 0.650024
Epoch 1.117: Loss = 0.75621
Epoch 1.118: Loss = 0.759613
Epoch 1.119: Loss = 0.806488
Epoch 1.120: Loss = 0.752197
TRAIN LOSS = 1.05705
TRAIN ACC = 65.6937 % (39418/60000)
Loss = 0.680069
Loss = 0.777557
Loss = 0.770035
Loss = 0.694351
Loss = 0.692352
Loss = 0.831207
Loss = 0.85434
Loss = 0.799866
Loss = 0.733231
Loss = 0.684738
Loss = 0.800552
Loss = 0.759827
Loss = 0.765411
Loss = 0.773849
Loss = 0.735062
Loss = 0.802109
Loss = 0.727615
Loss = 0.755692
Loss = 0.811981
Loss = 0.757538
TEST LOSS = 0.760369
TEST ACC = 394.179 % (7382/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.735992
Epoch 2.2: Loss = 0.814804
Epoch 2.3: Loss = 0.726685
Epoch 2.4: Loss = 0.756317
Epoch 2.5: Loss = 0.789688
Epoch 2.6: Loss = 0.72908
Epoch 2.7: Loss = 0.764862
Epoch 2.8: Loss = 0.667984
Epoch 2.9: Loss = 0.695618
Epoch 2.10: Loss = 0.765244
Epoch 2.11: Loss = 0.737381
Epoch 2.12: Loss = 0.761322
Epoch 2.13: Loss = 0.746994
Epoch 2.14: Loss = 0.75444
Epoch 2.15: Loss = 0.719162
Epoch 2.16: Loss = 0.788727
Epoch 2.17: Loss = 0.706436
Epoch 2.18: Loss = 0.759552
Epoch 2.19: Loss = 0.712814
Epoch 2.20: Loss = 0.805252
Epoch 2.21: Loss = 0.730713
Epoch 2.22: Loss = 0.70372
Epoch 2.23: Loss = 0.748749
Epoch 2.24: Loss = 0.733994
Epoch 2.25: Loss = 0.756363
Epoch 2.26: Loss = 0.693726
Epoch 2.27: Loss = 0.724838
Epoch 2.28: Loss = 0.648224
Epoch 2.29: Loss = 0.799576
Epoch 2.30: Loss = 0.731506
Epoch 2.31: Loss = 0.69606
Epoch 2.32: Loss = 0.710052
Epoch 2.33: Loss = 0.746841
Epoch 2.34: Loss = 0.710266
Epoch 2.35: Loss = 0.690262
Epoch 2.36: Loss = 0.733734
Epoch 2.37: Loss = 0.692795
Epoch 2.38: Loss = 0.673096
Epoch 2.39: Loss = 0.698944
Epoch 2.40: Loss = 0.740784
Epoch 2.41: Loss = 0.697403
Epoch 2.42: Loss = 0.771622
Epoch 2.43: Loss = 0.692749
Epoch 2.44: Loss = 0.790695
Epoch 2.45: Loss = 0.718842
Epoch 2.46: Loss = 0.612808
Epoch 2.47: Loss = 0.703506
Epoch 2.48: Loss = 0.617966
Epoch 2.49: Loss = 0.65744
Epoch 2.50: Loss = 0.703125
Epoch 2.51: Loss = 0.73233
Epoch 2.52: Loss = 0.625092
Epoch 2.53: Loss = 0.696808
Epoch 2.54: Loss = 0.693848
Epoch 2.55: Loss = 0.654221
Epoch 2.56: Loss = 0.742813
Epoch 2.57: Loss = 0.716888
Epoch 2.58: Loss = 0.75351
Epoch 2.59: Loss = 0.711823
Epoch 2.60: Loss = 0.720871
Epoch 2.61: Loss = 0.74939
Epoch 2.62: Loss = 0.693771
Epoch 2.63: Loss = 0.68631
Epoch 2.64: Loss = 0.633041
Epoch 2.65: Loss = 0.682434
Epoch 2.66: Loss = 0.662537
Epoch 2.67: Loss = 0.664001
Epoch 2.68: Loss = 0.695709
Epoch 2.69: Loss = 0.719635
Epoch 2.70: Loss = 0.690506
Epoch 2.71: Loss = 0.740417
Epoch 2.72: Loss = 0.620621
Epoch 2.73: Loss = 0.720276
Epoch 2.74: Loss = 0.699997
Epoch 2.75: Loss = 0.724716
Epoch 2.76: Loss = 0.638748
Epoch 2.77: Loss = 0.680878
Epoch 2.78: Loss = 0.595352
Epoch 2.79: Loss = 0.666702
Epoch 2.80: Loss = 0.753891
Epoch 2.81: Loss = 0.653732
Epoch 2.82: Loss = 0.587723
Epoch 2.83: Loss = 0.720596
Epoch 2.84: Loss = 0.685837
Epoch 2.85: Loss = 0.772797
Epoch 2.86: Loss = 0.665863
Epoch 2.87: Loss = 0.717484
Epoch 2.88: Loss = 0.726166
Epoch 2.89: Loss = 0.661224
Epoch 2.90: Loss = 0.664734
Epoch 2.91: Loss = 0.707657
Epoch 2.92: Loss = 0.616928
Epoch 2.93: Loss = 0.634018
Epoch 2.94: Loss = 0.711258
Epoch 2.95: Loss = 0.691803
Epoch 2.96: Loss = 0.762466
Epoch 2.97: Loss = 0.60025
Epoch 2.98: Loss = 0.745895
Epoch 2.99: Loss = 0.700989
Epoch 2.100: Loss = 0.617935
Epoch 2.101: Loss = 0.79599
Epoch 2.102: Loss = 0.692245
Epoch 2.103: Loss = 0.893433
Epoch 2.104: Loss = 0.619888
Epoch 2.105: Loss = 0.728912
Epoch 2.106: Loss = 0.716324
Epoch 2.107: Loss = 0.613663
Epoch 2.108: Loss = 0.694595
Epoch 2.109: Loss = 0.771011
Epoch 2.110: Loss = 0.654953
Epoch 2.111: Loss = 0.687424
Epoch 2.112: Loss = 0.638626
Epoch 2.113: Loss = 0.661011
Epoch 2.114: Loss = 0.681686
Epoch 2.115: Loss = 0.688583
Epoch 2.116: Loss = 0.75293
Epoch 2.117: Loss = 0.583206
Epoch 2.118: Loss = 0.756317
Epoch 2.119: Loss = 0.697983
Epoch 2.120: Loss = 0.648132
TRAIN LOSS = 0.70549
TRAIN ACC = 76.4923 % (45898/60000)
Loss = 0.630951
Loss = 0.721069
Loss = 0.703873
Loss = 0.6185
Loss = 0.649048
Loss = 0.812393
Loss = 0.822052
Loss = 0.772873
Loss = 0.699173
Loss = 0.6353
Loss = 0.793365
Loss = 0.78331
Loss = 0.71582
Loss = 0.710495
Loss = 0.713852
Loss = 0.75441
Loss = 0.67189
Loss = 0.73201
Loss = 0.797729
Loss = 0.687469
TEST LOSS = 0.721279
TEST ACC = 458.98 % (7622/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.71109
Epoch 3.2: Loss = 0.756592
Epoch 3.3: Loss = 0.709656
Epoch 3.4: Loss = 0.697327
Epoch 3.5: Loss = 0.631256
Epoch 3.6: Loss = 0.698181
Epoch 3.7: Loss = 0.679138
Epoch 3.8: Loss = 0.665237
Epoch 3.9: Loss = 0.697433
Epoch 3.10: Loss = 0.651367
Epoch 3.11: Loss = 0.608154
Epoch 3.12: Loss = 0.566696
Epoch 3.13: Loss = 0.702621
Epoch 3.14: Loss = 0.622208
Epoch 3.15: Loss = 0.631714
Epoch 3.16: Loss = 0.629364
Epoch 3.17: Loss = 0.622589
Epoch 3.18: Loss = 0.675583
Epoch 3.19: Loss = 0.745972
Epoch 3.20: Loss = 0.635788
Epoch 3.21: Loss = 0.654129
Epoch 3.22: Loss = 0.611496
Epoch 3.23: Loss = 0.68808
Epoch 3.24: Loss = 0.709976
Epoch 3.25: Loss = 0.644684
Epoch 3.26: Loss = 0.759048
Epoch 3.27: Loss = 0.725037
Epoch 3.28: Loss = 0.720184
Epoch 3.29: Loss = 0.671982
Epoch 3.30: Loss = 0.705627
Epoch 3.31: Loss = 0.724854
Epoch 3.32: Loss = 0.661987
Epoch 3.33: Loss = 0.672424
Epoch 3.34: Loss = 0.690109
Epoch 3.35: Loss = 0.667847
Epoch 3.36: Loss = 0.657791
Epoch 3.37: Loss = 0.750549
Epoch 3.38: Loss = 0.62796
Epoch 3.39: Loss = 0.664856
Epoch 3.40: Loss = 0.640488
Epoch 3.41: Loss = 0.676407
Epoch 3.42: Loss = 0.705612
Epoch 3.43: Loss = 0.772705
Epoch 3.44: Loss = 0.629257
Epoch 3.45: Loss = 0.716904
Epoch 3.46: Loss = 0.693771
Epoch 3.47: Loss = 0.643326
Epoch 3.48: Loss = 0.79631
Epoch 3.49: Loss = 0.680954
Epoch 3.50: Loss = 0.696487
Epoch 3.51: Loss = 0.582184
Epoch 3.52: Loss = 0.611801
Epoch 3.53: Loss = 0.629532
Epoch 3.54: Loss = 0.631195
Epoch 3.55: Loss = 0.683289
Epoch 3.56: Loss = 0.596924
Epoch 3.57: Loss = 0.641418
Epoch 3.58: Loss = 0.679962
Epoch 3.59: Loss = 0.723785
Epoch 3.60: Loss = 0.67865
Epoch 3.61: Loss = 0.712296
Epoch 3.62: Loss = 0.755432
Epoch 3.63: Loss = 0.72551
Epoch 3.64: Loss = 0.605637
Epoch 3.65: Loss = 0.639343
Epoch 3.66: Loss = 0.63858
Epoch 3.67: Loss = 0.639832
Epoch 3.68: Loss = 0.67865
Epoch 3.69: Loss = 0.677643
Epoch 3.70: Loss = 0.604141
Epoch 3.71: Loss = 0.637619
Epoch 3.72: Loss = 0.629562
Epoch 3.73: Loss = 0.758835
Epoch 3.74: Loss = 0.753296
Epoch 3.75: Loss = 0.583633
Epoch 3.76: Loss = 0.631332
Epoch 3.77: Loss = 0.637589
Epoch 3.78: Loss = 0.666061
Epoch 3.79: Loss = 0.631912
Epoch 3.80: Loss = 0.721191
Epoch 3.81: Loss = 0.621902
Epoch 3.82: Loss = 0.589111
Epoch 3.83: Loss = 0.618362
Epoch 3.84: Loss = 0.693573
Epoch 3.85: Loss = 0.599686
Epoch 3.86: Loss = 0.656433
Epoch 3.87: Loss = 0.682465
Epoch 3.88: Loss = 0.613693
Epoch 3.89: Loss = 0.677643
Epoch 3.90: Loss = 0.619843
Epoch 3.91: Loss = 0.63826
Epoch 3.92: Loss = 0.722046
Epoch 3.93: Loss = 0.653839
Epoch 3.94: Loss = 0.646622
Epoch 3.95: Loss = 0.633408
Epoch 3.96: Loss = 0.665115
Epoch 3.97: Loss = 0.613815
Epoch 3.98: Loss = 0.738907
Epoch 3.99: Loss = 0.663452
Epoch 3.100: Loss = 0.57869
Epoch 3.101: Loss = 0.716125
Epoch 3.102: Loss = 0.677734
Epoch 3.103: Loss = 0.854599
Epoch 3.104: Loss = 0.687683
Epoch 3.105: Loss = 0.61557
Epoch 3.106: Loss = 0.608444
Epoch 3.107: Loss = 0.510422
Epoch 3.108: Loss = 0.623779
Epoch 3.109: Loss = 0.64447
Epoch 3.110: Loss = 0.639862
Epoch 3.111: Loss = 0.61319
Epoch 3.112: Loss = 0.69574
Epoch 3.113: Loss = 0.544525
Epoch 3.114: Loss = 0.617783
Epoch 3.115: Loss = 0.666946
Epoch 3.116: Loss = 0.770874
Epoch 3.117: Loss = 0.700317
Epoch 3.118: Loss = 0.575394
Epoch 3.119: Loss = 0.637451
Epoch 3.120: Loss = 0.690018
TRAIN LOSS = 0.665054
TRAIN ACC = 78.3508 % (47013/60000)
Loss = 0.582596
Loss = 0.668213
Loss = 0.66124
Loss = 0.543701
Loss = 0.608795
Loss = 0.780273
Loss = 0.788391
Loss = 0.738281
Loss = 0.670242
Loss = 0.604309
Loss = 0.784912
Loss = 0.75322
Loss = 0.663193
Loss = 0.649963
Loss = 0.648621
Loss = 0.713303
Loss = 0.619919
Loss = 0.697815
Loss = 0.728745
Loss = 0.669342
TEST LOSS = 0.678753
TEST ACC = 470.129 % (7804/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.548325
Epoch 4.2: Loss = 0.599991
Epoch 4.3: Loss = 0.695221
Epoch 4.4: Loss = 0.537109
Epoch 4.5: Loss = 0.664963
Epoch 4.6: Loss = 0.697586
Epoch 4.7: Loss = 0.665558
Epoch 4.8: Loss = 0.567566
Epoch 4.9: Loss = 0.635834
Epoch 4.10: Loss = 0.706146
Epoch 4.11: Loss = 0.5905
Epoch 4.12: Loss = 0.689011
Epoch 4.13: Loss = 0.615143
Epoch 4.14: Loss = 0.728928
Epoch 4.15: Loss = 0.682648
Epoch 4.16: Loss = 0.630859
Epoch 4.17: Loss = 0.663483
Epoch 4.18: Loss = 0.646164
Epoch 4.19: Loss = 0.663651
Epoch 4.20: Loss = 0.564423
Epoch 4.21: Loss = 0.673386
Epoch 4.22: Loss = 0.611618
Epoch 4.23: Loss = 0.682556
Epoch 4.24: Loss = 0.748108
Epoch 4.25: Loss = 0.677643
Epoch 4.26: Loss = 0.625931
Epoch 4.27: Loss = 0.603699
Epoch 4.28: Loss = 0.628052
Epoch 4.29: Loss = 0.607208
Epoch 4.30: Loss = 0.555359
Epoch 4.31: Loss = 0.725891
Epoch 4.32: Loss = 0.601761
Epoch 4.33: Loss = 0.584076
Epoch 4.34: Loss = 0.716766
Epoch 4.35: Loss = 0.7061
Epoch 4.36: Loss = 0.704315
Epoch 4.37: Loss = 0.605698
Epoch 4.38: Loss = 0.606308
Epoch 4.39: Loss = 0.721268
Epoch 4.40: Loss = 0.57547
Epoch 4.41: Loss = 0.662033
Epoch 4.42: Loss = 0.606567
Epoch 4.43: Loss = 0.625824
Epoch 4.44: Loss = 0.66861
Epoch 4.45: Loss = 0.696182
Epoch 4.46: Loss = 0.635712
Epoch 4.47: Loss = 0.638916
Epoch 4.48: Loss = 0.584
Epoch 4.49: Loss = 0.600739
Epoch 4.50: Loss = 0.709671
Epoch 4.51: Loss = 0.55481
Epoch 4.52: Loss = 0.655838
Epoch 4.53: Loss = 0.676086
Epoch 4.54: Loss = 0.779877
Epoch 4.55: Loss = 0.640747
Epoch 4.56: Loss = 0.688019
Epoch 4.57: Loss = 0.710358
Epoch 4.58: Loss = 0.761414
Epoch 4.59: Loss = 0.644211
Epoch 4.60: Loss = 0.543961
Epoch 4.61: Loss = 0.59761
Epoch 4.62: Loss = 0.672012
Epoch 4.63: Loss = 0.602829
Epoch 4.64: Loss = 0.617386
Epoch 4.65: Loss = 0.568405
Epoch 4.66: Loss = 0.629837
Epoch 4.67: Loss = 0.684631
Epoch 4.68: Loss = 0.596146
Epoch 4.69: Loss = 0.630295
Epoch 4.70: Loss = 0.659637
Epoch 4.71: Loss = 0.65744
Epoch 4.72: Loss = 0.676712
Epoch 4.73: Loss = 0.674057
Epoch 4.74: Loss = 0.710342
Epoch 4.75: Loss = 0.687637
Epoch 4.76: Loss = 0.716278
Epoch 4.77: Loss = 0.651062
Epoch 4.78: Loss = 0.6306
Epoch 4.79: Loss = 0.775696
Epoch 4.80: Loss = 0.637436
Epoch 4.81: Loss = 0.643448
Epoch 4.82: Loss = 0.591721
Epoch 4.83: Loss = 0.715134
Epoch 4.84: Loss = 0.675629
Epoch 4.85: Loss = 0.679489
Epoch 4.86: Loss = 0.67244
Epoch 4.87: Loss = 0.553787
Epoch 4.88: Loss = 0.721436
Epoch 4.89: Loss = 0.681458
Epoch 4.90: Loss = 0.720627
Epoch 4.91: Loss = 0.65506
Epoch 4.92: Loss = 0.668121
Epoch 4.93: Loss = 0.639206
Epoch 4.94: Loss = 0.620163
Epoch 4.95: Loss = 0.527542
Epoch 4.96: Loss = 0.728226
Epoch 4.97: Loss = 0.673553
Epoch 4.98: Loss = 0.597
Epoch 4.99: Loss = 0.804169
Epoch 4.100: Loss = 0.558411
Epoch 4.101: Loss = 0.741699
Epoch 4.102: Loss = 0.642288
Epoch 4.103: Loss = 0.584473
Epoch 4.104: Loss = 0.776764
Epoch 4.105: Loss = 0.7034
Epoch 4.106: Loss = 0.757629
Epoch 4.107: Loss = 0.558609
Epoch 4.108: Loss = 0.647629
Epoch 4.109: Loss = 0.66922
Epoch 4.110: Loss = 0.605698
Epoch 4.111: Loss = 0.66127
Epoch 4.112: Loss = 0.598938
Epoch 4.113: Loss = 0.718796
Epoch 4.114: Loss = 0.579895
Epoch 4.115: Loss = 0.651596
Epoch 4.116: Loss = 0.723724
Epoch 4.117: Loss = 0.661163
Epoch 4.118: Loss = 0.564056
Epoch 4.119: Loss = 0.503922
Epoch 4.120: Loss = 0.684097
TRAIN LOSS = 0.650635
TRAIN ACC = 79.3915 % (47637/60000)
Loss = 0.565918
Loss = 0.653824
Loss = 0.655701
Loss = 0.562653
Loss = 0.611404
Loss = 0.744034
Loss = 0.802078
Loss = 0.720978
Loss = 0.651932
Loss = 0.599152
Loss = 0.776764
Loss = 0.768829
Loss = 0.667145
Loss = 0.671631
Loss = 0.66304
Loss = 0.696228
Loss = 0.630661
Loss = 0.692032
Loss = 0.732941
Loss = 0.649246
TEST LOSS = 0.675809
TEST ACC = 476.369 % (7868/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.674896
Epoch 5.2: Loss = 0.575516
Epoch 5.3: Loss = 0.53006
Epoch 5.4: Loss = 0.651443
Epoch 5.5: Loss = 0.621902
Epoch 5.6: Loss = 0.603058
Epoch 5.7: Loss = 0.662445
Epoch 5.8: Loss = 0.596313
Epoch 5.9: Loss = 0.572769
Epoch 5.10: Loss = 0.59938
Epoch 5.11: Loss = 0.61322
Epoch 5.12: Loss = 0.56131
Epoch 5.13: Loss = 0.750626
Epoch 5.14: Loss = 0.617813
Epoch 5.15: Loss = 0.570404
Epoch 5.16: Loss = 0.640106
Epoch 5.17: Loss = 0.67276
Epoch 5.18: Loss = 0.583817
Epoch 5.19: Loss = 0.656677
Epoch 5.20: Loss = 0.644852
Epoch 5.21: Loss = 0.716904
Epoch 5.22: Loss = 0.7276
Epoch 5.23: Loss = 0.664459
Epoch 5.24: Loss = 0.628326
Epoch 5.25: Loss = 0.688248
Epoch 5.26: Loss = 0.7686
Epoch 5.27: Loss = 0.77417
Epoch 5.28: Loss = 0.700821
Epoch 5.29: Loss = 0.736115
Epoch 5.30: Loss = 0.644379
Epoch 5.31: Loss = 0.619553
Epoch 5.32: Loss = 0.623871
Epoch 5.33: Loss = 0.624252
Epoch 5.34: Loss = 0.659058
Epoch 5.35: Loss = 0.603546
Epoch 5.36: Loss = 0.572266
Epoch 5.37: Loss = 0.609268
Epoch 5.38: Loss = 0.563034
Epoch 5.39: Loss = 0.618439
Epoch 5.40: Loss = 0.65033
Epoch 5.41: Loss = 0.691849
Epoch 5.42: Loss = 0.644394
Epoch 5.43: Loss = 0.593582
Epoch 5.44: Loss = 0.642624
Epoch 5.45: Loss = 0.592361
Epoch 5.46: Loss = 0.608704
Epoch 5.47: Loss = 0.617828
Epoch 5.48: Loss = 0.597061
Epoch 5.49: Loss = 0.788696
Epoch 5.50: Loss = 0.679886
Epoch 5.51: Loss = 0.554031
Epoch 5.52: Loss = 0.603928
Epoch 5.53: Loss = 0.696152
Epoch 5.54: Loss = 0.591751
Epoch 5.55: Loss = 0.637634
Epoch 5.56: Loss = 0.601257
Epoch 5.57: Loss = 0.752594
Epoch 5.58: Loss = 0.557297
Epoch 5.59: Loss = 0.64389
Epoch 5.60: Loss = 0.698929
Epoch 5.61: Loss = 0.721771
Epoch 5.62: Loss = 0.57132
Epoch 5.63: Loss = 0.67868
Epoch 5.64: Loss = 0.571915
Epoch 5.65: Loss = 0.723557
Epoch 5.66: Loss = 0.702881
Epoch 5.67: Loss = 0.6259
Epoch 5.68: Loss = 0.795883
Epoch 5.69: Loss = 0.716644
Epoch 5.70: Loss = 0.713608
Epoch 5.71: Loss = 0.586227
Epoch 5.72: Loss = 0.63736
Epoch 5.73: Loss = 0.692139
Epoch 5.74: Loss = 0.617737
Epoch 5.75: Loss = 0.606796
Epoch 5.76: Loss = 0.547226
Epoch 5.77: Loss = 0.649475
Epoch 5.78: Loss = 0.655579
Epoch 5.79: Loss = 0.630707
Epoch 5.80: Loss = 0.638092
Epoch 5.81: Loss = 0.716766
Epoch 5.82: Loss = 0.464401
Epoch 5.83: Loss = 0.611557
Epoch 5.84: Loss = 0.740234
Epoch 5.85: Loss = 0.665222
Epoch 5.86: Loss = 0.621857
Epoch 5.87: Loss = 0.619476
Epoch 5.88: Loss = 0.71933
Epoch 5.89: Loss = 0.546097
Epoch 5.90: Loss = 0.727402
Epoch 5.91: Loss = 0.708618
Epoch 5.92: Loss = 0.615158
Epoch 5.93: Loss = 0.646942
Epoch 5.94: Loss = 0.518021
Epoch 5.95: Loss = 0.553192
Epoch 5.96: Loss = 0.652328
Epoch 5.97: Loss = 0.764404
Epoch 5.98: Loss = 0.617004
Epoch 5.99: Loss = 0.725174
Epoch 5.100: Loss = 0.689774
Epoch 5.101: Loss = 0.699844
Epoch 5.102: Loss = 0.634735
Epoch 5.103: Loss = 0.649231
Epoch 5.104: Loss = 0.612961
Epoch 5.105: Loss = 0.54126
Epoch 5.106: Loss = 0.694733
Epoch 5.107: Loss = 0.648666
Epoch 5.108: Loss = 0.594925
Epoch 5.109: Loss = 0.710922
Epoch 5.110: Loss = 0.698883
Epoch 5.111: Loss = 0.686096
Epoch 5.112: Loss = 0.65329
Epoch 5.113: Loss = 0.564941
Epoch 5.114: Loss = 0.652161
Epoch 5.115: Loss = 0.644928
Epoch 5.116: Loss = 0.613586
Epoch 5.117: Loss = 0.696671
Epoch 5.118: Loss = 0.764877
Epoch 5.119: Loss = 0.624008
Epoch 5.120: Loss = 0.620499
TRAIN LOSS = 0.64502
TRAIN ACC = 79.982 % (47991/60000)
Loss = 0.578293
Loss = 0.648666
Loss = 0.638016
Loss = 0.541992
Loss = 0.607803
Loss = 0.718643
Loss = 0.776611
Loss = 0.711853
Loss = 0.650742
Loss = 0.594772
Loss = 0.791946
Loss = 0.77002
Loss = 0.657547
Loss = 0.653259
Loss = 0.619064
Loss = 0.682312
Loss = 0.612259
Loss = 0.697647
Loss = 0.714035
Loss = 0.647903
TEST LOSS = 0.665669
TEST ACC = 479.909 % (7939/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.512848
Epoch 6.2: Loss = 0.5737
Epoch 6.3: Loss = 0.697174
Epoch 6.4: Loss = 0.65863
Epoch 6.5: Loss = 0.622131
Epoch 6.6: Loss = 0.665009
Epoch 6.7: Loss = 0.736176
Epoch 6.8: Loss = 0.60318
Epoch 6.9: Loss = 0.570465
Epoch 6.10: Loss = 0.69812
Epoch 6.11: Loss = 0.599518
Epoch 6.12: Loss = 0.657944
Epoch 6.13: Loss = 0.533524
Epoch 6.14: Loss = 0.693787
Epoch 6.15: Loss = 0.744919
Epoch 6.16: Loss = 0.479095
Epoch 6.17: Loss = 0.594177
Epoch 6.18: Loss = 0.608719
Epoch 6.19: Loss = 0.618805
Epoch 6.20: Loss = 0.624634
Epoch 6.21: Loss = 0.584305
Epoch 6.22: Loss = 0.570435
Epoch 6.23: Loss = 0.650009
Epoch 6.24: Loss = 0.667908
Epoch 6.25: Loss = 0.6371
Epoch 6.26: Loss = 0.586777
Epoch 6.27: Loss = 0.603745
Epoch 6.28: Loss = 0.613892
Epoch 6.29: Loss = 0.61171
Epoch 6.30: Loss = 0.730148
Epoch 6.31: Loss = 0.616409
Epoch 6.32: Loss = 0.584183
Epoch 6.33: Loss = 0.608444
Epoch 6.34: Loss = 0.721878
Epoch 6.35: Loss = 0.655777
Epoch 6.36: Loss = 0.651825
Epoch 6.37: Loss = 0.583603
Epoch 6.38: Loss = 0.689224
Epoch 6.39: Loss = 0.561432
Epoch 6.40: Loss = 0.680801
Epoch 6.41: Loss = 0.671173
Epoch 6.42: Loss = 0.571136
Epoch 6.43: Loss = 0.658981
Epoch 6.44: Loss = 0.543259
Epoch 6.45: Loss = 0.53714
Epoch 6.46: Loss = 0.649979
Epoch 6.47: Loss = 0.626953
Epoch 6.48: Loss = 0.661926
Epoch 6.49: Loss = 0.772018
Epoch 6.50: Loss = 0.609177
Epoch 6.51: Loss = 0.603729
Epoch 6.52: Loss = 0.757263
Epoch 6.53: Loss = 0.553909
Epoch 6.54: Loss = 0.7323
Epoch 6.55: Loss = 0.644073
Epoch 6.56: Loss = 0.594604
Epoch 6.57: Loss = 0.525955
Epoch 6.58: Loss = 0.58757
Epoch 6.59: Loss = 0.644119
Epoch 6.60: Loss = 0.672134
Epoch 6.61: Loss = 0.746078
Epoch 6.62: Loss = 0.642319
Epoch 6.63: Loss = 0.600143
Epoch 6.64: Loss = 0.671463
Epoch 6.65: Loss = 0.64798
Epoch 6.66: Loss = 0.485901
Epoch 6.67: Loss = 0.634659
Epoch 6.68: Loss = 0.800415
Epoch 6.69: Loss = 0.605865
Epoch 6.70: Loss = 0.546249
Epoch 6.71: Loss = 0.733032
Epoch 6.72: Loss = 0.713333
Epoch 6.73: Loss = 0.686508
Epoch 6.74: Loss = 0.68718
Epoch 6.75: Loss = 0.717407
Epoch 6.76: Loss = 0.580566
Epoch 6.77: Loss = 0.569458
Epoch 6.78: Loss = 0.563263
Epoch 6.79: Loss = 0.594208
Epoch 6.80: Loss = 0.589737
Epoch 6.81: Loss = 0.609955
Epoch 6.82: Loss = 0.606247
Epoch 6.83: Loss = 0.547699
Epoch 6.84: Loss = 0.669601
Epoch 6.85: Loss = 0.687073
Epoch 6.86: Loss = 0.608383
Epoch 6.87: Loss = 0.622437
Epoch 6.88: Loss = 0.660645
Epoch 6.89: Loss = 0.648666
Epoch 6.90: Loss = 0.555359
Epoch 6.91: Loss = 0.655045
Epoch 6.92: Loss = 0.530685
Epoch 6.93: Loss = 0.626114
Epoch 6.94: Loss = 0.731064
Epoch 6.95: Loss = 0.669052
Epoch 6.96: Loss = 0.605118
Epoch 6.97: Loss = 0.498825
Epoch 6.98: Loss = 0.662704
Epoch 6.99: Loss = 0.685791
Epoch 6.100: Loss = 0.612717
Epoch 6.101: Loss = 0.636154
Epoch 6.102: Loss = 0.783173
Epoch 6.103: Loss = 0.703354
Epoch 6.104: Loss = 0.743561
Epoch 6.105: Loss = 0.736206
Epoch 6.106: Loss = 0.726318
Epoch 6.107: Loss = 0.688934
Epoch 6.108: Loss = 0.631531
Epoch 6.109: Loss = 0.737503
Epoch 6.110: Loss = 0.598953
Epoch 6.111: Loss = 0.635178
Epoch 6.112: Loss = 0.696136
Epoch 6.113: Loss = 0.561584
Epoch 6.114: Loss = 0.707397
Epoch 6.115: Loss = 0.618637
Epoch 6.116: Loss = 0.695313
Epoch 6.117: Loss = 0.629547
Epoch 6.118: Loss = 0.805069
Epoch 6.119: Loss = 0.536804
Epoch 6.120: Loss = 0.544617
TRAIN LOSS = 0.637054
TRAIN ACC = 80.4672 % (48282/60000)
Loss = 0.580048
Loss = 0.665436
Loss = 0.649445
Loss = 0.548691
Loss = 0.62941
Loss = 0.743256
Loss = 0.784195
Loss = 0.698486
Loss = 0.650421
Loss = 0.567322
Loss = 0.818954
Loss = 0.785828
Loss = 0.694489
Loss = 0.692673
Loss = 0.667236
Loss = 0.732346
Loss = 0.674377
Loss = 0.731293
Loss = 0.731079
Loss = 0.655838
TEST LOSS = 0.685041
TEST ACC = 482.819 % (7951/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.667282
Epoch 7.2: Loss = 0.618271
Epoch 7.3: Loss = 0.648605
Epoch 7.4: Loss = 0.729675
Epoch 7.5: Loss = 0.519608
Epoch 7.6: Loss = 0.578079
Epoch 7.7: Loss = 0.565048
Epoch 7.8: Loss = 0.693527
Epoch 7.9: Loss = 0.659592
Epoch 7.10: Loss = 0.626953
Epoch 7.11: Loss = 0.639343
Epoch 7.12: Loss = 0.576675
Epoch 7.13: Loss = 0.637558
Epoch 7.14: Loss = 0.568542
Epoch 7.15: Loss = 0.62738
Epoch 7.16: Loss = 0.671906
Epoch 7.17: Loss = 0.700974
Epoch 7.18: Loss = 0.799057
Epoch 7.19: Loss = 0.532501
Epoch 7.20: Loss = 0.596252
Epoch 7.21: Loss = 0.711578
Epoch 7.22: Loss = 0.667419
Epoch 7.23: Loss = 0.614349
Epoch 7.24: Loss = 0.703552
Epoch 7.25: Loss = 0.707336
Epoch 7.26: Loss = 0.619415
Epoch 7.27: Loss = 0.602081
Epoch 7.28: Loss = 0.628891
Epoch 7.29: Loss = 0.709808
Epoch 7.30: Loss = 0.580505
Epoch 7.31: Loss = 0.61261
Epoch 7.32: Loss = 0.74115
Epoch 7.33: Loss = 0.593857
Epoch 7.34: Loss = 0.589859
Epoch 7.35: Loss = 0.806107
Epoch 7.36: Loss = 0.68924
Epoch 7.37: Loss = 0.723984
Epoch 7.38: Loss = 0.654617
Epoch 7.39: Loss = 0.574234
Epoch 7.40: Loss = 0.658966
Epoch 7.41: Loss = 0.63031
Epoch 7.42: Loss = 0.654144
Epoch 7.43: Loss = 0.682205
Epoch 7.44: Loss = 0.706818
Epoch 7.45: Loss = 0.745758
Epoch 7.46: Loss = 0.707611
Epoch 7.47: Loss = 0.609833
Epoch 7.48: Loss = 0.560852
Epoch 7.49: Loss = 0.626892
Epoch 7.50: Loss = 0.678589
Epoch 7.51: Loss = 0.741028
Epoch 7.52: Loss = 0.741486
Epoch 7.53: Loss = 0.659424
Epoch 7.54: Loss = 0.68634
Epoch 7.55: Loss = 0.747711
Epoch 7.56: Loss = 0.637054
Epoch 7.57: Loss = 0.687561
Epoch 7.58: Loss = 0.711761
Epoch 7.59: Loss = 0.56218
Epoch 7.60: Loss = 0.779282
Epoch 7.61: Loss = 0.735474
Epoch 7.62: Loss = 0.743057
Epoch 7.63: Loss = 0.685974
Epoch 7.64: Loss = 0.680588
Epoch 7.65: Loss = 0.646133
Epoch 7.66: Loss = 0.67717
Epoch 7.67: Loss = 0.712143
Epoch 7.68: Loss = 0.685944
Epoch 7.69: Loss = 0.728897
Epoch 7.70: Loss = 0.738037
Epoch 7.71: Loss = 0.720657
Epoch 7.72: Loss = 0.750214
Epoch 7.73: Loss = 0.69455
Epoch 7.74: Loss = 0.623932
Epoch 7.75: Loss = 0.613525
Epoch 7.76: Loss = 0.658569
Epoch 7.77: Loss = 0.587006
Epoch 7.78: Loss = 0.649368
Epoch 7.79: Loss = 0.713562
Epoch 7.80: Loss = 0.559982
Epoch 7.81: Loss = 0.595016
Epoch 7.82: Loss = 0.691971
Epoch 7.83: Loss = 0.668686
Epoch 7.84: Loss = 0.717209
Epoch 7.85: Loss = 0.653061
Epoch 7.86: Loss = 0.728806
Epoch 7.87: Loss = 0.550415
Epoch 7.88: Loss = 0.731293
Epoch 7.89: Loss = 0.682404
Epoch 7.90: Loss = 0.636154
Epoch 7.91: Loss = 0.709839
Epoch 7.92: Loss = 0.619568
Epoch 7.93: Loss = 0.687866
Epoch 7.94: Loss = 0.536621
Epoch 7.95: Loss = 0.699722
Epoch 7.96: Loss = 0.649612
Epoch 7.97: Loss = 0.701263
Epoch 7.98: Loss = 0.653748
Epoch 7.99: Loss = 0.631607
Epoch 7.100: Loss = 0.688217
Epoch 7.101: Loss = 0.723557
Epoch 7.102: Loss = 0.654083
Epoch 7.103: Loss = 0.792786
Epoch 7.104: Loss = 0.693924
Epoch 7.105: Loss = 0.70694
Epoch 7.106: Loss = 0.653
Epoch 7.107: Loss = 0.595596
Epoch 7.108: Loss = 0.643997
Epoch 7.109: Loss = 0.5457
Epoch 7.110: Loss = 0.845871
Epoch 7.111: Loss = 0.692383
Epoch 7.112: Loss = 0.80014
Epoch 7.113: Loss = 0.598938
Epoch 7.114: Loss = 0.600601
Epoch 7.115: Loss = 0.538467
Epoch 7.116: Loss = 0.604492
Epoch 7.117: Loss = 0.623093
Epoch 7.118: Loss = 0.705429
Epoch 7.119: Loss = 0.658356
Epoch 7.120: Loss = 0.634644
TRAIN LOSS = 0.662964
TRAIN ACC = 80.5313 % (48321/60000)
Loss = 0.590179
Loss = 0.686493
Loss = 0.655899
Loss = 0.565964
Loss = 0.618622
Loss = 0.759415
Loss = 0.824127
Loss = 0.706116
Loss = 0.642197
Loss = 0.592148
Loss = 0.830215
Loss = 0.796143
Loss = 0.679245
Loss = 0.691879
Loss = 0.656189
Loss = 0.730637
Loss = 0.665268
Loss = 0.740097
Loss = 0.726776
Loss = 0.665512
TEST LOSS = 0.691156
TEST ACC = 483.209 % (8002/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.722809
Epoch 8.2: Loss = 0.630478
Epoch 8.3: Loss = 0.707855
Epoch 8.4: Loss = 0.629257
Epoch 8.5: Loss = 0.531448
Epoch 8.6: Loss = 0.683151
Epoch 8.7: Loss = 0.647797
Epoch 8.8: Loss = 0.688614
Epoch 8.9: Loss = 0.779068
Epoch 8.10: Loss = 0.660034
Epoch 8.11: Loss = 0.725052
Epoch 8.12: Loss = 0.552948
Epoch 8.13: Loss = 0.785461
Epoch 8.14: Loss = 0.611908
Epoch 8.15: Loss = 0.592514
Epoch 8.16: Loss = 0.673904
Epoch 8.17: Loss = 0.552963
Epoch 8.18: Loss = 0.631332
Epoch 8.19: Loss = 0.598969
Epoch 8.20: Loss = 0.734467
Epoch 8.21: Loss = 0.684753
Epoch 8.22: Loss = 0.731049
Epoch 8.23: Loss = 0.577057
Epoch 8.24: Loss = 0.622833
Epoch 8.25: Loss = 0.459656
Epoch 8.26: Loss = 0.601913
Epoch 8.27: Loss = 0.641251
Epoch 8.28: Loss = 0.699554
Epoch 8.29: Loss = 0.757309
Epoch 8.30: Loss = 0.689102
Epoch 8.31: Loss = 0.623428
Epoch 8.32: Loss = 0.788589
Epoch 8.33: Loss = 0.594208
Epoch 8.34: Loss = 0.652435
Epoch 8.35: Loss = 0.707809
Epoch 8.36: Loss = 0.668518
Epoch 8.37: Loss = 0.699951
Epoch 8.38: Loss = 0.720184
Epoch 8.39: Loss = 0.721924
Epoch 8.40: Loss = 0.707108
Epoch 8.41: Loss = 0.643311
Epoch 8.42: Loss = 0.673416
Epoch 8.43: Loss = 0.637482
Epoch 8.44: Loss = 0.519196
Epoch 8.45: Loss = 0.675842
Epoch 8.46: Loss = 0.659698
Epoch 8.47: Loss = 0.634781
Epoch 8.48: Loss = 0.838226
Epoch 8.49: Loss = 0.607086
Epoch 8.50: Loss = 0.735107
Epoch 8.51: Loss = 0.677795
Epoch 8.52: Loss = 0.638962
Epoch 8.53: Loss = 0.65387
Epoch 8.54: Loss = 0.632477
Epoch 8.55: Loss = 0.675812
Epoch 8.56: Loss = 0.674438
Epoch 8.57: Loss = 0.659332
Epoch 8.58: Loss = 0.675476
Epoch 8.59: Loss = 0.636459
Epoch 8.60: Loss = 0.654266
Epoch 8.61: Loss = 0.639084
Epoch 8.62: Loss = 0.611755
Epoch 8.63: Loss = 0.675934
Epoch 8.64: Loss = 0.738632
Epoch 8.65: Loss = 0.748077
Epoch 8.66: Loss = 0.651657
Epoch 8.67: Loss = 0.745575
Epoch 8.68: Loss = 0.621475
Epoch 8.69: Loss = 0.687881
Epoch 8.70: Loss = 0.601273
Epoch 8.71: Loss = 0.752182
Epoch 8.72: Loss = 0.630112
Epoch 8.73: Loss = 0.661987
Epoch 8.74: Loss = 0.747849
Epoch 8.75: Loss = 0.685471
Epoch 8.76: Loss = 0.679993
Epoch 8.77: Loss = 0.62439
Epoch 8.78: Loss = 0.637589
Epoch 8.79: Loss = 0.766617
Epoch 8.80: Loss = 0.71405
Epoch 8.81: Loss = 0.692093
Epoch 8.82: Loss = 0.492065
Epoch 8.83: Loss = 0.604309
Epoch 8.84: Loss = 0.752548
Epoch 8.85: Loss = 0.715118
Epoch 8.86: Loss = 0.602097
Epoch 8.87: Loss = 0.615768
Epoch 8.88: Loss = 0.588654
Epoch 8.89: Loss = 0.621323
Epoch 8.90: Loss = 0.597885
Epoch 8.91: Loss = 0.720215
Epoch 8.92: Loss = 0.621628
Epoch 8.93: Loss = 0.652237
Epoch 8.94: Loss = 0.657654
Epoch 8.95: Loss = 0.715149
Epoch 8.96: Loss = 0.775085
Epoch 8.97: Loss = 0.742325
Epoch 8.98: Loss = 0.66626
Epoch 8.99: Loss = 0.672287
Epoch 8.100: Loss = 0.615906
Epoch 8.101: Loss = 0.592392
Epoch 8.102: Loss = 0.743088
Epoch 8.103: Loss = 0.682877
Epoch 8.104: Loss = 0.623901
Epoch 8.105: Loss = 0.590485
Epoch 8.106: Loss = 0.652969
Epoch 8.107: Loss = 0.740234
Epoch 8.108: Loss = 0.689896
Epoch 8.109: Loss = 0.693207
Epoch 8.110: Loss = 0.714066
Epoch 8.111: Loss = 0.656906
Epoch 8.112: Loss = 0.78125
Epoch 8.113: Loss = 0.642044
Epoch 8.114: Loss = 0.604736
Epoch 8.115: Loss = 0.663483
Epoch 8.116: Loss = 0.534317
Epoch 8.117: Loss = 0.714462
Epoch 8.118: Loss = 0.575485
Epoch 8.119: Loss = 0.740189
Epoch 8.120: Loss = 0.473755
TRAIN LOSS = 0.662888
TRAIN ACC = 80.9311 % (48561/60000)
Loss = 0.591934
Loss = 0.724396
Loss = 0.677765
Loss = 0.566467
Loss = 0.635696
Loss = 0.778427
Loss = 0.82869
Loss = 0.77774
Loss = 0.659393
Loss = 0.616882
Loss = 0.884247
Loss = 0.844421
Loss = 0.728104
Loss = 0.692444
Loss = 0.682434
Loss = 0.743515
Loss = 0.683273
Loss = 0.747086
Loss = 0.770035
Loss = 0.710022
TEST LOSS = 0.717148
TEST ACC = 485.609 % (8007/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.711426
Epoch 9.2: Loss = 0.554688
Epoch 9.3: Loss = 0.674713
Epoch 9.4: Loss = 0.640976
Epoch 9.5: Loss = 0.580948
Epoch 9.6: Loss = 0.573502
Epoch 9.7: Loss = 0.731445
Epoch 9.8: Loss = 0.647034
Epoch 9.9: Loss = 0.6362
Epoch 9.10: Loss = 0.62999
Epoch 9.11: Loss = 0.651352
Epoch 9.12: Loss = 0.694107
Epoch 9.13: Loss = 0.686432
Epoch 9.14: Loss = 0.653961
Epoch 9.15: Loss = 0.719116
Epoch 9.16: Loss = 0.593323
Epoch 9.17: Loss = 0.602875
Epoch 9.18: Loss = 0.664825
Epoch 9.19: Loss = 0.746735
Epoch 9.20: Loss = 0.700607
Epoch 9.21: Loss = 0.772018
Epoch 9.22: Loss = 0.628799
Epoch 9.23: Loss = 0.606613
Epoch 9.24: Loss = 0.69281
Epoch 9.25: Loss = 0.758392
Epoch 9.26: Loss = 0.614578
Epoch 9.27: Loss = 0.698532
Epoch 9.28: Loss = 0.608261
Epoch 9.29: Loss = 0.603271
Epoch 9.30: Loss = 0.682281
Epoch 9.31: Loss = 0.634872
Epoch 9.32: Loss = 0.61055
Epoch 9.33: Loss = 0.595734
Epoch 9.34: Loss = 0.546738
Epoch 9.35: Loss = 0.570343
Epoch 9.36: Loss = 0.71257
Epoch 9.37: Loss = 0.630066
Epoch 9.38: Loss = 0.714401
Epoch 9.39: Loss = 0.765396
Epoch 9.40: Loss = 0.617767
Epoch 9.41: Loss = 0.613113
Epoch 9.42: Loss = 0.695755
Epoch 9.43: Loss = 0.459961
Epoch 9.44: Loss = 0.724823
Epoch 9.45: Loss = 0.715576
Epoch 9.46: Loss = 0.704269
Epoch 9.47: Loss = 0.69371
Epoch 9.48: Loss = 0.732513
Epoch 9.49: Loss = 0.652176
Epoch 9.50: Loss = 0.660278
Epoch 9.51: Loss = 0.611938
Epoch 9.52: Loss = 0.699173
Epoch 9.53: Loss = 0.654572
Epoch 9.54: Loss = 0.691238
Epoch 9.55: Loss = 0.748489
Epoch 9.56: Loss = 0.744232
Epoch 9.57: Loss = 0.698685
Epoch 9.58: Loss = 0.701233
Epoch 9.59: Loss = 0.820801
Epoch 9.60: Loss = 0.639175
Epoch 9.61: Loss = 0.696457
Epoch 9.62: Loss = 0.633484
Epoch 9.63: Loss = 0.625885
Epoch 9.64: Loss = 0.644989
Epoch 9.65: Loss = 0.602478
Epoch 9.66: Loss = 0.653366
Epoch 9.67: Loss = 0.780609
Epoch 9.68: Loss = 0.768143
Epoch 9.69: Loss = 0.667191
Epoch 9.70: Loss = 0.692184
Epoch 9.71: Loss = 0.581848
Epoch 9.72: Loss = 0.775726
Epoch 9.73: Loss = 0.682907
Epoch 9.74: Loss = 0.616272
Epoch 9.75: Loss = 0.716156
Epoch 9.76: Loss = 0.746094
Epoch 9.77: Loss = 0.680908
Epoch 9.78: Loss = 0.776749
Epoch 9.79: Loss = 0.755615
Epoch 9.80: Loss = 0.578888
Epoch 9.81: Loss = 0.621552
Epoch 9.82: Loss = 0.724014
Epoch 9.83: Loss = 0.540543
Epoch 9.84: Loss = 0.645721
Epoch 9.85: Loss = 0.586212
Epoch 9.86: Loss = 0.689163
Epoch 9.87: Loss = 0.710297
Epoch 9.88: Loss = 0.726196
Epoch 9.89: Loss = 0.729538
Epoch 9.90: Loss = 0.888031
Epoch 9.91: Loss = 0.6707
Epoch 9.92: Loss = 0.764038
Epoch 9.93: Loss = 0.606293
Epoch 9.94: Loss = 0.845947
Epoch 9.95: Loss = 0.706421
Epoch 9.96: Loss = 0.575485
Epoch 9.97: Loss = 0.582672
Epoch 9.98: Loss = 0.605606
Epoch 9.99: Loss = 0.64476
Epoch 9.100: Loss = 0.687943
Epoch 9.101: Loss = 0.739136
Epoch 9.102: Loss = 0.796738
Epoch 9.103: Loss = 0.549255
Epoch 9.104: Loss = 0.636276
Epoch 9.105: Loss = 0.797134
Epoch 9.106: Loss = 0.793808
Epoch 9.107: Loss = 0.627914
Epoch 9.108: Loss = 0.676559
Epoch 9.109: Loss = 0.670197
Epoch 9.110: Loss = 0.722412
Epoch 9.111: Loss = 0.750671
Epoch 9.112: Loss = 0.837051
Epoch 9.113: Loss = 0.670242
Epoch 9.114: Loss = 0.727005
Epoch 9.115: Loss = 0.559586
Epoch 9.116: Loss = 0.66478
Epoch 9.117: Loss = 0.745544
Epoch 9.118: Loss = 0.713211
Epoch 9.119: Loss = 0.746109
Epoch 9.120: Loss = 0.687119
TRAIN LOSS = 0.676331
TRAIN ACC = 81.0455 % (48629/60000)
Loss = 0.583298
Loss = 0.70459
Loss = 0.709885
Loss = 0.566635
Loss = 0.664764
Loss = 0.786636
Loss = 0.863663
Loss = 0.779449
Loss = 0.68808
Loss = 0.641251
Loss = 0.887894
Loss = 0.89006
Loss = 0.712692
Loss = 0.706009
Loss = 0.706314
Loss = 0.745773
Loss = 0.705643
Loss = 0.768967
Loss = 0.78624
Loss = 0.720566
TEST LOSS = 0.73092
TEST ACC = 486.288 % (8050/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.749191
Epoch 10.2: Loss = 0.766083
Epoch 10.3: Loss = 0.693161
Epoch 10.4: Loss = 0.846771
Epoch 10.5: Loss = 0.616119
Epoch 10.6: Loss = 0.675751
Epoch 10.7: Loss = 0.606049
Epoch 10.8: Loss = 0.692169
Epoch 10.9: Loss = 0.718582
Epoch 10.10: Loss = 0.749344
Epoch 10.11: Loss = 0.598984
Epoch 10.12: Loss = 0.745224
Epoch 10.13: Loss = 0.660965
Epoch 10.14: Loss = 0.89183
Epoch 10.15: Loss = 0.556671
Epoch 10.16: Loss = 0.608963
Epoch 10.17: Loss = 0.799774
Epoch 10.18: Loss = 0.598495
Epoch 10.19: Loss = 0.732376
Epoch 10.20: Loss = 0.696381
Epoch 10.21: Loss = 0.719238
Epoch 10.22: Loss = 0.711273
Epoch 10.23: Loss = 0.653824
Epoch 10.24: Loss = 0.721741
Epoch 10.25: Loss = 0.665588
Epoch 10.26: Loss = 0.716873
Epoch 10.27: Loss = 0.706787
Epoch 10.28: Loss = 0.651016
Epoch 10.29: Loss = 0.723175
Epoch 10.30: Loss = 0.605682
Epoch 10.31: Loss = 0.812027
Epoch 10.32: Loss = 0.605713
Epoch 10.33: Loss = 0.738968
Epoch 10.34: Loss = 0.709976
Epoch 10.35: Loss = 0.664383
Epoch 10.36: Loss = 0.694534
Epoch 10.37: Loss = 0.776978
Epoch 10.38: Loss = 0.683914
Epoch 10.39: Loss = 0.623459
Epoch 10.40: Loss = 0.72702
Epoch 10.41: Loss = 0.586166
Epoch 10.42: Loss = 0.761139
Epoch 10.43: Loss = 0.65416
Epoch 10.44: Loss = 0.540787
Epoch 10.45: Loss = 0.709457
Epoch 10.46: Loss = 0.731216
Epoch 10.47: Loss = 0.734619
Epoch 10.48: Loss = 0.607574
Epoch 10.49: Loss = 0.683273
Epoch 10.50: Loss = 0.657135
Epoch 10.51: Loss = 0.681381
Epoch 10.52: Loss = 0.737213
Epoch 10.53: Loss = 0.646255
Epoch 10.54: Loss = 0.682877
Epoch 10.55: Loss = 0.721939
Epoch 10.56: Loss = 0.552078
Epoch 10.57: Loss = 0.723602
Epoch 10.58: Loss = 0.605453
Epoch 10.59: Loss = 0.669449
Epoch 10.60: Loss = 0.724258
Epoch 10.61: Loss = 0.778397
Epoch 10.62: Loss = 0.684998
Epoch 10.63: Loss = 0.689667
Epoch 10.64: Loss = 0.766129
Epoch 10.65: Loss = 0.663971
Epoch 10.66: Loss = 0.735199
Epoch 10.67: Loss = 0.693283
Epoch 10.68: Loss = 0.586868
Epoch 10.69: Loss = 0.698456
Epoch 10.70: Loss = 0.599228
Epoch 10.71: Loss = 0.705521
Epoch 10.72: Loss = 0.586929
Epoch 10.73: Loss = 0.597122
Epoch 10.74: Loss = 0.65184
Epoch 10.75: Loss = 0.647888
Epoch 10.76: Loss = 0.670532
Epoch 10.77: Loss = 0.524872
Epoch 10.78: Loss = 0.771164
Epoch 10.79: Loss = 0.647614
Epoch 10.80: Loss = 0.637497
Epoch 10.81: Loss = 0.640915
Epoch 10.82: Loss = 0.777054
Epoch 10.83: Loss = 0.652832
Epoch 10.84: Loss = 0.793701
Epoch 10.85: Loss = 0.713242
Epoch 10.86: Loss = 0.628632
Epoch 10.87: Loss = 0.630295
Epoch 10.88: Loss = 0.703934
Epoch 10.89: Loss = 0.722687
Epoch 10.90: Loss = 0.612976
Epoch 10.91: Loss = 0.615677
Epoch 10.92: Loss = 0.657745
Epoch 10.93: Loss = 0.687714
Epoch 10.94: Loss = 0.722488
Epoch 10.95: Loss = 0.633316
Epoch 10.96: Loss = 0.652527
Epoch 10.97: Loss = 0.617813
Epoch 10.98: Loss = 0.698929
Epoch 10.99: Loss = 0.666946
Epoch 10.100: Loss = 0.736267
Epoch 10.101: Loss = 0.619064
Epoch 10.102: Loss = 0.648132
Epoch 10.103: Loss = 0.731125
Epoch 10.104: Loss = 0.657486
Epoch 10.105: Loss = 0.567963
Epoch 10.106: Loss = 0.631821
Epoch 10.107: Loss = 0.747253
Epoch 10.108: Loss = 0.742996
Epoch 10.109: Loss = 0.762314
Epoch 10.110: Loss = 0.693619
Epoch 10.111: Loss = 0.774399
Epoch 10.112: Loss = 0.760696
Epoch 10.113: Loss = 0.763107
Epoch 10.114: Loss = 0.84346
Epoch 10.115: Loss = 0.720322
Epoch 10.116: Loss = 0.764374
Epoch 10.117: Loss = 0.679047
Epoch 10.118: Loss = 0.69426
Epoch 10.119: Loss = 0.693115
Epoch 10.120: Loss = 0.654953
TRAIN LOSS = 0.686493
TRAIN ACC = 81.2576 % (48756/60000)
Loss = 0.598465
Loss = 0.690033
Loss = 0.704971
Loss = 0.55278
Loss = 0.677277
Loss = 0.791397
Loss = 0.867615
Loss = 0.778931
Loss = 0.725632
Loss = 0.616547
Loss = 0.87793
Loss = 0.856064
Loss = 0.7211
Loss = 0.705933
Loss = 0.708878
Loss = 0.718735
Loss = 0.714874
Loss = 0.759506
Loss = 0.779205
Loss = 0.730026
TEST LOSS = 0.728795
TEST ACC = 487.56 % (8055/10000)

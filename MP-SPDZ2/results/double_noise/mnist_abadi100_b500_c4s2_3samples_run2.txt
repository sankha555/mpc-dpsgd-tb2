Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.45244
Epoch 1.2: Loss = 2.38416
Epoch 1.3: Loss = 2.28395
Epoch 1.4: Loss = 2.26707
Epoch 1.5: Loss = 2.20741
Epoch 1.6: Loss = 2.15686
Epoch 1.7: Loss = 2.11499
Epoch 1.8: Loss = 2.0787
Epoch 1.9: Loss = 2.05338
Epoch 1.10: Loss = 2.01157
Epoch 1.11: Loss = 1.91052
Epoch 1.12: Loss = 1.97391
Epoch 1.13: Loss = 1.87924
Epoch 1.14: Loss = 1.86745
Epoch 1.15: Loss = 1.80743
Epoch 1.16: Loss = 1.8331
Epoch 1.17: Loss = 1.78813
Epoch 1.18: Loss = 1.76012
Epoch 1.19: Loss = 1.73296
Epoch 1.20: Loss = 1.70261
Epoch 1.21: Loss = 1.62842
Epoch 1.22: Loss = 1.59796
Epoch 1.23: Loss = 1.62221
Epoch 1.24: Loss = 1.58833
Epoch 1.25: Loss = 1.50835
Epoch 1.26: Loss = 1.56372
Epoch 1.27: Loss = 1.41704
Epoch 1.28: Loss = 1.42894
Epoch 1.29: Loss = 1.40785
Epoch 1.30: Loss = 1.39287
Epoch 1.31: Loss = 1.39673
Epoch 1.32: Loss = 1.4194
Epoch 1.33: Loss = 1.30641
Epoch 1.34: Loss = 1.31352
Epoch 1.35: Loss = 1.3217
Epoch 1.36: Loss = 1.29532
Epoch 1.37: Loss = 1.26556
Epoch 1.38: Loss = 1.24756
Epoch 1.39: Loss = 1.17708
Epoch 1.40: Loss = 1.17125
Epoch 1.41: Loss = 1.18549
Epoch 1.42: Loss = 1.15492
Epoch 1.43: Loss = 1.13005
Epoch 1.44: Loss = 1.15028
Epoch 1.45: Loss = 1.10201
Epoch 1.46: Loss = 1.13304
Epoch 1.47: Loss = 1.09709
Epoch 1.48: Loss = 1.04445
Epoch 1.49: Loss = 1.07684
Epoch 1.50: Loss = 1.0412
Epoch 1.51: Loss = 1.02473
Epoch 1.52: Loss = 0.990143
Epoch 1.53: Loss = 1.01834
Epoch 1.54: Loss = 0.997803
Epoch 1.55: Loss = 1.01184
Epoch 1.56: Loss = 0.943207
Epoch 1.57: Loss = 0.986725
Epoch 1.58: Loss = 0.927612
Epoch 1.59: Loss = 0.973328
Epoch 1.60: Loss = 0.877441
Epoch 1.61: Loss = 0.943527
Epoch 1.62: Loss = 0.879349
Epoch 1.63: Loss = 0.792374
Epoch 1.64: Loss = 0.885452
Epoch 1.65: Loss = 0.90921
Epoch 1.66: Loss = 0.893951
Epoch 1.67: Loss = 0.831131
Epoch 1.68: Loss = 0.76947
Epoch 1.69: Loss = 0.83461
Epoch 1.70: Loss = 0.864594
Epoch 1.71: Loss = 0.812653
Epoch 1.72: Loss = 0.865997
Epoch 1.73: Loss = 0.836105
Epoch 1.74: Loss = 0.854218
Epoch 1.75: Loss = 0.776672
Epoch 1.76: Loss = 0.762924
Epoch 1.77: Loss = 0.772751
Epoch 1.78: Loss = 0.730072
Epoch 1.79: Loss = 0.711899
Epoch 1.80: Loss = 0.818619
Epoch 1.81: Loss = 0.748184
Epoch 1.82: Loss = 0.721313
Epoch 1.83: Loss = 0.764786
Epoch 1.84: Loss = 0.702484
Epoch 1.85: Loss = 0.703491
Epoch 1.86: Loss = 0.711609
Epoch 1.87: Loss = 0.753616
Epoch 1.88: Loss = 0.732147
Epoch 1.89: Loss = 0.734512
Epoch 1.90: Loss = 0.637405
Epoch 1.91: Loss = 0.672882
Epoch 1.92: Loss = 0.735275
Epoch 1.93: Loss = 0.756348
Epoch 1.94: Loss = 0.675476
Epoch 1.95: Loss = 0.724976
Epoch 1.96: Loss = 0.663971
Epoch 1.97: Loss = 0.646957
Epoch 1.98: Loss = 0.655396
Epoch 1.99: Loss = 0.66629
Epoch 1.100: Loss = 0.639297
Epoch 1.101: Loss = 0.692337
Epoch 1.102: Loss = 0.606888
Epoch 1.103: Loss = 0.681976
Epoch 1.104: Loss = 0.639709
Epoch 1.105: Loss = 0.65123
Epoch 1.106: Loss = 0.635529
Epoch 1.107: Loss = 0.632309
Epoch 1.108: Loss = 0.600662
Epoch 1.109: Loss = 0.599442
Epoch 1.110: Loss = 0.618988
Epoch 1.111: Loss = 0.589111
Epoch 1.112: Loss = 0.551361
Epoch 1.113: Loss = 0.593353
Epoch 1.114: Loss = 0.620911
Epoch 1.115: Loss = 0.594315
Epoch 1.116: Loss = 0.602371
Epoch 1.117: Loss = 0.63446
Epoch 1.118: Loss = 0.608063
Epoch 1.119: Loss = 0.62587
Epoch 1.120: Loss = 0.615936
TRAIN LOSS = 1.10161
TRAIN ACC = 69.3512 % (41613/60000)
Loss = 0.608856
Loss = 0.614731
Loss = 0.733414
Loss = 0.707672
Loss = 0.718246
Loss = 0.63092
Loss = 0.585922
Loss = 0.747253
Loss = 0.711868
Loss = 0.661285
Loss = 0.357147
Loss = 0.480927
Loss = 0.396301
Loss = 0.575073
Loss = 0.463638
Loss = 0.467819
Loss = 0.444946
Loss = 0.228577
Loss = 0.44545
Loss = 0.688156
TEST LOSS = 0.56341
TEST ACC = 416.129 % (8419/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.61174
Epoch 2.2: Loss = 0.540283
Epoch 2.3: Loss = 0.617935
Epoch 2.4: Loss = 0.580231
Epoch 2.5: Loss = 0.511322
Epoch 2.6: Loss = 0.583817
Epoch 2.7: Loss = 0.598694
Epoch 2.8: Loss = 0.552765
Epoch 2.9: Loss = 0.615128
Epoch 2.10: Loss = 0.593658
Epoch 2.11: Loss = 0.509521
Epoch 2.12: Loss = 0.566803
Epoch 2.13: Loss = 0.538849
Epoch 2.14: Loss = 0.658173
Epoch 2.15: Loss = 0.550232
Epoch 2.16: Loss = 0.552155
Epoch 2.17: Loss = 0.577988
Epoch 2.18: Loss = 0.537125
Epoch 2.19: Loss = 0.60524
Epoch 2.20: Loss = 0.63884
Epoch 2.21: Loss = 0.532318
Epoch 2.22: Loss = 0.578995
Epoch 2.23: Loss = 0.510727
Epoch 2.24: Loss = 0.553146
Epoch 2.25: Loss = 0.540649
Epoch 2.26: Loss = 0.513306
Epoch 2.27: Loss = 0.53598
Epoch 2.28: Loss = 0.543823
Epoch 2.29: Loss = 0.438568
Epoch 2.30: Loss = 0.538406
Epoch 2.31: Loss = 0.516388
Epoch 2.32: Loss = 0.565735
Epoch 2.33: Loss = 0.538803
Epoch 2.34: Loss = 0.544434
Epoch 2.35: Loss = 0.527359
Epoch 2.36: Loss = 0.526642
Epoch 2.37: Loss = 0.486664
Epoch 2.38: Loss = 0.551331
Epoch 2.39: Loss = 0.512985
Epoch 2.40: Loss = 0.526047
Epoch 2.41: Loss = 0.588623
Epoch 2.42: Loss = 0.48053
Epoch 2.43: Loss = 0.510132
Epoch 2.44: Loss = 0.4543
Epoch 2.45: Loss = 0.440979
Epoch 2.46: Loss = 0.511887
Epoch 2.47: Loss = 0.450867
Epoch 2.48: Loss = 0.503464
Epoch 2.49: Loss = 0.477539
Epoch 2.50: Loss = 0.532883
Epoch 2.51: Loss = 0.446671
Epoch 2.52: Loss = 0.542725
Epoch 2.53: Loss = 0.48291
Epoch 2.54: Loss = 0.467575
Epoch 2.55: Loss = 0.508057
Epoch 2.56: Loss = 0.509064
Epoch 2.57: Loss = 0.471161
Epoch 2.58: Loss = 0.58255
Epoch 2.59: Loss = 0.500977
Epoch 2.60: Loss = 0.590118
Epoch 2.61: Loss = 0.478333
Epoch 2.62: Loss = 0.50322
Epoch 2.63: Loss = 0.501068
Epoch 2.64: Loss = 0.507858
Epoch 2.65: Loss = 0.437286
Epoch 2.66: Loss = 0.59581
Epoch 2.67: Loss = 0.519333
Epoch 2.68: Loss = 0.574631
Epoch 2.69: Loss = 0.427811
Epoch 2.70: Loss = 0.511566
Epoch 2.71: Loss = 0.454025
Epoch 2.72: Loss = 0.529114
Epoch 2.73: Loss = 0.540466
Epoch 2.74: Loss = 0.492874
Epoch 2.75: Loss = 0.497421
Epoch 2.76: Loss = 0.426575
Epoch 2.77: Loss = 0.365936
Epoch 2.78: Loss = 0.484512
Epoch 2.79: Loss = 0.458755
Epoch 2.80: Loss = 0.483795
Epoch 2.81: Loss = 0.511932
Epoch 2.82: Loss = 0.475494
Epoch 2.83: Loss = 0.45752
Epoch 2.84: Loss = 0.461349
Epoch 2.85: Loss = 0.468155
Epoch 2.86: Loss = 0.47673
Epoch 2.87: Loss = 0.455887
Epoch 2.88: Loss = 0.45961
Epoch 2.89: Loss = 0.541031
Epoch 2.90: Loss = 0.465683
Epoch 2.91: Loss = 0.481796
Epoch 2.92: Loss = 0.524231
Epoch 2.93: Loss = 0.470764
Epoch 2.94: Loss = 0.466492
Epoch 2.95: Loss = 0.460663
Epoch 2.96: Loss = 0.354156
Epoch 2.97: Loss = 0.47934
Epoch 2.98: Loss = 0.503754
Epoch 2.99: Loss = 0.523193
Epoch 2.100: Loss = 0.420929
Epoch 2.101: Loss = 0.543503
Epoch 2.102: Loss = 0.469162
Epoch 2.103: Loss = 0.466385
Epoch 2.104: Loss = 0.548813
Epoch 2.105: Loss = 0.479095
Epoch 2.106: Loss = 0.47879
Epoch 2.107: Loss = 0.503098
Epoch 2.108: Loss = 0.459091
Epoch 2.109: Loss = 0.438416
Epoch 2.110: Loss = 0.603287
Epoch 2.111: Loss = 0.499649
Epoch 2.112: Loss = 0.424667
Epoch 2.113: Loss = 0.374664
Epoch 2.114: Loss = 0.441162
Epoch 2.115: Loss = 0.444565
Epoch 2.116: Loss = 0.393799
Epoch 2.117: Loss = 0.499847
Epoch 2.118: Loss = 0.442581
Epoch 2.119: Loss = 0.434738
Epoch 2.120: Loss = 0.41951
TRAIN LOSS = 0.506592
TRAIN ACC = 85.1608 % (51099/60000)
Loss = 0.444931
Loss = 0.497314
Loss = 0.580978
Loss = 0.573715
Loss = 0.593155
Loss = 0.465866
Loss = 0.432175
Loss = 0.629669
Loss = 0.576691
Loss = 0.541428
Loss = 0.246109
Loss = 0.360962
Loss = 0.315643
Loss = 0.442627
Loss = 0.300415
Loss = 0.349564
Loss = 0.301025
Loss = 0.113144
Loss = 0.306793
Loss = 0.582123
TEST LOSS = 0.432716
TEST ACC = 510.989 % (8716/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.480118
Epoch 3.2: Loss = 0.440277
Epoch 3.3: Loss = 0.448959
Epoch 3.4: Loss = 0.376831
Epoch 3.5: Loss = 0.434586
Epoch 3.6: Loss = 0.46434
Epoch 3.7: Loss = 0.392578
Epoch 3.8: Loss = 0.505463
Epoch 3.9: Loss = 0.456635
Epoch 3.10: Loss = 0.525833
Epoch 3.11: Loss = 0.40863
Epoch 3.12: Loss = 0.494507
Epoch 3.13: Loss = 0.473755
Epoch 3.14: Loss = 0.453857
Epoch 3.15: Loss = 0.452057
Epoch 3.16: Loss = 0.449524
Epoch 3.17: Loss = 0.389481
Epoch 3.18: Loss = 0.396408
Epoch 3.19: Loss = 0.431763
Epoch 3.20: Loss = 0.393951
Epoch 3.21: Loss = 0.471222
Epoch 3.22: Loss = 0.443741
Epoch 3.23: Loss = 0.445084
Epoch 3.24: Loss = 0.399338
Epoch 3.25: Loss = 0.433655
Epoch 3.26: Loss = 0.49472
Epoch 3.27: Loss = 0.493301
Epoch 3.28: Loss = 0.481766
Epoch 3.29: Loss = 0.341736
Epoch 3.30: Loss = 0.565292
Epoch 3.31: Loss = 0.458267
Epoch 3.32: Loss = 0.349899
Epoch 3.33: Loss = 0.476105
Epoch 3.34: Loss = 0.365829
Epoch 3.35: Loss = 0.469635
Epoch 3.36: Loss = 0.454681
Epoch 3.37: Loss = 0.460388
Epoch 3.38: Loss = 0.409866
Epoch 3.39: Loss = 0.458267
Epoch 3.40: Loss = 0.435745
Epoch 3.41: Loss = 0.422958
Epoch 3.42: Loss = 0.424011
Epoch 3.43: Loss = 0.430252
Epoch 3.44: Loss = 0.422073
Epoch 3.45: Loss = 0.458527
Epoch 3.46: Loss = 0.445984
Epoch 3.47: Loss = 0.532913
Epoch 3.48: Loss = 0.382492
Epoch 3.49: Loss = 0.426178
Epoch 3.50: Loss = 0.441147
Epoch 3.51: Loss = 0.518234
Epoch 3.52: Loss = 0.415848
Epoch 3.53: Loss = 0.473572
Epoch 3.54: Loss = 0.52948
Epoch 3.55: Loss = 0.412903
Epoch 3.56: Loss = 0.373291
Epoch 3.57: Loss = 0.349197
Epoch 3.58: Loss = 0.384689
Epoch 3.59: Loss = 0.455276
Epoch 3.60: Loss = 0.42366
Epoch 3.61: Loss = 0.471161
Epoch 3.62: Loss = 0.407928
Epoch 3.63: Loss = 0.42012
Epoch 3.64: Loss = 0.505997
Epoch 3.65: Loss = 0.504654
Epoch 3.66: Loss = 0.433014
Epoch 3.67: Loss = 0.352142
Epoch 3.68: Loss = 0.458115
Epoch 3.69: Loss = 0.43457
Epoch 3.70: Loss = 0.446732
Epoch 3.71: Loss = 0.419495
Epoch 3.72: Loss = 0.401688
Epoch 3.73: Loss = 0.419769
Epoch 3.74: Loss = 0.468033
Epoch 3.75: Loss = 0.493408
Epoch 3.76: Loss = 0.458145
Epoch 3.77: Loss = 0.38913
Epoch 3.78: Loss = 0.406235
Epoch 3.79: Loss = 0.363358
Epoch 3.80: Loss = 0.475128
Epoch 3.81: Loss = 0.388779
Epoch 3.82: Loss = 0.459763
Epoch 3.83: Loss = 0.455566
Epoch 3.84: Loss = 0.418716
Epoch 3.85: Loss = 0.508957
Epoch 3.86: Loss = 0.387543
Epoch 3.87: Loss = 0.428726
Epoch 3.88: Loss = 0.412857
Epoch 3.89: Loss = 0.402069
Epoch 3.90: Loss = 0.433533
Epoch 3.91: Loss = 0.419601
Epoch 3.92: Loss = 0.414627
Epoch 3.93: Loss = 0.486465
Epoch 3.94: Loss = 0.404297
Epoch 3.95: Loss = 0.443253
Epoch 3.96: Loss = 0.428238
Epoch 3.97: Loss = 0.409851
Epoch 3.98: Loss = 0.369232
Epoch 3.99: Loss = 0.411575
Epoch 3.100: Loss = 0.32428
Epoch 3.101: Loss = 0.494095
Epoch 3.102: Loss = 0.463318
Epoch 3.103: Loss = 0.34935
Epoch 3.104: Loss = 0.429504
Epoch 3.105: Loss = 0.393173
Epoch 3.106: Loss = 0.430832
Epoch 3.107: Loss = 0.430908
Epoch 3.108: Loss = 0.404007
Epoch 3.109: Loss = 0.410583
Epoch 3.110: Loss = 0.483307
Epoch 3.111: Loss = 0.391159
Epoch 3.112: Loss = 0.38887
Epoch 3.113: Loss = 0.539429
Epoch 3.114: Loss = 0.439575
Epoch 3.115: Loss = 0.43782
Epoch 3.116: Loss = 0.394165
Epoch 3.117: Loss = 0.406219
Epoch 3.118: Loss = 0.366974
Epoch 3.119: Loss = 0.403152
Epoch 3.120: Loss = 0.383713
TRAIN LOSS = 0.433975
TRAIN ACC = 87.1704 % (52305/60000)
Loss = 0.40213
Loss = 0.481659
Loss = 0.532944
Loss = 0.546707
Loss = 0.550537
Loss = 0.425201
Loss = 0.392624
Loss = 0.617676
Loss = 0.553467
Loss = 0.501923
Loss = 0.201828
Loss = 0.313675
Loss = 0.289322
Loss = 0.390747
Loss = 0.254181
Loss = 0.301544
Loss = 0.257416
Loss = 0.0821075
Loss = 0.253586
Loss = 0.550751
TEST LOSS = 0.395001
TEST ACC = 523.048 % (8863/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.384644
Epoch 4.2: Loss = 0.433502
Epoch 4.3: Loss = 0.494629
Epoch 4.4: Loss = 0.401962
Epoch 4.5: Loss = 0.405731
Epoch 4.6: Loss = 0.46756
Epoch 4.7: Loss = 0.483643
Epoch 4.8: Loss = 0.450699
Epoch 4.9: Loss = 0.435791
Epoch 4.10: Loss = 0.354019
Epoch 4.11: Loss = 0.40892
Epoch 4.12: Loss = 0.471085
Epoch 4.13: Loss = 0.439972
Epoch 4.14: Loss = 0.457138
Epoch 4.15: Loss = 0.433624
Epoch 4.16: Loss = 0.439285
Epoch 4.17: Loss = 0.427856
Epoch 4.18: Loss = 0.45253
Epoch 4.19: Loss = 0.420471
Epoch 4.20: Loss = 0.374207
Epoch 4.21: Loss = 0.446335
Epoch 4.22: Loss = 0.409714
Epoch 4.23: Loss = 0.424866
Epoch 4.24: Loss = 0.462509
Epoch 4.25: Loss = 0.408615
Epoch 4.26: Loss = 0.360382
Epoch 4.27: Loss = 0.434753
Epoch 4.28: Loss = 0.397018
Epoch 4.29: Loss = 0.344147
Epoch 4.30: Loss = 0.365036
Epoch 4.31: Loss = 0.41272
Epoch 4.32: Loss = 0.385727
Epoch 4.33: Loss = 0.375046
Epoch 4.34: Loss = 0.365463
Epoch 4.35: Loss = 0.399658
Epoch 4.36: Loss = 0.425888
Epoch 4.37: Loss = 0.42215
Epoch 4.38: Loss = 0.40007
Epoch 4.39: Loss = 0.507919
Epoch 4.40: Loss = 0.462967
Epoch 4.41: Loss = 0.376892
Epoch 4.42: Loss = 0.437866
Epoch 4.43: Loss = 0.364807
Epoch 4.44: Loss = 0.409897
Epoch 4.45: Loss = 0.442825
Epoch 4.46: Loss = 0.475296
Epoch 4.47: Loss = 0.473434
Epoch 4.48: Loss = 0.371521
Epoch 4.49: Loss = 0.352417
Epoch 4.50: Loss = 0.435577
Epoch 4.51: Loss = 0.401382
Epoch 4.52: Loss = 0.395615
Epoch 4.53: Loss = 0.404755
Epoch 4.54: Loss = 0.410278
Epoch 4.55: Loss = 0.431625
Epoch 4.56: Loss = 0.425201
Epoch 4.57: Loss = 0.379562
Epoch 4.58: Loss = 0.4133
Epoch 4.59: Loss = 0.368027
Epoch 4.60: Loss = 0.421036
Epoch 4.61: Loss = 0.338959
Epoch 4.62: Loss = 0.332687
Epoch 4.63: Loss = 0.491837
Epoch 4.64: Loss = 0.459717
Epoch 4.65: Loss = 0.46579
Epoch 4.66: Loss = 0.34082
Epoch 4.67: Loss = 0.427856
Epoch 4.68: Loss = 0.448441
Epoch 4.69: Loss = 0.387222
Epoch 4.70: Loss = 0.334167
Epoch 4.71: Loss = 0.430359
Epoch 4.72: Loss = 0.419632
Epoch 4.73: Loss = 0.397522
Epoch 4.74: Loss = 0.391281
Epoch 4.75: Loss = 0.385956
Epoch 4.76: Loss = 0.346527
Epoch 4.77: Loss = 0.42688
Epoch 4.78: Loss = 0.346146
Epoch 4.79: Loss = 0.357208
Epoch 4.80: Loss = 0.444168
Epoch 4.81: Loss = 0.385849
Epoch 4.82: Loss = 0.406738
Epoch 4.83: Loss = 0.444916
Epoch 4.84: Loss = 0.396576
Epoch 4.85: Loss = 0.418488
Epoch 4.86: Loss = 0.338959
Epoch 4.87: Loss = 0.392944
Epoch 4.88: Loss = 0.379745
Epoch 4.89: Loss = 0.465195
Epoch 4.90: Loss = 0.395203
Epoch 4.91: Loss = 0.350677
Epoch 4.92: Loss = 0.363632
Epoch 4.93: Loss = 0.406891
Epoch 4.94: Loss = 0.336548
Epoch 4.95: Loss = 0.486023
Epoch 4.96: Loss = 0.39917
Epoch 4.97: Loss = 0.428238
Epoch 4.98: Loss = 0.400742
Epoch 4.99: Loss = 0.434906
Epoch 4.100: Loss = 0.336227
Epoch 4.101: Loss = 0.485458
Epoch 4.102: Loss = 0.44519
Epoch 4.103: Loss = 0.368759
Epoch 4.104: Loss = 0.428665
Epoch 4.105: Loss = 0.352341
Epoch 4.106: Loss = 0.466064
Epoch 4.107: Loss = 0.408325
Epoch 4.108: Loss = 0.355316
Epoch 4.109: Loss = 0.354355
Epoch 4.110: Loss = 0.321609
Epoch 4.111: Loss = 0.33522
Epoch 4.112: Loss = 0.452087
Epoch 4.113: Loss = 0.355408
Epoch 4.114: Loss = 0.41481
Epoch 4.115: Loss = 0.39415
Epoch 4.116: Loss = 0.391525
Epoch 4.117: Loss = 0.381989
Epoch 4.118: Loss = 0.410629
Epoch 4.119: Loss = 0.439575
Epoch 4.120: Loss = 0.345657
TRAIN LOSS = 0.40741
TRAIN ACC = 88.0142 % (52811/60000)
Loss = 0.369263
Loss = 0.460251
Loss = 0.508072
Loss = 0.523804
Loss = 0.540588
Loss = 0.407806
Loss = 0.368118
Loss = 0.629333
Loss = 0.53273
Loss = 0.480911
Loss = 0.191467
Loss = 0.297592
Loss = 0.290649
Loss = 0.379532
Loss = 0.211914
Loss = 0.307739
Loss = 0.227951
Loss = 0.0691528
Loss = 0.238068
Loss = 0.525208
TEST LOSS = 0.378007
TEST ACC = 528.11 % (8915/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.465317
Epoch 5.2: Loss = 0.36528
Epoch 5.3: Loss = 0.356476
Epoch 5.4: Loss = 0.344604
Epoch 5.5: Loss = 0.398499
Epoch 5.6: Loss = 0.46904
Epoch 5.7: Loss = 0.330276
Epoch 5.8: Loss = 0.372391
Epoch 5.9: Loss = 0.469986
Epoch 5.10: Loss = 0.518417
Epoch 5.11: Loss = 0.501251
Epoch 5.12: Loss = 0.396851
Epoch 5.13: Loss = 0.354874
Epoch 5.14: Loss = 0.349472
Epoch 5.15: Loss = 0.408798
Epoch 5.16: Loss = 0.440811
Epoch 5.17: Loss = 0.396896
Epoch 5.18: Loss = 0.38588
Epoch 5.19: Loss = 0.39653
Epoch 5.20: Loss = 0.43541
Epoch 5.21: Loss = 0.396866
Epoch 5.22: Loss = 0.350082
Epoch 5.23: Loss = 0.428726
Epoch 5.24: Loss = 0.290146
Epoch 5.25: Loss = 0.339005
Epoch 5.26: Loss = 0.393387
Epoch 5.27: Loss = 0.430237
Epoch 5.28: Loss = 0.431396
Epoch 5.29: Loss = 0.422302
Epoch 5.30: Loss = 0.304733
Epoch 5.31: Loss = 0.387482
Epoch 5.32: Loss = 0.364334
Epoch 5.33: Loss = 0.335663
Epoch 5.34: Loss = 0.437668
Epoch 5.35: Loss = 0.444565
Epoch 5.36: Loss = 0.431854
Epoch 5.37: Loss = 0.33696
Epoch 5.38: Loss = 0.368484
Epoch 5.39: Loss = 0.429794
Epoch 5.40: Loss = 0.418503
Epoch 5.41: Loss = 0.369614
Epoch 5.42: Loss = 0.438324
Epoch 5.43: Loss = 0.490494
Epoch 5.44: Loss = 0.44928
Epoch 5.45: Loss = 0.442276
Epoch 5.46: Loss = 0.346878
Epoch 5.47: Loss = 0.375137
Epoch 5.48: Loss = 0.424759
Epoch 5.49: Loss = 0.343781
Epoch 5.50: Loss = 0.302231
Epoch 5.51: Loss = 0.305267
Epoch 5.52: Loss = 0.391129
Epoch 5.53: Loss = 0.334473
Epoch 5.54: Loss = 0.297485
Epoch 5.55: Loss = 0.353638
Epoch 5.56: Loss = 0.41832
Epoch 5.57: Loss = 0.477036
Epoch 5.58: Loss = 0.349182
Epoch 5.59: Loss = 0.40564
Epoch 5.60: Loss = 0.40271
Epoch 5.61: Loss = 0.43721
Epoch 5.62: Loss = 0.382797
Epoch 5.63: Loss = 0.495956
Epoch 5.64: Loss = 0.470688
Epoch 5.65: Loss = 0.379288
Epoch 5.66: Loss = 0.348404
Epoch 5.67: Loss = 0.422775
Epoch 5.68: Loss = 0.381119
Epoch 5.69: Loss = 0.484634
Epoch 5.70: Loss = 0.334686
Epoch 5.71: Loss = 0.397491
Epoch 5.72: Loss = 0.397766
Epoch 5.73: Loss = 0.48024
Epoch 5.74: Loss = 0.330261
Epoch 5.75: Loss = 0.391998
Epoch 5.76: Loss = 0.473007
Epoch 5.77: Loss = 0.346054
Epoch 5.78: Loss = 0.368622
Epoch 5.79: Loss = 0.340927
Epoch 5.80: Loss = 0.33078
Epoch 5.81: Loss = 0.373245
Epoch 5.82: Loss = 0.439713
Epoch 5.83: Loss = 0.463043
Epoch 5.84: Loss = 0.353165
Epoch 5.85: Loss = 0.419174
Epoch 5.86: Loss = 0.335632
Epoch 5.87: Loss = 0.370148
Epoch 5.88: Loss = 0.365967
Epoch 5.89: Loss = 0.453903
Epoch 5.90: Loss = 0.291122
Epoch 5.91: Loss = 0.320602
Epoch 5.92: Loss = 0.351028
Epoch 5.93: Loss = 0.450546
Epoch 5.94: Loss = 0.350311
Epoch 5.95: Loss = 0.322342
Epoch 5.96: Loss = 0.338562
Epoch 5.97: Loss = 0.419464
Epoch 5.98: Loss = 0.367722
Epoch 5.99: Loss = 0.316269
Epoch 5.100: Loss = 0.357849
Epoch 5.101: Loss = 0.416016
Epoch 5.102: Loss = 0.39473
Epoch 5.103: Loss = 0.417603
Epoch 5.104: Loss = 0.41803
Epoch 5.105: Loss = 0.49173
Epoch 5.106: Loss = 0.374039
Epoch 5.107: Loss = 0.444
Epoch 5.108: Loss = 0.419144
Epoch 5.109: Loss = 0.417725
Epoch 5.110: Loss = 0.456207
Epoch 5.111: Loss = 0.360916
Epoch 5.112: Loss = 0.437927
Epoch 5.113: Loss = 0.402481
Epoch 5.114: Loss = 0.395218
Epoch 5.115: Loss = 0.375473
Epoch 5.116: Loss = 0.363907
Epoch 5.117: Loss = 0.428421
Epoch 5.118: Loss = 0.267471
Epoch 5.119: Loss = 0.514389
Epoch 5.120: Loss = 0.320343
TRAIN LOSS = 0.393219
TRAIN ACC = 88.6276 % (53179/60000)
Loss = 0.360458
Loss = 0.433578
Loss = 0.484955
Loss = 0.513062
Loss = 0.526169
Loss = 0.382126
Loss = 0.348877
Loss = 0.605133
Loss = 0.525116
Loss = 0.46579
Loss = 0.179703
Loss = 0.279526
Loss = 0.265289
Loss = 0.350845
Loss = 0.197296
Loss = 0.275909
Loss = 0.216248
Loss = 0.0614319
Loss = 0.217575
Loss = 0.516861
TEST LOSS = 0.360297
TEST ACC = 531.789 % (8967/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.398773
Epoch 6.2: Loss = 0.387665
Epoch 6.3: Loss = 0.397766
Epoch 6.4: Loss = 0.394516
Epoch 6.5: Loss = 0.420364
Epoch 6.6: Loss = 0.455795
Epoch 6.7: Loss = 0.355606
Epoch 6.8: Loss = 0.29747
Epoch 6.9: Loss = 0.386993
Epoch 6.10: Loss = 0.392166
Epoch 6.11: Loss = 0.333801
Epoch 6.12: Loss = 0.362732
Epoch 6.13: Loss = 0.43924
Epoch 6.14: Loss = 0.390884
Epoch 6.15: Loss = 0.309128
Epoch 6.16: Loss = 0.429184
Epoch 6.17: Loss = 0.429108
Epoch 6.18: Loss = 0.435226
Epoch 6.19: Loss = 0.466156
Epoch 6.20: Loss = 0.356796
Epoch 6.21: Loss = 0.428833
Epoch 6.22: Loss = 0.436432
Epoch 6.23: Loss = 0.34552
Epoch 6.24: Loss = 0.349335
Epoch 6.25: Loss = 0.412643
Epoch 6.26: Loss = 0.385941
Epoch 6.27: Loss = 0.403107
Epoch 6.28: Loss = 0.38501
Epoch 6.29: Loss = 0.30867
Epoch 6.30: Loss = 0.337982
Epoch 6.31: Loss = 0.410172
Epoch 6.32: Loss = 0.348633
Epoch 6.33: Loss = 0.407257
Epoch 6.34: Loss = 0.336838
Epoch 6.35: Loss = 0.311798
Epoch 6.36: Loss = 0.336823
Epoch 6.37: Loss = 0.413422
Epoch 6.38: Loss = 0.417816
Epoch 6.39: Loss = 0.340637
Epoch 6.40: Loss = 0.38974
Epoch 6.41: Loss = 0.373749
Epoch 6.42: Loss = 0.38765
Epoch 6.43: Loss = 0.395065
Epoch 6.44: Loss = 0.367569
Epoch 6.45: Loss = 0.304245
Epoch 6.46: Loss = 0.393204
Epoch 6.47: Loss = 0.418442
Epoch 6.48: Loss = 0.42099
Epoch 6.49: Loss = 0.299026
Epoch 6.50: Loss = 0.345596
Epoch 6.51: Loss = 0.347137
Epoch 6.52: Loss = 0.29303
Epoch 6.53: Loss = 0.332016
Epoch 6.54: Loss = 0.372696
Epoch 6.55: Loss = 0.375381
Epoch 6.56: Loss = 0.432129
Epoch 6.57: Loss = 0.418488
Epoch 6.58: Loss = 0.415863
Epoch 6.59: Loss = 0.433624
Epoch 6.60: Loss = 0.530319
Epoch 6.61: Loss = 0.365005
Epoch 6.62: Loss = 0.372879
Epoch 6.63: Loss = 0.359833
Epoch 6.64: Loss = 0.351624
Epoch 6.65: Loss = 0.371552
Epoch 6.66: Loss = 0.396744
Epoch 6.67: Loss = 0.472336
Epoch 6.68: Loss = 0.410736
Epoch 6.69: Loss = 0.335922
Epoch 6.70: Loss = 0.421463
Epoch 6.71: Loss = 0.374573
Epoch 6.72: Loss = 0.499603
Epoch 6.73: Loss = 0.370316
Epoch 6.74: Loss = 0.378418
Epoch 6.75: Loss = 0.4021
Epoch 6.76: Loss = 0.366425
Epoch 6.77: Loss = 0.35437
Epoch 6.78: Loss = 0.398026
Epoch 6.79: Loss = 0.420959
Epoch 6.80: Loss = 0.414154
Epoch 6.81: Loss = 0.382721
Epoch 6.82: Loss = 0.370895
Epoch 6.83: Loss = 0.407562
Epoch 6.84: Loss = 0.316788
Epoch 6.85: Loss = 0.332779
Epoch 6.86: Loss = 0.450882
Epoch 6.87: Loss = 0.501389
Epoch 6.88: Loss = 0.392029
Epoch 6.89: Loss = 0.373215
Epoch 6.90: Loss = 0.309006
Epoch 6.91: Loss = 0.344528
Epoch 6.92: Loss = 0.436279
Epoch 6.93: Loss = 0.39003
Epoch 6.94: Loss = 0.424454
Epoch 6.95: Loss = 0.395615
Epoch 6.96: Loss = 0.356537
Epoch 6.97: Loss = 0.459351
Epoch 6.98: Loss = 0.356598
Epoch 6.99: Loss = 0.398621
Epoch 6.100: Loss = 0.361481
Epoch 6.101: Loss = 0.402527
Epoch 6.102: Loss = 0.402878
Epoch 6.103: Loss = 0.386536
Epoch 6.104: Loss = 0.360916
Epoch 6.105: Loss = 0.311035
Epoch 6.106: Loss = 0.474243
Epoch 6.107: Loss = 0.358231
Epoch 6.108: Loss = 0.447372
Epoch 6.109: Loss = 0.38916
Epoch 6.110: Loss = 0.387604
Epoch 6.111: Loss = 0.435181
Epoch 6.112: Loss = 0.319183
Epoch 6.113: Loss = 0.405441
Epoch 6.114: Loss = 0.407089
Epoch 6.115: Loss = 0.298325
Epoch 6.116: Loss = 0.372299
Epoch 6.117: Loss = 0.379517
Epoch 6.118: Loss = 0.482132
Epoch 6.119: Loss = 0.376938
Epoch 6.120: Loss = 0.357498
TRAIN LOSS = 0.386276
TRAIN ACC = 88.9175 % (53353/60000)
Loss = 0.368088
Loss = 0.433334
Loss = 0.493332
Loss = 0.516586
Loss = 0.536667
Loss = 0.385208
Loss = 0.33934
Loss = 0.603348
Loss = 0.522385
Loss = 0.46373
Loss = 0.18222
Loss = 0.273605
Loss = 0.291977
Loss = 0.351746
Loss = 0.199371
Loss = 0.26918
Loss = 0.21904
Loss = 0.0602112
Loss = 0.212326
Loss = 0.543213
TEST LOSS = 0.363245
TEST ACC = 533.53 % (8947/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.442719
Epoch 7.2: Loss = 0.351486
Epoch 7.3: Loss = 0.426437
Epoch 7.4: Loss = 0.446732
Epoch 7.5: Loss = 0.358505
Epoch 7.6: Loss = 0.390335
Epoch 7.7: Loss = 0.473557
Epoch 7.8: Loss = 0.396622
Epoch 7.9: Loss = 0.320099
Epoch 7.10: Loss = 0.298691
Epoch 7.11: Loss = 0.368027
Epoch 7.12: Loss = 0.28009
Epoch 7.13: Loss = 0.372971
Epoch 7.14: Loss = 0.322449
Epoch 7.15: Loss = 0.355759
Epoch 7.16: Loss = 0.415131
Epoch 7.17: Loss = 0.426147
Epoch 7.18: Loss = 0.431656
Epoch 7.19: Loss = 0.376221
Epoch 7.20: Loss = 0.325424
Epoch 7.21: Loss = 0.404922
Epoch 7.22: Loss = 0.418182
Epoch 7.23: Loss = 0.398117
Epoch 7.24: Loss = 0.378143
Epoch 7.25: Loss = 0.427643
Epoch 7.26: Loss = 0.37915
Epoch 7.27: Loss = 0.466721
Epoch 7.28: Loss = 0.409134
Epoch 7.29: Loss = 0.347458
Epoch 7.30: Loss = 0.405014
Epoch 7.31: Loss = 0.470306
Epoch 7.32: Loss = 0.373062
Epoch 7.33: Loss = 0.407745
Epoch 7.34: Loss = 0.409866
Epoch 7.35: Loss = 0.380997
Epoch 7.36: Loss = 0.396622
Epoch 7.37: Loss = 0.314789
Epoch 7.38: Loss = 0.418854
Epoch 7.39: Loss = 0.406708
Epoch 7.40: Loss = 0.458755
Epoch 7.41: Loss = 0.323059
Epoch 7.42: Loss = 0.377747
Epoch 7.43: Loss = 0.357635
Epoch 7.44: Loss = 0.348282
Epoch 7.45: Loss = 0.428284
Epoch 7.46: Loss = 0.386642
Epoch 7.47: Loss = 0.45842
Epoch 7.48: Loss = 0.387543
Epoch 7.49: Loss = 0.379333
Epoch 7.50: Loss = 0.349335
Epoch 7.51: Loss = 0.431198
Epoch 7.52: Loss = 0.364334
Epoch 7.53: Loss = 0.401474
Epoch 7.54: Loss = 0.366425
Epoch 7.55: Loss = 0.367584
Epoch 7.56: Loss = 0.439133
Epoch 7.57: Loss = 0.353714
Epoch 7.58: Loss = 0.433975
Epoch 7.59: Loss = 0.373184
Epoch 7.60: Loss = 0.321564
Epoch 7.61: Loss = 0.533585
Epoch 7.62: Loss = 0.454819
Epoch 7.63: Loss = 0.428909
Epoch 7.64: Loss = 0.424042
Epoch 7.65: Loss = 0.390823
Epoch 7.66: Loss = 0.369675
Epoch 7.67: Loss = 0.308792
Epoch 7.68: Loss = 0.419647
Epoch 7.69: Loss = 0.386597
Epoch 7.70: Loss = 0.371628
Epoch 7.71: Loss = 0.309052
Epoch 7.72: Loss = 0.30014
Epoch 7.73: Loss = 0.358673
Epoch 7.74: Loss = 0.35556
Epoch 7.75: Loss = 0.348907
Epoch 7.76: Loss = 0.458405
Epoch 7.77: Loss = 0.398468
Epoch 7.78: Loss = 0.3255
Epoch 7.79: Loss = 0.290298
Epoch 7.80: Loss = 0.316437
Epoch 7.81: Loss = 0.393066
Epoch 7.82: Loss = 0.369797
Epoch 7.83: Loss = 0.421677
Epoch 7.84: Loss = 0.372269
Epoch 7.85: Loss = 0.343735
Epoch 7.86: Loss = 0.425842
Epoch 7.87: Loss = 0.34314
Epoch 7.88: Loss = 0.324585
Epoch 7.89: Loss = 0.418564
Epoch 7.90: Loss = 0.320343
Epoch 7.91: Loss = 0.416931
Epoch 7.92: Loss = 0.351624
Epoch 7.93: Loss = 0.370972
Epoch 7.94: Loss = 0.407761
Epoch 7.95: Loss = 0.335266
Epoch 7.96: Loss = 0.349655
Epoch 7.97: Loss = 0.337479
Epoch 7.98: Loss = 0.344452
Epoch 7.99: Loss = 0.302612
Epoch 7.100: Loss = 0.337326
Epoch 7.101: Loss = 0.372284
Epoch 7.102: Loss = 0.48938
Epoch 7.103: Loss = 0.328918
Epoch 7.104: Loss = 0.380707
Epoch 7.105: Loss = 0.401855
Epoch 7.106: Loss = 0.335678
Epoch 7.107: Loss = 0.361847
Epoch 7.108: Loss = 0.394791
Epoch 7.109: Loss = 0.411636
Epoch 7.110: Loss = 0.417847
Epoch 7.111: Loss = 0.361115
Epoch 7.112: Loss = 0.434052
Epoch 7.113: Loss = 0.4272
Epoch 7.114: Loss = 0.351685
Epoch 7.115: Loss = 0.443756
Epoch 7.116: Loss = 0.359543
Epoch 7.117: Loss = 0.336777
Epoch 7.118: Loss = 0.378555
Epoch 7.119: Loss = 0.391907
Epoch 7.120: Loss = 0.36528
TRAIN LOSS = 0.382355
TRAIN ACC = 89.122 % (53476/60000)
Loss = 0.371368
Loss = 0.427444
Loss = 0.49205
Loss = 0.499603
Loss = 0.549255
Loss = 0.375259
Loss = 0.325989
Loss = 0.633987
Loss = 0.540955
Loss = 0.457031
Loss = 0.176514
Loss = 0.263535
Loss = 0.305862
Loss = 0.357239
Loss = 0.173965
Loss = 0.268219
Loss = 0.240799
Loss = 0.0531921
Loss = 0.226074
Loss = 0.535553
TEST LOSS = 0.363695
TEST ACC = 534.76 % (8968/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.371796
Epoch 8.2: Loss = 0.411667
Epoch 8.3: Loss = 0.373978
Epoch 8.4: Loss = 0.37561
Epoch 8.5: Loss = 0.361084
Epoch 8.6: Loss = 0.292282
Epoch 8.7: Loss = 0.330124
Epoch 8.8: Loss = 0.347015
Epoch 8.9: Loss = 0.357742
Epoch 8.10: Loss = 0.388336
Epoch 8.11: Loss = 0.375275
Epoch 8.12: Loss = 0.303604
Epoch 8.13: Loss = 0.3414
Epoch 8.14: Loss = 0.411224
Epoch 8.15: Loss = 0.373047
Epoch 8.16: Loss = 0.384293
Epoch 8.17: Loss = 0.546356
Epoch 8.18: Loss = 0.334427
Epoch 8.19: Loss = 0.371201
Epoch 8.20: Loss = 0.269836
Epoch 8.21: Loss = 0.404633
Epoch 8.22: Loss = 0.356674
Epoch 8.23: Loss = 0.296432
Epoch 8.24: Loss = 0.37178
Epoch 8.25: Loss = 0.398804
Epoch 8.26: Loss = 0.310242
Epoch 8.27: Loss = 0.377182
Epoch 8.28: Loss = 0.370041
Epoch 8.29: Loss = 0.31311
Epoch 8.30: Loss = 0.362076
Epoch 8.31: Loss = 0.416519
Epoch 8.32: Loss = 0.319931
Epoch 8.33: Loss = 0.375671
Epoch 8.34: Loss = 0.428116
Epoch 8.35: Loss = 0.392868
Epoch 8.36: Loss = 0.382736
Epoch 8.37: Loss = 0.333221
Epoch 8.38: Loss = 0.434418
Epoch 8.39: Loss = 0.446884
Epoch 8.40: Loss = 0.297928
Epoch 8.41: Loss = 0.305222
Epoch 8.42: Loss = 0.356537
Epoch 8.43: Loss = 0.494186
Epoch 8.44: Loss = 0.392944
Epoch 8.45: Loss = 0.428253
Epoch 8.46: Loss = 0.322495
Epoch 8.47: Loss = 0.497772
Epoch 8.48: Loss = 0.455185
Epoch 8.49: Loss = 0.476471
Epoch 8.50: Loss = 0.538635
Epoch 8.51: Loss = 0.401871
Epoch 8.52: Loss = 0.401703
Epoch 8.53: Loss = 0.272186
Epoch 8.54: Loss = 0.446014
Epoch 8.55: Loss = 0.376968
Epoch 8.56: Loss = 0.282623
Epoch 8.57: Loss = 0.393158
Epoch 8.58: Loss = 0.371582
Epoch 8.59: Loss = 0.413437
Epoch 8.60: Loss = 0.401932
Epoch 8.61: Loss = 0.289566
Epoch 8.62: Loss = 0.3591
Epoch 8.63: Loss = 0.426743
Epoch 8.64: Loss = 0.405182
Epoch 8.65: Loss = 0.394348
Epoch 8.66: Loss = 0.471085
Epoch 8.67: Loss = 0.381226
Epoch 8.68: Loss = 0.307114
Epoch 8.69: Loss = 0.388885
Epoch 8.70: Loss = 0.334061
Epoch 8.71: Loss = 0.512161
Epoch 8.72: Loss = 0.393814
Epoch 8.73: Loss = 0.393524
Epoch 8.74: Loss = 0.363907
Epoch 8.75: Loss = 0.305496
Epoch 8.76: Loss = 0.488831
Epoch 8.77: Loss = 0.349075
Epoch 8.78: Loss = 0.359146
Epoch 8.79: Loss = 0.331314
Epoch 8.80: Loss = 0.221909
Epoch 8.81: Loss = 0.414612
Epoch 8.82: Loss = 0.402039
Epoch 8.83: Loss = 0.395645
Epoch 8.84: Loss = 0.387573
Epoch 8.85: Loss = 0.433792
Epoch 8.86: Loss = 0.362549
Epoch 8.87: Loss = 0.491623
Epoch 8.88: Loss = 0.441772
Epoch 8.89: Loss = 0.440079
Epoch 8.90: Loss = 0.381577
Epoch 8.91: Loss = 0.319275
Epoch 8.92: Loss = 0.318329
Epoch 8.93: Loss = 0.352371
Epoch 8.94: Loss = 0.382339
Epoch 8.95: Loss = 0.393402
Epoch 8.96: Loss = 0.388794
Epoch 8.97: Loss = 0.357681
Epoch 8.98: Loss = 0.419922
Epoch 8.99: Loss = 0.351028
Epoch 8.100: Loss = 0.39183
Epoch 8.101: Loss = 0.414322
Epoch 8.102: Loss = 0.40451
Epoch 8.103: Loss = 0.500015
Epoch 8.104: Loss = 0.358826
Epoch 8.105: Loss = 0.36496
Epoch 8.106: Loss = 0.299973
Epoch 8.107: Loss = 0.380402
Epoch 8.108: Loss = 0.370834
Epoch 8.109: Loss = 0.385269
Epoch 8.110: Loss = 0.360825
Epoch 8.111: Loss = 0.43425
Epoch 8.112: Loss = 0.450928
Epoch 8.113: Loss = 0.424271
Epoch 8.114: Loss = 0.319565
Epoch 8.115: Loss = 0.354736
Epoch 8.116: Loss = 0.316132
Epoch 8.117: Loss = 0.359756
Epoch 8.118: Loss = 0.357086
Epoch 8.119: Loss = 0.419083
Epoch 8.120: Loss = 0.246277
TRAIN LOSS = 0.3797
TRAIN ACC = 89.3387 % (53606/60000)
Loss = 0.363846
Loss = 0.420074
Loss = 0.490189
Loss = 0.50061
Loss = 0.529846
Loss = 0.367523
Loss = 0.327896
Loss = 0.618011
Loss = 0.516617
Loss = 0.444855
Loss = 0.170914
Loss = 0.258835
Loss = 0.30426
Loss = 0.353516
Loss = 0.173462
Loss = 0.279205
Loss = 0.229385
Loss = 0.0533447
Loss = 0.222473
Loss = 0.548309
TEST LOSS = 0.358658
TEST ACC = 536.06 % (9007/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.44519
Epoch 9.2: Loss = 0.41246
Epoch 9.3: Loss = 0.451523
Epoch 9.4: Loss = 0.319565
Epoch 9.5: Loss = 0.334503
Epoch 9.6: Loss = 0.42572
Epoch 9.7: Loss = 0.376389
Epoch 9.8: Loss = 0.438416
Epoch 9.9: Loss = 0.349579
Epoch 9.10: Loss = 0.445221
Epoch 9.11: Loss = 0.295731
Epoch 9.12: Loss = 0.329056
Epoch 9.13: Loss = 0.480316
Epoch 9.14: Loss = 0.313293
Epoch 9.15: Loss = 0.409042
Epoch 9.16: Loss = 0.391312
Epoch 9.17: Loss = 0.356049
Epoch 9.18: Loss = 0.480469
Epoch 9.19: Loss = 0.313324
Epoch 9.20: Loss = 0.340942
Epoch 9.21: Loss = 0.358856
Epoch 9.22: Loss = 0.331863
Epoch 9.23: Loss = 0.385788
Epoch 9.24: Loss = 0.358505
Epoch 9.25: Loss = 0.288498
Epoch 9.26: Loss = 0.421616
Epoch 9.27: Loss = 0.388672
Epoch 9.28: Loss = 0.287155
Epoch 9.29: Loss = 0.397537
Epoch 9.30: Loss = 0.387405
Epoch 9.31: Loss = 0.429855
Epoch 9.32: Loss = 0.348679
Epoch 9.33: Loss = 0.36998
Epoch 9.34: Loss = 0.363312
Epoch 9.35: Loss = 0.34816
Epoch 9.36: Loss = 0.370087
Epoch 9.37: Loss = 0.406815
Epoch 9.38: Loss = 0.390854
Epoch 9.39: Loss = 0.344543
Epoch 9.40: Loss = 0.368225
Epoch 9.41: Loss = 0.323395
Epoch 9.42: Loss = 0.388229
Epoch 9.43: Loss = 0.405121
Epoch 9.44: Loss = 0.351898
Epoch 9.45: Loss = 0.319427
Epoch 9.46: Loss = 0.321426
Epoch 9.47: Loss = 0.416153
Epoch 9.48: Loss = 0.455536
Epoch 9.49: Loss = 0.362045
Epoch 9.50: Loss = 0.320297
Epoch 9.51: Loss = 0.386246
Epoch 9.52: Loss = 0.306168
Epoch 9.53: Loss = 0.396912
Epoch 9.54: Loss = 0.427856
Epoch 9.55: Loss = 0.563171
Epoch 9.56: Loss = 0.320709
Epoch 9.57: Loss = 0.4758
Epoch 9.58: Loss = 0.398575
Epoch 9.59: Loss = 0.342163
Epoch 9.60: Loss = 0.388092
Epoch 9.61: Loss = 0.420013
Epoch 9.62: Loss = 0.437714
Epoch 9.63: Loss = 0.388504
Epoch 9.64: Loss = 0.342194
Epoch 9.65: Loss = 0.377151
Epoch 9.66: Loss = 0.435989
Epoch 9.67: Loss = 0.451324
Epoch 9.68: Loss = 0.392319
Epoch 9.69: Loss = 0.345093
Epoch 9.70: Loss = 0.454361
Epoch 9.71: Loss = 0.344345
Epoch 9.72: Loss = 0.380356
Epoch 9.73: Loss = 0.476959
Epoch 9.74: Loss = 0.409897
Epoch 9.75: Loss = 0.343475
Epoch 9.76: Loss = 0.374115
Epoch 9.77: Loss = 0.351547
Epoch 9.78: Loss = 0.375732
Epoch 9.79: Loss = 0.299942
Epoch 9.80: Loss = 0.362091
Epoch 9.81: Loss = 0.406784
Epoch 9.82: Loss = 0.397507
Epoch 9.83: Loss = 0.439468
Epoch 9.84: Loss = 0.379242
Epoch 9.85: Loss = 0.229828
Epoch 9.86: Loss = 0.338257
Epoch 9.87: Loss = 0.350739
Epoch 9.88: Loss = 0.44426
Epoch 9.89: Loss = 0.466003
Epoch 9.90: Loss = 0.402252
Epoch 9.91: Loss = 0.326004
Epoch 9.92: Loss = 0.439209
Epoch 9.93: Loss = 0.410217
Epoch 9.94: Loss = 0.353745
Epoch 9.95: Loss = 0.365097
Epoch 9.96: Loss = 0.437744
Epoch 9.97: Loss = 0.385941
Epoch 9.98: Loss = 0.302383
Epoch 9.99: Loss = 0.245407
Epoch 9.100: Loss = 0.39209
Epoch 9.101: Loss = 0.411545
Epoch 9.102: Loss = 0.449005
Epoch 9.103: Loss = 0.309906
Epoch 9.104: Loss = 0.398636
Epoch 9.105: Loss = 0.365005
Epoch 9.106: Loss = 0.413025
Epoch 9.107: Loss = 0.415848
Epoch 9.108: Loss = 0.453522
Epoch 9.109: Loss = 0.392258
Epoch 9.110: Loss = 0.232452
Epoch 9.111: Loss = 0.422821
Epoch 9.112: Loss = 0.412109
Epoch 9.113: Loss = 0.429138
Epoch 9.114: Loss = 0.340057
Epoch 9.115: Loss = 0.414444
Epoch 9.116: Loss = 0.364944
Epoch 9.117: Loss = 0.318481
Epoch 9.118: Loss = 0.357468
Epoch 9.119: Loss = 0.424103
Epoch 9.120: Loss = 0.354355
TRAIN LOSS = 0.380646
TRAIN ACC = 89.5432 % (53728/60000)
Loss = 0.364639
Loss = 0.422974
Loss = 0.486938
Loss = 0.521027
Loss = 0.535324
Loss = 0.378494
Loss = 0.323944
Loss = 0.637146
Loss = 0.504105
Loss = 0.453308
Loss = 0.172699
Loss = 0.261292
Loss = 0.300171
Loss = 0.355072
Loss = 0.165527
Loss = 0.255737
Loss = 0.222824
Loss = 0.059433
Loss = 0.234192
Loss = 0.542084
TEST LOSS = 0.359846
TEST ACC = 537.279 % (9021/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.411087
Epoch 10.2: Loss = 0.359375
Epoch 10.3: Loss = 0.37468
Epoch 10.4: Loss = 0.404999
Epoch 10.5: Loss = 0.404968
Epoch 10.6: Loss = 0.381592
Epoch 10.7: Loss = 0.385162
Epoch 10.8: Loss = 0.36882
Epoch 10.9: Loss = 0.313995
Epoch 10.10: Loss = 0.452942
Epoch 10.11: Loss = 0.317291
Epoch 10.12: Loss = 0.318924
Epoch 10.13: Loss = 0.294235
Epoch 10.14: Loss = 0.333344
Epoch 10.15: Loss = 0.385651
Epoch 10.16: Loss = 0.355865
Epoch 10.17: Loss = 0.45314
Epoch 10.18: Loss = 0.313995
Epoch 10.19: Loss = 0.418167
Epoch 10.20: Loss = 0.428772
Epoch 10.21: Loss = 0.393997
Epoch 10.22: Loss = 0.403015
Epoch 10.23: Loss = 0.337265
Epoch 10.24: Loss = 0.312866
Epoch 10.25: Loss = 0.434158
Epoch 10.26: Loss = 0.362869
Epoch 10.27: Loss = 0.319214
Epoch 10.28: Loss = 0.315231
Epoch 10.29: Loss = 0.341522
Epoch 10.30: Loss = 0.352264
Epoch 10.31: Loss = 0.352585
Epoch 10.32: Loss = 0.382584
Epoch 10.33: Loss = 0.42305
Epoch 10.34: Loss = 0.392609
Epoch 10.35: Loss = 0.2883
Epoch 10.36: Loss = 0.398651
Epoch 10.37: Loss = 0.35997
Epoch 10.38: Loss = 0.384521
Epoch 10.39: Loss = 0.356018
Epoch 10.40: Loss = 0.34021
Epoch 10.41: Loss = 0.261017
Epoch 10.42: Loss = 0.494308
Epoch 10.43: Loss = 0.377625
Epoch 10.44: Loss = 0.412186
Epoch 10.45: Loss = 0.372742
Epoch 10.46: Loss = 0.389267
Epoch 10.47: Loss = 0.281662
Epoch 10.48: Loss = 0.388611
Epoch 10.49: Loss = 0.457703
Epoch 10.50: Loss = 0.355423
Epoch 10.51: Loss = 0.2686
Epoch 10.52: Loss = 0.488113
Epoch 10.53: Loss = 0.365051
Epoch 10.54: Loss = 0.427521
Epoch 10.55: Loss = 0.403961
Epoch 10.56: Loss = 0.30159
Epoch 10.57: Loss = 0.386734
Epoch 10.58: Loss = 0.344406
Epoch 10.59: Loss = 0.460922
Epoch 10.60: Loss = 0.344681
Epoch 10.61: Loss = 0.336578
Epoch 10.62: Loss = 0.317413
Epoch 10.63: Loss = 0.285706
Epoch 10.64: Loss = 0.450226
Epoch 10.65: Loss = 0.427444
Epoch 10.66: Loss = 0.440598
Epoch 10.67: Loss = 0.385193
Epoch 10.68: Loss = 0.447693
Epoch 10.69: Loss = 0.318497
Epoch 10.70: Loss = 0.302292
Epoch 10.71: Loss = 0.361847
Epoch 10.72: Loss = 0.370209
Epoch 10.73: Loss = 0.439468
Epoch 10.74: Loss = 0.461227
Epoch 10.75: Loss = 0.395889
Epoch 10.76: Loss = 0.349945
Epoch 10.77: Loss = 0.290283
Epoch 10.78: Loss = 0.396957
Epoch 10.79: Loss = 0.32663
Epoch 10.80: Loss = 0.35791
Epoch 10.81: Loss = 0.408249
Epoch 10.82: Loss = 0.426971
Epoch 10.83: Loss = 0.392059
Epoch 10.84: Loss = 0.346283
Epoch 10.85: Loss = 0.318405
Epoch 10.86: Loss = 0.503098
Epoch 10.87: Loss = 0.309036
Epoch 10.88: Loss = 0.388199
Epoch 10.89: Loss = 0.420197
Epoch 10.90: Loss = 0.320953
Epoch 10.91: Loss = 0.340515
Epoch 10.92: Loss = 0.368332
Epoch 10.93: Loss = 0.402328
Epoch 10.94: Loss = 0.348602
Epoch 10.95: Loss = 0.34874
Epoch 10.96: Loss = 0.412247
Epoch 10.97: Loss = 0.271805
Epoch 10.98: Loss = 0.463165
Epoch 10.99: Loss = 0.435623
Epoch 10.100: Loss = 0.338699
Epoch 10.101: Loss = 0.450378
Epoch 10.102: Loss = 0.386292
Epoch 10.103: Loss = 0.415604
Epoch 10.104: Loss = 0.330139
Epoch 10.105: Loss = 0.290527
Epoch 10.106: Loss = 0.337021
Epoch 10.107: Loss = 0.548859
Epoch 10.108: Loss = 0.406128
Epoch 10.109: Loss = 0.323685
Epoch 10.110: Loss = 0.378906
Epoch 10.111: Loss = 0.464325
Epoch 10.112: Loss = 0.409637
Epoch 10.113: Loss = 0.423401
Epoch 10.114: Loss = 0.326431
Epoch 10.115: Loss = 0.382782
Epoch 10.116: Loss = 0.359375
Epoch 10.117: Loss = 0.290939
Epoch 10.118: Loss = 0.44342
Epoch 10.119: Loss = 0.284103
Epoch 10.120: Loss = 0.444412
TRAIN LOSS = 0.375259
TRAIN ACC = 89.7995 % (53882/60000)
Loss = 0.358215
Loss = 0.414398
Loss = 0.480545
Loss = 0.511505
Loss = 0.537552
Loss = 0.369858
Loss = 0.318985
Loss = 0.62532
Loss = 0.493088
Loss = 0.449844
Loss = 0.158951
Loss = 0.261322
Loss = 0.286697
Loss = 0.336639
Loss = 0.160568
Loss = 0.264587
Loss = 0.189911
Loss = 0.0543213
Loss = 0.225998
Loss = 0.518051
TEST LOSS = 0.350818
TEST ACC = 538.818 % (9076/10000)

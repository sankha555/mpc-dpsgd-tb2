Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.38754
Epoch 1.2: Loss = 2.29517
Epoch 1.3: Loss = 2.18573
Epoch 1.4: Loss = 2.14168
Epoch 1.5: Loss = 2.03375
Epoch 1.6: Loss = 2.0128
Epoch 1.7: Loss = 1.92664
Epoch 1.8: Loss = 1.87758
Epoch 1.9: Loss = 1.84511
Epoch 1.10: Loss = 1.77498
Epoch 1.11: Loss = 1.72047
Epoch 1.12: Loss = 1.67952
Epoch 1.13: Loss = 1.66054
Epoch 1.14: Loss = 1.63115
Epoch 1.15: Loss = 1.57632
Epoch 1.16: Loss = 1.53558
Epoch 1.17: Loss = 1.51109
Epoch 1.18: Loss = 1.48882
Epoch 1.19: Loss = 1.4599
Epoch 1.20: Loss = 1.39525
Epoch 1.21: Loss = 1.40352
Epoch 1.22: Loss = 1.41998
Epoch 1.23: Loss = 1.36806
Epoch 1.24: Loss = 1.29463
Epoch 1.25: Loss = 1.3334
Epoch 1.26: Loss = 1.23993
Epoch 1.27: Loss = 1.26311
Epoch 1.28: Loss = 1.25548
Epoch 1.29: Loss = 1.27684
Epoch 1.30: Loss = 1.20636
Epoch 1.31: Loss = 1.21368
Epoch 1.32: Loss = 1.24123
Epoch 1.33: Loss = 1.22083
Epoch 1.34: Loss = 1.13304
Epoch 1.35: Loss = 1.22942
Epoch 1.36: Loss = 1.12292
Epoch 1.37: Loss = 1.14084
Epoch 1.38: Loss = 1.11948
Epoch 1.39: Loss = 1.07314
Epoch 1.40: Loss = 1.08403
Epoch 1.41: Loss = 1.11136
Epoch 1.42: Loss = 1.00624
Epoch 1.43: Loss = 1.00381
Epoch 1.44: Loss = 1.00516
Epoch 1.45: Loss = 1.06448
Epoch 1.46: Loss = 1.0018
Epoch 1.47: Loss = 1.04381
Epoch 1.48: Loss = 0.963028
Epoch 1.49: Loss = 0.977173
Epoch 1.50: Loss = 1.04718
Epoch 1.51: Loss = 1.03473
Epoch 1.52: Loss = 1.00522
Epoch 1.53: Loss = 0.999237
Epoch 1.54: Loss = 0.943008
Epoch 1.55: Loss = 0.916077
Epoch 1.56: Loss = 0.971497
Epoch 1.57: Loss = 0.903854
Epoch 1.58: Loss = 0.932587
Epoch 1.59: Loss = 0.99762
Epoch 1.60: Loss = 0.963715
Epoch 1.61: Loss = 0.935928
Epoch 1.62: Loss = 0.921509
Epoch 1.63: Loss = 1.02344
Epoch 1.64: Loss = 0.928452
Epoch 1.65: Loss = 0.891937
Epoch 1.66: Loss = 0.858154
Epoch 1.67: Loss = 0.911301
Epoch 1.68: Loss = 0.888046
Epoch 1.69: Loss = 0.969193
Epoch 1.70: Loss = 0.842941
Epoch 1.71: Loss = 0.829544
Epoch 1.72: Loss = 0.893417
Epoch 1.73: Loss = 0.899414
Epoch 1.74: Loss = 0.844284
Epoch 1.75: Loss = 0.869141
Epoch 1.76: Loss = 0.913879
Epoch 1.77: Loss = 0.855057
Epoch 1.78: Loss = 0.82341
Epoch 1.79: Loss = 0.819443
Epoch 1.80: Loss = 0.862259
Epoch 1.81: Loss = 0.824738
Epoch 1.82: Loss = 0.796844
Epoch 1.83: Loss = 0.822617
Epoch 1.84: Loss = 0.803146
Epoch 1.85: Loss = 0.824951
Epoch 1.86: Loss = 0.767944
Epoch 1.87: Loss = 0.831329
Epoch 1.88: Loss = 0.713486
Epoch 1.89: Loss = 0.843887
Epoch 1.90: Loss = 0.850952
Epoch 1.91: Loss = 0.819214
Epoch 1.92: Loss = 0.818787
Epoch 1.93: Loss = 0.818542
Epoch 1.94: Loss = 0.802063
Epoch 1.95: Loss = 0.748825
Epoch 1.96: Loss = 0.809006
Epoch 1.97: Loss = 0.833939
Epoch 1.98: Loss = 0.729721
Epoch 1.99: Loss = 0.798721
Epoch 1.100: Loss = 0.775055
Epoch 1.101: Loss = 0.795975
Epoch 1.102: Loss = 0.814209
Epoch 1.103: Loss = 0.802612
Epoch 1.104: Loss = 0.742462
Epoch 1.105: Loss = 0.819931
Epoch 1.106: Loss = 0.783539
Epoch 1.107: Loss = 0.75766
Epoch 1.108: Loss = 0.773621
Epoch 1.109: Loss = 0.828506
Epoch 1.110: Loss = 0.853729
Epoch 1.111: Loss = 0.830902
Epoch 1.112: Loss = 0.736679
Epoch 1.113: Loss = 0.695999
Epoch 1.114: Loss = 0.756104
Epoch 1.115: Loss = 0.715088
Epoch 1.116: Loss = 0.71553
Epoch 1.117: Loss = 0.747253
Epoch 1.118: Loss = 0.795792
Epoch 1.119: Loss = 0.808624
Epoch 1.120: Loss = 0.768188
TRAIN LOSS = 1.08357
TRAIN ACC = 63.829 % (38299/60000)
Loss = 0.709824
Loss = 0.791794
Loss = 0.790649
Loss = 0.734116
Loss = 0.696655
Loss = 0.847244
Loss = 0.902969
Loss = 0.836594
Loss = 0.748108
Loss = 0.732712
Loss = 0.827805
Loss = 0.815475
Loss = 0.78006
Loss = 0.795456
Loss = 0.757706
Loss = 0.807983
Loss = 0.737091
Loss = 0.746445
Loss = 0.816101
Loss = 0.76825
TEST LOSS = 0.782152
TEST ACC = 382.99 % (7230/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.70311
Epoch 2.2: Loss = 0.707825
Epoch 2.3: Loss = 0.725098
Epoch 2.4: Loss = 0.784821
Epoch 2.5: Loss = 0.757553
Epoch 2.6: Loss = 0.828018
Epoch 2.7: Loss = 0.690186
Epoch 2.8: Loss = 0.714813
Epoch 2.9: Loss = 0.72113
Epoch 2.10: Loss = 0.803909
Epoch 2.11: Loss = 0.71431
Epoch 2.12: Loss = 0.76358
Epoch 2.13: Loss = 0.755203
Epoch 2.14: Loss = 0.712692
Epoch 2.15: Loss = 0.729248
Epoch 2.16: Loss = 0.81221
Epoch 2.17: Loss = 0.884613
Epoch 2.18: Loss = 0.778122
Epoch 2.19: Loss = 0.690247
Epoch 2.20: Loss = 0.743774
Epoch 2.21: Loss = 0.720795
Epoch 2.22: Loss = 0.778519
Epoch 2.23: Loss = 0.738113
Epoch 2.24: Loss = 0.718399
Epoch 2.25: Loss = 0.723099
Epoch 2.26: Loss = 0.768524
Epoch 2.27: Loss = 0.706253
Epoch 2.28: Loss = 0.689438
Epoch 2.29: Loss = 0.664185
Epoch 2.30: Loss = 0.820236
Epoch 2.31: Loss = 0.704269
Epoch 2.32: Loss = 0.79805
Epoch 2.33: Loss = 0.778305
Epoch 2.34: Loss = 0.76825
Epoch 2.35: Loss = 0.758118
Epoch 2.36: Loss = 0.745453
Epoch 2.37: Loss = 0.742035
Epoch 2.38: Loss = 0.707077
Epoch 2.39: Loss = 0.6129
Epoch 2.40: Loss = 0.70607
Epoch 2.41: Loss = 0.726624
Epoch 2.42: Loss = 0.726166
Epoch 2.43: Loss = 0.696884
Epoch 2.44: Loss = 0.773376
Epoch 2.45: Loss = 0.752075
Epoch 2.46: Loss = 0.752197
Epoch 2.47: Loss = 0.684967
Epoch 2.48: Loss = 0.78302
Epoch 2.49: Loss = 0.656052
Epoch 2.50: Loss = 0.767944
Epoch 2.51: Loss = 0.752151
Epoch 2.52: Loss = 0.749496
Epoch 2.53: Loss = 0.820175
Epoch 2.54: Loss = 0.776291
Epoch 2.55: Loss = 0.73912
Epoch 2.56: Loss = 0.713867
Epoch 2.57: Loss = 0.689728
Epoch 2.58: Loss = 0.67894
Epoch 2.59: Loss = 0.683121
Epoch 2.60: Loss = 0.725342
Epoch 2.61: Loss = 0.661957
Epoch 2.62: Loss = 0.719528
Epoch 2.63: Loss = 0.767883
Epoch 2.64: Loss = 0.706009
Epoch 2.65: Loss = 0.774033
Epoch 2.66: Loss = 0.654449
Epoch 2.67: Loss = 0.759064
Epoch 2.68: Loss = 0.647415
Epoch 2.69: Loss = 0.71933
Epoch 2.70: Loss = 0.669205
Epoch 2.71: Loss = 0.714325
Epoch 2.72: Loss = 0.597122
Epoch 2.73: Loss = 0.663162
Epoch 2.74: Loss = 0.691254
Epoch 2.75: Loss = 0.674805
Epoch 2.76: Loss = 0.735275
Epoch 2.77: Loss = 0.799545
Epoch 2.78: Loss = 0.693542
Epoch 2.79: Loss = 0.780197
Epoch 2.80: Loss = 0.631866
Epoch 2.81: Loss = 0.690231
Epoch 2.82: Loss = 0.762695
Epoch 2.83: Loss = 0.668686
Epoch 2.84: Loss = 0.78064
Epoch 2.85: Loss = 0.647064
Epoch 2.86: Loss = 0.695236
Epoch 2.87: Loss = 0.712524
Epoch 2.88: Loss = 0.671616
Epoch 2.89: Loss = 0.63031
Epoch 2.90: Loss = 0.688492
Epoch 2.91: Loss = 0.71759
Epoch 2.92: Loss = 0.701569
Epoch 2.93: Loss = 0.669174
Epoch 2.94: Loss = 0.687576
Epoch 2.95: Loss = 0.691879
Epoch 2.96: Loss = 0.710388
Epoch 2.97: Loss = 0.692917
Epoch 2.98: Loss = 0.681931
Epoch 2.99: Loss = 0.764435
Epoch 2.100: Loss = 0.649139
Epoch 2.101: Loss = 0.814392
Epoch 2.102: Loss = 0.684769
Epoch 2.103: Loss = 0.668457
Epoch 2.104: Loss = 0.6875
Epoch 2.105: Loss = 0.668045
Epoch 2.106: Loss = 0.774399
Epoch 2.107: Loss = 0.727585
Epoch 2.108: Loss = 0.700241
Epoch 2.109: Loss = 0.714493
Epoch 2.110: Loss = 0.717026
Epoch 2.111: Loss = 0.569321
Epoch 2.112: Loss = 0.710602
Epoch 2.113: Loss = 0.686691
Epoch 2.114: Loss = 0.748932
Epoch 2.115: Loss = 0.664658
Epoch 2.116: Loss = 0.768723
Epoch 2.117: Loss = 0.733231
Epoch 2.118: Loss = 0.735596
Epoch 2.119: Loss = 0.715836
Epoch 2.120: Loss = 0.617569
TRAIN LOSS = 0.720001
TRAIN ACC = 75.4303 % (45260/60000)
Loss = 0.638
Loss = 0.743683
Loss = 0.698486
Loss = 0.626648
Loss = 0.626816
Loss = 0.783142
Loss = 0.815094
Loss = 0.775711
Loss = 0.687546
Loss = 0.666504
Loss = 0.798126
Loss = 0.753677
Loss = 0.711609
Loss = 0.689819
Loss = 0.691147
Loss = 0.74263
Loss = 0.662704
Loss = 0.705673
Loss = 0.746338
Loss = 0.713165
TEST LOSS = 0.713826
TEST ACC = 452.599 % (7572/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.78833
Epoch 3.2: Loss = 0.784653
Epoch 3.3: Loss = 0.722015
Epoch 3.4: Loss = 0.697052
Epoch 3.5: Loss = 0.747391
Epoch 3.6: Loss = 0.662048
Epoch 3.7: Loss = 0.70784
Epoch 3.8: Loss = 0.708878
Epoch 3.9: Loss = 0.781357
Epoch 3.10: Loss = 0.654877
Epoch 3.11: Loss = 0.69989
Epoch 3.12: Loss = 0.605606
Epoch 3.13: Loss = 0.614517
Epoch 3.14: Loss = 0.637024
Epoch 3.15: Loss = 0.602676
Epoch 3.16: Loss = 0.59494
Epoch 3.17: Loss = 0.697189
Epoch 3.18: Loss = 0.707611
Epoch 3.19: Loss = 0.749237
Epoch 3.20: Loss = 0.745438
Epoch 3.21: Loss = 0.588135
Epoch 3.22: Loss = 0.739197
Epoch 3.23: Loss = 0.700699
Epoch 3.24: Loss = 0.706009
Epoch 3.25: Loss = 0.658478
Epoch 3.26: Loss = 0.670166
Epoch 3.27: Loss = 0.657242
Epoch 3.28: Loss = 0.66777
Epoch 3.29: Loss = 0.68634
Epoch 3.30: Loss = 0.686813
Epoch 3.31: Loss = 0.702789
Epoch 3.32: Loss = 0.694885
Epoch 3.33: Loss = 0.695038
Epoch 3.34: Loss = 0.674316
Epoch 3.35: Loss = 0.618332
Epoch 3.36: Loss = 0.652893
Epoch 3.37: Loss = 0.715347
Epoch 3.38: Loss = 0.713837
Epoch 3.39: Loss = 0.709641
Epoch 3.40: Loss = 0.69339
Epoch 3.41: Loss = 0.652008
Epoch 3.42: Loss = 0.652008
Epoch 3.43: Loss = 0.679077
Epoch 3.44: Loss = 0.683426
Epoch 3.45: Loss = 0.687775
Epoch 3.46: Loss = 0.607269
Epoch 3.47: Loss = 0.600449
Epoch 3.48: Loss = 0.657272
Epoch 3.49: Loss = 0.686768
Epoch 3.50: Loss = 0.743698
Epoch 3.51: Loss = 0.670502
Epoch 3.52: Loss = 0.737442
Epoch 3.53: Loss = 0.659439
Epoch 3.54: Loss = 0.711151
Epoch 3.55: Loss = 0.659134
Epoch 3.56: Loss = 0.714111
Epoch 3.57: Loss = 0.682236
Epoch 3.58: Loss = 0.597473
Epoch 3.59: Loss = 0.739059
Epoch 3.60: Loss = 0.63942
Epoch 3.61: Loss = 0.668198
Epoch 3.62: Loss = 0.573563
Epoch 3.63: Loss = 0.711044
Epoch 3.64: Loss = 0.751907
Epoch 3.65: Loss = 0.834152
Epoch 3.66: Loss = 0.706329
Epoch 3.67: Loss = 0.668289
Epoch 3.68: Loss = 0.593307
Epoch 3.69: Loss = 0.606232
Epoch 3.70: Loss = 0.702377
Epoch 3.71: Loss = 0.564758
Epoch 3.72: Loss = 0.656311
Epoch 3.73: Loss = 0.613037
Epoch 3.74: Loss = 0.632065
Epoch 3.75: Loss = 0.667923
Epoch 3.76: Loss = 0.672729
Epoch 3.77: Loss = 0.576874
Epoch 3.78: Loss = 0.688644
Epoch 3.79: Loss = 0.643387
Epoch 3.80: Loss = 0.651825
Epoch 3.81: Loss = 0.665237
Epoch 3.82: Loss = 0.682755
Epoch 3.83: Loss = 0.595001
Epoch 3.84: Loss = 0.75087
Epoch 3.85: Loss = 0.703186
Epoch 3.86: Loss = 0.627808
Epoch 3.87: Loss = 0.62767
Epoch 3.88: Loss = 0.606155
Epoch 3.89: Loss = 0.616882
Epoch 3.90: Loss = 0.543472
Epoch 3.91: Loss = 0.570724
Epoch 3.92: Loss = 0.710556
Epoch 3.93: Loss = 0.623489
Epoch 3.94: Loss = 0.645691
Epoch 3.95: Loss = 0.637268
Epoch 3.96: Loss = 0.689499
Epoch 3.97: Loss = 0.722565
Epoch 3.98: Loss = 0.652893
Epoch 3.99: Loss = 0.683594
Epoch 3.100: Loss = 0.646637
Epoch 3.101: Loss = 0.713837
Epoch 3.102: Loss = 0.645416
Epoch 3.103: Loss = 0.615555
Epoch 3.104: Loss = 0.727081
Epoch 3.105: Loss = 0.606201
Epoch 3.106: Loss = 0.66011
Epoch 3.107: Loss = 0.831146
Epoch 3.108: Loss = 0.646072
Epoch 3.109: Loss = 0.733795
Epoch 3.110: Loss = 0.671906
Epoch 3.111: Loss = 0.616348
Epoch 3.112: Loss = 0.585358
Epoch 3.113: Loss = 0.58139
Epoch 3.114: Loss = 0.737396
Epoch 3.115: Loss = 0.702728
Epoch 3.116: Loss = 0.695038
Epoch 3.117: Loss = 0.702118
Epoch 3.118: Loss = 0.655518
Epoch 3.119: Loss = 0.80957
Epoch 3.120: Loss = 0.80986
TRAIN LOSS = 0.674438
TRAIN ACC = 77.7023 % (46624/60000)
Loss = 0.600693
Loss = 0.704086
Loss = 0.665787
Loss = 0.58754
Loss = 0.60994
Loss = 0.756989
Loss = 0.804749
Loss = 0.744858
Loss = 0.662994
Loss = 0.629196
Loss = 0.792236
Loss = 0.747131
Loss = 0.706223
Loss = 0.680298
Loss = 0.664825
Loss = 0.71228
Loss = 0.648239
Loss = 0.707779
Loss = 0.743591
Loss = 0.684799
TEST LOSS = 0.692711
TEST ACC = 466.24 % (7714/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.696899
Epoch 4.2: Loss = 0.636047
Epoch 4.3: Loss = 0.618286
Epoch 4.4: Loss = 0.665497
Epoch 4.5: Loss = 0.638474
Epoch 4.6: Loss = 0.582535
Epoch 4.7: Loss = 0.622452
Epoch 4.8: Loss = 0.671265
Epoch 4.9: Loss = 0.811218
Epoch 4.10: Loss = 0.658875
Epoch 4.11: Loss = 0.634537
Epoch 4.12: Loss = 0.634079
Epoch 4.13: Loss = 0.564575
Epoch 4.14: Loss = 0.611252
Epoch 4.15: Loss = 0.614731
Epoch 4.16: Loss = 0.692642
Epoch 4.17: Loss = 0.63533
Epoch 4.18: Loss = 0.661087
Epoch 4.19: Loss = 0.648651
Epoch 4.20: Loss = 0.797302
Epoch 4.21: Loss = 0.664032
Epoch 4.22: Loss = 0.606842
Epoch 4.23: Loss = 0.688858
Epoch 4.24: Loss = 0.5905
Epoch 4.25: Loss = 0.637299
Epoch 4.26: Loss = 0.569183
Epoch 4.27: Loss = 0.674438
Epoch 4.28: Loss = 0.733566
Epoch 4.29: Loss = 0.768646
Epoch 4.30: Loss = 0.698029
Epoch 4.31: Loss = 0.686356
Epoch 4.32: Loss = 0.688431
Epoch 4.33: Loss = 0.670135
Epoch 4.34: Loss = 0.625854
Epoch 4.35: Loss = 0.664917
Epoch 4.36: Loss = 0.631027
Epoch 4.37: Loss = 0.540558
Epoch 4.38: Loss = 0.632492
Epoch 4.39: Loss = 0.674088
Epoch 4.40: Loss = 0.636932
Epoch 4.41: Loss = 0.616837
Epoch 4.42: Loss = 0.695175
Epoch 4.43: Loss = 0.600235
Epoch 4.44: Loss = 0.621643
Epoch 4.45: Loss = 0.722214
Epoch 4.46: Loss = 0.638885
Epoch 4.47: Loss = 0.668655
Epoch 4.48: Loss = 0.652405
Epoch 4.49: Loss = 0.607132
Epoch 4.50: Loss = 0.601166
Epoch 4.51: Loss = 0.613022
Epoch 4.52: Loss = 0.706238
Epoch 4.53: Loss = 0.704834
Epoch 4.54: Loss = 0.701401
Epoch 4.55: Loss = 0.56871
Epoch 4.56: Loss = 0.695404
Epoch 4.57: Loss = 0.684738
Epoch 4.58: Loss = 0.675629
Epoch 4.59: Loss = 0.606995
Epoch 4.60: Loss = 0.616455
Epoch 4.61: Loss = 0.680664
Epoch 4.62: Loss = 0.728134
Epoch 4.63: Loss = 0.643738
Epoch 4.64: Loss = 0.74733
Epoch 4.65: Loss = 0.627258
Epoch 4.66: Loss = 0.678619
Epoch 4.67: Loss = 0.664368
Epoch 4.68: Loss = 0.571732
Epoch 4.69: Loss = 0.579193
Epoch 4.70: Loss = 0.589478
Epoch 4.71: Loss = 0.721436
Epoch 4.72: Loss = 0.636581
Epoch 4.73: Loss = 0.73143
Epoch 4.74: Loss = 0.73317
Epoch 4.75: Loss = 0.64447
Epoch 4.76: Loss = 0.62085
Epoch 4.77: Loss = 0.68338
Epoch 4.78: Loss = 0.606247
Epoch 4.79: Loss = 0.606857
Epoch 4.80: Loss = 0.663147
Epoch 4.81: Loss = 0.562943
Epoch 4.82: Loss = 0.724503
Epoch 4.83: Loss = 0.694016
Epoch 4.84: Loss = 0.669479
Epoch 4.85: Loss = 0.602371
Epoch 4.86: Loss = 0.607483
Epoch 4.87: Loss = 0.648605
Epoch 4.88: Loss = 0.555161
Epoch 4.89: Loss = 0.699539
Epoch 4.90: Loss = 0.706772
Epoch 4.91: Loss = 0.783661
Epoch 4.92: Loss = 0.607178
Epoch 4.93: Loss = 0.561356
Epoch 4.94: Loss = 0.647842
Epoch 4.95: Loss = 0.749954
Epoch 4.96: Loss = 0.722229
Epoch 4.97: Loss = 0.734085
Epoch 4.98: Loss = 0.715958
Epoch 4.99: Loss = 0.669815
Epoch 4.100: Loss = 0.659912
Epoch 4.101: Loss = 0.672958
Epoch 4.102: Loss = 0.723816
Epoch 4.103: Loss = 0.492142
Epoch 4.104: Loss = 0.670609
Epoch 4.105: Loss = 0.618027
Epoch 4.106: Loss = 0.5905
Epoch 4.107: Loss = 0.633057
Epoch 4.108: Loss = 0.685684
Epoch 4.109: Loss = 0.607849
Epoch 4.110: Loss = 0.614288
Epoch 4.111: Loss = 0.622192
Epoch 4.112: Loss = 0.693802
Epoch 4.113: Loss = 0.706055
Epoch 4.114: Loss = 0.583038
Epoch 4.115: Loss = 0.676666
Epoch 4.116: Loss = 0.760651
Epoch 4.117: Loss = 0.674515
Epoch 4.118: Loss = 0.652893
Epoch 4.119: Loss = 0.634735
Epoch 4.120: Loss = 0.610123
TRAIN LOSS = 0.654846
TRAIN ACC = 78.7277 % (47239/60000)
Loss = 0.585571
Loss = 0.703369
Loss = 0.625809
Loss = 0.569122
Loss = 0.599319
Loss = 0.757568
Loss = 0.782837
Loss = 0.732117
Loss = 0.637741
Loss = 0.593796
Loss = 0.77858
Loss = 0.745804
Loss = 0.68425
Loss = 0.664963
Loss = 0.659012
Loss = 0.706039
Loss = 0.644821
Loss = 0.682053
Loss = 0.715286
Loss = 0.673096
TEST LOSS = 0.677057
TEST ACC = 472.389 % (7829/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.693008
Epoch 5.2: Loss = 0.682434
Epoch 5.3: Loss = 0.701752
Epoch 5.4: Loss = 0.627228
Epoch 5.5: Loss = 0.610977
Epoch 5.6: Loss = 0.653198
Epoch 5.7: Loss = 0.606644
Epoch 5.8: Loss = 0.648499
Epoch 5.9: Loss = 0.708282
Epoch 5.10: Loss = 0.643341
Epoch 5.11: Loss = 0.647461
Epoch 5.12: Loss = 0.622452
Epoch 5.13: Loss = 0.700348
Epoch 5.14: Loss = 0.595596
Epoch 5.15: Loss = 0.699356
Epoch 5.16: Loss = 0.670837
Epoch 5.17: Loss = 0.636642
Epoch 5.18: Loss = 0.636627
Epoch 5.19: Loss = 0.642456
Epoch 5.20: Loss = 0.68605
Epoch 5.21: Loss = 0.613495
Epoch 5.22: Loss = 0.578156
Epoch 5.23: Loss = 0.669144
Epoch 5.24: Loss = 0.608994
Epoch 5.25: Loss = 0.681259
Epoch 5.26: Loss = 0.626236
Epoch 5.27: Loss = 0.680893
Epoch 5.28: Loss = 0.630035
Epoch 5.29: Loss = 0.691162
Epoch 5.30: Loss = 0.609177
Epoch 5.31: Loss = 0.710281
Epoch 5.32: Loss = 0.608292
Epoch 5.33: Loss = 0.695862
Epoch 5.34: Loss = 0.698669
Epoch 5.35: Loss = 0.590439
Epoch 5.36: Loss = 0.676712
Epoch 5.37: Loss = 0.614731
Epoch 5.38: Loss = 0.620087
Epoch 5.39: Loss = 0.664902
Epoch 5.40: Loss = 0.608826
Epoch 5.41: Loss = 0.629623
Epoch 5.42: Loss = 0.697662
Epoch 5.43: Loss = 0.642609
Epoch 5.44: Loss = 0.634964
Epoch 5.45: Loss = 0.633942
Epoch 5.46: Loss = 0.637115
Epoch 5.47: Loss = 0.579895
Epoch 5.48: Loss = 0.534988
Epoch 5.49: Loss = 0.708603
Epoch 5.50: Loss = 0.582687
Epoch 5.51: Loss = 0.643158
Epoch 5.52: Loss = 0.650345
Epoch 5.53: Loss = 0.679031
Epoch 5.54: Loss = 0.597626
Epoch 5.55: Loss = 0.634354
Epoch 5.56: Loss = 0.652756
Epoch 5.57: Loss = 0.650574
Epoch 5.58: Loss = 0.652374
Epoch 5.59: Loss = 0.613586
Epoch 5.60: Loss = 0.682938
Epoch 5.61: Loss = 0.605011
Epoch 5.62: Loss = 0.715103
Epoch 5.63: Loss = 0.65509
Epoch 5.64: Loss = 0.692276
Epoch 5.65: Loss = 0.698029
Epoch 5.66: Loss = 0.639435
Epoch 5.67: Loss = 0.66098
Epoch 5.68: Loss = 0.66069
Epoch 5.69: Loss = 0.638092
Epoch 5.70: Loss = 0.623077
Epoch 5.71: Loss = 0.697861
Epoch 5.72: Loss = 0.53215
Epoch 5.73: Loss = 0.680222
Epoch 5.74: Loss = 0.757828
Epoch 5.75: Loss = 0.588486
Epoch 5.76: Loss = 0.621857
Epoch 5.77: Loss = 0.64473
Epoch 5.78: Loss = 0.672729
Epoch 5.79: Loss = 0.700699
Epoch 5.80: Loss = 0.674332
Epoch 5.81: Loss = 0.651596
Epoch 5.82: Loss = 0.598633
Epoch 5.83: Loss = 0.730103
Epoch 5.84: Loss = 0.669724
Epoch 5.85: Loss = 0.689407
Epoch 5.86: Loss = 0.676483
Epoch 5.87: Loss = 0.649185
Epoch 5.88: Loss = 0.699387
Epoch 5.89: Loss = 0.661072
Epoch 5.90: Loss = 0.541473
Epoch 5.91: Loss = 0.661163
Epoch 5.92: Loss = 0.720581
Epoch 5.93: Loss = 0.624466
Epoch 5.94: Loss = 0.521912
Epoch 5.95: Loss = 0.599625
Epoch 5.96: Loss = 0.63652
Epoch 5.97: Loss = 0.659241
Epoch 5.98: Loss = 0.644562
Epoch 5.99: Loss = 0.655609
Epoch 5.100: Loss = 0.791672
Epoch 5.101: Loss = 0.722504
Epoch 5.102: Loss = 0.672882
Epoch 5.103: Loss = 0.566025
Epoch 5.104: Loss = 0.687668
Epoch 5.105: Loss = 0.689804
Epoch 5.106: Loss = 0.599518
Epoch 5.107: Loss = 0.711777
Epoch 5.108: Loss = 0.530045
Epoch 5.109: Loss = 0.576431
Epoch 5.110: Loss = 0.714981
Epoch 5.111: Loss = 0.637314
Epoch 5.112: Loss = 0.744583
Epoch 5.113: Loss = 0.702301
Epoch 5.114: Loss = 0.663528
Epoch 5.115: Loss = 0.635452
Epoch 5.116: Loss = 0.693527
Epoch 5.117: Loss = 0.655945
Epoch 5.118: Loss = 0.57518
Epoch 5.119: Loss = 0.617996
Epoch 5.120: Loss = 0.704651
TRAIN LOSS = 0.65062
TRAIN ACC = 79.3182 % (47593/60000)
Loss = 0.572922
Loss = 0.697235
Loss = 0.652832
Loss = 0.573593
Loss = 0.617554
Loss = 0.760529
Loss = 0.813004
Loss = 0.730652
Loss = 0.633179
Loss = 0.58139
Loss = 0.788834
Loss = 0.750717
Loss = 0.692459
Loss = 0.67186
Loss = 0.65126
Loss = 0.695282
Loss = 0.662781
Loss = 0.677292
Loss = 0.713974
Loss = 0.680206
TEST LOSS = 0.680877
TEST ACC = 475.929 % (7852/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.719101
Epoch 6.2: Loss = 0.691849
Epoch 6.3: Loss = 0.701431
Epoch 6.4: Loss = 0.591385
Epoch 6.5: Loss = 0.758499
Epoch 6.6: Loss = 0.584
Epoch 6.7: Loss = 0.711609
Epoch 6.8: Loss = 0.707901
Epoch 6.9: Loss = 0.640854
Epoch 6.10: Loss = 0.608917
Epoch 6.11: Loss = 0.5784
Epoch 6.12: Loss = 0.666672
Epoch 6.13: Loss = 0.632202
Epoch 6.14: Loss = 0.680099
Epoch 6.15: Loss = 0.654266
Epoch 6.16: Loss = 0.734482
Epoch 6.17: Loss = 0.653351
Epoch 6.18: Loss = 0.557693
Epoch 6.19: Loss = 0.634155
Epoch 6.20: Loss = 0.651855
Epoch 6.21: Loss = 0.653839
Epoch 6.22: Loss = 0.7621
Epoch 6.23: Loss = 0.674057
Epoch 6.24: Loss = 0.655258
Epoch 6.25: Loss = 0.693237
Epoch 6.26: Loss = 0.62764
Epoch 6.27: Loss = 0.60347
Epoch 6.28: Loss = 0.767609
Epoch 6.29: Loss = 0.642578
Epoch 6.30: Loss = 0.723526
Epoch 6.31: Loss = 0.529312
Epoch 6.32: Loss = 0.677536
Epoch 6.33: Loss = 0.657043
Epoch 6.34: Loss = 0.663651
Epoch 6.35: Loss = 0.666122
Epoch 6.36: Loss = 0.624878
Epoch 6.37: Loss = 0.698151
Epoch 6.38: Loss = 0.59198
Epoch 6.39: Loss = 0.564896
Epoch 6.40: Loss = 0.607834
Epoch 6.41: Loss = 0.589645
Epoch 6.42: Loss = 0.637451
Epoch 6.43: Loss = 0.719299
Epoch 6.44: Loss = 0.576889
Epoch 6.45: Loss = 0.573456
Epoch 6.46: Loss = 0.534103
Epoch 6.47: Loss = 0.717285
Epoch 6.48: Loss = 0.598587
Epoch 6.49: Loss = 0.7211
Epoch 6.50: Loss = 0.608307
Epoch 6.51: Loss = 0.69017
Epoch 6.52: Loss = 0.695145
Epoch 6.53: Loss = 0.597855
Epoch 6.54: Loss = 0.637497
Epoch 6.55: Loss = 0.76561
Epoch 6.56: Loss = 0.619949
Epoch 6.57: Loss = 0.641449
Epoch 6.58: Loss = 0.748199
Epoch 6.59: Loss = 0.672424
Epoch 6.60: Loss = 0.684265
Epoch 6.61: Loss = 0.549484
Epoch 6.62: Loss = 0.66275
Epoch 6.63: Loss = 0.595169
Epoch 6.64: Loss = 0.614883
Epoch 6.65: Loss = 0.686935
Epoch 6.66: Loss = 0.675644
Epoch 6.67: Loss = 0.567062
Epoch 6.68: Loss = 0.553238
Epoch 6.69: Loss = 0.507828
Epoch 6.70: Loss = 0.626144
Epoch 6.71: Loss = 0.658096
Epoch 6.72: Loss = 0.641724
Epoch 6.73: Loss = 0.553879
Epoch 6.74: Loss = 0.621262
Epoch 6.75: Loss = 0.591385
Epoch 6.76: Loss = 0.644272
Epoch 6.77: Loss = 0.700607
Epoch 6.78: Loss = 0.70256
Epoch 6.79: Loss = 0.576416
Epoch 6.80: Loss = 0.573715
Epoch 6.81: Loss = 0.628586
Epoch 6.82: Loss = 0.701706
Epoch 6.83: Loss = 0.595078
Epoch 6.84: Loss = 0.640274
Epoch 6.85: Loss = 0.613022
Epoch 6.86: Loss = 0.663086
Epoch 6.87: Loss = 0.64801
Epoch 6.88: Loss = 0.628845
Epoch 6.89: Loss = 0.634842
Epoch 6.90: Loss = 0.596512
Epoch 6.91: Loss = 0.760971
Epoch 6.92: Loss = 0.673218
Epoch 6.93: Loss = 0.713898
Epoch 6.94: Loss = 0.616409
Epoch 6.95: Loss = 0.531219
Epoch 6.96: Loss = 0.602798
Epoch 6.97: Loss = 0.773422
Epoch 6.98: Loss = 0.668198
Epoch 6.99: Loss = 0.693832
Epoch 6.100: Loss = 0.769104
Epoch 6.101: Loss = 0.582855
Epoch 6.102: Loss = 0.65564
Epoch 6.103: Loss = 0.654968
Epoch 6.104: Loss = 0.620285
Epoch 6.105: Loss = 0.648117
Epoch 6.106: Loss = 0.665466
Epoch 6.107: Loss = 0.663879
Epoch 6.108: Loss = 0.604034
Epoch 6.109: Loss = 0.797638
Epoch 6.110: Loss = 0.612976
Epoch 6.111: Loss = 0.550369
Epoch 6.112: Loss = 0.636581
Epoch 6.113: Loss = 0.646332
Epoch 6.114: Loss = 0.570358
Epoch 6.115: Loss = 0.693039
Epoch 6.116: Loss = 0.647156
Epoch 6.117: Loss = 0.612259
Epoch 6.118: Loss = 0.643463
Epoch 6.119: Loss = 0.669983
Epoch 6.120: Loss = 0.613907
TRAIN LOSS = 0.645996
TRAIN ACC = 79.8645 % (47921/60000)
Loss = 0.55687
Loss = 0.703339
Loss = 0.643295
Loss = 0.558075
Loss = 0.61615
Loss = 0.753693
Loss = 0.825867
Loss = 0.734772
Loss = 0.634323
Loss = 0.592453
Loss = 0.80748
Loss = 0.745544
Loss = 0.691452
Loss = 0.682983
Loss = 0.650818
Loss = 0.686279
Loss = 0.667633
Loss = 0.691574
Loss = 0.709579
Loss = 0.684097
TEST LOSS = 0.681814
TEST ACC = 479.21 % (7932/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.642624
Epoch 7.2: Loss = 0.576157
Epoch 7.3: Loss = 0.494751
Epoch 7.4: Loss = 0.778305
Epoch 7.5: Loss = 0.664642
Epoch 7.6: Loss = 0.500641
Epoch 7.7: Loss = 0.638062
Epoch 7.8: Loss = 0.622437
Epoch 7.9: Loss = 0.654282
Epoch 7.10: Loss = 0.707947
Epoch 7.11: Loss = 0.750656
Epoch 7.12: Loss = 0.645203
Epoch 7.13: Loss = 0.640015
Epoch 7.14: Loss = 0.789551
Epoch 7.15: Loss = 0.691208
Epoch 7.16: Loss = 0.63385
Epoch 7.17: Loss = 0.607681
Epoch 7.18: Loss = 0.64386
Epoch 7.19: Loss = 0.723923
Epoch 7.20: Loss = 0.626389
Epoch 7.21: Loss = 0.657471
Epoch 7.22: Loss = 0.557892
Epoch 7.23: Loss = 0.561417
Epoch 7.24: Loss = 0.600082
Epoch 7.25: Loss = 0.548264
Epoch 7.26: Loss = 0.547409
Epoch 7.27: Loss = 0.606812
Epoch 7.28: Loss = 0.653671
Epoch 7.29: Loss = 0.630127
Epoch 7.30: Loss = 0.773392
Epoch 7.31: Loss = 0.579971
Epoch 7.32: Loss = 0.707169
Epoch 7.33: Loss = 0.547165
Epoch 7.34: Loss = 0.574615
Epoch 7.35: Loss = 0.58078
Epoch 7.36: Loss = 0.638962
Epoch 7.37: Loss = 0.619202
Epoch 7.38: Loss = 0.565201
Epoch 7.39: Loss = 0.681519
Epoch 7.40: Loss = 0.545563
Epoch 7.41: Loss = 0.633484
Epoch 7.42: Loss = 0.548538
Epoch 7.43: Loss = 0.505066
Epoch 7.44: Loss = 0.706543
Epoch 7.45: Loss = 0.732071
Epoch 7.46: Loss = 0.722412
Epoch 7.47: Loss = 0.69725
Epoch 7.48: Loss = 0.625229
Epoch 7.49: Loss = 0.78891
Epoch 7.50: Loss = 0.673798
Epoch 7.51: Loss = 0.680984
Epoch 7.52: Loss = 0.665878
Epoch 7.53: Loss = 0.733154
Epoch 7.54: Loss = 0.664764
Epoch 7.55: Loss = 0.642441
Epoch 7.56: Loss = 0.631989
Epoch 7.57: Loss = 0.649963
Epoch 7.58: Loss = 0.740112
Epoch 7.59: Loss = 0.694199
Epoch 7.60: Loss = 0.598267
Epoch 7.61: Loss = 0.6539
Epoch 7.62: Loss = 0.675827
Epoch 7.63: Loss = 0.70578
Epoch 7.64: Loss = 0.609146
Epoch 7.65: Loss = 0.67984
Epoch 7.66: Loss = 0.521362
Epoch 7.67: Loss = 0.631851
Epoch 7.68: Loss = 0.60611
Epoch 7.69: Loss = 0.617233
Epoch 7.70: Loss = 0.702896
Epoch 7.71: Loss = 0.690063
Epoch 7.72: Loss = 0.679962
Epoch 7.73: Loss = 0.651901
Epoch 7.74: Loss = 0.583633
Epoch 7.75: Loss = 0.524673
Epoch 7.76: Loss = 0.638504
Epoch 7.77: Loss = 0.619095
Epoch 7.78: Loss = 0.631668
Epoch 7.79: Loss = 0.560516
Epoch 7.80: Loss = 0.604614
Epoch 7.81: Loss = 0.712677
Epoch 7.82: Loss = 0.723923
Epoch 7.83: Loss = 0.707596
Epoch 7.84: Loss = 0.673279
Epoch 7.85: Loss = 0.602661
Epoch 7.86: Loss = 0.682266
Epoch 7.87: Loss = 0.655563
Epoch 7.88: Loss = 0.71254
Epoch 7.89: Loss = 0.603424
Epoch 7.90: Loss = 0.58522
Epoch 7.91: Loss = 0.728729
Epoch 7.92: Loss = 0.732529
Epoch 7.93: Loss = 0.603226
Epoch 7.94: Loss = 0.596909
Epoch 7.95: Loss = 0.53212
Epoch 7.96: Loss = 0.520737
Epoch 7.97: Loss = 0.645996
Epoch 7.98: Loss = 0.689697
Epoch 7.99: Loss = 0.620392
Epoch 7.100: Loss = 0.697739
Epoch 7.101: Loss = 0.530746
Epoch 7.102: Loss = 0.736542
Epoch 7.103: Loss = 0.655441
Epoch 7.104: Loss = 0.664413
Epoch 7.105: Loss = 0.713593
Epoch 7.106: Loss = 0.566101
Epoch 7.107: Loss = 0.584183
Epoch 7.108: Loss = 0.649719
Epoch 7.109: Loss = 0.523865
Epoch 7.110: Loss = 0.743927
Epoch 7.111: Loss = 0.676071
Epoch 7.112: Loss = 0.554352
Epoch 7.113: Loss = 0.638168
Epoch 7.114: Loss = 0.599319
Epoch 7.115: Loss = 0.648743
Epoch 7.116: Loss = 0.70549
Epoch 7.117: Loss = 0.569504
Epoch 7.118: Loss = 0.73912
Epoch 7.119: Loss = 0.63916
Epoch 7.120: Loss = 0.567123
TRAIN LOSS = 0.640549
TRAIN ACC = 80.2979 % (48181/60000)
Loss = 0.552612
Loss = 0.704895
Loss = 0.634613
Loss = 0.554428
Loss = 0.605667
Loss = 0.750336
Loss = 0.808136
Loss = 0.721481
Loss = 0.645615
Loss = 0.582428
Loss = 0.822098
Loss = 0.765625
Loss = 0.691879
Loss = 0.676956
Loss = 0.663651
Loss = 0.680496
Loss = 0.638763
Loss = 0.693039
Loss = 0.71286
Loss = 0.665787
TEST LOSS = 0.678568
TEST ACC = 481.808 % (7957/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.714127
Epoch 8.2: Loss = 0.504211
Epoch 8.3: Loss = 0.71962
Epoch 8.4: Loss = 0.621521
Epoch 8.5: Loss = 0.65773
Epoch 8.6: Loss = 0.637833
Epoch 8.7: Loss = 0.537582
Epoch 8.8: Loss = 0.604294
Epoch 8.9: Loss = 0.733948
Epoch 8.10: Loss = 0.77417
Epoch 8.11: Loss = 0.665161
Epoch 8.12: Loss = 0.660019
Epoch 8.13: Loss = 0.826233
Epoch 8.14: Loss = 0.556107
Epoch 8.15: Loss = 0.645889
Epoch 8.16: Loss = 0.650131
Epoch 8.17: Loss = 0.592194
Epoch 8.18: Loss = 0.572372
Epoch 8.19: Loss = 0.625977
Epoch 8.20: Loss = 0.659683
Epoch 8.21: Loss = 0.571426
Epoch 8.22: Loss = 0.659592
Epoch 8.23: Loss = 0.622742
Epoch 8.24: Loss = 0.663391
Epoch 8.25: Loss = 0.733078
Epoch 8.26: Loss = 0.644028
Epoch 8.27: Loss = 0.646317
Epoch 8.28: Loss = 0.627823
Epoch 8.29: Loss = 0.620483
Epoch 8.30: Loss = 0.649796
Epoch 8.31: Loss = 0.603851
Epoch 8.32: Loss = 0.624634
Epoch 8.33: Loss = 0.63974
Epoch 8.34: Loss = 0.657562
Epoch 8.35: Loss = 0.603119
Epoch 8.36: Loss = 0.661423
Epoch 8.37: Loss = 0.708267
Epoch 8.38: Loss = 0.587296
Epoch 8.39: Loss = 0.581482
Epoch 8.40: Loss = 0.738068
Epoch 8.41: Loss = 0.608902
Epoch 8.42: Loss = 0.678986
Epoch 8.43: Loss = 0.581161
Epoch 8.44: Loss = 0.624207
Epoch 8.45: Loss = 0.731415
Epoch 8.46: Loss = 0.506638
Epoch 8.47: Loss = 0.574829
Epoch 8.48: Loss = 0.535187
Epoch 8.49: Loss = 0.487305
Epoch 8.50: Loss = 0.677338
Epoch 8.51: Loss = 0.754059
Epoch 8.52: Loss = 0.605011
Epoch 8.53: Loss = 0.56929
Epoch 8.54: Loss = 0.560349
Epoch 8.55: Loss = 0.713715
Epoch 8.56: Loss = 0.641876
Epoch 8.57: Loss = 0.611374
Epoch 8.58: Loss = 0.63884
Epoch 8.59: Loss = 0.674011
Epoch 8.60: Loss = 0.658035
Epoch 8.61: Loss = 0.583023
Epoch 8.62: Loss = 0.619797
Epoch 8.63: Loss = 0.684372
Epoch 8.64: Loss = 0.553253
Epoch 8.65: Loss = 0.6642
Epoch 8.66: Loss = 0.714493
Epoch 8.67: Loss = 0.657578
Epoch 8.68: Loss = 0.575699
Epoch 8.69: Loss = 0.63382
Epoch 8.70: Loss = 0.591476
Epoch 8.71: Loss = 0.710587
Epoch 8.72: Loss = 0.656311
Epoch 8.73: Loss = 0.592484
Epoch 8.74: Loss = 0.600113
Epoch 8.75: Loss = 0.603775
Epoch 8.76: Loss = 0.69194
Epoch 8.77: Loss = 0.657227
Epoch 8.78: Loss = 0.632416
Epoch 8.79: Loss = 0.767212
Epoch 8.80: Loss = 0.669312
Epoch 8.81: Loss = 0.716278
Epoch 8.82: Loss = 0.681046
Epoch 8.83: Loss = 0.633987
Epoch 8.84: Loss = 0.617218
Epoch 8.85: Loss = 0.6138
Epoch 8.86: Loss = 0.671143
Epoch 8.87: Loss = 0.714737
Epoch 8.88: Loss = 0.560242
Epoch 8.89: Loss = 0.66095
Epoch 8.90: Loss = 0.699326
Epoch 8.91: Loss = 0.628784
Epoch 8.92: Loss = 0.666733
Epoch 8.93: Loss = 0.660141
Epoch 8.94: Loss = 0.618881
Epoch 8.95: Loss = 0.697678
Epoch 8.96: Loss = 0.613663
Epoch 8.97: Loss = 0.621643
Epoch 8.98: Loss = 0.557831
Epoch 8.99: Loss = 0.601791
Epoch 8.100: Loss = 0.634674
Epoch 8.101: Loss = 0.523804
Epoch 8.102: Loss = 0.693192
Epoch 8.103: Loss = 0.719391
Epoch 8.104: Loss = 0.563004
Epoch 8.105: Loss = 0.572296
Epoch 8.106: Loss = 0.542404
Epoch 8.107: Loss = 0.568802
Epoch 8.108: Loss = 0.680969
Epoch 8.109: Loss = 0.578293
Epoch 8.110: Loss = 0.6147
Epoch 8.111: Loss = 0.591156
Epoch 8.112: Loss = 0.620041
Epoch 8.113: Loss = 0.614716
Epoch 8.114: Loss = 0.600906
Epoch 8.115: Loss = 0.697495
Epoch 8.116: Loss = 0.584732
Epoch 8.117: Loss = 0.724121
Epoch 8.118: Loss = 0.599075
Epoch 8.119: Loss = 0.680038
Epoch 8.120: Loss = 0.613876
TRAIN LOSS = 0.636246
TRAIN ACC = 80.5786 % (48350/60000)
Loss = 0.544312
Loss = 0.683487
Loss = 0.622498
Loss = 0.541397
Loss = 0.597168
Loss = 0.736649
Loss = 0.835373
Loss = 0.700256
Loss = 0.641098
Loss = 0.570633
Loss = 0.835587
Loss = 0.750122
Loss = 0.700104
Loss = 0.694656
Loss = 0.656952
Loss = 0.654907
Loss = 0.639099
Loss = 0.660645
Loss = 0.68364
Loss = 0.66777
TEST LOSS = 0.670817
TEST ACC = 483.499 % (7977/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.587173
Epoch 9.2: Loss = 0.657257
Epoch 9.3: Loss = 0.623947
Epoch 9.4: Loss = 0.631851
Epoch 9.5: Loss = 0.645935
Epoch 9.6: Loss = 0.644989
Epoch 9.7: Loss = 0.626907
Epoch 9.8: Loss = 0.577667
Epoch 9.9: Loss = 0.563339
Epoch 9.10: Loss = 0.575378
Epoch 9.11: Loss = 0.632507
Epoch 9.12: Loss = 0.608292
Epoch 9.13: Loss = 0.671219
Epoch 9.14: Loss = 0.640396
Epoch 9.15: Loss = 0.714569
Epoch 9.16: Loss = 0.618332
Epoch 9.17: Loss = 0.671875
Epoch 9.18: Loss = 0.658936
Epoch 9.19: Loss = 0.639771
Epoch 9.20: Loss = 0.662689
Epoch 9.21: Loss = 0.730865
Epoch 9.22: Loss = 0.647614
Epoch 9.23: Loss = 0.569092
Epoch 9.24: Loss = 0.704178
Epoch 9.25: Loss = 0.556442
Epoch 9.26: Loss = 0.704193
Epoch 9.27: Loss = 0.627304
Epoch 9.28: Loss = 0.588348
Epoch 9.29: Loss = 0.673813
Epoch 9.30: Loss = 0.594589
Epoch 9.31: Loss = 0.576859
Epoch 9.32: Loss = 0.591217
Epoch 9.33: Loss = 0.59581
Epoch 9.34: Loss = 0.664215
Epoch 9.35: Loss = 0.591476
Epoch 9.36: Loss = 0.670471
Epoch 9.37: Loss = 0.621262
Epoch 9.38: Loss = 0.708405
Epoch 9.39: Loss = 0.656555
Epoch 9.40: Loss = 0.613358
Epoch 9.41: Loss = 0.618942
Epoch 9.42: Loss = 0.669937
Epoch 9.43: Loss = 0.620575
Epoch 9.44: Loss = 0.725601
Epoch 9.45: Loss = 0.586151
Epoch 9.46: Loss = 0.740341
Epoch 9.47: Loss = 0.564209
Epoch 9.48: Loss = 0.648972
Epoch 9.49: Loss = 0.662216
Epoch 9.50: Loss = 0.538544
Epoch 9.51: Loss = 0.765121
Epoch 9.52: Loss = 0.716766
Epoch 9.53: Loss = 0.646149
Epoch 9.54: Loss = 0.571991
Epoch 9.55: Loss = 0.549881
Epoch 9.56: Loss = 0.591187
Epoch 9.57: Loss = 0.704132
Epoch 9.58: Loss = 0.704895
Epoch 9.59: Loss = 0.66983
Epoch 9.60: Loss = 0.61557
Epoch 9.61: Loss = 0.660507
Epoch 9.62: Loss = 0.515823
Epoch 9.63: Loss = 0.522659
Epoch 9.64: Loss = 0.605316
Epoch 9.65: Loss = 0.58728
Epoch 9.66: Loss = 0.67067
Epoch 9.67: Loss = 0.779709
Epoch 9.68: Loss = 0.661514
Epoch 9.69: Loss = 0.749771
Epoch 9.70: Loss = 0.616806
Epoch 9.71: Loss = 0.656235
Epoch 9.72: Loss = 0.644073
Epoch 9.73: Loss = 0.665726
Epoch 9.74: Loss = 0.621246
Epoch 9.75: Loss = 0.635452
Epoch 9.76: Loss = 0.619461
Epoch 9.77: Loss = 0.629242
Epoch 9.78: Loss = 0.587723
Epoch 9.79: Loss = 0.645599
Epoch 9.80: Loss = 0.606339
Epoch 9.81: Loss = 0.653702
Epoch 9.82: Loss = 0.613602
Epoch 9.83: Loss = 0.652542
Epoch 9.84: Loss = 0.695953
Epoch 9.85: Loss = 0.669586
Epoch 9.86: Loss = 0.601379
Epoch 9.87: Loss = 0.687088
Epoch 9.88: Loss = 0.587952
Epoch 9.89: Loss = 0.761642
Epoch 9.90: Loss = 0.614853
Epoch 9.91: Loss = 0.541153
Epoch 9.92: Loss = 0.617538
Epoch 9.93: Loss = 0.689713
Epoch 9.94: Loss = 0.721802
Epoch 9.95: Loss = 0.693573
Epoch 9.96: Loss = 0.578827
Epoch 9.97: Loss = 0.678741
Epoch 9.98: Loss = 0.762817
Epoch 9.99: Loss = 0.579834
Epoch 9.100: Loss = 0.541809
Epoch 9.101: Loss = 0.701569
Epoch 9.102: Loss = 0.671051
Epoch 9.103: Loss = 0.616135
Epoch 9.104: Loss = 0.574219
Epoch 9.105: Loss = 0.651932
Epoch 9.106: Loss = 0.655701
Epoch 9.107: Loss = 0.596649
Epoch 9.108: Loss = 0.704208
Epoch 9.109: Loss = 0.668259
Epoch 9.110: Loss = 0.578354
Epoch 9.111: Loss = 0.671295
Epoch 9.112: Loss = 0.621368
Epoch 9.113: Loss = 0.864716
Epoch 9.114: Loss = 0.486313
Epoch 9.115: Loss = 0.644577
Epoch 9.116: Loss = 0.614899
Epoch 9.117: Loss = 0.711258
Epoch 9.118: Loss = 0.532211
Epoch 9.119: Loss = 0.681793
Epoch 9.120: Loss = 0.641052
TRAIN LOSS = 0.639709
TRAIN ACC = 80.7236 % (48436/60000)
Loss = 0.545074
Loss = 0.677094
Loss = 0.625229
Loss = 0.563461
Loss = 0.619293
Loss = 0.741486
Loss = 0.838638
Loss = 0.703674
Loss = 0.647766
Loss = 0.573822
Loss = 0.834503
Loss = 0.753021
Loss = 0.687897
Loss = 0.677368
Loss = 0.67627
Loss = 0.675751
Loss = 0.650513
Loss = 0.660049
Loss = 0.696838
Loss = 0.676895
TEST LOSS = 0.676232
TEST ACC = 484.36 % (7975/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.633087
Epoch 10.2: Loss = 0.611038
Epoch 10.3: Loss = 0.730194
Epoch 10.4: Loss = 0.789459
Epoch 10.5: Loss = 0.599396
Epoch 10.6: Loss = 0.835678
Epoch 10.7: Loss = 0.604477
Epoch 10.8: Loss = 0.626678
Epoch 10.9: Loss = 0.62735
Epoch 10.10: Loss = 0.591568
Epoch 10.11: Loss = 0.649704
Epoch 10.12: Loss = 0.662369
Epoch 10.13: Loss = 0.789246
Epoch 10.14: Loss = 0.619141
Epoch 10.15: Loss = 0.726364
Epoch 10.16: Loss = 0.728821
Epoch 10.17: Loss = 0.587936
Epoch 10.18: Loss = 0.663544
Epoch 10.19: Loss = 0.711777
Epoch 10.20: Loss = 0.68306
Epoch 10.21: Loss = 0.711166
Epoch 10.22: Loss = 0.619934
Epoch 10.23: Loss = 0.634872
Epoch 10.24: Loss = 0.719193
Epoch 10.25: Loss = 0.623856
Epoch 10.26: Loss = 0.750198
Epoch 10.27: Loss = 0.597717
Epoch 10.28: Loss = 0.61084
Epoch 10.29: Loss = 0.617874
Epoch 10.30: Loss = 0.65451
Epoch 10.31: Loss = 0.494568
Epoch 10.32: Loss = 0.591171
Epoch 10.33: Loss = 0.777618
Epoch 10.34: Loss = 0.666245
Epoch 10.35: Loss = 0.513062
Epoch 10.36: Loss = 0.792938
Epoch 10.37: Loss = 0.662857
Epoch 10.38: Loss = 0.621689
Epoch 10.39: Loss = 0.571625
Epoch 10.40: Loss = 0.544525
Epoch 10.41: Loss = 0.651581
Epoch 10.42: Loss = 0.67865
Epoch 10.43: Loss = 0.749084
Epoch 10.44: Loss = 0.557404
Epoch 10.45: Loss = 0.715302
Epoch 10.46: Loss = 0.600449
Epoch 10.47: Loss = 0.613098
Epoch 10.48: Loss = 0.792984
Epoch 10.49: Loss = 0.665833
Epoch 10.50: Loss = 0.548569
Epoch 10.51: Loss = 0.639038
Epoch 10.52: Loss = 0.641159
Epoch 10.53: Loss = 0.622131
Epoch 10.54: Loss = 0.630692
Epoch 10.55: Loss = 0.56926
Epoch 10.56: Loss = 0.687012
Epoch 10.57: Loss = 0.707779
Epoch 10.58: Loss = 0.644501
Epoch 10.59: Loss = 0.542282
Epoch 10.60: Loss = 0.706375
Epoch 10.61: Loss = 0.65567
Epoch 10.62: Loss = 0.57959
Epoch 10.63: Loss = 0.59166
Epoch 10.64: Loss = 0.575073
Epoch 10.65: Loss = 0.563278
Epoch 10.66: Loss = 0.787445
Epoch 10.67: Loss = 0.676239
Epoch 10.68: Loss = 0.6586
Epoch 10.69: Loss = 0.691376
Epoch 10.70: Loss = 0.805176
Epoch 10.71: Loss = 0.726715
Epoch 10.72: Loss = 0.632507
Epoch 10.73: Loss = 0.702179
Epoch 10.74: Loss = 0.693939
Epoch 10.75: Loss = 0.672516
Epoch 10.76: Loss = 0.6483
Epoch 10.77: Loss = 0.602509
Epoch 10.78: Loss = 0.645355
Epoch 10.79: Loss = 0.648148
Epoch 10.80: Loss = 0.527176
Epoch 10.81: Loss = 0.620956
Epoch 10.82: Loss = 0.706985
Epoch 10.83: Loss = 0.701736
Epoch 10.84: Loss = 0.710632
Epoch 10.85: Loss = 0.614578
Epoch 10.86: Loss = 0.467499
Epoch 10.87: Loss = 0.620865
Epoch 10.88: Loss = 0.554779
Epoch 10.89: Loss = 0.581604
Epoch 10.90: Loss = 0.702271
Epoch 10.91: Loss = 0.697311
Epoch 10.92: Loss = 0.613312
Epoch 10.93: Loss = 0.58812
Epoch 10.94: Loss = 0.61792
Epoch 10.95: Loss = 0.590637
Epoch 10.96: Loss = 0.662247
Epoch 10.97: Loss = 0.517929
Epoch 10.98: Loss = 0.725891
Epoch 10.99: Loss = 0.540619
Epoch 10.100: Loss = 0.636215
Epoch 10.101: Loss = 0.587646
Epoch 10.102: Loss = 0.533417
Epoch 10.103: Loss = 0.652847
Epoch 10.104: Loss = 0.666733
Epoch 10.105: Loss = 0.584167
Epoch 10.106: Loss = 0.615311
Epoch 10.107: Loss = 0.722565
Epoch 10.108: Loss = 0.684402
Epoch 10.109: Loss = 0.658951
Epoch 10.110: Loss = 0.631317
Epoch 10.111: Loss = 0.587723
Epoch 10.112: Loss = 0.600525
Epoch 10.113: Loss = 0.696091
Epoch 10.114: Loss = 0.633286
Epoch 10.115: Loss = 0.629807
Epoch 10.116: Loss = 0.692841
Epoch 10.117: Loss = 0.587494
Epoch 10.118: Loss = 0.639023
Epoch 10.119: Loss = 0.581985
Epoch 10.120: Loss = 0.638535
TRAIN LOSS = 0.644928
TRAIN ACC = 80.864 % (48520/60000)
Loss = 0.536621
Loss = 0.671951
Loss = 0.599167
Loss = 0.53978
Loss = 0.598373
Loss = 0.746597
Loss = 0.836838
Loss = 0.705338
Loss = 0.630997
Loss = 0.546402
Loss = 0.830276
Loss = 0.733368
Loss = 0.679413
Loss = 0.69252
Loss = 0.635239
Loss = 0.670547
Loss = 0.631958
Loss = 0.643692
Loss = 0.686356
Loss = 0.651428
TEST LOSS = 0.663343
TEST ACC = 485.199 % (7984/10000)

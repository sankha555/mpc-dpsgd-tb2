Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.43555
Epoch 1.2: Loss = 2.32805
Epoch 1.3: Loss = 2.21524
Epoch 1.4: Loss = 2.08485
Epoch 1.5: Loss = 2.04195
Epoch 1.6: Loss = 1.96793
Epoch 1.7: Loss = 1.91075
Epoch 1.8: Loss = 1.84549
Epoch 1.9: Loss = 1.80969
Epoch 1.10: Loss = 1.7621
Epoch 1.11: Loss = 1.70396
Epoch 1.12: Loss = 1.62033
Epoch 1.13: Loss = 1.61781
Epoch 1.14: Loss = 1.61029
Epoch 1.15: Loss = 1.52179
Epoch 1.16: Loss = 1.51514
Epoch 1.17: Loss = 1.42961
Epoch 1.18: Loss = 1.40163
Epoch 1.19: Loss = 1.37218
Epoch 1.20: Loss = 1.37094
Epoch 1.21: Loss = 1.31288
Epoch 1.22: Loss = 1.27194
Epoch 1.23: Loss = 1.2912
Epoch 1.24: Loss = 1.25166
Epoch 1.25: Loss = 1.22356
Epoch 1.26: Loss = 1.20839
Epoch 1.27: Loss = 1.17989
Epoch 1.28: Loss = 1.19838
Epoch 1.29: Loss = 1.14192
Epoch 1.30: Loss = 1.13821
Epoch 1.31: Loss = 1.15456
Epoch 1.32: Loss = 1.10466
Epoch 1.33: Loss = 1.04829
Epoch 1.34: Loss = 1.10701
Epoch 1.35: Loss = 1.08711
Epoch 1.36: Loss = 1.07512
Epoch 1.37: Loss = 1.08372
Epoch 1.38: Loss = 1.06946
Epoch 1.39: Loss = 1.01556
Epoch 1.40: Loss = 1.05327
Epoch 1.41: Loss = 1.02524
Epoch 1.42: Loss = 0.993729
Epoch 1.43: Loss = 1.00525
Epoch 1.44: Loss = 0.948807
Epoch 1.45: Loss = 0.999039
Epoch 1.46: Loss = 0.927963
Epoch 1.47: Loss = 0.92984
Epoch 1.48: Loss = 0.920792
Epoch 1.49: Loss = 0.894012
Epoch 1.50: Loss = 0.93364
Epoch 1.51: Loss = 0.906815
Epoch 1.52: Loss = 0.865433
Epoch 1.53: Loss = 0.88623
Epoch 1.54: Loss = 0.952942
Epoch 1.55: Loss = 0.828659
Epoch 1.56: Loss = 0.972153
Epoch 1.57: Loss = 0.875748
Epoch 1.58: Loss = 0.881104
Epoch 1.59: Loss = 0.897202
Epoch 1.60: Loss = 0.950623
Epoch 1.61: Loss = 0.849518
Epoch 1.62: Loss = 0.803986
Epoch 1.63: Loss = 0.870941
Epoch 1.64: Loss = 0.865494
Epoch 1.65: Loss = 0.818069
Epoch 1.66: Loss = 0.801086
Epoch 1.67: Loss = 0.853973
Epoch 1.68: Loss = 0.837967
Epoch 1.69: Loss = 0.807236
Epoch 1.70: Loss = 0.826309
Epoch 1.71: Loss = 0.817734
Epoch 1.72: Loss = 0.897324
Epoch 1.73: Loss = 0.837784
Epoch 1.74: Loss = 0.779129
Epoch 1.75: Loss = 0.860153
Epoch 1.76: Loss = 0.780289
Epoch 1.77: Loss = 0.8255
Epoch 1.78: Loss = 0.738541
Epoch 1.79: Loss = 0.785095
Epoch 1.80: Loss = 0.835403
Epoch 1.81: Loss = 0.848846
Epoch 1.82: Loss = 0.835037
Epoch 1.83: Loss = 0.772293
Epoch 1.84: Loss = 0.786774
Epoch 1.85: Loss = 0.786636
Epoch 1.86: Loss = 0.779831
Epoch 1.87: Loss = 0.73822
Epoch 1.88: Loss = 0.829056
Epoch 1.89: Loss = 0.774582
Epoch 1.90: Loss = 0.795486
Epoch 1.91: Loss = 0.68721
Epoch 1.92: Loss = 0.767761
Epoch 1.93: Loss = 0.812988
Epoch 1.94: Loss = 0.786301
Epoch 1.95: Loss = 0.814041
Epoch 1.96: Loss = 0.737762
Epoch 1.97: Loss = 0.747513
Epoch 1.98: Loss = 0.676666
Epoch 1.99: Loss = 0.770447
Epoch 1.100: Loss = 0.749481
Epoch 1.101: Loss = 0.752914
Epoch 1.102: Loss = 0.800262
Epoch 1.103: Loss = 0.677429
Epoch 1.104: Loss = 0.747055
Epoch 1.105: Loss = 0.745407
Epoch 1.106: Loss = 0.662003
Epoch 1.107: Loss = 0.80777
Epoch 1.108: Loss = 0.784302
Epoch 1.109: Loss = 0.765747
Epoch 1.110: Loss = 0.777618
Epoch 1.111: Loss = 0.688354
Epoch 1.112: Loss = 0.722443
Epoch 1.113: Loss = 0.795227
Epoch 1.114: Loss = 0.723328
Epoch 1.115: Loss = 0.753128
Epoch 1.116: Loss = 0.716904
Epoch 1.117: Loss = 0.698242
Epoch 1.118: Loss = 0.678696
Epoch 1.119: Loss = 0.719849
Epoch 1.120: Loss = 0.72644
TRAIN LOSS = 1.03259
TRAIN ACC = 66.5909 % (39957/60000)
Loss = 0.662216
Loss = 0.77359
Loss = 0.758606
Loss = 0.663483
Loss = 0.68248
Loss = 0.843063
Loss = 0.843704
Loss = 0.771469
Loss = 0.713898
Loss = 0.673508
Loss = 0.794601
Loss = 0.760162
Loss = 0.757843
Loss = 0.778488
Loss = 0.730972
Loss = 0.796448
Loss = 0.690308
Loss = 0.761963
Loss = 0.790161
Loss = 0.73671
TEST LOSS = 0.749183
TEST ACC = 399.57 % (7420/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.745209
Epoch 2.2: Loss = 0.707397
Epoch 2.3: Loss = 0.676392
Epoch 2.4: Loss = 0.63884
Epoch 2.5: Loss = 0.822495
Epoch 2.6: Loss = 0.751404
Epoch 2.7: Loss = 0.819458
Epoch 2.8: Loss = 0.643326
Epoch 2.9: Loss = 0.749771
Epoch 2.10: Loss = 0.660782
Epoch 2.11: Loss = 0.696426
Epoch 2.12: Loss = 0.742126
Epoch 2.13: Loss = 0.822098
Epoch 2.14: Loss = 0.781113
Epoch 2.15: Loss = 0.686264
Epoch 2.16: Loss = 0.630722
Epoch 2.17: Loss = 0.69043
Epoch 2.18: Loss = 0.776428
Epoch 2.19: Loss = 0.675674
Epoch 2.20: Loss = 0.663834
Epoch 2.21: Loss = 0.676483
Epoch 2.22: Loss = 0.694244
Epoch 2.23: Loss = 0.737762
Epoch 2.24: Loss = 0.692154
Epoch 2.25: Loss = 0.6642
Epoch 2.26: Loss = 0.645966
Epoch 2.27: Loss = 0.733093
Epoch 2.28: Loss = 0.71492
Epoch 2.29: Loss = 0.769012
Epoch 2.30: Loss = 0.769928
Epoch 2.31: Loss = 0.691162
Epoch 2.32: Loss = 0.679184
Epoch 2.33: Loss = 0.79541
Epoch 2.34: Loss = 0.591385
Epoch 2.35: Loss = 0.644028
Epoch 2.36: Loss = 0.655289
Epoch 2.37: Loss = 0.74733
Epoch 2.38: Loss = 0.709549
Epoch 2.39: Loss = 0.707245
Epoch 2.40: Loss = 0.710999
Epoch 2.41: Loss = 0.648392
Epoch 2.42: Loss = 0.676682
Epoch 2.43: Loss = 0.691299
Epoch 2.44: Loss = 0.69426
Epoch 2.45: Loss = 0.643997
Epoch 2.46: Loss = 0.68425
Epoch 2.47: Loss = 0.640442
Epoch 2.48: Loss = 0.717682
Epoch 2.49: Loss = 0.665955
Epoch 2.50: Loss = 0.701797
Epoch 2.51: Loss = 0.598221
Epoch 2.52: Loss = 0.697998
Epoch 2.53: Loss = 0.682602
Epoch 2.54: Loss = 0.624283
Epoch 2.55: Loss = 0.707855
Epoch 2.56: Loss = 0.678299
Epoch 2.57: Loss = 0.761642
Epoch 2.58: Loss = 0.714172
Epoch 2.59: Loss = 0.728714
Epoch 2.60: Loss = 0.728989
Epoch 2.61: Loss = 0.724228
Epoch 2.62: Loss = 0.703857
Epoch 2.63: Loss = 0.680893
Epoch 2.64: Loss = 0.688492
Epoch 2.65: Loss = 0.635422
Epoch 2.66: Loss = 0.66687
Epoch 2.67: Loss = 0.649506
Epoch 2.68: Loss = 0.777634
Epoch 2.69: Loss = 0.764542
Epoch 2.70: Loss = 0.730804
Epoch 2.71: Loss = 0.734512
Epoch 2.72: Loss = 0.728516
Epoch 2.73: Loss = 0.681061
Epoch 2.74: Loss = 0.695633
Epoch 2.75: Loss = 0.674606
Epoch 2.76: Loss = 0.710846
Epoch 2.77: Loss = 0.667709
Epoch 2.78: Loss = 0.632248
Epoch 2.79: Loss = 0.630951
Epoch 2.80: Loss = 0.708755
Epoch 2.81: Loss = 0.59967
Epoch 2.82: Loss = 0.677994
Epoch 2.83: Loss = 0.642029
Epoch 2.84: Loss = 0.78772
Epoch 2.85: Loss = 0.692154
Epoch 2.86: Loss = 0.66124
Epoch 2.87: Loss = 0.695908
Epoch 2.88: Loss = 0.770691
Epoch 2.89: Loss = 0.64946
Epoch 2.90: Loss = 0.68309
Epoch 2.91: Loss = 0.678452
Epoch 2.92: Loss = 0.746292
Epoch 2.93: Loss = 0.634109
Epoch 2.94: Loss = 0.637848
Epoch 2.95: Loss = 0.652054
Epoch 2.96: Loss = 0.68425
Epoch 2.97: Loss = 0.625198
Epoch 2.98: Loss = 0.810867
Epoch 2.99: Loss = 0.649063
Epoch 2.100: Loss = 0.635498
Epoch 2.101: Loss = 0.678482
Epoch 2.102: Loss = 0.617294
Epoch 2.103: Loss = 0.671478
Epoch 2.104: Loss = 0.687927
Epoch 2.105: Loss = 0.708237
Epoch 2.106: Loss = 0.645294
Epoch 2.107: Loss = 0.714859
Epoch 2.108: Loss = 0.737427
Epoch 2.109: Loss = 0.719162
Epoch 2.110: Loss = 0.659027
Epoch 2.111: Loss = 0.614136
Epoch 2.112: Loss = 0.750839
Epoch 2.113: Loss = 0.697693
Epoch 2.114: Loss = 0.618393
Epoch 2.115: Loss = 0.663788
Epoch 2.116: Loss = 0.718506
Epoch 2.117: Loss = 0.683411
Epoch 2.118: Loss = 0.690933
Epoch 2.119: Loss = 0.67334
Epoch 2.120: Loss = 0.663864
TRAIN LOSS = 0.693451
TRAIN ACC = 77.2171 % (46332/60000)
Loss = 0.584
Loss = 0.732147
Loss = 0.676895
Loss = 0.579453
Loss = 0.613464
Loss = 0.759918
Loss = 0.809097
Loss = 0.733078
Loss = 0.682495
Loss = 0.609238
Loss = 0.765961
Loss = 0.731003
Loss = 0.680161
Loss = 0.694
Loss = 0.679855
Loss = 0.735031
Loss = 0.626572
Loss = 0.69751
Loss = 0.739838
Loss = 0.669418
TEST LOSS = 0.689956
TEST ACC = 463.319 % (7734/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.64859
Epoch 3.2: Loss = 0.637711
Epoch 3.3: Loss = 0.681824
Epoch 3.4: Loss = 0.611237
Epoch 3.5: Loss = 0.747604
Epoch 3.6: Loss = 0.671127
Epoch 3.7: Loss = 0.664307
Epoch 3.8: Loss = 0.700043
Epoch 3.9: Loss = 0.710388
Epoch 3.10: Loss = 0.658539
Epoch 3.11: Loss = 0.632599
Epoch 3.12: Loss = 0.663361
Epoch 3.13: Loss = 0.61586
Epoch 3.14: Loss = 0.639969
Epoch 3.15: Loss = 0.779953
Epoch 3.16: Loss = 0.604538
Epoch 3.17: Loss = 0.628433
Epoch 3.18: Loss = 0.657532
Epoch 3.19: Loss = 0.758484
Epoch 3.20: Loss = 0.750977
Epoch 3.21: Loss = 0.646835
Epoch 3.22: Loss = 0.646088
Epoch 3.23: Loss = 0.690826
Epoch 3.24: Loss = 0.68335
Epoch 3.25: Loss = 0.591858
Epoch 3.26: Loss = 0.632629
Epoch 3.27: Loss = 0.594543
Epoch 3.28: Loss = 0.724197
Epoch 3.29: Loss = 0.63678
Epoch 3.30: Loss = 0.679092
Epoch 3.31: Loss = 0.571686
Epoch 3.32: Loss = 0.719711
Epoch 3.33: Loss = 0.624207
Epoch 3.34: Loss = 0.709976
Epoch 3.35: Loss = 0.706467
Epoch 3.36: Loss = 0.681946
Epoch 3.37: Loss = 0.619263
Epoch 3.38: Loss = 0.628922
Epoch 3.39: Loss = 0.695053
Epoch 3.40: Loss = 0.684402
Epoch 3.41: Loss = 0.784546
Epoch 3.42: Loss = 0.640991
Epoch 3.43: Loss = 0.721451
Epoch 3.44: Loss = 0.673492
Epoch 3.45: Loss = 0.591995
Epoch 3.46: Loss = 0.685196
Epoch 3.47: Loss = 0.64325
Epoch 3.48: Loss = 0.632645
Epoch 3.49: Loss = 0.613403
Epoch 3.50: Loss = 0.650467
Epoch 3.51: Loss = 0.67041
Epoch 3.52: Loss = 0.804337
Epoch 3.53: Loss = 0.669022
Epoch 3.54: Loss = 0.617081
Epoch 3.55: Loss = 0.680878
Epoch 3.56: Loss = 0.62648
Epoch 3.57: Loss = 0.539047
Epoch 3.58: Loss = 0.588409
Epoch 3.59: Loss = 0.757553
Epoch 3.60: Loss = 0.63913
Epoch 3.61: Loss = 0.652634
Epoch 3.62: Loss = 0.772369
Epoch 3.63: Loss = 0.598267
Epoch 3.64: Loss = 0.565353
Epoch 3.65: Loss = 0.652512
Epoch 3.66: Loss = 0.609314
Epoch 3.67: Loss = 0.6017
Epoch 3.68: Loss = 0.707138
Epoch 3.69: Loss = 0.766418
Epoch 3.70: Loss = 0.590012
Epoch 3.71: Loss = 0.64827
Epoch 3.72: Loss = 0.619736
Epoch 3.73: Loss = 0.596695
Epoch 3.74: Loss = 0.776398
Epoch 3.75: Loss = 0.639618
Epoch 3.76: Loss = 0.679123
Epoch 3.77: Loss = 0.733337
Epoch 3.78: Loss = 0.677612
Epoch 3.79: Loss = 0.658432
Epoch 3.80: Loss = 0.789703
Epoch 3.81: Loss = 0.751129
Epoch 3.82: Loss = 0.63385
Epoch 3.83: Loss = 0.793472
Epoch 3.84: Loss = 0.668594
Epoch 3.85: Loss = 0.642059
Epoch 3.86: Loss = 0.760223
Epoch 3.87: Loss = 0.581146
Epoch 3.88: Loss = 0.658951
Epoch 3.89: Loss = 0.573105
Epoch 3.90: Loss = 0.574219
Epoch 3.91: Loss = 0.713989
Epoch 3.92: Loss = 0.627182
Epoch 3.93: Loss = 0.645187
Epoch 3.94: Loss = 0.624176
Epoch 3.95: Loss = 0.606415
Epoch 3.96: Loss = 0.650513
Epoch 3.97: Loss = 0.682327
Epoch 3.98: Loss = 0.621979
Epoch 3.99: Loss = 0.622803
Epoch 3.100: Loss = 0.704208
Epoch 3.101: Loss = 0.651794
Epoch 3.102: Loss = 0.665207
Epoch 3.103: Loss = 0.801544
Epoch 3.104: Loss = 0.595673
Epoch 3.105: Loss = 0.773376
Epoch 3.106: Loss = 0.536484
Epoch 3.107: Loss = 0.650055
Epoch 3.108: Loss = 0.53186
Epoch 3.109: Loss = 0.633148
Epoch 3.110: Loss = 0.742935
Epoch 3.111: Loss = 0.577225
Epoch 3.112: Loss = 0.745605
Epoch 3.113: Loss = 0.594574
Epoch 3.114: Loss = 0.727066
Epoch 3.115: Loss = 0.63855
Epoch 3.116: Loss = 0.685486
Epoch 3.117: Loss = 0.680283
Epoch 3.118: Loss = 0.723053
Epoch 3.119: Loss = 0.643661
Epoch 3.120: Loss = 0.659607
TRAIN LOSS = 0.663254
TRAIN ACC = 78.8834 % (47332/60000)
Loss = 0.576202
Loss = 0.741089
Loss = 0.672867
Loss = 0.548782
Loss = 0.614853
Loss = 0.776779
Loss = 0.812515
Loss = 0.726166
Loss = 0.696014
Loss = 0.627014
Loss = 0.806229
Loss = 0.72966
Loss = 0.7108
Loss = 0.690582
Loss = 0.666641
Loss = 0.728531
Loss = 0.619308
Loss = 0.735901
Loss = 0.725067
Loss = 0.687042
TEST LOSS = 0.694602
TEST ACC = 473.318 % (7771/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.649887
Epoch 4.2: Loss = 0.668915
Epoch 4.3: Loss = 0.592377
Epoch 4.4: Loss = 0.728302
Epoch 4.5: Loss = 0.648056
Epoch 4.6: Loss = 0.600296
Epoch 4.7: Loss = 0.670609
Epoch 4.8: Loss = 0.636002
Epoch 4.9: Loss = 0.625366
Epoch 4.10: Loss = 0.676712
Epoch 4.11: Loss = 0.63266
Epoch 4.12: Loss = 0.702881
Epoch 4.13: Loss = 0.666641
Epoch 4.14: Loss = 0.649124
Epoch 4.15: Loss = 0.538712
Epoch 4.16: Loss = 0.596878
Epoch 4.17: Loss = 0.607193
Epoch 4.18: Loss = 0.694016
Epoch 4.19: Loss = 0.638977
Epoch 4.20: Loss = 0.65976
Epoch 4.21: Loss = 0.793213
Epoch 4.22: Loss = 0.572083
Epoch 4.23: Loss = 0.720108
Epoch 4.24: Loss = 0.64949
Epoch 4.25: Loss = 0.618408
Epoch 4.26: Loss = 0.729233
Epoch 4.27: Loss = 0.639618
Epoch 4.28: Loss = 0.602936
Epoch 4.29: Loss = 0.694839
Epoch 4.30: Loss = 0.666718
Epoch 4.31: Loss = 0.582779
Epoch 4.32: Loss = 0.706818
Epoch 4.33: Loss = 0.571594
Epoch 4.34: Loss = 0.666794
Epoch 4.35: Loss = 0.895081
Epoch 4.36: Loss = 0.686081
Epoch 4.37: Loss = 0.634445
Epoch 4.38: Loss = 0.578247
Epoch 4.39: Loss = 0.705368
Epoch 4.40: Loss = 0.753342
Epoch 4.41: Loss = 0.669647
Epoch 4.42: Loss = 0.688354
Epoch 4.43: Loss = 0.638184
Epoch 4.44: Loss = 0.604416
Epoch 4.45: Loss = 0.73938
Epoch 4.46: Loss = 0.691452
Epoch 4.47: Loss = 0.632751
Epoch 4.48: Loss = 0.61174
Epoch 4.49: Loss = 0.744415
Epoch 4.50: Loss = 0.705963
Epoch 4.51: Loss = 0.660995
Epoch 4.52: Loss = 0.719284
Epoch 4.53: Loss = 0.685135
Epoch 4.54: Loss = 0.631973
Epoch 4.55: Loss = 0.69429
Epoch 4.56: Loss = 0.546509
Epoch 4.57: Loss = 0.721359
Epoch 4.58: Loss = 0.691574
Epoch 4.59: Loss = 0.643448
Epoch 4.60: Loss = 0.573074
Epoch 4.61: Loss = 0.679703
Epoch 4.62: Loss = 0.74472
Epoch 4.63: Loss = 0.6073
Epoch 4.64: Loss = 0.675949
Epoch 4.65: Loss = 0.603699
Epoch 4.66: Loss = 0.62323
Epoch 4.67: Loss = 0.781174
Epoch 4.68: Loss = 0.676697
Epoch 4.69: Loss = 0.629974
Epoch 4.70: Loss = 0.678482
Epoch 4.71: Loss = 0.71257
Epoch 4.72: Loss = 0.730728
Epoch 4.73: Loss = 0.681015
Epoch 4.74: Loss = 0.705017
Epoch 4.75: Loss = 0.679047
Epoch 4.76: Loss = 0.664032
Epoch 4.77: Loss = 0.613693
Epoch 4.78: Loss = 0.648026
Epoch 4.79: Loss = 0.727463
Epoch 4.80: Loss = 0.723312
Epoch 4.81: Loss = 0.580353
Epoch 4.82: Loss = 0.795319
Epoch 4.83: Loss = 0.664261
Epoch 4.84: Loss = 0.656601
Epoch 4.85: Loss = 0.689682
Epoch 4.86: Loss = 0.542725
Epoch 4.87: Loss = 0.600571
Epoch 4.88: Loss = 0.555374
Epoch 4.89: Loss = 0.630875
Epoch 4.90: Loss = 0.636795
Epoch 4.91: Loss = 0.61908
Epoch 4.92: Loss = 0.583786
Epoch 4.93: Loss = 0.500824
Epoch 4.94: Loss = 0.705887
Epoch 4.95: Loss = 0.624359
Epoch 4.96: Loss = 0.626465
Epoch 4.97: Loss = 0.64325
Epoch 4.98: Loss = 0.781036
Epoch 4.99: Loss = 0.561905
Epoch 4.100: Loss = 0.736191
Epoch 4.101: Loss = 0.660172
Epoch 4.102: Loss = 0.689682
Epoch 4.103: Loss = 0.624619
Epoch 4.104: Loss = 0.633026
Epoch 4.105: Loss = 0.593781
Epoch 4.106: Loss = 0.644073
Epoch 4.107: Loss = 0.556839
Epoch 4.108: Loss = 0.62973
Epoch 4.109: Loss = 0.663391
Epoch 4.110: Loss = 0.714508
Epoch 4.111: Loss = 0.661514
Epoch 4.112: Loss = 0.53746
Epoch 4.113: Loss = 0.671677
Epoch 4.114: Loss = 0.499786
Epoch 4.115: Loss = 0.55632
Epoch 4.116: Loss = 0.594574
Epoch 4.117: Loss = 0.586975
Epoch 4.118: Loss = 0.605331
Epoch 4.119: Loss = 0.617218
Epoch 4.120: Loss = 0.751892
TRAIN LOSS = 0.653824
TRAIN ACC = 79.4525 % (47674/60000)
Loss = 0.581253
Loss = 0.710251
Loss = 0.65921
Loss = 0.543655
Loss = 0.585159
Loss = 0.753036
Loss = 0.811462
Loss = 0.729538
Loss = 0.690475
Loss = 0.616608
Loss = 0.811066
Loss = 0.751389
Loss = 0.684769
Loss = 0.692047
Loss = 0.671539
Loss = 0.709061
Loss = 0.61174
Loss = 0.724152
Loss = 0.723969
Loss = 0.686554
TEST LOSS = 0.687346
TEST ACC = 476.74 % (7880/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.64183
Epoch 5.2: Loss = 0.659698
Epoch 5.3: Loss = 0.712372
Epoch 5.4: Loss = 0.663971
Epoch 5.5: Loss = 0.740173
Epoch 5.6: Loss = 0.570572
Epoch 5.7: Loss = 0.731079
Epoch 5.8: Loss = 0.660004
Epoch 5.9: Loss = 0.562607
Epoch 5.10: Loss = 0.651657
Epoch 5.11: Loss = 0.532089
Epoch 5.12: Loss = 0.738068
Epoch 5.13: Loss = 0.599609
Epoch 5.14: Loss = 0.623032
Epoch 5.15: Loss = 0.679825
Epoch 5.16: Loss = 0.625748
Epoch 5.17: Loss = 0.634903
Epoch 5.18: Loss = 0.614609
Epoch 5.19: Loss = 0.556076
Epoch 5.20: Loss = 0.626556
Epoch 5.21: Loss = 0.640305
Epoch 5.22: Loss = 0.591293
Epoch 5.23: Loss = 0.606216
Epoch 5.24: Loss = 0.623383
Epoch 5.25: Loss = 0.536682
Epoch 5.26: Loss = 0.739456
Epoch 5.27: Loss = 0.621307
Epoch 5.28: Loss = 0.671204
Epoch 5.29: Loss = 0.668808
Epoch 5.30: Loss = 0.628693
Epoch 5.31: Loss = 0.621231
Epoch 5.32: Loss = 0.670639
Epoch 5.33: Loss = 0.649033
Epoch 5.34: Loss = 0.658142
Epoch 5.35: Loss = 0.626526
Epoch 5.36: Loss = 0.600327
Epoch 5.37: Loss = 0.649506
Epoch 5.38: Loss = 0.568832
Epoch 5.39: Loss = 0.59404
Epoch 5.40: Loss = 0.644882
Epoch 5.41: Loss = 0.557877
Epoch 5.42: Loss = 0.681229
Epoch 5.43: Loss = 0.676773
Epoch 5.44: Loss = 0.664169
Epoch 5.45: Loss = 0.559448
Epoch 5.46: Loss = 0.609772
Epoch 5.47: Loss = 0.674957
Epoch 5.48: Loss = 0.61084
Epoch 5.49: Loss = 0.663055
Epoch 5.50: Loss = 0.568695
Epoch 5.51: Loss = 0.649567
Epoch 5.52: Loss = 0.637817
Epoch 5.53: Loss = 0.705017
Epoch 5.54: Loss = 0.6436
Epoch 5.55: Loss = 0.698074
Epoch 5.56: Loss = 0.727753
Epoch 5.57: Loss = 0.7491
Epoch 5.58: Loss = 0.656876
Epoch 5.59: Loss = 0.638626
Epoch 5.60: Loss = 0.675354
Epoch 5.61: Loss = 0.635437
Epoch 5.62: Loss = 0.606216
Epoch 5.63: Loss = 0.5466
Epoch 5.64: Loss = 0.550156
Epoch 5.65: Loss = 0.556778
Epoch 5.66: Loss = 0.560852
Epoch 5.67: Loss = 0.610138
Epoch 5.68: Loss = 0.681244
Epoch 5.69: Loss = 0.587494
Epoch 5.70: Loss = 0.646057
Epoch 5.71: Loss = 0.716095
Epoch 5.72: Loss = 0.671188
Epoch 5.73: Loss = 0.628784
Epoch 5.74: Loss = 0.646393
Epoch 5.75: Loss = 0.550476
Epoch 5.76: Loss = 0.674118
Epoch 5.77: Loss = 0.70961
Epoch 5.78: Loss = 0.561081
Epoch 5.79: Loss = 0.716522
Epoch 5.80: Loss = 0.68161
Epoch 5.81: Loss = 0.54422
Epoch 5.82: Loss = 0.733765
Epoch 5.83: Loss = 0.575211
Epoch 5.84: Loss = 0.648544
Epoch 5.85: Loss = 0.544968
Epoch 5.86: Loss = 0.664642
Epoch 5.87: Loss = 0.602463
Epoch 5.88: Loss = 0.552322
Epoch 5.89: Loss = 0.678864
Epoch 5.90: Loss = 0.551926
Epoch 5.91: Loss = 0.683762
Epoch 5.92: Loss = 0.698624
Epoch 5.93: Loss = 0.681885
Epoch 5.94: Loss = 0.643814
Epoch 5.95: Loss = 0.587265
Epoch 5.96: Loss = 0.857925
Epoch 5.97: Loss = 0.720215
Epoch 5.98: Loss = 0.7444
Epoch 5.99: Loss = 0.654694
Epoch 5.100: Loss = 0.631836
Epoch 5.101: Loss = 0.595016
Epoch 5.102: Loss = 0.64679
Epoch 5.103: Loss = 0.69725
Epoch 5.104: Loss = 0.662186
Epoch 5.105: Loss = 0.609222
Epoch 5.106: Loss = 0.617233
Epoch 5.107: Loss = 0.614227
Epoch 5.108: Loss = 0.58847
Epoch 5.109: Loss = 0.631058
Epoch 5.110: Loss = 0.591385
Epoch 5.111: Loss = 0.665909
Epoch 5.112: Loss = 0.598251
Epoch 5.113: Loss = 0.65686
Epoch 5.114: Loss = 0.734009
Epoch 5.115: Loss = 0.796127
Epoch 5.116: Loss = 0.738846
Epoch 5.117: Loss = 0.589432
Epoch 5.118: Loss = 0.610123
Epoch 5.119: Loss = 0.620026
Epoch 5.120: Loss = 0.526489
TRAIN LOSS = 0.640106
TRAIN ACC = 80.1514 % (48093/60000)
Loss = 0.565186
Loss = 0.706543
Loss = 0.645844
Loss = 0.518372
Loss = 0.557037
Loss = 0.73465
Loss = 0.816055
Loss = 0.710297
Loss = 0.65097
Loss = 0.567673
Loss = 0.796326
Loss = 0.714615
Loss = 0.674408
Loss = 0.686295
Loss = 0.629105
Loss = 0.675583
Loss = 0.609009
Loss = 0.694092
Loss = 0.6716
Loss = 0.656754
TEST LOSS = 0.66402
TEST ACC = 480.93 % (7979/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.557693
Epoch 6.2: Loss = 0.698517
Epoch 6.3: Loss = 0.535843
Epoch 6.4: Loss = 0.614731
Epoch 6.5: Loss = 0.632889
Epoch 6.6: Loss = 0.577301
Epoch 6.7: Loss = 0.690628
Epoch 6.8: Loss = 0.674622
Epoch 6.9: Loss = 0.600891
Epoch 6.10: Loss = 0.619705
Epoch 6.11: Loss = 0.562393
Epoch 6.12: Loss = 0.624588
Epoch 6.13: Loss = 0.65831
Epoch 6.14: Loss = 0.606766
Epoch 6.15: Loss = 0.628738
Epoch 6.16: Loss = 0.660095
Epoch 6.17: Loss = 0.705353
Epoch 6.18: Loss = 0.568756
Epoch 6.19: Loss = 0.54689
Epoch 6.20: Loss = 0.684097
Epoch 6.21: Loss = 0.58638
Epoch 6.22: Loss = 0.563538
Epoch 6.23: Loss = 0.653305
Epoch 6.24: Loss = 0.656952
Epoch 6.25: Loss = 0.690247
Epoch 6.26: Loss = 0.669556
Epoch 6.27: Loss = 0.604843
Epoch 6.28: Loss = 0.628647
Epoch 6.29: Loss = 0.656296
Epoch 6.30: Loss = 0.680496
Epoch 6.31: Loss = 0.652435
Epoch 6.32: Loss = 0.525665
Epoch 6.33: Loss = 0.661423
Epoch 6.34: Loss = 0.637085
Epoch 6.35: Loss = 0.638916
Epoch 6.36: Loss = 0.594925
Epoch 6.37: Loss = 0.703339
Epoch 6.38: Loss = 0.609222
Epoch 6.39: Loss = 0.797577
Epoch 6.40: Loss = 0.659744
Epoch 6.41: Loss = 0.566727
Epoch 6.42: Loss = 0.647461
Epoch 6.43: Loss = 0.616791
Epoch 6.44: Loss = 0.680023
Epoch 6.45: Loss = 0.664612
Epoch 6.46: Loss = 0.646378
Epoch 6.47: Loss = 0.699203
Epoch 6.48: Loss = 0.640411
Epoch 6.49: Loss = 0.544632
Epoch 6.50: Loss = 0.644653
Epoch 6.51: Loss = 0.541107
Epoch 6.52: Loss = 0.566544
Epoch 6.53: Loss = 0.591858
Epoch 6.54: Loss = 0.708755
Epoch 6.55: Loss = 0.624207
Epoch 6.56: Loss = 0.641861
Epoch 6.57: Loss = 0.64006
Epoch 6.58: Loss = 0.6353
Epoch 6.59: Loss = 0.678162
Epoch 6.60: Loss = 0.660156
Epoch 6.61: Loss = 0.567429
Epoch 6.62: Loss = 0.654236
Epoch 6.63: Loss = 0.74353
Epoch 6.64: Loss = 0.57782
Epoch 6.65: Loss = 0.672089
Epoch 6.66: Loss = 0.597641
Epoch 6.67: Loss = 0.734329
Epoch 6.68: Loss = 0.57077
Epoch 6.69: Loss = 0.668884
Epoch 6.70: Loss = 0.636765
Epoch 6.71: Loss = 0.660095
Epoch 6.72: Loss = 0.698547
Epoch 6.73: Loss = 0.656982
Epoch 6.74: Loss = 0.67659
Epoch 6.75: Loss = 0.654846
Epoch 6.76: Loss = 0.656876
Epoch 6.77: Loss = 0.598648
Epoch 6.78: Loss = 0.643509
Epoch 6.79: Loss = 0.635574
Epoch 6.80: Loss = 0.685898
Epoch 6.81: Loss = 0.661011
Epoch 6.82: Loss = 0.624573
Epoch 6.83: Loss = 0.648605
Epoch 6.84: Loss = 0.527557
Epoch 6.85: Loss = 0.710709
Epoch 6.86: Loss = 0.694046
Epoch 6.87: Loss = 0.608521
Epoch 6.88: Loss = 0.720581
Epoch 6.89: Loss = 0.653305
Epoch 6.90: Loss = 0.74971
Epoch 6.91: Loss = 0.796494
Epoch 6.92: Loss = 0.718674
Epoch 6.93: Loss = 0.647842
Epoch 6.94: Loss = 0.643692
Epoch 6.95: Loss = 0.532883
Epoch 6.96: Loss = 0.714127
Epoch 6.97: Loss = 0.544815
Epoch 6.98: Loss = 0.614044
Epoch 6.99: Loss = 0.496506
Epoch 6.100: Loss = 0.686615
Epoch 6.101: Loss = 0.670288
Epoch 6.102: Loss = 0.626648
Epoch 6.103: Loss = 0.642578
Epoch 6.104: Loss = 0.685806
Epoch 6.105: Loss = 0.754105
Epoch 6.106: Loss = 0.571411
Epoch 6.107: Loss = 0.702957
Epoch 6.108: Loss = 0.647751
Epoch 6.109: Loss = 0.652039
Epoch 6.110: Loss = 0.668472
Epoch 6.111: Loss = 0.707779
Epoch 6.112: Loss = 0.65239
Epoch 6.113: Loss = 0.737549
Epoch 6.114: Loss = 0.60553
Epoch 6.115: Loss = 0.834457
Epoch 6.116: Loss = 0.603836
Epoch 6.117: Loss = 0.608444
Epoch 6.118: Loss = 0.749817
Epoch 6.119: Loss = 0.664307
Epoch 6.120: Loss = 0.55983
TRAIN LOSS = 0.644089
TRAIN ACC = 80.5267 % (48318/60000)
Loss = 0.585815
Loss = 0.727173
Loss = 0.660629
Loss = 0.560181
Loss = 0.582092
Loss = 0.739426
Loss = 0.84201
Loss = 0.727493
Loss = 0.637253
Loss = 0.581009
Loss = 0.839722
Loss = 0.776398
Loss = 0.685425
Loss = 0.715973
Loss = 0.664185
Loss = 0.691422
Loss = 0.661469
Loss = 0.713348
Loss = 0.696228
Loss = 0.71402
TEST LOSS = 0.690063
TEST ACC = 483.179 % (7963/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.583023
Epoch 7.2: Loss = 0.601395
Epoch 7.3: Loss = 0.610214
Epoch 7.4: Loss = 0.530685
Epoch 7.5: Loss = 0.686707
Epoch 7.6: Loss = 0.637787
Epoch 7.7: Loss = 0.635635
Epoch 7.8: Loss = 0.634964
Epoch 7.9: Loss = 0.582001
Epoch 7.10: Loss = 0.612915
Epoch 7.11: Loss = 0.632782
Epoch 7.12: Loss = 0.696793
Epoch 7.13: Loss = 0.685562
Epoch 7.14: Loss = 0.553604
Epoch 7.15: Loss = 0.717453
Epoch 7.16: Loss = 0.651947
Epoch 7.17: Loss = 0.706116
Epoch 7.18: Loss = 0.739761
Epoch 7.19: Loss = 0.661514
Epoch 7.20: Loss = 0.70314
Epoch 7.21: Loss = 0.563629
Epoch 7.22: Loss = 0.614105
Epoch 7.23: Loss = 0.695358
Epoch 7.24: Loss = 0.668655
Epoch 7.25: Loss = 0.763931
Epoch 7.26: Loss = 0.792313
Epoch 7.27: Loss = 0.607468
Epoch 7.28: Loss = 0.564682
Epoch 7.29: Loss = 0.712097
Epoch 7.30: Loss = 0.728363
Epoch 7.31: Loss = 0.707565
Epoch 7.32: Loss = 0.803223
Epoch 7.33: Loss = 0.594894
Epoch 7.34: Loss = 0.714203
Epoch 7.35: Loss = 0.618927
Epoch 7.36: Loss = 0.575424
Epoch 7.37: Loss = 0.730286
Epoch 7.38: Loss = 0.538513
Epoch 7.39: Loss = 0.72403
Epoch 7.40: Loss = 0.578949
Epoch 7.41: Loss = 0.609528
Epoch 7.42: Loss = 0.628433
Epoch 7.43: Loss = 0.558304
Epoch 7.44: Loss = 0.552139
Epoch 7.45: Loss = 0.666168
Epoch 7.46: Loss = 0.675064
Epoch 7.47: Loss = 0.747284
Epoch 7.48: Loss = 0.589615
Epoch 7.49: Loss = 0.675888
Epoch 7.50: Loss = 0.709534
Epoch 7.51: Loss = 0.531738
Epoch 7.52: Loss = 0.599426
Epoch 7.53: Loss = 0.529358
Epoch 7.54: Loss = 0.686951
Epoch 7.55: Loss = 0.647705
Epoch 7.56: Loss = 0.549652
Epoch 7.57: Loss = 0.778473
Epoch 7.58: Loss = 0.648788
Epoch 7.59: Loss = 0.668716
Epoch 7.60: Loss = 0.650345
Epoch 7.61: Loss = 0.619385
Epoch 7.62: Loss = 0.623947
Epoch 7.63: Loss = 0.677643
Epoch 7.64: Loss = 0.751114
Epoch 7.65: Loss = 0.644028
Epoch 7.66: Loss = 0.676514
Epoch 7.67: Loss = 0.705505
Epoch 7.68: Loss = 0.671173
Epoch 7.69: Loss = 0.699371
Epoch 7.70: Loss = 0.648178
Epoch 7.71: Loss = 0.684326
Epoch 7.72: Loss = 0.670303
Epoch 7.73: Loss = 0.594177
Epoch 7.74: Loss = 0.747726
Epoch 7.75: Loss = 0.650269
Epoch 7.76: Loss = 0.667664
Epoch 7.77: Loss = 0.770401
Epoch 7.78: Loss = 0.621323
Epoch 7.79: Loss = 0.719543
Epoch 7.80: Loss = 0.612213
Epoch 7.81: Loss = 0.651688
Epoch 7.82: Loss = 0.755432
Epoch 7.83: Loss = 0.546677
Epoch 7.84: Loss = 0.808929
Epoch 7.85: Loss = 0.764984
Epoch 7.86: Loss = 0.594986
Epoch 7.87: Loss = 0.624496
Epoch 7.88: Loss = 0.728256
Epoch 7.89: Loss = 0.704575
Epoch 7.90: Loss = 0.664001
Epoch 7.91: Loss = 0.672363
Epoch 7.92: Loss = 0.654266
Epoch 7.93: Loss = 0.615845
Epoch 7.94: Loss = 0.715942
Epoch 7.95: Loss = 0.647766
Epoch 7.96: Loss = 0.648331
Epoch 7.97: Loss = 0.684402
Epoch 7.98: Loss = 0.625061
Epoch 7.99: Loss = 0.820816
Epoch 7.100: Loss = 0.687515
Epoch 7.101: Loss = 0.699829
Epoch 7.102: Loss = 0.661652
Epoch 7.103: Loss = 0.668655
Epoch 7.104: Loss = 0.684921
Epoch 7.105: Loss = 0.615448
Epoch 7.106: Loss = 0.703278
Epoch 7.107: Loss = 0.580368
Epoch 7.108: Loss = 0.656006
Epoch 7.109: Loss = 0.574173
Epoch 7.110: Loss = 0.563568
Epoch 7.111: Loss = 0.690994
Epoch 7.112: Loss = 0.6185
Epoch 7.113: Loss = 0.638062
Epoch 7.114: Loss = 0.650513
Epoch 7.115: Loss = 0.800415
Epoch 7.116: Loss = 0.745621
Epoch 7.117: Loss = 0.626038
Epoch 7.118: Loss = 0.696838
Epoch 7.119: Loss = 0.645035
Epoch 7.120: Loss = 0.716721
TRAIN LOSS = 0.65979
TRAIN ACC = 80.4871 % (48294/60000)
Loss = 0.573334
Loss = 0.75296
Loss = 0.683762
Loss = 0.573181
Loss = 0.602676
Loss = 0.812668
Loss = 0.852554
Loss = 0.725418
Loss = 0.677048
Loss = 0.576721
Loss = 0.871246
Loss = 0.773453
Loss = 0.715927
Loss = 0.710983
Loss = 0.641968
Loss = 0.714493
Loss = 0.670868
Loss = 0.775284
Loss = 0.723312
Loss = 0.71254
TEST LOSS = 0.70702
TEST ACC = 482.939 % (8012/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.539612
Epoch 8.2: Loss = 0.692505
Epoch 8.3: Loss = 0.646027
Epoch 8.4: Loss = 0.65921
Epoch 8.5: Loss = 0.756973
Epoch 8.6: Loss = 0.664642
Epoch 8.7: Loss = 0.732132
Epoch 8.8: Loss = 0.745743
Epoch 8.9: Loss = 0.656128
Epoch 8.10: Loss = 0.681015
Epoch 8.11: Loss = 0.583633
Epoch 8.12: Loss = 0.58577
Epoch 8.13: Loss = 0.634628
Epoch 8.14: Loss = 0.623291
Epoch 8.15: Loss = 0.682526
Epoch 8.16: Loss = 0.709152
Epoch 8.17: Loss = 0.65274
Epoch 8.18: Loss = 0.639603
Epoch 8.19: Loss = 0.523941
Epoch 8.20: Loss = 0.533478
Epoch 8.21: Loss = 0.649872
Epoch 8.22: Loss = 0.771088
Epoch 8.23: Loss = 0.564133
Epoch 8.24: Loss = 0.709717
Epoch 8.25: Loss = 0.665695
Epoch 8.26: Loss = 0.49823
Epoch 8.27: Loss = 0.610519
Epoch 8.28: Loss = 0.608154
Epoch 8.29: Loss = 0.732208
Epoch 8.30: Loss = 0.50766
Epoch 8.31: Loss = 0.676941
Epoch 8.32: Loss = 0.664719
Epoch 8.33: Loss = 0.708008
Epoch 8.34: Loss = 0.717926
Epoch 8.35: Loss = 0.634232
Epoch 8.36: Loss = 0.667038
Epoch 8.37: Loss = 0.674408
Epoch 8.38: Loss = 0.668457
Epoch 8.39: Loss = 0.715637
Epoch 8.40: Loss = 0.602554
Epoch 8.41: Loss = 0.561707
Epoch 8.42: Loss = 0.713394
Epoch 8.43: Loss = 0.788391
Epoch 8.44: Loss = 0.577621
Epoch 8.45: Loss = 0.647446
Epoch 8.46: Loss = 0.657761
Epoch 8.47: Loss = 0.680649
Epoch 8.48: Loss = 0.668106
Epoch 8.49: Loss = 0.572235
Epoch 8.50: Loss = 0.652466
Epoch 8.51: Loss = 0.675995
Epoch 8.52: Loss = 0.71344
Epoch 8.53: Loss = 0.533569
Epoch 8.54: Loss = 0.594391
Epoch 8.55: Loss = 0.612976
Epoch 8.56: Loss = 0.577728
Epoch 8.57: Loss = 0.587723
Epoch 8.58: Loss = 0.667007
Epoch 8.59: Loss = 0.625015
Epoch 8.60: Loss = 0.623505
Epoch 8.61: Loss = 0.699341
Epoch 8.62: Loss = 0.737823
Epoch 8.63: Loss = 0.595795
Epoch 8.64: Loss = 0.731659
Epoch 8.65: Loss = 0.708603
Epoch 8.66: Loss = 0.719772
Epoch 8.67: Loss = 0.69873
Epoch 8.68: Loss = 0.678894
Epoch 8.69: Loss = 0.721573
Epoch 8.70: Loss = 0.715729
Epoch 8.71: Loss = 0.684814
Epoch 8.72: Loss = 0.572311
Epoch 8.73: Loss = 0.588379
Epoch 8.74: Loss = 0.567551
Epoch 8.75: Loss = 0.671738
Epoch 8.76: Loss = 0.611938
Epoch 8.77: Loss = 0.582275
Epoch 8.78: Loss = 0.878296
Epoch 8.79: Loss = 0.78212
Epoch 8.80: Loss = 0.927597
Epoch 8.81: Loss = 0.729477
Epoch 8.82: Loss = 0.642853
Epoch 8.83: Loss = 0.602859
Epoch 8.84: Loss = 0.666183
Epoch 8.85: Loss = 0.659073
Epoch 8.86: Loss = 0.55069
Epoch 8.87: Loss = 0.625214
Epoch 8.88: Loss = 0.597382
Epoch 8.89: Loss = 0.593567
Epoch 8.90: Loss = 0.722336
Epoch 8.91: Loss = 0.575165
Epoch 8.92: Loss = 0.796646
Epoch 8.93: Loss = 0.619263
Epoch 8.94: Loss = 0.60524
Epoch 8.95: Loss = 0.664169
Epoch 8.96: Loss = 0.722168
Epoch 8.97: Loss = 0.618835
Epoch 8.98: Loss = 0.68338
Epoch 8.99: Loss = 0.850143
Epoch 8.100: Loss = 0.631516
Epoch 8.101: Loss = 0.845245
Epoch 8.102: Loss = 0.771851
Epoch 8.103: Loss = 0.667938
Epoch 8.104: Loss = 0.602982
Epoch 8.105: Loss = 0.634872
Epoch 8.106: Loss = 0.606949
Epoch 8.107: Loss = 0.63652
Epoch 8.108: Loss = 0.703949
Epoch 8.109: Loss = 0.732224
Epoch 8.110: Loss = 0.721024
Epoch 8.111: Loss = 0.548004
Epoch 8.112: Loss = 0.639709
Epoch 8.113: Loss = 0.640198
Epoch 8.114: Loss = 0.725281
Epoch 8.115: Loss = 0.752609
Epoch 8.116: Loss = 0.718018
Epoch 8.117: Loss = 0.669922
Epoch 8.118: Loss = 0.755646
Epoch 8.119: Loss = 0.67363
Epoch 8.120: Loss = 0.710464
TRAIN LOSS = 0.662598
TRAIN ACC = 80.8441 % (48508/60000)
Loss = 0.582352
Loss = 0.705002
Loss = 0.694717
Loss = 0.590073
Loss = 0.582245
Loss = 0.739563
Loss = 0.922638
Loss = 0.74292
Loss = 0.690323
Loss = 0.570465
Loss = 0.876755
Loss = 0.796097
Loss = 0.731613
Loss = 0.713318
Loss = 0.61235
Loss = 0.730179
Loss = 0.639954
Loss = 0.756866
Loss = 0.745346
Loss = 0.70723
TEST LOSS = 0.7065
TEST ACC = 485.078 % (8010/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.574539
Epoch 9.2: Loss = 0.728607
Epoch 9.3: Loss = 0.699768
Epoch 9.4: Loss = 0.712845
Epoch 9.5: Loss = 0.6353
Epoch 9.6: Loss = 0.618179
Epoch 9.7: Loss = 0.697998
Epoch 9.8: Loss = 0.682999
Epoch 9.9: Loss = 0.578903
Epoch 9.10: Loss = 0.605377
Epoch 9.11: Loss = 0.663223
Epoch 9.12: Loss = 0.738998
Epoch 9.13: Loss = 0.653976
Epoch 9.14: Loss = 0.751053
Epoch 9.15: Loss = 0.554825
Epoch 9.16: Loss = 0.645096
Epoch 9.17: Loss = 0.678253
Epoch 9.18: Loss = 0.802536
Epoch 9.19: Loss = 0.738327
Epoch 9.20: Loss = 0.706787
Epoch 9.21: Loss = 0.599014
Epoch 9.22: Loss = 0.611893
Epoch 9.23: Loss = 0.758926
Epoch 9.24: Loss = 0.730026
Epoch 9.25: Loss = 0.630096
Epoch 9.26: Loss = 0.670609
Epoch 9.27: Loss = 0.61795
Epoch 9.28: Loss = 0.771927
Epoch 9.29: Loss = 0.736145
Epoch 9.30: Loss = 0.799377
Epoch 9.31: Loss = 0.673309
Epoch 9.32: Loss = 0.662323
Epoch 9.33: Loss = 0.69516
Epoch 9.34: Loss = 0.494049
Epoch 9.35: Loss = 0.667648
Epoch 9.36: Loss = 0.653549
Epoch 9.37: Loss = 0.612381
Epoch 9.38: Loss = 0.71019
Epoch 9.39: Loss = 0.749161
Epoch 9.40: Loss = 0.726349
Epoch 9.41: Loss = 0.700485
Epoch 9.42: Loss = 0.682358
Epoch 9.43: Loss = 0.703796
Epoch 9.44: Loss = 0.593887
Epoch 9.45: Loss = 0.862671
Epoch 9.46: Loss = 0.692245
Epoch 9.47: Loss = 0.681396
Epoch 9.48: Loss = 0.716599
Epoch 9.49: Loss = 0.788086
Epoch 9.50: Loss = 0.601364
Epoch 9.51: Loss = 0.679611
Epoch 9.52: Loss = 0.690216
Epoch 9.53: Loss = 0.64798
Epoch 9.54: Loss = 0.623474
Epoch 9.55: Loss = 0.681564
Epoch 9.56: Loss = 0.713959
Epoch 9.57: Loss = 0.783005
Epoch 9.58: Loss = 0.720108
Epoch 9.59: Loss = 0.482498
Epoch 9.60: Loss = 0.590393
Epoch 9.61: Loss = 0.630875
Epoch 9.62: Loss = 0.697037
Epoch 9.63: Loss = 0.662109
Epoch 9.64: Loss = 0.787979
Epoch 9.65: Loss = 0.558167
Epoch 9.66: Loss = 0.68512
Epoch 9.67: Loss = 0.659821
Epoch 9.68: Loss = 0.688522
Epoch 9.69: Loss = 0.677063
Epoch 9.70: Loss = 0.735428
Epoch 9.71: Loss = 0.731171
Epoch 9.72: Loss = 0.638596
Epoch 9.73: Loss = 0.696671
Epoch 9.74: Loss = 0.725372
Epoch 9.75: Loss = 0.572098
Epoch 9.76: Loss = 0.721588
Epoch 9.77: Loss = 0.647217
Epoch 9.78: Loss = 0.590012
Epoch 9.79: Loss = 0.62915
Epoch 9.80: Loss = 0.5578
Epoch 9.81: Loss = 0.797836
Epoch 9.82: Loss = 0.800522
Epoch 9.83: Loss = 0.664932
Epoch 9.84: Loss = 0.628571
Epoch 9.85: Loss = 0.825638
Epoch 9.86: Loss = 0.71669
Epoch 9.87: Loss = 0.676102
Epoch 9.88: Loss = 0.739243
Epoch 9.89: Loss = 0.730011
Epoch 9.90: Loss = 0.695526
Epoch 9.91: Loss = 0.752167
Epoch 9.92: Loss = 0.70163
Epoch 9.93: Loss = 0.634338
Epoch 9.94: Loss = 0.628342
Epoch 9.95: Loss = 0.675583
Epoch 9.96: Loss = 0.682037
Epoch 9.97: Loss = 0.586548
Epoch 9.98: Loss = 0.691254
Epoch 9.99: Loss = 0.530701
Epoch 9.100: Loss = 0.639847
Epoch 9.101: Loss = 0.60257
Epoch 9.102: Loss = 0.720032
Epoch 9.103: Loss = 0.569839
Epoch 9.104: Loss = 0.688919
Epoch 9.105: Loss = 0.643356
Epoch 9.106: Loss = 0.695648
Epoch 9.107: Loss = 0.677032
Epoch 9.108: Loss = 0.659668
Epoch 9.109: Loss = 0.707718
Epoch 9.110: Loss = 0.70224
Epoch 9.111: Loss = 0.688705
Epoch 9.112: Loss = 0.628372
Epoch 9.113: Loss = 0.59317
Epoch 9.114: Loss = 0.715927
Epoch 9.115: Loss = 0.710129
Epoch 9.116: Loss = 0.677048
Epoch 9.117: Loss = 0.695984
Epoch 9.118: Loss = 0.857864
Epoch 9.119: Loss = 0.703918
Epoch 9.120: Loss = 0.572739
TRAIN LOSS = 0.677292
TRAIN ACC = 80.9509 % (48573/60000)
Loss = 0.582748
Loss = 0.699219
Loss = 0.688721
Loss = 0.549561
Loss = 0.608047
Loss = 0.754181
Loss = 0.874573
Loss = 0.739944
Loss = 0.674927
Loss = 0.5802
Loss = 0.917328
Loss = 0.800934
Loss = 0.712036
Loss = 0.727615
Loss = 0.619751
Loss = 0.73233
Loss = 0.629059
Loss = 0.762634
Loss = 0.741089
Loss = 0.73494
TEST LOSS = 0.706492
TEST ACC = 485.728 % (8034/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.725418
Epoch 10.2: Loss = 0.590485
Epoch 10.3: Loss = 0.707184
Epoch 10.4: Loss = 0.54451
Epoch 10.5: Loss = 0.634781
Epoch 10.6: Loss = 0.671112
Epoch 10.7: Loss = 0.784576
Epoch 10.8: Loss = 0.66362
Epoch 10.9: Loss = 0.644913
Epoch 10.10: Loss = 0.581207
Epoch 10.11: Loss = 0.622925
Epoch 10.12: Loss = 0.730423
Epoch 10.13: Loss = 0.833191
Epoch 10.14: Loss = 0.738541
Epoch 10.15: Loss = 0.76944
Epoch 10.16: Loss = 0.668137
Epoch 10.17: Loss = 0.538513
Epoch 10.18: Loss = 0.602417
Epoch 10.19: Loss = 0.683884
Epoch 10.20: Loss = 0.598724
Epoch 10.21: Loss = 0.68959
Epoch 10.22: Loss = 0.561523
Epoch 10.23: Loss = 0.72757
Epoch 10.24: Loss = 0.691345
Epoch 10.25: Loss = 0.654388
Epoch 10.26: Loss = 0.768311
Epoch 10.27: Loss = 0.652695
Epoch 10.28: Loss = 0.714142
Epoch 10.29: Loss = 0.685074
Epoch 10.30: Loss = 0.570999
Epoch 10.31: Loss = 0.617966
Epoch 10.32: Loss = 0.875183
Epoch 10.33: Loss = 0.799362
Epoch 10.34: Loss = 0.576218
Epoch 10.35: Loss = 0.675842
Epoch 10.36: Loss = 0.749176
Epoch 10.37: Loss = 0.574661
Epoch 10.38: Loss = 0.793503
Epoch 10.39: Loss = 0.631897
Epoch 10.40: Loss = 0.806519
Epoch 10.41: Loss = 0.550873
Epoch 10.42: Loss = 0.6315
Epoch 10.43: Loss = 0.660614
Epoch 10.44: Loss = 0.612198
Epoch 10.45: Loss = 0.664291
Epoch 10.46: Loss = 0.615829
Epoch 10.47: Loss = 0.778564
Epoch 10.48: Loss = 0.657303
Epoch 10.49: Loss = 0.599869
Epoch 10.50: Loss = 0.75647
Epoch 10.51: Loss = 0.689194
Epoch 10.52: Loss = 0.607391
Epoch 10.53: Loss = 0.583664
Epoch 10.54: Loss = 0.627304
Epoch 10.55: Loss = 0.757858
Epoch 10.56: Loss = 0.626343
Epoch 10.57: Loss = 0.843597
Epoch 10.58: Loss = 0.67923
Epoch 10.59: Loss = 0.723404
Epoch 10.60: Loss = 0.616074
Epoch 10.61: Loss = 0.623001
Epoch 10.62: Loss = 0.617142
Epoch 10.63: Loss = 0.661194
Epoch 10.64: Loss = 0.777969
Epoch 10.65: Loss = 0.681915
Epoch 10.66: Loss = 0.540588
Epoch 10.67: Loss = 0.716309
Epoch 10.68: Loss = 0.688477
Epoch 10.69: Loss = 0.652908
Epoch 10.70: Loss = 0.733902
Epoch 10.71: Loss = 0.809143
Epoch 10.72: Loss = 0.823547
Epoch 10.73: Loss = 0.67514
Epoch 10.74: Loss = 0.598221
Epoch 10.75: Loss = 0.714844
Epoch 10.76: Loss = 0.59317
Epoch 10.77: Loss = 0.762268
Epoch 10.78: Loss = 0.682739
Epoch 10.79: Loss = 0.679092
Epoch 10.80: Loss = 0.734756
Epoch 10.81: Loss = 0.725464
Epoch 10.82: Loss = 0.723206
Epoch 10.83: Loss = 0.685211
Epoch 10.84: Loss = 0.62265
Epoch 10.85: Loss = 0.706863
Epoch 10.86: Loss = 0.672058
Epoch 10.87: Loss = 0.582169
Epoch 10.88: Loss = 0.635956
Epoch 10.89: Loss = 0.57756
Epoch 10.90: Loss = 0.58522
Epoch 10.91: Loss = 0.678467
Epoch 10.92: Loss = 0.716187
Epoch 10.93: Loss = 0.637405
Epoch 10.94: Loss = 0.714005
Epoch 10.95: Loss = 0.658005
Epoch 10.96: Loss = 0.590103
Epoch 10.97: Loss = 0.535934
Epoch 10.98: Loss = 0.700851
Epoch 10.99: Loss = 0.664047
Epoch 10.100: Loss = 0.631454
Epoch 10.101: Loss = 0.623657
Epoch 10.102: Loss = 0.668686
Epoch 10.103: Loss = 0.682083
Epoch 10.104: Loss = 0.659393
Epoch 10.105: Loss = 0.642807
Epoch 10.106: Loss = 0.707748
Epoch 10.107: Loss = 0.745392
Epoch 10.108: Loss = 0.682678
Epoch 10.109: Loss = 0.611359
Epoch 10.110: Loss = 0.591599
Epoch 10.111: Loss = 0.608093
Epoch 10.112: Loss = 0.680191
Epoch 10.113: Loss = 0.664719
Epoch 10.114: Loss = 0.660782
Epoch 10.115: Loss = 0.703369
Epoch 10.116: Loss = 0.716919
Epoch 10.117: Loss = 0.695023
Epoch 10.118: Loss = 0.658997
Epoch 10.119: Loss = 0.610596
Epoch 10.120: Loss = 0.700485
TRAIN LOSS = 0.67131
TRAIN ACC = 81.0898 % (48656/60000)
Loss = 0.592117
Loss = 0.733704
Loss = 0.730835
Loss = 0.566864
Loss = 0.606995
Loss = 0.780029
Loss = 0.880325
Loss = 0.752213
Loss = 0.693832
Loss = 0.596939
Loss = 0.923203
Loss = 0.74321
Loss = 0.731232
Loss = 0.748291
Loss = 0.615158
Loss = 0.716797
Loss = 0.647858
Loss = 0.780258
Loss = 0.741043
Loss = 0.705109
TEST LOSS = 0.7143
TEST ACC = 486.559 % (8027/10000)

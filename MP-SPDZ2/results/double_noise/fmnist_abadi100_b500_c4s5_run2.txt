Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.37886
Epoch 1.2: Loss = 2.28242
Epoch 1.3: Loss = 2.20346
Epoch 1.4: Loss = 2.11137
Epoch 1.5: Loss = 2.04483
Epoch 1.6: Loss = 2.00832
Epoch 1.7: Loss = 1.98267
Epoch 1.8: Loss = 1.88153
Epoch 1.9: Loss = 1.88521
Epoch 1.10: Loss = 1.77757
Epoch 1.11: Loss = 1.7836
Epoch 1.12: Loss = 1.71497
Epoch 1.13: Loss = 1.69183
Epoch 1.14: Loss = 1.61325
Epoch 1.15: Loss = 1.57304
Epoch 1.16: Loss = 1.57118
Epoch 1.17: Loss = 1.47134
Epoch 1.18: Loss = 1.50949
Epoch 1.19: Loss = 1.49429
Epoch 1.20: Loss = 1.44972
Epoch 1.21: Loss = 1.4072
Epoch 1.22: Loss = 1.41357
Epoch 1.23: Loss = 1.33418
Epoch 1.24: Loss = 1.31311
Epoch 1.25: Loss = 1.30223
Epoch 1.26: Loss = 1.29111
Epoch 1.27: Loss = 1.28384
Epoch 1.28: Loss = 1.28125
Epoch 1.29: Loss = 1.17531
Epoch 1.30: Loss = 1.21527
Epoch 1.31: Loss = 1.15552
Epoch 1.32: Loss = 1.15039
Epoch 1.33: Loss = 1.19304
Epoch 1.34: Loss = 1.19531
Epoch 1.35: Loss = 1.11098
Epoch 1.36: Loss = 1.2196
Epoch 1.37: Loss = 1.08932
Epoch 1.38: Loss = 1.1106
Epoch 1.39: Loss = 1.11394
Epoch 1.40: Loss = 1.11263
Epoch 1.41: Loss = 1.10208
Epoch 1.42: Loss = 1.10939
Epoch 1.43: Loss = 1.09738
Epoch 1.44: Loss = 1.06438
Epoch 1.45: Loss = 1.05319
Epoch 1.46: Loss = 1.02658
Epoch 1.47: Loss = 1.0192
Epoch 1.48: Loss = 1.09941
Epoch 1.49: Loss = 0.966522
Epoch 1.50: Loss = 0.978653
Epoch 1.51: Loss = 0.921082
Epoch 1.52: Loss = 0.968094
Epoch 1.53: Loss = 0.977173
Epoch 1.54: Loss = 0.930618
Epoch 1.55: Loss = 1.02299
Epoch 1.56: Loss = 1.02402
Epoch 1.57: Loss = 0.930542
Epoch 1.58: Loss = 0.866623
Epoch 1.59: Loss = 0.918518
Epoch 1.60: Loss = 0.986847
Epoch 1.61: Loss = 0.965927
Epoch 1.62: Loss = 0.862732
Epoch 1.63: Loss = 0.897873
Epoch 1.64: Loss = 0.941605
Epoch 1.65: Loss = 0.883469
Epoch 1.66: Loss = 0.925629
Epoch 1.67: Loss = 0.94632
Epoch 1.68: Loss = 0.928543
Epoch 1.69: Loss = 0.914032
Epoch 1.70: Loss = 0.911865
Epoch 1.71: Loss = 0.854553
Epoch 1.72: Loss = 0.889023
Epoch 1.73: Loss = 0.883057
Epoch 1.74: Loss = 0.88887
Epoch 1.75: Loss = 0.91188
Epoch 1.76: Loss = 0.923447
Epoch 1.77: Loss = 0.967255
Epoch 1.78: Loss = 0.842377
Epoch 1.79: Loss = 0.87616
Epoch 1.80: Loss = 0.872345
Epoch 1.81: Loss = 0.894135
Epoch 1.82: Loss = 0.908401
Epoch 1.83: Loss = 0.793976
Epoch 1.84: Loss = 0.88327
Epoch 1.85: Loss = 0.8414
Epoch 1.86: Loss = 0.806335
Epoch 1.87: Loss = 0.855728
Epoch 1.88: Loss = 0.870102
Epoch 1.89: Loss = 0.811279
Epoch 1.90: Loss = 0.850067
Epoch 1.91: Loss = 0.891525
Epoch 1.92: Loss = 0.83107
Epoch 1.93: Loss = 0.852539
Epoch 1.94: Loss = 0.803772
Epoch 1.95: Loss = 0.761917
Epoch 1.96: Loss = 0.842163
Epoch 1.97: Loss = 0.800995
Epoch 1.98: Loss = 0.848572
Epoch 1.99: Loss = 0.860168
Epoch 1.100: Loss = 0.783005
Epoch 1.101: Loss = 0.78215
Epoch 1.102: Loss = 0.798233
Epoch 1.103: Loss = 0.778091
Epoch 1.104: Loss = 0.822449
Epoch 1.105: Loss = 0.858871
Epoch 1.106: Loss = 0.840042
Epoch 1.107: Loss = 0.799164
Epoch 1.108: Loss = 0.819489
Epoch 1.109: Loss = 0.830505
Epoch 1.110: Loss = 0.763031
Epoch 1.111: Loss = 0.823059
Epoch 1.112: Loss = 0.8246
Epoch 1.113: Loss = 0.840103
Epoch 1.114: Loss = 0.824188
Epoch 1.115: Loss = 0.744507
Epoch 1.116: Loss = 0.80368
Epoch 1.117: Loss = 0.746857
Epoch 1.118: Loss = 0.830063
Epoch 1.119: Loss = 0.748245
Epoch 1.120: Loss = 0.772583
TRAIN LOSS = 1.09909
TRAIN ACC = 62.5488 % (37531/60000)
Loss = 0.729431
Loss = 0.808441
Loss = 0.821548
Loss = 0.74556
Loss = 0.724152
Loss = 0.851608
Loss = 0.897095
Loss = 0.855865
Loss = 0.759689
Loss = 0.730759
Loss = 0.895981
Loss = 0.787109
Loss = 0.811966
Loss = 0.822662
Loss = 0.767029
Loss = 0.82399
Loss = 0.752396
Loss = 0.782471
Loss = 0.835754
Loss = 0.791351
TEST LOSS = 0.799743
TEST ACC = 375.31 % (7160/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.742035
Epoch 2.2: Loss = 0.756622
Epoch 2.3: Loss = 0.828537
Epoch 2.4: Loss = 0.772293
Epoch 2.5: Loss = 0.775024
Epoch 2.6: Loss = 0.813492
Epoch 2.7: Loss = 0.833282
Epoch 2.8: Loss = 0.715439
Epoch 2.9: Loss = 0.758713
Epoch 2.10: Loss = 0.765396
Epoch 2.11: Loss = 0.781754
Epoch 2.12: Loss = 0.808304
Epoch 2.13: Loss = 0.747406
Epoch 2.14: Loss = 0.797989
Epoch 2.15: Loss = 0.753952
Epoch 2.16: Loss = 0.797379
Epoch 2.17: Loss = 0.780853
Epoch 2.18: Loss = 0.722519
Epoch 2.19: Loss = 0.791626
Epoch 2.20: Loss = 0.787674
Epoch 2.21: Loss = 0.80896
Epoch 2.22: Loss = 0.755798
Epoch 2.23: Loss = 0.735474
Epoch 2.24: Loss = 0.687027
Epoch 2.25: Loss = 0.676147
Epoch 2.26: Loss = 0.861526
Epoch 2.27: Loss = 0.747589
Epoch 2.28: Loss = 0.685638
Epoch 2.29: Loss = 0.740524
Epoch 2.30: Loss = 0.813141
Epoch 2.31: Loss = 0.81636
Epoch 2.32: Loss = 0.779709
Epoch 2.33: Loss = 0.803528
Epoch 2.34: Loss = 0.738037
Epoch 2.35: Loss = 0.672913
Epoch 2.36: Loss = 0.789795
Epoch 2.37: Loss = 0.751968
Epoch 2.38: Loss = 0.777817
Epoch 2.39: Loss = 0.821747
Epoch 2.40: Loss = 0.772034
Epoch 2.41: Loss = 0.716782
Epoch 2.42: Loss = 0.744751
Epoch 2.43: Loss = 0.797211
Epoch 2.44: Loss = 0.74707
Epoch 2.45: Loss = 0.764236
Epoch 2.46: Loss = 0.739243
Epoch 2.47: Loss = 0.72998
Epoch 2.48: Loss = 0.738235
Epoch 2.49: Loss = 0.680466
Epoch 2.50: Loss = 0.709717
Epoch 2.51: Loss = 0.815369
Epoch 2.52: Loss = 0.791336
Epoch 2.53: Loss = 0.722519
Epoch 2.54: Loss = 0.768997
Epoch 2.55: Loss = 0.731384
Epoch 2.56: Loss = 0.730072
Epoch 2.57: Loss = 0.809799
Epoch 2.58: Loss = 0.704575
Epoch 2.59: Loss = 0.712158
Epoch 2.60: Loss = 0.719772
Epoch 2.61: Loss = 0.787628
Epoch 2.62: Loss = 0.709641
Epoch 2.63: Loss = 0.714264
Epoch 2.64: Loss = 0.770264
Epoch 2.65: Loss = 0.751404
Epoch 2.66: Loss = 0.731308
Epoch 2.67: Loss = 0.687714
Epoch 2.68: Loss = 0.764664
Epoch 2.69: Loss = 0.734482
Epoch 2.70: Loss = 0.736893
Epoch 2.71: Loss = 0.736359
Epoch 2.72: Loss = 0.697342
Epoch 2.73: Loss = 0.771484
Epoch 2.74: Loss = 0.822952
Epoch 2.75: Loss = 0.762375
Epoch 2.76: Loss = 0.758423
Epoch 2.77: Loss = 0.744141
Epoch 2.78: Loss = 0.721893
Epoch 2.79: Loss = 0.801926
Epoch 2.80: Loss = 0.764923
Epoch 2.81: Loss = 0.605042
Epoch 2.82: Loss = 0.722641
Epoch 2.83: Loss = 0.759109
Epoch 2.84: Loss = 0.688141
Epoch 2.85: Loss = 0.744949
Epoch 2.86: Loss = 0.673172
Epoch 2.87: Loss = 0.659149
Epoch 2.88: Loss = 0.732101
Epoch 2.89: Loss = 0.70195
Epoch 2.90: Loss = 0.765747
Epoch 2.91: Loss = 0.687302
Epoch 2.92: Loss = 0.716476
Epoch 2.93: Loss = 0.737976
Epoch 2.94: Loss = 0.750778
Epoch 2.95: Loss = 0.824722
Epoch 2.96: Loss = 0.637619
Epoch 2.97: Loss = 0.709534
Epoch 2.98: Loss = 0.760574
Epoch 2.99: Loss = 0.727615
Epoch 2.100: Loss = 0.702713
Epoch 2.101: Loss = 0.742416
Epoch 2.102: Loss = 0.771439
Epoch 2.103: Loss = 0.761795
Epoch 2.104: Loss = 0.747421
Epoch 2.105: Loss = 0.746964
Epoch 2.106: Loss = 0.767273
Epoch 2.107: Loss = 0.722534
Epoch 2.108: Loss = 0.670456
Epoch 2.109: Loss = 0.785965
Epoch 2.110: Loss = 0.76384
Epoch 2.111: Loss = 0.725113
Epoch 2.112: Loss = 0.827652
Epoch 2.113: Loss = 0.649109
Epoch 2.114: Loss = 0.696365
Epoch 2.115: Loss = 0.821259
Epoch 2.116: Loss = 0.788773
Epoch 2.117: Loss = 0.679993
Epoch 2.118: Loss = 0.677811
Epoch 2.119: Loss = 0.716019
Epoch 2.120: Loss = 0.669403
TRAIN LOSS = 0.747086
TRAIN ACC = 74.2294 % (44540/60000)
Loss = 0.66951
Loss = 0.749802
Loss = 0.735825
Loss = 0.681168
Loss = 0.662979
Loss = 0.784027
Loss = 0.88472
Loss = 0.792435
Loss = 0.725128
Loss = 0.661209
Loss = 0.826828
Loss = 0.759827
Loss = 0.768127
Loss = 0.766495
Loss = 0.724747
Loss = 0.775314
Loss = 0.697906
Loss = 0.74675
Loss = 0.770203
Loss = 0.721222
TEST LOSS = 0.745211
TEST ACC = 445.399 % (7468/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.82019
Epoch 3.2: Loss = 0.747772
Epoch 3.3: Loss = 0.777664
Epoch 3.4: Loss = 0.681732
Epoch 3.5: Loss = 0.621841
Epoch 3.6: Loss = 0.720413
Epoch 3.7: Loss = 0.661621
Epoch 3.8: Loss = 0.757706
Epoch 3.9: Loss = 0.65419
Epoch 3.10: Loss = 0.752762
Epoch 3.11: Loss = 0.834656
Epoch 3.12: Loss = 0.712402
Epoch 3.13: Loss = 0.615921
Epoch 3.14: Loss = 0.703293
Epoch 3.15: Loss = 0.732498
Epoch 3.16: Loss = 0.727402
Epoch 3.17: Loss = 0.77359
Epoch 3.18: Loss = 0.748291
Epoch 3.19: Loss = 0.733841
Epoch 3.20: Loss = 0.819168
Epoch 3.21: Loss = 0.863861
Epoch 3.22: Loss = 0.651016
Epoch 3.23: Loss = 0.794098
Epoch 3.24: Loss = 0.783386
Epoch 3.25: Loss = 0.687454
Epoch 3.26: Loss = 0.79538
Epoch 3.27: Loss = 0.683548
Epoch 3.28: Loss = 0.76535
Epoch 3.29: Loss = 0.641403
Epoch 3.30: Loss = 0.682083
Epoch 3.31: Loss = 0.625885
Epoch 3.32: Loss = 0.689606
Epoch 3.33: Loss = 0.61058
Epoch 3.34: Loss = 0.768356
Epoch 3.35: Loss = 0.646149
Epoch 3.36: Loss = 0.674362
Epoch 3.37: Loss = 0.756088
Epoch 3.38: Loss = 0.679123
Epoch 3.39: Loss = 0.773666
Epoch 3.40: Loss = 0.652451
Epoch 3.41: Loss = 0.723923
Epoch 3.42: Loss = 0.762817
Epoch 3.43: Loss = 0.732635
Epoch 3.44: Loss = 0.688843
Epoch 3.45: Loss = 0.715088
Epoch 3.46: Loss = 0.704193
Epoch 3.47: Loss = 0.810974
Epoch 3.48: Loss = 0.782272
Epoch 3.49: Loss = 0.716339
Epoch 3.50: Loss = 0.790375
Epoch 3.51: Loss = 0.648209
Epoch 3.52: Loss = 0.68219
Epoch 3.53: Loss = 0.658524
Epoch 3.54: Loss = 0.618271
Epoch 3.55: Loss = 0.730362
Epoch 3.56: Loss = 0.809479
Epoch 3.57: Loss = 0.774582
Epoch 3.58: Loss = 0.704727
Epoch 3.59: Loss = 0.699081
Epoch 3.60: Loss = 0.654099
Epoch 3.61: Loss = 0.793198
Epoch 3.62: Loss = 0.821014
Epoch 3.63: Loss = 0.65773
Epoch 3.64: Loss = 0.723022
Epoch 3.65: Loss = 0.728607
Epoch 3.66: Loss = 0.736252
Epoch 3.67: Loss = 0.644669
Epoch 3.68: Loss = 0.694153
Epoch 3.69: Loss = 0.683533
Epoch 3.70: Loss = 0.725052
Epoch 3.71: Loss = 0.727386
Epoch 3.72: Loss = 0.723389
Epoch 3.73: Loss = 0.77562
Epoch 3.74: Loss = 0.752121
Epoch 3.75: Loss = 0.65274
Epoch 3.76: Loss = 0.774216
Epoch 3.77: Loss = 0.735931
Epoch 3.78: Loss = 0.730835
Epoch 3.79: Loss = 0.649612
Epoch 3.80: Loss = 0.780014
Epoch 3.81: Loss = 0.773819
Epoch 3.82: Loss = 0.666031
Epoch 3.83: Loss = 0.766678
Epoch 3.84: Loss = 0.653809
Epoch 3.85: Loss = 0.722794
Epoch 3.86: Loss = 0.719513
Epoch 3.87: Loss = 0.684189
Epoch 3.88: Loss = 0.684906
Epoch 3.89: Loss = 0.648346
Epoch 3.90: Loss = 0.71344
Epoch 3.91: Loss = 0.845612
Epoch 3.92: Loss = 0.59996
Epoch 3.93: Loss = 0.684174
Epoch 3.94: Loss = 0.624634
Epoch 3.95: Loss = 0.773224
Epoch 3.96: Loss = 0.630188
Epoch 3.97: Loss = 0.760056
Epoch 3.98: Loss = 0.708267
Epoch 3.99: Loss = 0.727707
Epoch 3.100: Loss = 0.718491
Epoch 3.101: Loss = 0.81015
Epoch 3.102: Loss = 0.847549
Epoch 3.103: Loss = 0.579819
Epoch 3.104: Loss = 0.692551
Epoch 3.105: Loss = 0.696793
Epoch 3.106: Loss = 0.675201
Epoch 3.107: Loss = 0.724991
Epoch 3.108: Loss = 0.876251
Epoch 3.109: Loss = 0.809799
Epoch 3.110: Loss = 0.723022
Epoch 3.111: Loss = 0.650696
Epoch 3.112: Loss = 0.692673
Epoch 3.113: Loss = 0.71048
Epoch 3.114: Loss = 0.753265
Epoch 3.115: Loss = 0.600586
Epoch 3.116: Loss = 0.650818
Epoch 3.117: Loss = 0.680862
Epoch 3.118: Loss = 0.72699
Epoch 3.119: Loss = 0.752609
Epoch 3.120: Loss = 0.675415
TRAIN LOSS = 0.718124
TRAIN ACC = 76.1429 % (45688/60000)
Loss = 0.632523
Loss = 0.718857
Loss = 0.676819
Loss = 0.647568
Loss = 0.624786
Loss = 0.727478
Loss = 0.85199
Loss = 0.739182
Loss = 0.715546
Loss = 0.620865
Loss = 0.818253
Loss = 0.758255
Loss = 0.744904
Loss = 0.73671
Loss = 0.717438
Loss = 0.73201
Loss = 0.65242
Loss = 0.703339
Loss = 0.751373
Loss = 0.691132
TEST LOSS = 0.713072
TEST ACC = 456.879 % (7673/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.707733
Epoch 4.2: Loss = 0.671356
Epoch 4.3: Loss = 0.716614
Epoch 4.4: Loss = 0.838791
Epoch 4.5: Loss = 0.670364
Epoch 4.6: Loss = 0.635666
Epoch 4.7: Loss = 0.71666
Epoch 4.8: Loss = 0.74411
Epoch 4.9: Loss = 0.708466
Epoch 4.10: Loss = 0.610748
Epoch 4.11: Loss = 0.701462
Epoch 4.12: Loss = 0.737518
Epoch 4.13: Loss = 0.744827
Epoch 4.14: Loss = 0.592728
Epoch 4.15: Loss = 0.804657
Epoch 4.16: Loss = 0.636276
Epoch 4.17: Loss = 0.80658
Epoch 4.18: Loss = 0.637344
Epoch 4.19: Loss = 0.729294
Epoch 4.20: Loss = 0.654053
Epoch 4.21: Loss = 0.653442
Epoch 4.22: Loss = 0.749191
Epoch 4.23: Loss = 0.585114
Epoch 4.24: Loss = 0.700775
Epoch 4.25: Loss = 0.736176
Epoch 4.26: Loss = 0.666138
Epoch 4.27: Loss = 0.806458
Epoch 4.28: Loss = 0.797104
Epoch 4.29: Loss = 0.783051
Epoch 4.30: Loss = 0.690674
Epoch 4.31: Loss = 0.735489
Epoch 4.32: Loss = 0.735092
Epoch 4.33: Loss = 0.76062
Epoch 4.34: Loss = 0.721207
Epoch 4.35: Loss = 0.572281
Epoch 4.36: Loss = 0.697632
Epoch 4.37: Loss = 0.675446
Epoch 4.38: Loss = 0.656372
Epoch 4.39: Loss = 0.731369
Epoch 4.40: Loss = 0.658752
Epoch 4.41: Loss = 0.679565
Epoch 4.42: Loss = 0.811539
Epoch 4.43: Loss = 0.690048
Epoch 4.44: Loss = 0.752213
Epoch 4.45: Loss = 0.721985
Epoch 4.46: Loss = 0.699341
Epoch 4.47: Loss = 0.708054
Epoch 4.48: Loss = 0.75029
Epoch 4.49: Loss = 0.735687
Epoch 4.50: Loss = 0.666534
Epoch 4.51: Loss = 0.620758
Epoch 4.52: Loss = 0.650208
Epoch 4.53: Loss = 0.685104
Epoch 4.54: Loss = 0.714615
Epoch 4.55: Loss = 0.684006
Epoch 4.56: Loss = 0.526108
Epoch 4.57: Loss = 0.810287
Epoch 4.58: Loss = 0.691132
Epoch 4.59: Loss = 0.698257
Epoch 4.60: Loss = 0.759277
Epoch 4.61: Loss = 0.668121
Epoch 4.62: Loss = 0.67244
Epoch 4.63: Loss = 0.727722
Epoch 4.64: Loss = 0.773926
Epoch 4.65: Loss = 0.745407
Epoch 4.66: Loss = 0.734772
Epoch 4.67: Loss = 0.608414
Epoch 4.68: Loss = 0.816025
Epoch 4.69: Loss = 0.752106
Epoch 4.70: Loss = 0.753891
Epoch 4.71: Loss = 0.62233
Epoch 4.72: Loss = 0.799332
Epoch 4.73: Loss = 0.737656
Epoch 4.74: Loss = 0.649445
Epoch 4.75: Loss = 0.771072
Epoch 4.76: Loss = 0.686584
Epoch 4.77: Loss = 0.621674
Epoch 4.78: Loss = 0.696747
Epoch 4.79: Loss = 0.715057
Epoch 4.80: Loss = 0.653229
Epoch 4.81: Loss = 0.801559
Epoch 4.82: Loss = 0.692398
Epoch 4.83: Loss = 0.708572
Epoch 4.84: Loss = 0.646469
Epoch 4.85: Loss = 0.689407
Epoch 4.86: Loss = 0.764221
Epoch 4.87: Loss = 0.678192
Epoch 4.88: Loss = 0.670837
Epoch 4.89: Loss = 0.79422
Epoch 4.90: Loss = 0.700378
Epoch 4.91: Loss = 0.658218
Epoch 4.92: Loss = 0.764633
Epoch 4.93: Loss = 0.783371
Epoch 4.94: Loss = 0.723602
Epoch 4.95: Loss = 0.72435
Epoch 4.96: Loss = 0.762268
Epoch 4.97: Loss = 0.782089
Epoch 4.98: Loss = 0.889481
Epoch 4.99: Loss = 0.780548
Epoch 4.100: Loss = 0.677704
Epoch 4.101: Loss = 0.75827
Epoch 4.102: Loss = 0.635239
Epoch 4.103: Loss = 0.734665
Epoch 4.104: Loss = 0.738113
Epoch 4.105: Loss = 0.648911
Epoch 4.106: Loss = 0.628571
Epoch 4.107: Loss = 0.719742
Epoch 4.108: Loss = 0.705521
Epoch 4.109: Loss = 0.643707
Epoch 4.110: Loss = 0.702301
Epoch 4.111: Loss = 0.701889
Epoch 4.112: Loss = 0.657501
Epoch 4.113: Loss = 0.71373
Epoch 4.114: Loss = 0.787613
Epoch 4.115: Loss = 0.689514
Epoch 4.116: Loss = 0.661118
Epoch 4.117: Loss = 0.826202
Epoch 4.118: Loss = 0.721451
Epoch 4.119: Loss = 0.70079
Epoch 4.120: Loss = 0.717361
TRAIN LOSS = 0.710205
TRAIN ACC = 77.4796 % (46490/60000)
Loss = 0.641922
Loss = 0.73558
Loss = 0.672592
Loss = 0.640533
Loss = 0.665741
Loss = 0.76123
Loss = 0.884872
Loss = 0.797272
Loss = 0.714203
Loss = 0.605621
Loss = 0.879791
Loss = 0.781143
Loss = 0.747223
Loss = 0.742249
Loss = 0.732376
Loss = 0.757553
Loss = 0.632751
Loss = 0.730194
Loss = 0.767685
Loss = 0.690842
TEST LOSS = 0.729069
TEST ACC = 464.899 % (7730/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.656708
Epoch 5.2: Loss = 0.750153
Epoch 5.3: Loss = 0.644165
Epoch 5.4: Loss = 0.758636
Epoch 5.5: Loss = 0.674347
Epoch 5.6: Loss = 0.683502
Epoch 5.7: Loss = 0.730713
Epoch 5.8: Loss = 0.723999
Epoch 5.9: Loss = 0.793655
Epoch 5.10: Loss = 0.627029
Epoch 5.11: Loss = 0.733902
Epoch 5.12: Loss = 0.707458
Epoch 5.13: Loss = 0.866318
Epoch 5.14: Loss = 0.728271
Epoch 5.15: Loss = 0.730804
Epoch 5.16: Loss = 0.746658
Epoch 5.17: Loss = 0.601974
Epoch 5.18: Loss = 0.686218
Epoch 5.19: Loss = 0.681686
Epoch 5.20: Loss = 0.726517
Epoch 5.21: Loss = 0.662827
Epoch 5.22: Loss = 0.762833
Epoch 5.23: Loss = 0.638504
Epoch 5.24: Loss = 0.674576
Epoch 5.25: Loss = 0.689316
Epoch 5.26: Loss = 0.808243
Epoch 5.27: Loss = 0.691757
Epoch 5.28: Loss = 0.713562
Epoch 5.29: Loss = 0.758514
Epoch 5.30: Loss = 0.651031
Epoch 5.31: Loss = 0.693237
Epoch 5.32: Loss = 0.692947
Epoch 5.33: Loss = 0.677475
Epoch 5.34: Loss = 0.699783
Epoch 5.35: Loss = 0.773148
Epoch 5.36: Loss = 0.753662
Epoch 5.37: Loss = 0.70961
Epoch 5.38: Loss = 0.65654
Epoch 5.39: Loss = 0.694656
Epoch 5.40: Loss = 0.779175
Epoch 5.41: Loss = 0.756226
Epoch 5.42: Loss = 0.699615
Epoch 5.43: Loss = 0.62825
Epoch 5.44: Loss = 0.618423
Epoch 5.45: Loss = 0.753647
Epoch 5.46: Loss = 0.723114
Epoch 5.47: Loss = 0.67627
Epoch 5.48: Loss = 0.686508
Epoch 5.49: Loss = 0.555618
Epoch 5.50: Loss = 0.73909
Epoch 5.51: Loss = 0.757935
Epoch 5.52: Loss = 0.682648
Epoch 5.53: Loss = 0.801132
Epoch 5.54: Loss = 0.745468
Epoch 5.55: Loss = 0.712631
Epoch 5.56: Loss = 0.730988
Epoch 5.57: Loss = 0.730362
Epoch 5.58: Loss = 0.713379
Epoch 5.59: Loss = 0.61203
Epoch 5.60: Loss = 0.744843
Epoch 5.61: Loss = 0.593552
Epoch 5.62: Loss = 0.641907
Epoch 5.63: Loss = 0.810135
Epoch 5.64: Loss = 0.653931
Epoch 5.65: Loss = 0.797775
Epoch 5.66: Loss = 0.655411
Epoch 5.67: Loss = 0.745514
Epoch 5.68: Loss = 0.744125
Epoch 5.69: Loss = 0.694244
Epoch 5.70: Loss = 0.786713
Epoch 5.71: Loss = 0.7435
Epoch 5.72: Loss = 0.690506
Epoch 5.73: Loss = 0.712997
Epoch 5.74: Loss = 0.796692
Epoch 5.75: Loss = 0.532532
Epoch 5.76: Loss = 0.794983
Epoch 5.77: Loss = 0.571991
Epoch 5.78: Loss = 0.659882
Epoch 5.79: Loss = 0.725143
Epoch 5.80: Loss = 0.641785
Epoch 5.81: Loss = 0.755417
Epoch 5.82: Loss = 0.766159
Epoch 5.83: Loss = 0.760773
Epoch 5.84: Loss = 0.673828
Epoch 5.85: Loss = 0.605545
Epoch 5.86: Loss = 0.744278
Epoch 5.87: Loss = 0.709747
Epoch 5.88: Loss = 0.664413
Epoch 5.89: Loss = 0.84137
Epoch 5.90: Loss = 0.773895
Epoch 5.91: Loss = 0.724274
Epoch 5.92: Loss = 0.70462
Epoch 5.93: Loss = 0.699127
Epoch 5.94: Loss = 0.69809
Epoch 5.95: Loss = 0.61618
Epoch 5.96: Loss = 0.770706
Epoch 5.97: Loss = 0.904007
Epoch 5.98: Loss = 0.702301
Epoch 5.99: Loss = 0.705338
Epoch 5.100: Loss = 0.693329
Epoch 5.101: Loss = 0.661407
Epoch 5.102: Loss = 0.691635
Epoch 5.103: Loss = 0.654144
Epoch 5.104: Loss = 0.752914
Epoch 5.105: Loss = 0.661163
Epoch 5.106: Loss = 0.810776
Epoch 5.107: Loss = 0.804565
Epoch 5.108: Loss = 0.777603
Epoch 5.109: Loss = 0.627396
Epoch 5.110: Loss = 0.806137
Epoch 5.111: Loss = 0.756653
Epoch 5.112: Loss = 0.785461
Epoch 5.113: Loss = 0.666397
Epoch 5.114: Loss = 0.726685
Epoch 5.115: Loss = 0.750763
Epoch 5.116: Loss = 0.693756
Epoch 5.117: Loss = 0.585236
Epoch 5.118: Loss = 0.719391
Epoch 5.119: Loss = 0.656693
Epoch 5.120: Loss = 0.706741
TRAIN LOSS = 0.710876
TRAIN ACC = 78.0579 % (46837/60000)
Loss = 0.608032
Loss = 0.727951
Loss = 0.671249
Loss = 0.610474
Loss = 0.679535
Loss = 0.781738
Loss = 0.896713
Loss = 0.786606
Loss = 0.738098
Loss = 0.610657
Loss = 0.896988
Loss = 0.815948
Loss = 0.7285
Loss = 0.754456
Loss = 0.736893
Loss = 0.744888
Loss = 0.650055
Loss = 0.754349
Loss = 0.83342
Loss = 0.726837
TEST LOSS = 0.737669
TEST ACC = 468.369 % (7741/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.635315
Epoch 6.2: Loss = 0.766953
Epoch 6.3: Loss = 0.811264
Epoch 6.4: Loss = 0.680878
Epoch 6.5: Loss = 0.786469
Epoch 6.6: Loss = 0.673477
Epoch 6.7: Loss = 0.765915
Epoch 6.8: Loss = 0.716782
Epoch 6.9: Loss = 0.699631
Epoch 6.10: Loss = 0.711502
Epoch 6.11: Loss = 0.726379
Epoch 6.12: Loss = 0.69104
Epoch 6.13: Loss = 0.654419
Epoch 6.14: Loss = 0.752563
Epoch 6.15: Loss = 0.688934
Epoch 6.16: Loss = 0.791092
Epoch 6.17: Loss = 0.620499
Epoch 6.18: Loss = 0.733734
Epoch 6.19: Loss = 0.73175
Epoch 6.20: Loss = 0.784424
Epoch 6.21: Loss = 0.612259
Epoch 6.22: Loss = 0.774338
Epoch 6.23: Loss = 0.679367
Epoch 6.24: Loss = 0.746429
Epoch 6.25: Loss = 0.565872
Epoch 6.26: Loss = 0.581436
Epoch 6.27: Loss = 0.76561
Epoch 6.28: Loss = 0.799973
Epoch 6.29: Loss = 0.727585
Epoch 6.30: Loss = 0.700043
Epoch 6.31: Loss = 0.789642
Epoch 6.32: Loss = 0.776382
Epoch 6.33: Loss = 0.675293
Epoch 6.34: Loss = 0.620651
Epoch 6.35: Loss = 0.632767
Epoch 6.36: Loss = 0.720963
Epoch 6.37: Loss = 0.800247
Epoch 6.38: Loss = 0.789932
Epoch 6.39: Loss = 0.680695
Epoch 6.40: Loss = 0.742401
Epoch 6.41: Loss = 0.670715
Epoch 6.42: Loss = 0.67128
Epoch 6.43: Loss = 0.587967
Epoch 6.44: Loss = 0.770264
Epoch 6.45: Loss = 0.706757
Epoch 6.46: Loss = 0.709381
Epoch 6.47: Loss = 0.728027
Epoch 6.48: Loss = 0.686172
Epoch 6.49: Loss = 0.688721
Epoch 6.50: Loss = 0.766663
Epoch 6.51: Loss = 0.775955
Epoch 6.52: Loss = 0.753525
Epoch 6.53: Loss = 0.739853
Epoch 6.54: Loss = 0.744736
Epoch 6.55: Loss = 0.717422
Epoch 6.56: Loss = 0.78302
Epoch 6.57: Loss = 0.767899
Epoch 6.58: Loss = 0.8125
Epoch 6.59: Loss = 0.689667
Epoch 6.60: Loss = 0.690887
Epoch 6.61: Loss = 0.661423
Epoch 6.62: Loss = 0.792389
Epoch 6.63: Loss = 0.665039
Epoch 6.64: Loss = 0.616669
Epoch 6.65: Loss = 0.713608
Epoch 6.66: Loss = 0.841644
Epoch 6.67: Loss = 0.72937
Epoch 6.68: Loss = 0.790695
Epoch 6.69: Loss = 0.656631
Epoch 6.70: Loss = 0.618042
Epoch 6.71: Loss = 0.67569
Epoch 6.72: Loss = 0.711868
Epoch 6.73: Loss = 0.752716
Epoch 6.74: Loss = 0.695465
Epoch 6.75: Loss = 0.712646
Epoch 6.76: Loss = 0.71051
Epoch 6.77: Loss = 0.725662
Epoch 6.78: Loss = 0.756256
Epoch 6.79: Loss = 0.675735
Epoch 6.80: Loss = 0.777939
Epoch 6.81: Loss = 0.651947
Epoch 6.82: Loss = 0.641342
Epoch 6.83: Loss = 0.74147
Epoch 6.84: Loss = 0.678116
Epoch 6.85: Loss = 0.656723
Epoch 6.86: Loss = 0.708633
Epoch 6.87: Loss = 0.571915
Epoch 6.88: Loss = 0.604279
Epoch 6.89: Loss = 0.67366
Epoch 6.90: Loss = 0.670197
Epoch 6.91: Loss = 0.840744
Epoch 6.92: Loss = 0.774704
Epoch 6.93: Loss = 0.624954
Epoch 6.94: Loss = 0.733047
Epoch 6.95: Loss = 0.753098
Epoch 6.96: Loss = 0.64212
Epoch 6.97: Loss = 0.670975
Epoch 6.98: Loss = 0.649643
Epoch 6.99: Loss = 0.676682
Epoch 6.100: Loss = 0.653809
Epoch 6.101: Loss = 0.893616
Epoch 6.102: Loss = 0.730988
Epoch 6.103: Loss = 0.6642
Epoch 6.104: Loss = 0.604004
Epoch 6.105: Loss = 0.733566
Epoch 6.106: Loss = 0.64357
Epoch 6.107: Loss = 0.755569
Epoch 6.108: Loss = 0.722015
Epoch 6.109: Loss = 0.667343
Epoch 6.110: Loss = 0.704575
Epoch 6.111: Loss = 0.733444
Epoch 6.112: Loss = 0.626465
Epoch 6.113: Loss = 0.698303
Epoch 6.114: Loss = 0.810318
Epoch 6.115: Loss = 0.704712
Epoch 6.116: Loss = 0.685516
Epoch 6.117: Loss = 0.779663
Epoch 6.118: Loss = 0.73909
Epoch 6.119: Loss = 0.667252
Epoch 6.120: Loss = 0.676956
TRAIN LOSS = 0.710052
TRAIN ACC = 78.4363 % (47064/60000)
Loss = 0.62326
Loss = 0.779099
Loss = 0.670364
Loss = 0.647232
Loss = 0.687225
Loss = 0.777954
Loss = 0.920609
Loss = 0.755142
Loss = 0.757706
Loss = 0.625732
Loss = 0.901825
Loss = 0.801407
Loss = 0.734543
Loss = 0.724274
Loss = 0.733459
Loss = 0.746353
Loss = 0.66272
Loss = 0.737976
Loss = 0.805099
Loss = 0.720551
TEST LOSS = 0.740626
TEST ACC = 470.639 % (7768/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.653595
Epoch 7.2: Loss = 0.65773
Epoch 7.3: Loss = 0.779526
Epoch 7.4: Loss = 0.647766
Epoch 7.5: Loss = 0.760117
Epoch 7.6: Loss = 0.684799
Epoch 7.7: Loss = 0.661774
Epoch 7.8: Loss = 0.58812
Epoch 7.9: Loss = 0.873108
Epoch 7.10: Loss = 0.651962
Epoch 7.11: Loss = 0.821655
Epoch 7.12: Loss = 0.551773
Epoch 7.13: Loss = 0.662857
Epoch 7.14: Loss = 0.716522
Epoch 7.15: Loss = 0.661713
Epoch 7.16: Loss = 0.660095
Epoch 7.17: Loss = 0.629227
Epoch 7.18: Loss = 0.70697
Epoch 7.19: Loss = 0.654099
Epoch 7.20: Loss = 0.700577
Epoch 7.21: Loss = 0.623917
Epoch 7.22: Loss = 0.70343
Epoch 7.23: Loss = 0.878052
Epoch 7.24: Loss = 0.639389
Epoch 7.25: Loss = 0.788147
Epoch 7.26: Loss = 0.652374
Epoch 7.27: Loss = 0.617325
Epoch 7.28: Loss = 0.709534
Epoch 7.29: Loss = 0.730316
Epoch 7.30: Loss = 0.77951
Epoch 7.31: Loss = 0.712204
Epoch 7.32: Loss = 0.725693
Epoch 7.33: Loss = 0.630356
Epoch 7.34: Loss = 0.663055
Epoch 7.35: Loss = 0.862167
Epoch 7.36: Loss = 0.69426
Epoch 7.37: Loss = 0.708298
Epoch 7.38: Loss = 0.793564
Epoch 7.39: Loss = 0.816025
Epoch 7.40: Loss = 0.607712
Epoch 7.41: Loss = 0.820923
Epoch 7.42: Loss = 0.625961
Epoch 7.43: Loss = 0.773514
Epoch 7.44: Loss = 0.77655
Epoch 7.45: Loss = 0.650497
Epoch 7.46: Loss = 0.710617
Epoch 7.47: Loss = 0.555954
Epoch 7.48: Loss = 0.642044
Epoch 7.49: Loss = 0.76799
Epoch 7.50: Loss = 0.733749
Epoch 7.51: Loss = 0.670486
Epoch 7.52: Loss = 0.743713
Epoch 7.53: Loss = 0.79715
Epoch 7.54: Loss = 0.707031
Epoch 7.55: Loss = 0.646011
Epoch 7.56: Loss = 0.738968
Epoch 7.57: Loss = 0.669769
Epoch 7.58: Loss = 0.678238
Epoch 7.59: Loss = 0.824875
Epoch 7.60: Loss = 0.796066
Epoch 7.61: Loss = 0.821274
Epoch 7.62: Loss = 0.797455
Epoch 7.63: Loss = 0.865753
Epoch 7.64: Loss = 0.667633
Epoch 7.65: Loss = 0.674484
Epoch 7.66: Loss = 0.763641
Epoch 7.67: Loss = 0.830841
Epoch 7.68: Loss = 0.691208
Epoch 7.69: Loss = 0.678436
Epoch 7.70: Loss = 0.680252
Epoch 7.71: Loss = 0.750259
Epoch 7.72: Loss = 0.758682
Epoch 7.73: Loss = 0.782074
Epoch 7.74: Loss = 0.802414
Epoch 7.75: Loss = 0.696564
Epoch 7.76: Loss = 0.727692
Epoch 7.77: Loss = 0.854141
Epoch 7.78: Loss = 0.737335
Epoch 7.79: Loss = 0.685135
Epoch 7.80: Loss = 0.753967
Epoch 7.81: Loss = 0.682922
Epoch 7.82: Loss = 0.654495
Epoch 7.83: Loss = 0.735489
Epoch 7.84: Loss = 0.735626
Epoch 7.85: Loss = 0.653244
Epoch 7.86: Loss = 0.804306
Epoch 7.87: Loss = 0.694336
Epoch 7.88: Loss = 0.769928
Epoch 7.89: Loss = 0.823334
Epoch 7.90: Loss = 0.822083
Epoch 7.91: Loss = 0.65712
Epoch 7.92: Loss = 0.628296
Epoch 7.93: Loss = 0.861557
Epoch 7.94: Loss = 0.768921
Epoch 7.95: Loss = 0.813965
Epoch 7.96: Loss = 0.736252
Epoch 7.97: Loss = 0.687881
Epoch 7.98: Loss = 0.833618
Epoch 7.99: Loss = 0.661682
Epoch 7.100: Loss = 0.742462
Epoch 7.101: Loss = 0.742935
Epoch 7.102: Loss = 0.742767
Epoch 7.103: Loss = 0.735092
Epoch 7.104: Loss = 0.754944
Epoch 7.105: Loss = 0.657089
Epoch 7.106: Loss = 0.872772
Epoch 7.107: Loss = 0.763748
Epoch 7.108: Loss = 0.719543
Epoch 7.109: Loss = 0.789581
Epoch 7.110: Loss = 0.788422
Epoch 7.111: Loss = 0.685837
Epoch 7.112: Loss = 0.794479
Epoch 7.113: Loss = 0.756882
Epoch 7.114: Loss = 0.793182
Epoch 7.115: Loss = 0.845627
Epoch 7.116: Loss = 0.691345
Epoch 7.117: Loss = 0.666962
Epoch 7.118: Loss = 0.699493
Epoch 7.119: Loss = 0.689285
Epoch 7.120: Loss = 0.92041
TRAIN LOSS = 0.727402
TRAIN ACC = 78.5873 % (47155/60000)
Loss = 0.647705
Loss = 0.832199
Loss = 0.672623
Loss = 0.682388
Loss = 0.680634
Loss = 0.830368
Loss = 0.958862
Loss = 0.799667
Loss = 0.791626
Loss = 0.666046
Loss = 0.980484
Loss = 0.843765
Loss = 0.777771
Loss = 0.725189
Loss = 0.736847
Loss = 0.818436
Loss = 0.702484
Loss = 0.826569
Loss = 0.826279
Loss = 0.730621
TEST LOSS = 0.776528
TEST ACC = 471.548 % (7734/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.720306
Epoch 8.2: Loss = 0.875854
Epoch 8.3: Loss = 0.852615
Epoch 8.4: Loss = 0.822983
Epoch 8.5: Loss = 0.660553
Epoch 8.6: Loss = 0.625137
Epoch 8.7: Loss = 0.714417
Epoch 8.8: Loss = 0.747055
Epoch 8.9: Loss = 0.762955
Epoch 8.10: Loss = 0.732224
Epoch 8.11: Loss = 0.737381
Epoch 8.12: Loss = 0.751556
Epoch 8.13: Loss = 0.838287
Epoch 8.14: Loss = 0.748352
Epoch 8.15: Loss = 0.645218
Epoch 8.16: Loss = 0.600708
Epoch 8.17: Loss = 0.684998
Epoch 8.18: Loss = 0.766571
Epoch 8.19: Loss = 0.880844
Epoch 8.20: Loss = 0.676025
Epoch 8.21: Loss = 0.787094
Epoch 8.22: Loss = 0.646713
Epoch 8.23: Loss = 0.778687
Epoch 8.24: Loss = 0.656219
Epoch 8.25: Loss = 0.757568
Epoch 8.26: Loss = 0.788544
Epoch 8.27: Loss = 0.764679
Epoch 8.28: Loss = 0.843201
Epoch 8.29: Loss = 0.884201
Epoch 8.30: Loss = 0.785614
Epoch 8.31: Loss = 0.703339
Epoch 8.32: Loss = 0.886749
Epoch 8.33: Loss = 0.691162
Epoch 8.34: Loss = 0.881897
Epoch 8.35: Loss = 0.746826
Epoch 8.36: Loss = 0.830902
Epoch 8.37: Loss = 0.65358
Epoch 8.38: Loss = 0.689682
Epoch 8.39: Loss = 0.652328
Epoch 8.40: Loss = 0.762085
Epoch 8.41: Loss = 0.647919
Epoch 8.42: Loss = 0.704453
Epoch 8.43: Loss = 0.712906
Epoch 8.44: Loss = 0.751236
Epoch 8.45: Loss = 0.619995
Epoch 8.46: Loss = 0.744141
Epoch 8.47: Loss = 0.817429
Epoch 8.48: Loss = 0.731186
Epoch 8.49: Loss = 0.687958
Epoch 8.50: Loss = 0.824341
Epoch 8.51: Loss = 0.841217
Epoch 8.52: Loss = 0.767166
Epoch 8.53: Loss = 0.718903
Epoch 8.54: Loss = 0.71994
Epoch 8.55: Loss = 0.804962
Epoch 8.56: Loss = 0.856049
Epoch 8.57: Loss = 0.866913
Epoch 8.58: Loss = 0.831314
Epoch 8.59: Loss = 0.892258
Epoch 8.60: Loss = 0.569351
Epoch 8.61: Loss = 0.77063
Epoch 8.62: Loss = 0.826324
Epoch 8.63: Loss = 0.900146
Epoch 8.64: Loss = 0.716034
Epoch 8.65: Loss = 0.748886
Epoch 8.66: Loss = 0.877899
Epoch 8.67: Loss = 0.888031
Epoch 8.68: Loss = 0.675385
Epoch 8.69: Loss = 0.679703
Epoch 8.70: Loss = 0.805771
Epoch 8.71: Loss = 0.946976
Epoch 8.72: Loss = 0.736465
Epoch 8.73: Loss = 0.799332
Epoch 8.74: Loss = 0.725449
Epoch 8.75: Loss = 0.726791
Epoch 8.76: Loss = 0.590256
Epoch 8.77: Loss = 0.695511
Epoch 8.78: Loss = 0.747009
Epoch 8.79: Loss = 0.883133
Epoch 8.80: Loss = 0.837021
Epoch 8.81: Loss = 0.751251
Epoch 8.82: Loss = 0.744705
Epoch 8.83: Loss = 0.673141
Epoch 8.84: Loss = 0.738647
Epoch 8.85: Loss = 0.833191
Epoch 8.86: Loss = 0.795364
Epoch 8.87: Loss = 0.741257
Epoch 8.88: Loss = 0.770508
Epoch 8.89: Loss = 0.638367
Epoch 8.90: Loss = 0.637939
Epoch 8.91: Loss = 0.740707
Epoch 8.92: Loss = 0.75528
Epoch 8.93: Loss = 0.703873
Epoch 8.94: Loss = 0.852982
Epoch 8.95: Loss = 0.67244
Epoch 8.96: Loss = 0.723404
Epoch 8.97: Loss = 0.685379
Epoch 8.98: Loss = 0.725983
Epoch 8.99: Loss = 0.702454
Epoch 8.100: Loss = 0.808289
Epoch 8.101: Loss = 0.759613
Epoch 8.102: Loss = 0.75856
Epoch 8.103: Loss = 0.796143
Epoch 8.104: Loss = 0.758698
Epoch 8.105: Loss = 0.802979
Epoch 8.106: Loss = 0.784622
Epoch 8.107: Loss = 0.711853
Epoch 8.108: Loss = 0.85495
Epoch 8.109: Loss = 0.718536
Epoch 8.110: Loss = 0.883667
Epoch 8.111: Loss = 0.724701
Epoch 8.112: Loss = 0.776215
Epoch 8.113: Loss = 0.748734
Epoch 8.114: Loss = 0.642258
Epoch 8.115: Loss = 0.856323
Epoch 8.116: Loss = 0.699646
Epoch 8.117: Loss = 0.732895
Epoch 8.118: Loss = 0.847305
Epoch 8.119: Loss = 0.688599
Epoch 8.120: Loss = 0.746109
TRAIN LOSS = 0.75592
TRAIN ACC = 78.6942 % (47218/60000)
Loss = 0.66481
Loss = 0.854401
Loss = 0.698029
Loss = 0.706436
Loss = 0.709686
Loss = 0.859894
Loss = 1.01147
Loss = 0.850266
Loss = 0.827286
Loss = 0.696594
Loss = 0.995514
Loss = 0.896515
Loss = 0.786148
Loss = 0.732391
Loss = 0.792328
Loss = 0.836594
Loss = 0.768616
Loss = 0.849899
Loss = 0.876022
Loss = 0.787933
TEST LOSS = 0.810042
TEST ACC = 472.179 % (7740/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.828445
Epoch 9.2: Loss = 0.775482
Epoch 9.3: Loss = 0.843994
Epoch 9.4: Loss = 0.740402
Epoch 9.5: Loss = 0.687424
Epoch 9.6: Loss = 0.676483
Epoch 9.7: Loss = 0.773926
Epoch 9.8: Loss = 0.793182
Epoch 9.9: Loss = 0.924301
Epoch 9.10: Loss = 0.861328
Epoch 9.11: Loss = 0.833817
Epoch 9.12: Loss = 0.837265
Epoch 9.13: Loss = 0.724625
Epoch 9.14: Loss = 0.81456
Epoch 9.15: Loss = 0.785919
Epoch 9.16: Loss = 0.72171
Epoch 9.17: Loss = 0.749969
Epoch 9.18: Loss = 1.02058
Epoch 9.19: Loss = 0.785309
Epoch 9.20: Loss = 0.797958
Epoch 9.21: Loss = 0.788467
Epoch 9.22: Loss = 0.792007
Epoch 9.23: Loss = 0.79422
Epoch 9.24: Loss = 0.798065
Epoch 9.25: Loss = 0.78891
Epoch 9.26: Loss = 0.73967
Epoch 9.27: Loss = 0.841766
Epoch 9.28: Loss = 0.70253
Epoch 9.29: Loss = 0.888641
Epoch 9.30: Loss = 0.762543
Epoch 9.31: Loss = 0.717072
Epoch 9.32: Loss = 0.705368
Epoch 9.33: Loss = 0.843201
Epoch 9.34: Loss = 0.689072
Epoch 9.35: Loss = 0.737518
Epoch 9.36: Loss = 0.590668
Epoch 9.37: Loss = 0.822205
Epoch 9.38: Loss = 0.83197
Epoch 9.39: Loss = 0.723343
Epoch 9.40: Loss = 0.714172
Epoch 9.41: Loss = 0.835999
Epoch 9.42: Loss = 0.750702
Epoch 9.43: Loss = 0.644104
Epoch 9.44: Loss = 0.808731
Epoch 9.45: Loss = 0.78981
Epoch 9.46: Loss = 0.746323
Epoch 9.47: Loss = 0.615738
Epoch 9.48: Loss = 0.679001
Epoch 9.49: Loss = 0.823883
Epoch 9.50: Loss = 0.825058
Epoch 9.51: Loss = 0.669815
Epoch 9.52: Loss = 0.796616
Epoch 9.53: Loss = 0.847412
Epoch 9.54: Loss = 0.854019
Epoch 9.55: Loss = 0.708908
Epoch 9.56: Loss = 0.831894
Epoch 9.57: Loss = 0.744003
Epoch 9.58: Loss = 0.755051
Epoch 9.59: Loss = 0.703476
Epoch 9.60: Loss = 0.88829
Epoch 9.61: Loss = 0.705032
Epoch 9.62: Loss = 0.739441
Epoch 9.63: Loss = 0.662292
Epoch 9.64: Loss = 0.864182
Epoch 9.65: Loss = 0.719315
Epoch 9.66: Loss = 0.848694
Epoch 9.67: Loss = 0.759079
Epoch 9.68: Loss = 0.836594
Epoch 9.69: Loss = 0.691116
Epoch 9.70: Loss = 0.790848
Epoch 9.71: Loss = 0.699997
Epoch 9.72: Loss = 0.896393
Epoch 9.73: Loss = 0.659775
Epoch 9.74: Loss = 0.758408
Epoch 9.75: Loss = 0.79425
Epoch 9.76: Loss = 0.668182
Epoch 9.77: Loss = 0.798264
Epoch 9.78: Loss = 0.897018
Epoch 9.79: Loss = 0.766998
Epoch 9.80: Loss = 0.692764
Epoch 9.81: Loss = 0.749619
Epoch 9.82: Loss = 0.642807
Epoch 9.83: Loss = 0.782181
Epoch 9.84: Loss = 0.672653
Epoch 9.85: Loss = 0.773361
Epoch 9.86: Loss = 0.775009
Epoch 9.87: Loss = 0.637787
Epoch 9.88: Loss = 0.693848
Epoch 9.89: Loss = 0.646149
Epoch 9.90: Loss = 0.64769
Epoch 9.91: Loss = 0.795258
Epoch 9.92: Loss = 0.874725
Epoch 9.93: Loss = 0.705139
Epoch 9.94: Loss = 0.752579
Epoch 9.95: Loss = 0.706146
Epoch 9.96: Loss = 0.879623
Epoch 9.97: Loss = 0.842255
Epoch 9.98: Loss = 0.81459
Epoch 9.99: Loss = 0.724289
Epoch 9.100: Loss = 0.944748
Epoch 9.101: Loss = 0.69046
Epoch 9.102: Loss = 0.773819
Epoch 9.103: Loss = 0.793259
Epoch 9.104: Loss = 0.748688
Epoch 9.105: Loss = 0.751511
Epoch 9.106: Loss = 0.771179
Epoch 9.107: Loss = 0.922699
Epoch 9.108: Loss = 0.72966
Epoch 9.109: Loss = 0.807892
Epoch 9.110: Loss = 0.663559
Epoch 9.111: Loss = 0.864838
Epoch 9.112: Loss = 0.927383
Epoch 9.113: Loss = 0.668381
Epoch 9.114: Loss = 0.825226
Epoch 9.115: Loss = 0.70459
Epoch 9.116: Loss = 0.858566
Epoch 9.117: Loss = 0.776718
Epoch 9.118: Loss = 0.723938
Epoch 9.119: Loss = 0.733856
Epoch 9.120: Loss = 0.76709
TRAIN LOSS = 0.769836
TRAIN ACC = 78.5767 % (47148/60000)
Loss = 0.683197
Loss = 0.887634
Loss = 0.723557
Loss = 0.694992
Loss = 0.711533
Loss = 0.863815
Loss = 1.00076
Loss = 0.854721
Loss = 0.825745
Loss = 0.713959
Loss = 0.949234
Loss = 0.910919
Loss = 0.849655
Loss = 0.758011
Loss = 0.794769
Loss = 0.86618
Loss = 0.793976
Loss = 0.842422
Loss = 0.891556
Loss = 0.801971
TEST LOSS = 0.82093
TEST ACC = 471.48 % (7740/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.747025
Epoch 10.2: Loss = 0.777939
Epoch 10.3: Loss = 0.81546
Epoch 10.4: Loss = 0.761154
Epoch 10.5: Loss = 0.718735
Epoch 10.6: Loss = 0.77951
Epoch 10.7: Loss = 0.731262
Epoch 10.8: Loss = 0.758057
Epoch 10.9: Loss = 0.825378
Epoch 10.10: Loss = 0.643967
Epoch 10.11: Loss = 0.855652
Epoch 10.12: Loss = 0.717438
Epoch 10.13: Loss = 0.778061
Epoch 10.14: Loss = 0.801743
Epoch 10.15: Loss = 0.818253
Epoch 10.16: Loss = 0.791138
Epoch 10.17: Loss = 0.759323
Epoch 10.18: Loss = 0.794891
Epoch 10.19: Loss = 0.744751
Epoch 10.20: Loss = 0.826767
Epoch 10.21: Loss = 0.90184
Epoch 10.22: Loss = 0.626053
Epoch 10.23: Loss = 0.638718
Epoch 10.24: Loss = 0.808411
Epoch 10.25: Loss = 0.601913
Epoch 10.26: Loss = 0.764313
Epoch 10.27: Loss = 0.834015
Epoch 10.28: Loss = 0.853058
Epoch 10.29: Loss = 0.799744
Epoch 10.30: Loss = 0.77121
Epoch 10.31: Loss = 0.706161
Epoch 10.32: Loss = 0.801926
Epoch 10.33: Loss = 0.828674
Epoch 10.34: Loss = 0.851715
Epoch 10.35: Loss = 0.703796
Epoch 10.36: Loss = 0.757202
Epoch 10.37: Loss = 0.718475
Epoch 10.38: Loss = 0.83075
Epoch 10.39: Loss = 0.74794
Epoch 10.40: Loss = 0.737045
Epoch 10.41: Loss = 0.723389
Epoch 10.42: Loss = 0.801025
Epoch 10.43: Loss = 0.746445
Epoch 10.44: Loss = 0.804382
Epoch 10.45: Loss = 0.738602
Epoch 10.46: Loss = 0.764938
Epoch 10.47: Loss = 0.710327
Epoch 10.48: Loss = 0.754074
Epoch 10.49: Loss = 0.807404
Epoch 10.50: Loss = 0.81601
Epoch 10.51: Loss = 0.822601
Epoch 10.52: Loss = 0.73439
Epoch 10.53: Loss = 0.807938
Epoch 10.54: Loss = 0.715897
Epoch 10.55: Loss = 0.781479
Epoch 10.56: Loss = 0.790039
Epoch 10.57: Loss = 0.836502
Epoch 10.58: Loss = 0.64035
Epoch 10.59: Loss = 0.805222
Epoch 10.60: Loss = 0.741226
Epoch 10.61: Loss = 0.687759
Epoch 10.62: Loss = 0.649979
Epoch 10.63: Loss = 0.777435
Epoch 10.64: Loss = 0.776642
Epoch 10.65: Loss = 0.865097
Epoch 10.66: Loss = 0.785553
Epoch 10.67: Loss = 0.890701
Epoch 10.68: Loss = 0.816742
Epoch 10.69: Loss = 0.875214
Epoch 10.70: Loss = 0.87851
Epoch 10.71: Loss = 0.849594
Epoch 10.72: Loss = 0.732071
Epoch 10.73: Loss = 0.911545
Epoch 10.74: Loss = 0.755142
Epoch 10.75: Loss = 0.711136
Epoch 10.76: Loss = 0.704849
Epoch 10.77: Loss = 0.876099
Epoch 10.78: Loss = 0.738129
Epoch 10.79: Loss = 0.830597
Epoch 10.80: Loss = 0.756546
Epoch 10.81: Loss = 0.731323
Epoch 10.82: Loss = 0.838654
Epoch 10.83: Loss = 0.743835
Epoch 10.84: Loss = 0.711823
Epoch 10.85: Loss = 0.780365
Epoch 10.86: Loss = 0.759552
Epoch 10.87: Loss = 0.912125
Epoch 10.88: Loss = 0.708374
Epoch 10.89: Loss = 0.924561
Epoch 10.90: Loss = 0.85849
Epoch 10.91: Loss = 0.82048
Epoch 10.92: Loss = 1.02835
Epoch 10.93: Loss = 0.632614
Epoch 10.94: Loss = 0.834808
Epoch 10.95: Loss = 0.748825
Epoch 10.96: Loss = 0.87851
Epoch 10.97: Loss = 0.748566
Epoch 10.98: Loss = 0.837509
Epoch 10.99: Loss = 0.688629
Epoch 10.100: Loss = 0.773254
Epoch 10.101: Loss = 0.9552
Epoch 10.102: Loss = 0.740616
Epoch 10.103: Loss = 0.764832
Epoch 10.104: Loss = 0.695618
Epoch 10.105: Loss = 0.754868
Epoch 10.106: Loss = 0.745316
Epoch 10.107: Loss = 0.753662
Epoch 10.108: Loss = 0.796616
Epoch 10.109: Loss = 0.76152
Epoch 10.110: Loss = 0.820267
Epoch 10.111: Loss = 1.00677
Epoch 10.112: Loss = 0.918701
Epoch 10.113: Loss = 0.814026
Epoch 10.114: Loss = 0.850204
Epoch 10.115: Loss = 0.686874
Epoch 10.116: Loss = 0.74939
Epoch 10.117: Loss = 0.818436
Epoch 10.118: Loss = 0.902786
Epoch 10.119: Loss = 0.954376
Epoch 10.120: Loss = 0.825378
TRAIN LOSS = 0.785172
TRAIN ACC = 78.8605 % (47319/60000)
Loss = 0.705185
Loss = 0.881897
Loss = 0.744141
Loss = 0.690186
Loss = 0.776489
Loss = 0.913528
Loss = 1.01776
Loss = 0.873123
Loss = 0.814392
Loss = 0.749374
Loss = 0.999557
Loss = 0.93515
Loss = 0.836548
Loss = 0.792694
Loss = 0.854874
Loss = 0.891418
Loss = 0.768341
Loss = 0.905869
Loss = 0.872177
Loss = 0.825012
TEST LOSS = 0.842386
TEST ACC = 473.189 % (7758/10000)

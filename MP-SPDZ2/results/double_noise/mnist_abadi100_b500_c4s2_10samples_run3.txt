Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.34814
Epoch 1.2: Loss = 2.30415
Epoch 1.3: Loss = 2.25555
Epoch 1.4: Loss = 2.23885
Epoch 1.5: Loss = 2.21939
Epoch 1.6: Loss = 2.18483
Epoch 1.7: Loss = 2.10455
Epoch 1.8: Loss = 2.10356
Epoch 1.9: Loss = 2.05225
Epoch 1.10: Loss = 2.00256
Epoch 1.11: Loss = 1.9846
Epoch 1.12: Loss = 1.94568
Epoch 1.13: Loss = 1.94438
Epoch 1.14: Loss = 1.88925
Epoch 1.15: Loss = 1.86891
Epoch 1.16: Loss = 1.79164
Epoch 1.17: Loss = 1.80902
Epoch 1.18: Loss = 1.82156
Epoch 1.19: Loss = 1.70691
Epoch 1.20: Loss = 1.71069
Epoch 1.21: Loss = 1.642
Epoch 1.22: Loss = 1.66444
Epoch 1.23: Loss = 1.65491
Epoch 1.24: Loss = 1.57106
Epoch 1.25: Loss = 1.53201
Epoch 1.26: Loss = 1.48959
Epoch 1.27: Loss = 1.51785
Epoch 1.28: Loss = 1.49496
Epoch 1.29: Loss = 1.44577
Epoch 1.30: Loss = 1.41397
Epoch 1.31: Loss = 1.40303
Epoch 1.32: Loss = 1.39445
Epoch 1.33: Loss = 1.37987
Epoch 1.34: Loss = 1.30708
Epoch 1.35: Loss = 1.31931
Epoch 1.36: Loss = 1.25374
Epoch 1.37: Loss = 1.26775
Epoch 1.38: Loss = 1.23718
Epoch 1.39: Loss = 1.21956
Epoch 1.40: Loss = 1.21191
Epoch 1.41: Loss = 1.18776
Epoch 1.42: Loss = 1.18323
Epoch 1.43: Loss = 1.17218
Epoch 1.44: Loss = 1.14362
Epoch 1.45: Loss = 1.09821
Epoch 1.46: Loss = 1.11006
Epoch 1.47: Loss = 1.08952
Epoch 1.48: Loss = 1.16988
Epoch 1.49: Loss = 1.05714
Epoch 1.50: Loss = 1.07626
Epoch 1.51: Loss = 1.01823
Epoch 1.52: Loss = 1.00536
Epoch 1.53: Loss = 1.09763
Epoch 1.54: Loss = 1.06783
Epoch 1.55: Loss = 1.03992
Epoch 1.56: Loss = 1.06117
Epoch 1.57: Loss = 0.942596
Epoch 1.58: Loss = 0.957306
Epoch 1.59: Loss = 0.954575
Epoch 1.60: Loss = 0.90918
Epoch 1.61: Loss = 0.941254
Epoch 1.62: Loss = 0.969162
Epoch 1.63: Loss = 0.884125
Epoch 1.64: Loss = 0.932999
Epoch 1.65: Loss = 0.93959
Epoch 1.66: Loss = 0.929657
Epoch 1.67: Loss = 0.909317
Epoch 1.68: Loss = 0.856171
Epoch 1.69: Loss = 0.882019
Epoch 1.70: Loss = 0.87114
Epoch 1.71: Loss = 0.862503
Epoch 1.72: Loss = 0.916458
Epoch 1.73: Loss = 0.868683
Epoch 1.74: Loss = 0.904419
Epoch 1.75: Loss = 0.805588
Epoch 1.76: Loss = 0.850418
Epoch 1.77: Loss = 0.85701
Epoch 1.78: Loss = 0.828094
Epoch 1.79: Loss = 0.840149
Epoch 1.80: Loss = 0.837296
Epoch 1.81: Loss = 0.74942
Epoch 1.82: Loss = 0.824554
Epoch 1.83: Loss = 0.811996
Epoch 1.84: Loss = 0.764297
Epoch 1.85: Loss = 0.812759
Epoch 1.86: Loss = 0.739105
Epoch 1.87: Loss = 0.749115
Epoch 1.88: Loss = 0.746155
Epoch 1.89: Loss = 0.826294
Epoch 1.90: Loss = 0.718094
Epoch 1.91: Loss = 0.80806
Epoch 1.92: Loss = 0.717117
Epoch 1.93: Loss = 0.710754
Epoch 1.94: Loss = 0.802505
Epoch 1.95: Loss = 0.716766
Epoch 1.96: Loss = 0.81012
Epoch 1.97: Loss = 0.721863
Epoch 1.98: Loss = 0.766449
Epoch 1.99: Loss = 0.781052
Epoch 1.100: Loss = 0.69664
Epoch 1.101: Loss = 0.69191
Epoch 1.102: Loss = 0.702377
Epoch 1.103: Loss = 0.697159
Epoch 1.104: Loss = 0.70517
Epoch 1.105: Loss = 0.792679
Epoch 1.106: Loss = 0.703888
Epoch 1.107: Loss = 0.663101
Epoch 1.108: Loss = 0.759842
Epoch 1.109: Loss = 0.678818
Epoch 1.110: Loss = 0.636703
Epoch 1.111: Loss = 0.68219
Epoch 1.112: Loss = 0.679092
Epoch 1.113: Loss = 0.650711
Epoch 1.114: Loss = 0.703644
Epoch 1.115: Loss = 0.63205
Epoch 1.116: Loss = 0.702301
Epoch 1.117: Loss = 0.657379
Epoch 1.118: Loss = 0.729156
Epoch 1.119: Loss = 0.70903
Epoch 1.120: Loss = 0.705963
TRAIN LOSS = 1.14076
TRAIN ACC = 67.0044 % (40205/60000)
Loss = 0.688934
Loss = 0.728851
Loss = 0.848297
Loss = 0.750778
Loss = 0.802353
Loss = 0.706589
Loss = 0.662277
Loss = 0.827667
Loss = 0.79541
Loss = 0.745407
Loss = 0.404037
Loss = 0.643661
Loss = 0.396561
Loss = 0.640732
Loss = 0.538513
Loss = 0.475189
Loss = 0.480042
Loss = 0.302856
Loss = 0.480377
Loss = 0.758713
TEST LOSS = 0.633862
TEST ACC = 402.049 % (8129/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.645004
Epoch 2.2: Loss = 0.696625
Epoch 2.3: Loss = 0.714401
Epoch 2.4: Loss = 0.677048
Epoch 2.5: Loss = 0.645935
Epoch 2.6: Loss = 0.61377
Epoch 2.7: Loss = 0.579483
Epoch 2.8: Loss = 0.643311
Epoch 2.9: Loss = 0.614319
Epoch 2.10: Loss = 0.630127
Epoch 2.11: Loss = 0.672165
Epoch 2.12: Loss = 0.612793
Epoch 2.13: Loss = 0.598206
Epoch 2.14: Loss = 0.672043
Epoch 2.15: Loss = 0.617828
Epoch 2.16: Loss = 0.553207
Epoch 2.17: Loss = 0.648331
Epoch 2.18: Loss = 0.633774
Epoch 2.19: Loss = 0.639755
Epoch 2.20: Loss = 0.621475
Epoch 2.21: Loss = 0.613007
Epoch 2.22: Loss = 0.662521
Epoch 2.23: Loss = 0.657852
Epoch 2.24: Loss = 0.605057
Epoch 2.25: Loss = 0.610413
Epoch 2.26: Loss = 0.628876
Epoch 2.27: Loss = 0.5746
Epoch 2.28: Loss = 0.577454
Epoch 2.29: Loss = 0.551651
Epoch 2.30: Loss = 0.571411
Epoch 2.31: Loss = 0.635468
Epoch 2.32: Loss = 0.590088
Epoch 2.33: Loss = 0.526749
Epoch 2.34: Loss = 0.542465
Epoch 2.35: Loss = 0.533966
Epoch 2.36: Loss = 0.510651
Epoch 2.37: Loss = 0.598038
Epoch 2.38: Loss = 0.63501
Epoch 2.39: Loss = 0.591263
Epoch 2.40: Loss = 0.565338
Epoch 2.41: Loss = 0.617569
Epoch 2.42: Loss = 0.583298
Epoch 2.43: Loss = 0.517059
Epoch 2.44: Loss = 0.583939
Epoch 2.45: Loss = 0.608353
Epoch 2.46: Loss = 0.516541
Epoch 2.47: Loss = 0.608688
Epoch 2.48: Loss = 0.485474
Epoch 2.49: Loss = 0.515839
Epoch 2.50: Loss = 0.605148
Epoch 2.51: Loss = 0.630157
Epoch 2.52: Loss = 0.596359
Epoch 2.53: Loss = 0.606705
Epoch 2.54: Loss = 0.490295
Epoch 2.55: Loss = 0.519958
Epoch 2.56: Loss = 0.575195
Epoch 2.57: Loss = 0.480988
Epoch 2.58: Loss = 0.588272
Epoch 2.59: Loss = 0.443604
Epoch 2.60: Loss = 0.516342
Epoch 2.61: Loss = 0.528702
Epoch 2.62: Loss = 0.567612
Epoch 2.63: Loss = 0.557465
Epoch 2.64: Loss = 0.534363
Epoch 2.65: Loss = 0.556427
Epoch 2.66: Loss = 0.490341
Epoch 2.67: Loss = 0.528503
Epoch 2.68: Loss = 0.601547
Epoch 2.69: Loss = 0.601501
Epoch 2.70: Loss = 0.520096
Epoch 2.71: Loss = 0.508026
Epoch 2.72: Loss = 0.596039
Epoch 2.73: Loss = 0.514709
Epoch 2.74: Loss = 0.531738
Epoch 2.75: Loss = 0.55867
Epoch 2.76: Loss = 0.468735
Epoch 2.77: Loss = 0.544342
Epoch 2.78: Loss = 0.486877
Epoch 2.79: Loss = 0.508286
Epoch 2.80: Loss = 0.487976
Epoch 2.81: Loss = 0.499207
Epoch 2.82: Loss = 0.571457
Epoch 2.83: Loss = 0.619003
Epoch 2.84: Loss = 0.561188
Epoch 2.85: Loss = 0.478897
Epoch 2.86: Loss = 0.654785
Epoch 2.87: Loss = 0.529099
Epoch 2.88: Loss = 0.445572
Epoch 2.89: Loss = 0.475113
Epoch 2.90: Loss = 0.455734
Epoch 2.91: Loss = 0.512924
Epoch 2.92: Loss = 0.575333
Epoch 2.93: Loss = 0.493622
Epoch 2.94: Loss = 0.546646
Epoch 2.95: Loss = 0.496521
Epoch 2.96: Loss = 0.524261
Epoch 2.97: Loss = 0.44986
Epoch 2.98: Loss = 0.472229
Epoch 2.99: Loss = 0.461349
Epoch 2.100: Loss = 0.522522
Epoch 2.101: Loss = 0.509613
Epoch 2.102: Loss = 0.534607
Epoch 2.103: Loss = 0.458328
Epoch 2.104: Loss = 0.46347
Epoch 2.105: Loss = 0.5159
Epoch 2.106: Loss = 0.535156
Epoch 2.107: Loss = 0.437973
Epoch 2.108: Loss = 0.555588
Epoch 2.109: Loss = 0.566696
Epoch 2.110: Loss = 0.502777
Epoch 2.111: Loss = 0.534454
Epoch 2.112: Loss = 0.50174
Epoch 2.113: Loss = 0.479935
Epoch 2.114: Loss = 0.585419
Epoch 2.115: Loss = 0.457031
Epoch 2.116: Loss = 0.539413
Epoch 2.117: Loss = 0.556808
Epoch 2.118: Loss = 0.471085
Epoch 2.119: Loss = 0.551926
Epoch 2.120: Loss = 0.519913
TRAIN LOSS = 0.557648
TRAIN ACC = 83.1833 % (49912/60000)
Loss = 0.492294
Loss = 0.58989
Loss = 0.651154
Loss = 0.599121
Loss = 0.66391
Loss = 0.52623
Loss = 0.502045
Loss = 0.705872
Loss = 0.646973
Loss = 0.60231
Loss = 0.247787
Loss = 0.421249
Loss = 0.290756
Loss = 0.449646
Loss = 0.325089
Loss = 0.352219
Loss = 0.296219
Loss = 0.15007
Loss = 0.308167
Loss = 0.660553
TEST LOSS = 0.474077
TEST ACC = 499.12 % (8538/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.411652
Epoch 3.2: Loss = 0.471405
Epoch 3.3: Loss = 0.480652
Epoch 3.4: Loss = 0.440186
Epoch 3.5: Loss = 0.613708
Epoch 3.6: Loss = 0.419601
Epoch 3.7: Loss = 0.531418
Epoch 3.8: Loss = 0.52504
Epoch 3.9: Loss = 0.527191
Epoch 3.10: Loss = 0.492966
Epoch 3.11: Loss = 0.493896
Epoch 3.12: Loss = 0.464722
Epoch 3.13: Loss = 0.513016
Epoch 3.14: Loss = 0.44339
Epoch 3.15: Loss = 0.514954
Epoch 3.16: Loss = 0.489243
Epoch 3.17: Loss = 0.491943
Epoch 3.18: Loss = 0.426895
Epoch 3.19: Loss = 0.561752
Epoch 3.20: Loss = 0.428726
Epoch 3.21: Loss = 0.511658
Epoch 3.22: Loss = 0.559357
Epoch 3.23: Loss = 0.596634
Epoch 3.24: Loss = 0.511063
Epoch 3.25: Loss = 0.50412
Epoch 3.26: Loss = 0.421707
Epoch 3.27: Loss = 0.518341
Epoch 3.28: Loss = 0.537277
Epoch 3.29: Loss = 0.456772
Epoch 3.30: Loss = 0.483826
Epoch 3.31: Loss = 0.403625
Epoch 3.32: Loss = 0.488037
Epoch 3.33: Loss = 0.465454
Epoch 3.34: Loss = 0.522034
Epoch 3.35: Loss = 0.563553
Epoch 3.36: Loss = 0.493088
Epoch 3.37: Loss = 0.538437
Epoch 3.38: Loss = 0.536713
Epoch 3.39: Loss = 0.467377
Epoch 3.40: Loss = 0.468292
Epoch 3.41: Loss = 0.523239
Epoch 3.42: Loss = 0.482941
Epoch 3.43: Loss = 0.517548
Epoch 3.44: Loss = 0.414124
Epoch 3.45: Loss = 0.430038
Epoch 3.46: Loss = 0.447556
Epoch 3.47: Loss = 0.508118
Epoch 3.48: Loss = 0.501144
Epoch 3.49: Loss = 0.508728
Epoch 3.50: Loss = 0.513016
Epoch 3.51: Loss = 0.465714
Epoch 3.52: Loss = 0.515671
Epoch 3.53: Loss = 0.529709
Epoch 3.54: Loss = 0.562683
Epoch 3.55: Loss = 0.463409
Epoch 3.56: Loss = 0.476913
Epoch 3.57: Loss = 0.52002
Epoch 3.58: Loss = 0.501923
Epoch 3.59: Loss = 0.480453
Epoch 3.60: Loss = 0.499939
Epoch 3.61: Loss = 0.390976
Epoch 3.62: Loss = 0.43634
Epoch 3.63: Loss = 0.477875
Epoch 3.64: Loss = 0.515106
Epoch 3.65: Loss = 0.435074
Epoch 3.66: Loss = 0.445969
Epoch 3.67: Loss = 0.490067
Epoch 3.68: Loss = 0.502533
Epoch 3.69: Loss = 0.514496
Epoch 3.70: Loss = 0.432632
Epoch 3.71: Loss = 0.520569
Epoch 3.72: Loss = 0.420212
Epoch 3.73: Loss = 0.430832
Epoch 3.74: Loss = 0.42244
Epoch 3.75: Loss = 0.437485
Epoch 3.76: Loss = 0.476242
Epoch 3.77: Loss = 0.518051
Epoch 3.78: Loss = 0.51149
Epoch 3.79: Loss = 0.542084
Epoch 3.80: Loss = 0.449066
Epoch 3.81: Loss = 0.58429
Epoch 3.82: Loss = 0.513245
Epoch 3.83: Loss = 0.560211
Epoch 3.84: Loss = 0.484314
Epoch 3.85: Loss = 0.513733
Epoch 3.86: Loss = 0.357422
Epoch 3.87: Loss = 0.457733
Epoch 3.88: Loss = 0.394302
Epoch 3.89: Loss = 0.516586
Epoch 3.90: Loss = 0.457901
Epoch 3.91: Loss = 0.45488
Epoch 3.92: Loss = 0.50029
Epoch 3.93: Loss = 0.592102
Epoch 3.94: Loss = 0.424103
Epoch 3.95: Loss = 0.499176
Epoch 3.96: Loss = 0.482803
Epoch 3.97: Loss = 0.426514
Epoch 3.98: Loss = 0.495285
Epoch 3.99: Loss = 0.51059
Epoch 3.100: Loss = 0.567581
Epoch 3.101: Loss = 0.46257
Epoch 3.102: Loss = 0.437546
Epoch 3.103: Loss = 0.458878
Epoch 3.104: Loss = 0.452713
Epoch 3.105: Loss = 0.594254
Epoch 3.106: Loss = 0.512421
Epoch 3.107: Loss = 0.488632
Epoch 3.108: Loss = 0.47081
Epoch 3.109: Loss = 0.482849
Epoch 3.110: Loss = 0.425613
Epoch 3.111: Loss = 0.515656
Epoch 3.112: Loss = 0.400513
Epoch 3.113: Loss = 0.471497
Epoch 3.114: Loss = 0.463699
Epoch 3.115: Loss = 0.522629
Epoch 3.116: Loss = 0.456635
Epoch 3.117: Loss = 0.445236
Epoch 3.118: Loss = 0.425735
Epoch 3.119: Loss = 0.446289
Epoch 3.120: Loss = 0.604874
TRAIN LOSS = 0.486282
TRAIN ACC = 85.5377 % (51325/60000)
Loss = 0.429459
Loss = 0.565109
Loss = 0.61499
Loss = 0.577042
Loss = 0.633606
Loss = 0.448761
Loss = 0.443878
Loss = 0.660995
Loss = 0.594101
Loss = 0.540649
Loss = 0.223434
Loss = 0.354782
Loss = 0.280136
Loss = 0.419739
Loss = 0.289978
Loss = 0.343002
Loss = 0.260147
Loss = 0.10643
Loss = 0.27153
Loss = 0.6409
TEST LOSS = 0.434933
TEST ACC = 513.249 % (8687/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.461563
Epoch 4.2: Loss = 0.472977
Epoch 4.3: Loss = 0.480881
Epoch 4.4: Loss = 0.529282
Epoch 4.5: Loss = 0.461151
Epoch 4.6: Loss = 0.499176
Epoch 4.7: Loss = 0.515717
Epoch 4.8: Loss = 0.543274
Epoch 4.9: Loss = 0.472183
Epoch 4.10: Loss = 0.564377
Epoch 4.11: Loss = 0.463196
Epoch 4.12: Loss = 0.467224
Epoch 4.13: Loss = 0.50206
Epoch 4.14: Loss = 0.458008
Epoch 4.15: Loss = 0.538727
Epoch 4.16: Loss = 0.525177
Epoch 4.17: Loss = 0.520477
Epoch 4.18: Loss = 0.516434
Epoch 4.19: Loss = 0.484848
Epoch 4.20: Loss = 0.453445
Epoch 4.21: Loss = 0.418976
Epoch 4.22: Loss = 0.451111
Epoch 4.23: Loss = 0.434662
Epoch 4.24: Loss = 0.445236
Epoch 4.25: Loss = 0.561691
Epoch 4.26: Loss = 0.473587
Epoch 4.27: Loss = 0.519714
Epoch 4.28: Loss = 0.407639
Epoch 4.29: Loss = 0.478455
Epoch 4.30: Loss = 0.461517
Epoch 4.31: Loss = 0.397217
Epoch 4.32: Loss = 0.473785
Epoch 4.33: Loss = 0.588303
Epoch 4.34: Loss = 0.440353
Epoch 4.35: Loss = 0.546982
Epoch 4.36: Loss = 0.397797
Epoch 4.37: Loss = 0.499329
Epoch 4.38: Loss = 0.396744
Epoch 4.39: Loss = 0.429352
Epoch 4.40: Loss = 0.480698
Epoch 4.41: Loss = 0.52533
Epoch 4.42: Loss = 0.498428
Epoch 4.43: Loss = 0.469604
Epoch 4.44: Loss = 0.441498
Epoch 4.45: Loss = 0.508087
Epoch 4.46: Loss = 0.502777
Epoch 4.47: Loss = 0.443558
Epoch 4.48: Loss = 0.443542
Epoch 4.49: Loss = 0.478745
Epoch 4.50: Loss = 0.381668
Epoch 4.51: Loss = 0.473846
Epoch 4.52: Loss = 0.439484
Epoch 4.53: Loss = 0.459702
Epoch 4.54: Loss = 0.384415
Epoch 4.55: Loss = 0.435379
Epoch 4.56: Loss = 0.34523
Epoch 4.57: Loss = 0.395462
Epoch 4.58: Loss = 0.457977
Epoch 4.59: Loss = 0.424835
Epoch 4.60: Loss = 0.443634
Epoch 4.61: Loss = 0.420395
Epoch 4.62: Loss = 0.436111
Epoch 4.63: Loss = 0.511093
Epoch 4.64: Loss = 0.406586
Epoch 4.65: Loss = 0.469864
Epoch 4.66: Loss = 0.451431
Epoch 4.67: Loss = 0.53688
Epoch 4.68: Loss = 0.394379
Epoch 4.69: Loss = 0.515625
Epoch 4.70: Loss = 0.477859
Epoch 4.71: Loss = 0.520996
Epoch 4.72: Loss = 0.454269
Epoch 4.73: Loss = 0.50354
Epoch 4.74: Loss = 0.40152
Epoch 4.75: Loss = 0.40033
Epoch 4.76: Loss = 0.455658
Epoch 4.77: Loss = 0.575928
Epoch 4.78: Loss = 0.459061
Epoch 4.79: Loss = 0.546143
Epoch 4.80: Loss = 0.436508
Epoch 4.81: Loss = 0.359299
Epoch 4.82: Loss = 0.42218
Epoch 4.83: Loss = 0.562195
Epoch 4.84: Loss = 0.491348
Epoch 4.85: Loss = 0.448395
Epoch 4.86: Loss = 0.452255
Epoch 4.87: Loss = 0.529617
Epoch 4.88: Loss = 0.475067
Epoch 4.89: Loss = 0.561172
Epoch 4.90: Loss = 0.460861
Epoch 4.91: Loss = 0.615097
Epoch 4.92: Loss = 0.477615
Epoch 4.93: Loss = 0.498108
Epoch 4.94: Loss = 0.508057
Epoch 4.95: Loss = 0.435608
Epoch 4.96: Loss = 0.50415
Epoch 4.97: Loss = 0.384644
Epoch 4.98: Loss = 0.472534
Epoch 4.99: Loss = 0.431763
Epoch 4.100: Loss = 0.398148
Epoch 4.101: Loss = 0.543564
Epoch 4.102: Loss = 0.449646
Epoch 4.103: Loss = 0.381256
Epoch 4.104: Loss = 0.451736
Epoch 4.105: Loss = 0.599915
Epoch 4.106: Loss = 0.373077
Epoch 4.107: Loss = 0.455399
Epoch 4.108: Loss = 0.41214
Epoch 4.109: Loss = 0.505325
Epoch 4.110: Loss = 0.553467
Epoch 4.111: Loss = 0.53299
Epoch 4.112: Loss = 0.473328
Epoch 4.113: Loss = 0.451721
Epoch 4.114: Loss = 0.407562
Epoch 4.115: Loss = 0.503021
Epoch 4.116: Loss = 0.469055
Epoch 4.117: Loss = 0.437775
Epoch 4.118: Loss = 0.422104
Epoch 4.119: Loss = 0.441193
Epoch 4.120: Loss = 0.477997
TRAIN LOSS = 0.46994
TRAIN ACC = 86.2839 % (51773/60000)
Loss = 0.423553
Loss = 0.573273
Loss = 0.624741
Loss = 0.59581
Loss = 0.665314
Loss = 0.454498
Loss = 0.419785
Loss = 0.66803
Loss = 0.609604
Loss = 0.572098
Loss = 0.214493
Loss = 0.376541
Loss = 0.330627
Loss = 0.416977
Loss = 0.274414
Loss = 0.373596
Loss = 0.264664
Loss = 0.0947876
Loss = 0.269623
Loss = 0.608322
TEST LOSS = 0.441537
TEST ACC = 517.729 % (8713/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.547363
Epoch 5.2: Loss = 0.408905
Epoch 5.3: Loss = 0.498245
Epoch 5.4: Loss = 0.403351
Epoch 5.5: Loss = 0.492523
Epoch 5.6: Loss = 0.427414
Epoch 5.7: Loss = 0.424316
Epoch 5.8: Loss = 0.369156
Epoch 5.9: Loss = 0.453247
Epoch 5.10: Loss = 0.37709
Epoch 5.11: Loss = 0.473816
Epoch 5.12: Loss = 0.52594
Epoch 5.13: Loss = 0.401703
Epoch 5.14: Loss = 0.494995
Epoch 5.15: Loss = 0.461395
Epoch 5.16: Loss = 0.447067
Epoch 5.17: Loss = 0.440109
Epoch 5.18: Loss = 0.526901
Epoch 5.19: Loss = 0.456482
Epoch 5.20: Loss = 0.410248
Epoch 5.21: Loss = 0.481903
Epoch 5.22: Loss = 0.501099
Epoch 5.23: Loss = 0.515442
Epoch 5.24: Loss = 0.464264
Epoch 5.25: Loss = 0.436386
Epoch 5.26: Loss = 0.409576
Epoch 5.27: Loss = 0.429504
Epoch 5.28: Loss = 0.463837
Epoch 5.29: Loss = 0.40831
Epoch 5.30: Loss = 0.446564
Epoch 5.31: Loss = 0.570908
Epoch 5.32: Loss = 0.470291
Epoch 5.33: Loss = 0.513596
Epoch 5.34: Loss = 0.416763
Epoch 5.35: Loss = 0.532974
Epoch 5.36: Loss = 0.448563
Epoch 5.37: Loss = 0.446548
Epoch 5.38: Loss = 0.464096
Epoch 5.39: Loss = 0.591217
Epoch 5.40: Loss = 0.417694
Epoch 5.41: Loss = 0.470856
Epoch 5.42: Loss = 0.425781
Epoch 5.43: Loss = 0.450851
Epoch 5.44: Loss = 0.493088
Epoch 5.45: Loss = 0.469543
Epoch 5.46: Loss = 0.505737
Epoch 5.47: Loss = 0.422501
Epoch 5.48: Loss = 0.410431
Epoch 5.49: Loss = 0.448029
Epoch 5.50: Loss = 0.457031
Epoch 5.51: Loss = 0.446808
Epoch 5.52: Loss = 0.453796
Epoch 5.53: Loss = 0.488785
Epoch 5.54: Loss = 0.449921
Epoch 5.55: Loss = 0.371719
Epoch 5.56: Loss = 0.554169
Epoch 5.57: Loss = 0.492386
Epoch 5.58: Loss = 0.485016
Epoch 5.59: Loss = 0.485275
Epoch 5.60: Loss = 0.396622
Epoch 5.61: Loss = 0.387375
Epoch 5.62: Loss = 0.398956
Epoch 5.63: Loss = 0.433395
Epoch 5.64: Loss = 0.424683
Epoch 5.65: Loss = 0.423065
Epoch 5.66: Loss = 0.486008
Epoch 5.67: Loss = 0.453217
Epoch 5.68: Loss = 0.470673
Epoch 5.69: Loss = 0.490479
Epoch 5.70: Loss = 0.381027
Epoch 5.71: Loss = 0.377579
Epoch 5.72: Loss = 0.454636
Epoch 5.73: Loss = 0.440964
Epoch 5.74: Loss = 0.425079
Epoch 5.75: Loss = 0.56073
Epoch 5.76: Loss = 0.511963
Epoch 5.77: Loss = 0.444321
Epoch 5.78: Loss = 0.539093
Epoch 5.79: Loss = 0.450348
Epoch 5.80: Loss = 0.513046
Epoch 5.81: Loss = 0.448013
Epoch 5.82: Loss = 0.552048
Epoch 5.83: Loss = 0.434738
Epoch 5.84: Loss = 0.47641
Epoch 5.85: Loss = 0.552307
Epoch 5.86: Loss = 0.477661
Epoch 5.87: Loss = 0.458267
Epoch 5.88: Loss = 0.410477
Epoch 5.89: Loss = 0.445419
Epoch 5.90: Loss = 0.448334
Epoch 5.91: Loss = 0.457794
Epoch 5.92: Loss = 0.412216
Epoch 5.93: Loss = 0.442932
Epoch 5.94: Loss = 0.493729
Epoch 5.95: Loss = 0.476089
Epoch 5.96: Loss = 0.429871
Epoch 5.97: Loss = 0.445801
Epoch 5.98: Loss = 0.483994
Epoch 5.99: Loss = 0.469925
Epoch 5.100: Loss = 0.3983
Epoch 5.101: Loss = 0.469116
Epoch 5.102: Loss = 0.564056
Epoch 5.103: Loss = 0.51796
Epoch 5.104: Loss = 0.460815
Epoch 5.105: Loss = 0.473389
Epoch 5.106: Loss = 0.529587
Epoch 5.107: Loss = 0.362228
Epoch 5.108: Loss = 0.474869
Epoch 5.109: Loss = 0.391129
Epoch 5.110: Loss = 0.414398
Epoch 5.111: Loss = 0.508698
Epoch 5.112: Loss = 0.452774
Epoch 5.113: Loss = 0.424759
Epoch 5.114: Loss = 0.523621
Epoch 5.115: Loss = 0.396484
Epoch 5.116: Loss = 0.522903
Epoch 5.117: Loss = 0.426163
Epoch 5.118: Loss = 0.527328
Epoch 5.119: Loss = 0.54332
Epoch 5.120: Loss = 0.480011
TRAIN LOSS = 0.461594
TRAIN ACC = 86.7081 % (52027/60000)
Loss = 0.395309
Loss = 0.570206
Loss = 0.616257
Loss = 0.593735
Loss = 0.659393
Loss = 0.466293
Loss = 0.408432
Loss = 0.67189
Loss = 0.610352
Loss = 0.573715
Loss = 0.210358
Loss = 0.342987
Loss = 0.303314
Loss = 0.411713
Loss = 0.2444
Loss = 0.324341
Loss = 0.268356
Loss = 0.0939636
Loss = 0.271805
Loss = 0.625824
TEST LOSS = 0.433132
TEST ACC = 520.27 % (8769/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.568649
Epoch 6.2: Loss = 0.470947
Epoch 6.3: Loss = 0.391357
Epoch 6.4: Loss = 0.493546
Epoch 6.5: Loss = 0.507858
Epoch 6.6: Loss = 0.49556
Epoch 6.7: Loss = 0.493835
Epoch 6.8: Loss = 0.478256
Epoch 6.9: Loss = 0.469986
Epoch 6.10: Loss = 0.398972
Epoch 6.11: Loss = 0.652481
Epoch 6.12: Loss = 0.455048
Epoch 6.13: Loss = 0.493851
Epoch 6.14: Loss = 0.408005
Epoch 6.15: Loss = 0.445038
Epoch 6.16: Loss = 0.484177
Epoch 6.17: Loss = 0.329254
Epoch 6.18: Loss = 0.466553
Epoch 6.19: Loss = 0.613861
Epoch 6.20: Loss = 0.562637
Epoch 6.21: Loss = 0.427292
Epoch 6.22: Loss = 0.399582
Epoch 6.23: Loss = 0.481293
Epoch 6.24: Loss = 0.466339
Epoch 6.25: Loss = 0.558975
Epoch 6.26: Loss = 0.515808
Epoch 6.27: Loss = 0.46962
Epoch 6.28: Loss = 0.34729
Epoch 6.29: Loss = 0.340851
Epoch 6.30: Loss = 0.522995
Epoch 6.31: Loss = 0.391922
Epoch 6.32: Loss = 0.473465
Epoch 6.33: Loss = 0.53009
Epoch 6.34: Loss = 0.444565
Epoch 6.35: Loss = 0.605927
Epoch 6.36: Loss = 0.458633
Epoch 6.37: Loss = 0.503876
Epoch 6.38: Loss = 0.520584
Epoch 6.39: Loss = 0.519287
Epoch 6.40: Loss = 0.412918
Epoch 6.41: Loss = 0.492035
Epoch 6.42: Loss = 0.471771
Epoch 6.43: Loss = 0.45314
Epoch 6.44: Loss = 0.453293
Epoch 6.45: Loss = 0.449356
Epoch 6.46: Loss = 0.337082
Epoch 6.47: Loss = 0.429672
Epoch 6.48: Loss = 0.431625
Epoch 6.49: Loss = 0.530838
Epoch 6.50: Loss = 0.468979
Epoch 6.51: Loss = 0.452606
Epoch 6.52: Loss = 0.429993
Epoch 6.53: Loss = 0.463745
Epoch 6.54: Loss = 0.40242
Epoch 6.55: Loss = 0.509949
Epoch 6.56: Loss = 0.510345
Epoch 6.57: Loss = 0.485229
Epoch 6.58: Loss = 0.481232
Epoch 6.59: Loss = 0.442444
Epoch 6.60: Loss = 0.466263
Epoch 6.61: Loss = 0.482727
Epoch 6.62: Loss = 0.537262
Epoch 6.63: Loss = 0.477966
Epoch 6.64: Loss = 0.554932
Epoch 6.65: Loss = 0.453217
Epoch 6.66: Loss = 0.447342
Epoch 6.67: Loss = 0.48085
Epoch 6.68: Loss = 0.498077
Epoch 6.69: Loss = 0.421814
Epoch 6.70: Loss = 0.496674
Epoch 6.71: Loss = 0.485504
Epoch 6.72: Loss = 0.578979
Epoch 6.73: Loss = 0.485458
Epoch 6.74: Loss = 0.417603
Epoch 6.75: Loss = 0.566345
Epoch 6.76: Loss = 0.529785
Epoch 6.77: Loss = 0.502899
Epoch 6.78: Loss = 0.450485
Epoch 6.79: Loss = 0.494186
Epoch 6.80: Loss = 0.454681
Epoch 6.81: Loss = 0.436844
Epoch 6.82: Loss = 0.52037
Epoch 6.83: Loss = 0.466431
Epoch 6.84: Loss = 0.513077
Epoch 6.85: Loss = 0.407928
Epoch 6.86: Loss = 0.354736
Epoch 6.87: Loss = 0.440674
Epoch 6.88: Loss = 0.410217
Epoch 6.89: Loss = 0.513519
Epoch 6.90: Loss = 0.371048
Epoch 6.91: Loss = 0.517685
Epoch 6.92: Loss = 0.445526
Epoch 6.93: Loss = 0.533966
Epoch 6.94: Loss = 0.395935
Epoch 6.95: Loss = 0.328232
Epoch 6.96: Loss = 0.463303
Epoch 6.97: Loss = 0.383301
Epoch 6.98: Loss = 0.487198
Epoch 6.99: Loss = 0.525574
Epoch 6.100: Loss = 0.40596
Epoch 6.101: Loss = 0.422379
Epoch 6.102: Loss = 0.425262
Epoch 6.103: Loss = 0.523804
Epoch 6.104: Loss = 0.462006
Epoch 6.105: Loss = 0.44223
Epoch 6.106: Loss = 0.41124
Epoch 6.107: Loss = 0.507248
Epoch 6.108: Loss = 0.389954
Epoch 6.109: Loss = 0.329453
Epoch 6.110: Loss = 0.521164
Epoch 6.111: Loss = 0.412292
Epoch 6.112: Loss = 0.46933
Epoch 6.113: Loss = 0.513077
Epoch 6.114: Loss = 0.549911
Epoch 6.115: Loss = 0.42984
Epoch 6.116: Loss = 0.494675
Epoch 6.117: Loss = 0.504471
Epoch 6.118: Loss = 0.4505
Epoch 6.119: Loss = 0.434708
Epoch 6.120: Loss = 0.454544
TRAIN LOSS = 0.467651
TRAIN ACC = 86.6089 % (51968/60000)
Loss = 0.430283
Loss = 0.545959
Loss = 0.611298
Loss = 0.647461
Loss = 0.66011
Loss = 0.467911
Loss = 0.417786
Loss = 0.667145
Loss = 0.625275
Loss = 0.569122
Loss = 0.194702
Loss = 0.328445
Loss = 0.275269
Loss = 0.41687
Loss = 0.234711
Loss = 0.305466
Loss = 0.234879
Loss = 0.081131
Loss = 0.313721
Loss = 0.700668
TEST LOSS = 0.43641
TEST ACC = 519.679 % (8787/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.584534
Epoch 7.2: Loss = 0.474213
Epoch 7.3: Loss = 0.518021
Epoch 7.4: Loss = 0.515701
Epoch 7.5: Loss = 0.477295
Epoch 7.6: Loss = 0.407684
Epoch 7.7: Loss = 0.453461
Epoch 7.8: Loss = 0.454941
Epoch 7.9: Loss = 0.49234
Epoch 7.10: Loss = 0.529846
Epoch 7.11: Loss = 0.399261
Epoch 7.12: Loss = 0.372452
Epoch 7.13: Loss = 0.457092
Epoch 7.14: Loss = 0.503815
Epoch 7.15: Loss = 0.630478
Epoch 7.16: Loss = 0.483047
Epoch 7.17: Loss = 0.550507
Epoch 7.18: Loss = 0.380447
Epoch 7.19: Loss = 0.508087
Epoch 7.20: Loss = 0.494263
Epoch 7.21: Loss = 0.353317
Epoch 7.22: Loss = 0.536179
Epoch 7.23: Loss = 0.530945
Epoch 7.24: Loss = 0.495239
Epoch 7.25: Loss = 0.497543
Epoch 7.26: Loss = 0.42189
Epoch 7.27: Loss = 0.499847
Epoch 7.28: Loss = 0.521591
Epoch 7.29: Loss = 0.484116
Epoch 7.30: Loss = 0.464722
Epoch 7.31: Loss = 0.482376
Epoch 7.32: Loss = 0.424683
Epoch 7.33: Loss = 0.454361
Epoch 7.34: Loss = 0.597885
Epoch 7.35: Loss = 0.497925
Epoch 7.36: Loss = 0.501709
Epoch 7.37: Loss = 0.440155
Epoch 7.38: Loss = 0.454819
Epoch 7.39: Loss = 0.483749
Epoch 7.40: Loss = 0.507004
Epoch 7.41: Loss = 0.46225
Epoch 7.42: Loss = 0.373001
Epoch 7.43: Loss = 0.512573
Epoch 7.44: Loss = 0.472076
Epoch 7.45: Loss = 0.437073
Epoch 7.46: Loss = 0.497711
Epoch 7.47: Loss = 0.449097
Epoch 7.48: Loss = 0.416489
Epoch 7.49: Loss = 0.480835
Epoch 7.50: Loss = 0.513474
Epoch 7.51: Loss = 0.572433
Epoch 7.52: Loss = 0.510315
Epoch 7.53: Loss = 0.376312
Epoch 7.54: Loss = 0.532837
Epoch 7.55: Loss = 0.461258
Epoch 7.56: Loss = 0.402832
Epoch 7.57: Loss = 0.560394
Epoch 7.58: Loss = 0.402695
Epoch 7.59: Loss = 0.510864
Epoch 7.60: Loss = 0.635742
Epoch 7.61: Loss = 0.482895
Epoch 7.62: Loss = 0.434952
Epoch 7.63: Loss = 0.495056
Epoch 7.64: Loss = 0.442474
Epoch 7.65: Loss = 0.553345
Epoch 7.66: Loss = 0.547058
Epoch 7.67: Loss = 0.476471
Epoch 7.68: Loss = 0.528885
Epoch 7.69: Loss = 0.581833
Epoch 7.70: Loss = 0.601852
Epoch 7.71: Loss = 0.542648
Epoch 7.72: Loss = 0.503387
Epoch 7.73: Loss = 0.406219
Epoch 7.74: Loss = 0.540878
Epoch 7.75: Loss = 0.432968
Epoch 7.76: Loss = 0.385254
Epoch 7.77: Loss = 0.490448
Epoch 7.78: Loss = 0.466797
Epoch 7.79: Loss = 0.531158
Epoch 7.80: Loss = 0.459137
Epoch 7.81: Loss = 0.553589
Epoch 7.82: Loss = 0.560852
Epoch 7.83: Loss = 0.55899
Epoch 7.84: Loss = 0.480972
Epoch 7.85: Loss = 0.447159
Epoch 7.86: Loss = 0.495102
Epoch 7.87: Loss = 0.407318
Epoch 7.88: Loss = 0.461975
Epoch 7.89: Loss = 0.453125
Epoch 7.90: Loss = 0.374802
Epoch 7.91: Loss = 0.514984
Epoch 7.92: Loss = 0.481491
Epoch 7.93: Loss = 0.366013
Epoch 7.94: Loss = 0.479431
Epoch 7.95: Loss = 0.549698
Epoch 7.96: Loss = 0.41423
Epoch 7.97: Loss = 0.448547
Epoch 7.98: Loss = 0.489532
Epoch 7.99: Loss = 0.44548
Epoch 7.100: Loss = 0.398636
Epoch 7.101: Loss = 0.455551
Epoch 7.102: Loss = 0.444138
Epoch 7.103: Loss = 0.522186
Epoch 7.104: Loss = 0.406631
Epoch 7.105: Loss = 0.611282
Epoch 7.106: Loss = 0.484695
Epoch 7.107: Loss = 0.475372
Epoch 7.108: Loss = 0.336639
Epoch 7.109: Loss = 0.420868
Epoch 7.110: Loss = 0.485352
Epoch 7.111: Loss = 0.54631
Epoch 7.112: Loss = 0.387192
Epoch 7.113: Loss = 0.472504
Epoch 7.114: Loss = 0.463516
Epoch 7.115: Loss = 0.426514
Epoch 7.116: Loss = 0.361618
Epoch 7.117: Loss = 0.417068
Epoch 7.118: Loss = 0.436096
Epoch 7.119: Loss = 0.435013
Epoch 7.120: Loss = 0.514252
TRAIN LOSS = 0.477554
TRAIN ACC = 86.7477 % (52051/60000)
Loss = 0.454605
Loss = 0.576355
Loss = 0.629028
Loss = 0.667618
Loss = 0.687119
Loss = 0.468826
Loss = 0.401871
Loss = 0.698334
Loss = 0.629242
Loss = 0.571152
Loss = 0.209412
Loss = 0.365814
Loss = 0.258087
Loss = 0.423904
Loss = 0.248032
Loss = 0.303497
Loss = 0.248627
Loss = 0.0789948
Loss = 0.352142
Loss = 0.745132
TEST LOSS = 0.450889
TEST ACC = 520.509 % (8764/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.394318
Epoch 8.2: Loss = 0.404861
Epoch 8.3: Loss = 0.41275
Epoch 8.4: Loss = 0.425888
Epoch 8.5: Loss = 0.484619
Epoch 8.6: Loss = 0.534348
Epoch 8.7: Loss = 0.538528
Epoch 8.8: Loss = 0.447128
Epoch 8.9: Loss = 0.425415
Epoch 8.10: Loss = 0.481995
Epoch 8.11: Loss = 0.470474
Epoch 8.12: Loss = 0.443283
Epoch 8.13: Loss = 0.528748
Epoch 8.14: Loss = 0.461807
Epoch 8.15: Loss = 0.521027
Epoch 8.16: Loss = 0.4767
Epoch 8.17: Loss = 0.402771
Epoch 8.18: Loss = 0.48439
Epoch 8.19: Loss = 0.565552
Epoch 8.20: Loss = 0.482803
Epoch 8.21: Loss = 0.479813
Epoch 8.22: Loss = 0.462433
Epoch 8.23: Loss = 0.448959
Epoch 8.24: Loss = 0.47435
Epoch 8.25: Loss = 0.374237
Epoch 8.26: Loss = 0.462631
Epoch 8.27: Loss = 0.459915
Epoch 8.28: Loss = 0.469589
Epoch 8.29: Loss = 0.531509
Epoch 8.30: Loss = 0.604263
Epoch 8.31: Loss = 0.487915
Epoch 8.32: Loss = 0.490936
Epoch 8.33: Loss = 0.518402
Epoch 8.34: Loss = 0.402176
Epoch 8.35: Loss = 0.48291
Epoch 8.36: Loss = 0.433746
Epoch 8.37: Loss = 0.484421
Epoch 8.38: Loss = 0.439178
Epoch 8.39: Loss = 0.498428
Epoch 8.40: Loss = 0.426193
Epoch 8.41: Loss = 0.533081
Epoch 8.42: Loss = 0.414886
Epoch 8.43: Loss = 0.372498
Epoch 8.44: Loss = 0.497574
Epoch 8.45: Loss = 0.382828
Epoch 8.46: Loss = 0.559067
Epoch 8.47: Loss = 0.544739
Epoch 8.48: Loss = 0.589966
Epoch 8.49: Loss = 0.663849
Epoch 8.50: Loss = 0.443832
Epoch 8.51: Loss = 0.452316
Epoch 8.52: Loss = 0.510651
Epoch 8.53: Loss = 0.36525
Epoch 8.54: Loss = 0.362991
Epoch 8.55: Loss = 0.439804
Epoch 8.56: Loss = 0.568192
Epoch 8.57: Loss = 0.350708
Epoch 8.58: Loss = 0.468445
Epoch 8.59: Loss = 0.487701
Epoch 8.60: Loss = 0.592865
Epoch 8.61: Loss = 0.4673
Epoch 8.62: Loss = 0.50502
Epoch 8.63: Loss = 0.50827
Epoch 8.64: Loss = 0.394882
Epoch 8.65: Loss = 0.443176
Epoch 8.66: Loss = 0.434433
Epoch 8.67: Loss = 0.478973
Epoch 8.68: Loss = 0.511566
Epoch 8.69: Loss = 0.575684
Epoch 8.70: Loss = 0.452332
Epoch 8.71: Loss = 0.454758
Epoch 8.72: Loss = 0.596924
Epoch 8.73: Loss = 0.488724
Epoch 8.74: Loss = 0.447693
Epoch 8.75: Loss = 0.4897
Epoch 8.76: Loss = 0.479507
Epoch 8.77: Loss = 0.501862
Epoch 8.78: Loss = 0.435089
Epoch 8.79: Loss = 0.356201
Epoch 8.80: Loss = 0.527267
Epoch 8.81: Loss = 0.502487
Epoch 8.82: Loss = 0.51062
Epoch 8.83: Loss = 0.576965
Epoch 8.84: Loss = 0.465408
Epoch 8.85: Loss = 0.518784
Epoch 8.86: Loss = 0.499252
Epoch 8.87: Loss = 0.545624
Epoch 8.88: Loss = 0.494019
Epoch 8.89: Loss = 0.492477
Epoch 8.90: Loss = 0.461319
Epoch 8.91: Loss = 0.461044
Epoch 8.92: Loss = 0.423538
Epoch 8.93: Loss = 0.507889
Epoch 8.94: Loss = 0.538986
Epoch 8.95: Loss = 0.622925
Epoch 8.96: Loss = 0.539505
Epoch 8.97: Loss = 0.491608
Epoch 8.98: Loss = 0.435486
Epoch 8.99: Loss = 0.563095
Epoch 8.100: Loss = 0.473297
Epoch 8.101: Loss = 0.619797
Epoch 8.102: Loss = 0.410324
Epoch 8.103: Loss = 0.495758
Epoch 8.104: Loss = 0.370651
Epoch 8.105: Loss = 0.489609
Epoch 8.106: Loss = 0.503906
Epoch 8.107: Loss = 0.463348
Epoch 8.108: Loss = 0.487961
Epoch 8.109: Loss = 0.483658
Epoch 8.110: Loss = 0.556168
Epoch 8.111: Loss = 0.465561
Epoch 8.112: Loss = 0.527374
Epoch 8.113: Loss = 0.590622
Epoch 8.114: Loss = 0.483017
Epoch 8.115: Loss = 0.501053
Epoch 8.116: Loss = 0.452026
Epoch 8.117: Loss = 0.420029
Epoch 8.118: Loss = 0.552185
Epoch 8.119: Loss = 0.682404
Epoch 8.120: Loss = 0.532455
TRAIN LOSS = 0.484863
TRAIN ACC = 86.7493 % (52052/60000)
Loss = 0.422852
Loss = 0.618149
Loss = 0.66481
Loss = 0.742798
Loss = 0.738373
Loss = 0.526306
Loss = 0.417633
Loss = 0.755005
Loss = 0.672043
Loss = 0.601227
Loss = 0.238998
Loss = 0.354584
Loss = 0.321121
Loss = 0.477493
Loss = 0.235458
Loss = 0.346603
Loss = 0.244141
Loss = 0.0814972
Loss = 0.345123
Loss = 0.705338
TEST LOSS = 0.475477
TEST ACC = 520.518 % (8717/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.496277
Epoch 9.2: Loss = 0.464874
Epoch 9.3: Loss = 0.643356
Epoch 9.4: Loss = 0.525925
Epoch 9.5: Loss = 0.475784
Epoch 9.6: Loss = 0.393341
Epoch 9.7: Loss = 0.467972
Epoch 9.8: Loss = 0.470825
Epoch 9.9: Loss = 0.558212
Epoch 9.10: Loss = 0.578293
Epoch 9.11: Loss = 0.595993
Epoch 9.12: Loss = 0.534622
Epoch 9.13: Loss = 0.564407
Epoch 9.14: Loss = 0.446228
Epoch 9.15: Loss = 0.548553
Epoch 9.16: Loss = 0.520462
Epoch 9.17: Loss = 0.495789
Epoch 9.18: Loss = 0.379257
Epoch 9.19: Loss = 0.448105
Epoch 9.20: Loss = 0.574921
Epoch 9.21: Loss = 0.428604
Epoch 9.22: Loss = 0.535492
Epoch 9.23: Loss = 0.470993
Epoch 9.24: Loss = 0.551468
Epoch 9.25: Loss = 0.3591
Epoch 9.26: Loss = 0.482132
Epoch 9.27: Loss = 0.372849
Epoch 9.28: Loss = 0.53038
Epoch 9.29: Loss = 0.444504
Epoch 9.30: Loss = 0.519028
Epoch 9.31: Loss = 0.472305
Epoch 9.32: Loss = 0.375153
Epoch 9.33: Loss = 0.480789
Epoch 9.34: Loss = 0.4832
Epoch 9.35: Loss = 0.608078
Epoch 9.36: Loss = 0.556549
Epoch 9.37: Loss = 0.4086
Epoch 9.38: Loss = 0.386978
Epoch 9.39: Loss = 0.53476
Epoch 9.40: Loss = 0.628464
Epoch 9.41: Loss = 0.514938
Epoch 9.42: Loss = 0.43457
Epoch 9.43: Loss = 0.477463
Epoch 9.44: Loss = 0.523453
Epoch 9.45: Loss = 0.565735
Epoch 9.46: Loss = 0.429886
Epoch 9.47: Loss = 0.464142
Epoch 9.48: Loss = 0.385345
Epoch 9.49: Loss = 0.467102
Epoch 9.50: Loss = 0.498367
Epoch 9.51: Loss = 0.571594
Epoch 9.52: Loss = 0.433838
Epoch 9.53: Loss = 0.376694
Epoch 9.54: Loss = 0.582016
Epoch 9.55: Loss = 0.400986
Epoch 9.56: Loss = 0.53685
Epoch 9.57: Loss = 0.440613
Epoch 9.58: Loss = 0.512848
Epoch 9.59: Loss = 0.500641
Epoch 9.60: Loss = 0.462982
Epoch 9.61: Loss = 0.570435
Epoch 9.62: Loss = 0.409821
Epoch 9.63: Loss = 0.620804
Epoch 9.64: Loss = 0.580276
Epoch 9.65: Loss = 0.557663
Epoch 9.66: Loss = 0.467743
Epoch 9.67: Loss = 0.458542
Epoch 9.68: Loss = 0.449188
Epoch 9.69: Loss = 0.36203
Epoch 9.70: Loss = 0.54068
Epoch 9.71: Loss = 0.497009
Epoch 9.72: Loss = 0.503403
Epoch 9.73: Loss = 0.569275
Epoch 9.74: Loss = 0.520508
Epoch 9.75: Loss = 0.58284
Epoch 9.76: Loss = 0.573227
Epoch 9.77: Loss = 0.530991
Epoch 9.78: Loss = 0.46582
Epoch 9.79: Loss = 0.477982
Epoch 9.80: Loss = 0.535309
Epoch 9.81: Loss = 0.573761
Epoch 9.82: Loss = 0.547211
Epoch 9.83: Loss = 0.471878
Epoch 9.84: Loss = 0.602066
Epoch 9.85: Loss = 0.392258
Epoch 9.86: Loss = 0.591888
Epoch 9.87: Loss = 0.476395
Epoch 9.88: Loss = 0.496994
Epoch 9.89: Loss = 0.510239
Epoch 9.90: Loss = 0.471649
Epoch 9.91: Loss = 0.388107
Epoch 9.92: Loss = 0.514359
Epoch 9.93: Loss = 0.447311
Epoch 9.94: Loss = 0.50528
Epoch 9.95: Loss = 0.523163
Epoch 9.96: Loss = 0.512405
Epoch 9.97: Loss = 0.433258
Epoch 9.98: Loss = 0.477905
Epoch 9.99: Loss = 0.399277
Epoch 9.100: Loss = 0.492111
Epoch 9.101: Loss = 0.480881
Epoch 9.102: Loss = 0.432709
Epoch 9.103: Loss = 0.450928
Epoch 9.104: Loss = 0.454605
Epoch 9.105: Loss = 0.499146
Epoch 9.106: Loss = 0.345932
Epoch 9.107: Loss = 0.537613
Epoch 9.108: Loss = 0.497116
Epoch 9.109: Loss = 0.508774
Epoch 9.110: Loss = 0.489502
Epoch 9.111: Loss = 0.526917
Epoch 9.112: Loss = 0.501678
Epoch 9.113: Loss = 0.540085
Epoch 9.114: Loss = 0.445557
Epoch 9.115: Loss = 0.417145
Epoch 9.116: Loss = 0.443619
Epoch 9.117: Loss = 0.504959
Epoch 9.118: Loss = 0.711334
Epoch 9.119: Loss = 0.593002
Epoch 9.120: Loss = 0.349701
TRAIN LOSS = 0.493546
TRAIN ACC = 86.7844 % (52073/60000)
Loss = 0.408188
Loss = 0.624054
Loss = 0.663376
Loss = 0.699631
Loss = 0.716232
Loss = 0.514313
Loss = 0.4104
Loss = 0.741425
Loss = 0.677658
Loss = 0.605286
Loss = 0.219833
Loss = 0.414124
Loss = 0.339279
Loss = 0.463852
Loss = 0.228973
Loss = 0.340195
Loss = 0.235947
Loss = 0.0704346
Loss = 0.310791
Loss = 0.740387
TEST LOSS = 0.471219
TEST ACC = 520.729 % (8752/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.450012
Epoch 10.2: Loss = 0.548523
Epoch 10.3: Loss = 0.515228
Epoch 10.4: Loss = 0.542297
Epoch 10.5: Loss = 0.453857
Epoch 10.6: Loss = 0.614197
Epoch 10.7: Loss = 0.442078
Epoch 10.8: Loss = 0.510345
Epoch 10.9: Loss = 0.598511
Epoch 10.10: Loss = 0.544495
Epoch 10.11: Loss = 0.547729
Epoch 10.12: Loss = 0.48848
Epoch 10.13: Loss = 0.498184
Epoch 10.14: Loss = 0.525345
Epoch 10.15: Loss = 0.412323
Epoch 10.16: Loss = 0.470078
Epoch 10.17: Loss = 0.571396
Epoch 10.18: Loss = 0.688278
Epoch 10.19: Loss = 0.527603
Epoch 10.20: Loss = 0.448639
Epoch 10.21: Loss = 0.450317
Epoch 10.22: Loss = 0.473923
Epoch 10.23: Loss = 0.574524
Epoch 10.24: Loss = 0.649475
Epoch 10.25: Loss = 0.51445
Epoch 10.26: Loss = 0.585342
Epoch 10.27: Loss = 0.632538
Epoch 10.28: Loss = 0.628754
Epoch 10.29: Loss = 0.386063
Epoch 10.30: Loss = 0.568527
Epoch 10.31: Loss = 0.36882
Epoch 10.32: Loss = 0.579758
Epoch 10.33: Loss = 0.520554
Epoch 10.34: Loss = 0.58168
Epoch 10.35: Loss = 0.618164
Epoch 10.36: Loss = 0.468903
Epoch 10.37: Loss = 0.503967
Epoch 10.38: Loss = 0.419342
Epoch 10.39: Loss = 0.500732
Epoch 10.40: Loss = 0.632492
Epoch 10.41: Loss = 0.429092
Epoch 10.42: Loss = 0.46048
Epoch 10.43: Loss = 0.472443
Epoch 10.44: Loss = 0.439529
Epoch 10.45: Loss = 0.42424
Epoch 10.46: Loss = 0.428711
Epoch 10.47: Loss = 0.426498
Epoch 10.48: Loss = 0.486649
Epoch 10.49: Loss = 0.536469
Epoch 10.50: Loss = 0.586304
Epoch 10.51: Loss = 0.523972
Epoch 10.52: Loss = 0.446548
Epoch 10.53: Loss = 0.398636
Epoch 10.54: Loss = 0.450073
Epoch 10.55: Loss = 0.373245
Epoch 10.56: Loss = 0.496689
Epoch 10.57: Loss = 0.342438
Epoch 10.58: Loss = 0.367706
Epoch 10.59: Loss = 0.437912
Epoch 10.60: Loss = 0.546021
Epoch 10.61: Loss = 0.420868
Epoch 10.62: Loss = 0.545441
Epoch 10.63: Loss = 0.456528
Epoch 10.64: Loss = 0.473434
Epoch 10.65: Loss = 0.425659
Epoch 10.66: Loss = 0.451523
Epoch 10.67: Loss = 0.536255
Epoch 10.68: Loss = 0.481033
Epoch 10.69: Loss = 0.536041
Epoch 10.70: Loss = 0.494019
Epoch 10.71: Loss = 0.485977
Epoch 10.72: Loss = 0.458435
Epoch 10.73: Loss = 0.597946
Epoch 10.74: Loss = 0.435532
Epoch 10.75: Loss = 0.468063
Epoch 10.76: Loss = 0.460114
Epoch 10.77: Loss = 0.534912
Epoch 10.78: Loss = 0.523514
Epoch 10.79: Loss = 0.568878
Epoch 10.80: Loss = 0.424728
Epoch 10.81: Loss = 0.52124
Epoch 10.82: Loss = 0.471939
Epoch 10.83: Loss = 0.498779
Epoch 10.84: Loss = 0.557907
Epoch 10.85: Loss = 0.512634
Epoch 10.86: Loss = 0.460007
Epoch 10.87: Loss = 0.351364
Epoch 10.88: Loss = 0.399368
Epoch 10.89: Loss = 0.556
Epoch 10.90: Loss = 0.510147
Epoch 10.91: Loss = 0.520218
Epoch 10.92: Loss = 0.619797
Epoch 10.93: Loss = 0.45459
Epoch 10.94: Loss = 0.382996
Epoch 10.95: Loss = 0.509979
Epoch 10.96: Loss = 0.484894
Epoch 10.97: Loss = 0.55368
Epoch 10.98: Loss = 0.511429
Epoch 10.99: Loss = 0.423462
Epoch 10.100: Loss = 0.447739
Epoch 10.101: Loss = 0.398117
Epoch 10.102: Loss = 0.465118
Epoch 10.103: Loss = 0.457275
Epoch 10.104: Loss = 0.564026
Epoch 10.105: Loss = 0.405563
Epoch 10.106: Loss = 0.516342
Epoch 10.107: Loss = 0.492126
Epoch 10.108: Loss = 0.528793
Epoch 10.109: Loss = 0.481094
Epoch 10.110: Loss = 0.517471
Epoch 10.111: Loss = 0.397324
Epoch 10.112: Loss = 0.466812
Epoch 10.113: Loss = 0.507935
Epoch 10.114: Loss = 0.486679
Epoch 10.115: Loss = 0.539047
Epoch 10.116: Loss = 0.423706
Epoch 10.117: Loss = 0.397736
Epoch 10.118: Loss = 0.520355
Epoch 10.119: Loss = 0.485825
Epoch 10.120: Loss = 0.496643
TRAIN LOSS = 0.493378
TRAIN ACC = 87.1231 % (52276/60000)
Loss = 0.425629
Loss = 0.581375
Loss = 0.676575
Loss = 0.70575
Loss = 0.683487
Loss = 0.464966
Loss = 0.441193
Loss = 0.80838
Loss = 0.672333
Loss = 0.598099
Loss = 0.222443
Loss = 0.386322
Loss = 0.318802
Loss = 0.449524
Loss = 0.225327
Loss = 0.342087
Loss = 0.223801
Loss = 0.0704803
Loss = 0.302307
Loss = 0.709732
TEST LOSS = 0.46543
TEST ACC = 522.758 % (8781/10000)

Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.30573
Epoch 1.2: Loss = 2.24112
Epoch 1.3: Loss = 2.19049
Epoch 1.4: Loss = 2.12183
Epoch 1.5: Loss = 2.08563
Epoch 1.6: Loss = 2.01599
Epoch 1.7: Loss = 1.97218
Epoch 1.8: Loss = 1.89601
Epoch 1.9: Loss = 1.8744
Epoch 1.10: Loss = 1.78677
Epoch 1.11: Loss = 1.73544
Epoch 1.12: Loss = 1.7131
Epoch 1.13: Loss = 1.67863
Epoch 1.14: Loss = 1.58972
Epoch 1.15: Loss = 1.56648
Epoch 1.16: Loss = 1.54982
Epoch 1.17: Loss = 1.47307
Epoch 1.18: Loss = 1.47044
Epoch 1.19: Loss = 1.44727
Epoch 1.20: Loss = 1.39172
Epoch 1.21: Loss = 1.41571
Epoch 1.22: Loss = 1.36768
Epoch 1.23: Loss = 1.31126
Epoch 1.24: Loss = 1.31375
Epoch 1.25: Loss = 1.29057
Epoch 1.26: Loss = 1.3011
Epoch 1.27: Loss = 1.25751
Epoch 1.28: Loss = 1.25369
Epoch 1.29: Loss = 1.20811
Epoch 1.30: Loss = 1.17155
Epoch 1.31: Loss = 1.16396
Epoch 1.32: Loss = 1.22531
Epoch 1.33: Loss = 1.10329
Epoch 1.34: Loss = 1.08899
Epoch 1.35: Loss = 1.10735
Epoch 1.36: Loss = 1.14139
Epoch 1.37: Loss = 1.09955
Epoch 1.38: Loss = 1.08035
Epoch 1.39: Loss = 1.02914
Epoch 1.40: Loss = 1.03433
Epoch 1.41: Loss = 1.06633
Epoch 1.42: Loss = 1.03349
Epoch 1.43: Loss = 1.05432
Epoch 1.44: Loss = 0.997559
Epoch 1.45: Loss = 1.00478
Epoch 1.46: Loss = 1.02296
Epoch 1.47: Loss = 0.994278
Epoch 1.48: Loss = 0.963211
Epoch 1.49: Loss = 1.02071
Epoch 1.50: Loss = 0.990189
Epoch 1.51: Loss = 1.00177
Epoch 1.52: Loss = 0.903183
Epoch 1.53: Loss = 0.898361
Epoch 1.54: Loss = 0.959442
Epoch 1.55: Loss = 0.970413
Epoch 1.56: Loss = 0.936813
Epoch 1.57: Loss = 0.887497
Epoch 1.58: Loss = 0.956421
Epoch 1.59: Loss = 0.93222
Epoch 1.60: Loss = 0.930923
Epoch 1.61: Loss = 0.929916
Epoch 1.62: Loss = 0.896515
Epoch 1.63: Loss = 0.951187
Epoch 1.64: Loss = 0.90332
Epoch 1.65: Loss = 0.904358
Epoch 1.66: Loss = 0.9422
Epoch 1.67: Loss = 0.854767
Epoch 1.68: Loss = 0.905899
Epoch 1.69: Loss = 0.882355
Epoch 1.70: Loss = 0.814087
Epoch 1.71: Loss = 0.929184
Epoch 1.72: Loss = 0.793137
Epoch 1.73: Loss = 0.878326
Epoch 1.74: Loss = 0.846878
Epoch 1.75: Loss = 0.85878
Epoch 1.76: Loss = 0.821381
Epoch 1.77: Loss = 0.862686
Epoch 1.78: Loss = 0.797821
Epoch 1.79: Loss = 0.797562
Epoch 1.80: Loss = 0.851318
Epoch 1.81: Loss = 0.797607
Epoch 1.82: Loss = 0.834564
Epoch 1.83: Loss = 0.823318
Epoch 1.84: Loss = 0.87233
Epoch 1.85: Loss = 0.797058
Epoch 1.86: Loss = 0.822678
Epoch 1.87: Loss = 0.837845
Epoch 1.88: Loss = 0.891388
Epoch 1.89: Loss = 0.862762
Epoch 1.90: Loss = 0.744141
Epoch 1.91: Loss = 0.739792
Epoch 1.92: Loss = 0.818542
Epoch 1.93: Loss = 0.773193
Epoch 1.94: Loss = 0.853989
Epoch 1.95: Loss = 0.789322
Epoch 1.96: Loss = 0.822632
Epoch 1.97: Loss = 0.774323
Epoch 1.98: Loss = 0.792084
Epoch 1.99: Loss = 0.751801
Epoch 1.100: Loss = 0.753769
Epoch 1.101: Loss = 0.814117
Epoch 1.102: Loss = 0.805374
Epoch 1.103: Loss = 0.730606
Epoch 1.104: Loss = 0.780655
Epoch 1.105: Loss = 0.765915
Epoch 1.106: Loss = 0.761215
Epoch 1.107: Loss = 0.767242
Epoch 1.108: Loss = 0.742081
Epoch 1.109: Loss = 0.799042
Epoch 1.110: Loss = 0.841568
Epoch 1.111: Loss = 0.81929
Epoch 1.112: Loss = 0.735474
Epoch 1.113: Loss = 0.858414
Epoch 1.114: Loss = 0.827133
Epoch 1.115: Loss = 0.741913
Epoch 1.116: Loss = 0.820679
Epoch 1.117: Loss = 0.682343
Epoch 1.118: Loss = 0.785873
Epoch 1.119: Loss = 0.701553
Epoch 1.120: Loss = 0.761871
TRAIN LOSS = 1.07262
TRAIN ACC = 64.0869 % (38454/60000)
Loss = 0.706894
Loss = 0.807129
Loss = 0.808075
Loss = 0.709961
Loss = 0.71109
Loss = 0.85466
Loss = 0.876678
Loss = 0.830872
Loss = 0.748642
Loss = 0.71051
Loss = 0.821228
Loss = 0.782532
Loss = 0.783783
Loss = 0.779938
Loss = 0.746262
Loss = 0.805984
Loss = 0.754013
Loss = 0.772522
Loss = 0.811768
Loss = 0.759995
TEST LOSS = 0.779127
TEST ACC = 384.54 % (7197/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.773788
Epoch 2.2: Loss = 0.800323
Epoch 2.3: Loss = 0.719833
Epoch 2.4: Loss = 0.728668
Epoch 2.5: Loss = 0.702469
Epoch 2.6: Loss = 0.745804
Epoch 2.7: Loss = 0.63269
Epoch 2.8: Loss = 0.751312
Epoch 2.9: Loss = 0.7285
Epoch 2.10: Loss = 0.76413
Epoch 2.11: Loss = 0.796356
Epoch 2.12: Loss = 0.709564
Epoch 2.13: Loss = 0.774399
Epoch 2.14: Loss = 0.73584
Epoch 2.15: Loss = 0.700058
Epoch 2.16: Loss = 0.741028
Epoch 2.17: Loss = 0.791351
Epoch 2.18: Loss = 0.731461
Epoch 2.19: Loss = 0.69104
Epoch 2.20: Loss = 0.691437
Epoch 2.21: Loss = 0.837463
Epoch 2.22: Loss = 0.636688
Epoch 2.23: Loss = 0.703125
Epoch 2.24: Loss = 0.646774
Epoch 2.25: Loss = 0.746506
Epoch 2.26: Loss = 0.771576
Epoch 2.27: Loss = 0.718887
Epoch 2.28: Loss = 0.774658
Epoch 2.29: Loss = 0.742874
Epoch 2.30: Loss = 0.723053
Epoch 2.31: Loss = 0.698563
Epoch 2.32: Loss = 0.771866
Epoch 2.33: Loss = 0.708618
Epoch 2.34: Loss = 0.740982
Epoch 2.35: Loss = 0.688354
Epoch 2.36: Loss = 0.751236
Epoch 2.37: Loss = 0.665207
Epoch 2.38: Loss = 0.674316
Epoch 2.39: Loss = 0.737671
Epoch 2.40: Loss = 0.704575
Epoch 2.41: Loss = 0.702728
Epoch 2.42: Loss = 0.84523
Epoch 2.43: Loss = 0.654144
Epoch 2.44: Loss = 0.629257
Epoch 2.45: Loss = 0.814346
Epoch 2.46: Loss = 0.741776
Epoch 2.47: Loss = 0.712967
Epoch 2.48: Loss = 0.690674
Epoch 2.49: Loss = 0.663864
Epoch 2.50: Loss = 0.762527
Epoch 2.51: Loss = 0.815948
Epoch 2.52: Loss = 0.703934
Epoch 2.53: Loss = 0.776459
Epoch 2.54: Loss = 0.771408
Epoch 2.55: Loss = 0.703308
Epoch 2.56: Loss = 0.74202
Epoch 2.57: Loss = 0.612534
Epoch 2.58: Loss = 0.666153
Epoch 2.59: Loss = 0.72525
Epoch 2.60: Loss = 0.766556
Epoch 2.61: Loss = 0.689468
Epoch 2.62: Loss = 0.717667
Epoch 2.63: Loss = 0.833755
Epoch 2.64: Loss = 0.742218
Epoch 2.65: Loss = 0.736801
Epoch 2.66: Loss = 0.661591
Epoch 2.67: Loss = 0.803528
Epoch 2.68: Loss = 0.66777
Epoch 2.69: Loss = 0.778183
Epoch 2.70: Loss = 0.756149
Epoch 2.71: Loss = 0.703613
Epoch 2.72: Loss = 0.692535
Epoch 2.73: Loss = 0.672287
Epoch 2.74: Loss = 0.684174
Epoch 2.75: Loss = 0.703751
Epoch 2.76: Loss = 0.640808
Epoch 2.77: Loss = 0.699799
Epoch 2.78: Loss = 0.683578
Epoch 2.79: Loss = 0.679749
Epoch 2.80: Loss = 0.648087
Epoch 2.81: Loss = 0.713074
Epoch 2.82: Loss = 0.708115
Epoch 2.83: Loss = 0.799713
Epoch 2.84: Loss = 0.677368
Epoch 2.85: Loss = 0.616516
Epoch 2.86: Loss = 0.789352
Epoch 2.87: Loss = 0.653641
Epoch 2.88: Loss = 0.661606
Epoch 2.89: Loss = 0.658997
Epoch 2.90: Loss = 0.751587
Epoch 2.91: Loss = 0.761627
Epoch 2.92: Loss = 0.742325
Epoch 2.93: Loss = 0.682419
Epoch 2.94: Loss = 0.631958
Epoch 2.95: Loss = 0.728943
Epoch 2.96: Loss = 0.66478
Epoch 2.97: Loss = 0.63588
Epoch 2.98: Loss = 0.727768
Epoch 2.99: Loss = 0.744431
Epoch 2.100: Loss = 0.69754
Epoch 2.101: Loss = 0.705231
Epoch 2.102: Loss = 0.659943
Epoch 2.103: Loss = 0.671707
Epoch 2.104: Loss = 0.743668
Epoch 2.105: Loss = 0.676117
Epoch 2.106: Loss = 0.691376
Epoch 2.107: Loss = 0.687408
Epoch 2.108: Loss = 0.680069
Epoch 2.109: Loss = 0.710815
Epoch 2.110: Loss = 0.702591
Epoch 2.111: Loss = 0.709457
Epoch 2.112: Loss = 0.754929
Epoch 2.113: Loss = 0.756744
Epoch 2.114: Loss = 0.740433
Epoch 2.115: Loss = 0.683212
Epoch 2.116: Loss = 0.753098
Epoch 2.117: Loss = 0.637222
Epoch 2.118: Loss = 0.679337
Epoch 2.119: Loss = 0.533859
Epoch 2.120: Loss = 0.676895
TRAIN LOSS = 0.7146
TRAIN ACC = 75.2609 % (45158/60000)
Loss = 0.621918
Loss = 0.739975
Loss = 0.689697
Loss = 0.604858
Loss = 0.627167
Loss = 0.795074
Loss = 0.80864
Loss = 0.771744
Loss = 0.68367
Loss = 0.639709
Loss = 0.767105
Loss = 0.734207
Loss = 0.712448
Loss = 0.685486
Loss = 0.667694
Loss = 0.714798
Loss = 0.675812
Loss = 0.701035
Loss = 0.735733
Loss = 0.692963
TEST LOSS = 0.703486
TEST ACC = 451.579 % (7599/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.61409
Epoch 3.2: Loss = 0.77121
Epoch 3.3: Loss = 0.71434
Epoch 3.4: Loss = 0.586212
Epoch 3.5: Loss = 0.726074
Epoch 3.6: Loss = 0.670761
Epoch 3.7: Loss = 0.764175
Epoch 3.8: Loss = 0.811081
Epoch 3.9: Loss = 0.719589
Epoch 3.10: Loss = 0.756073
Epoch 3.11: Loss = 0.629837
Epoch 3.12: Loss = 0.658127
Epoch 3.13: Loss = 0.736465
Epoch 3.14: Loss = 0.708252
Epoch 3.15: Loss = 0.640152
Epoch 3.16: Loss = 0.710388
Epoch 3.17: Loss = 0.650543
Epoch 3.18: Loss = 0.726425
Epoch 3.19: Loss = 0.639404
Epoch 3.20: Loss = 0.661133
Epoch 3.21: Loss = 0.775101
Epoch 3.22: Loss = 0.673676
Epoch 3.23: Loss = 0.70047
Epoch 3.24: Loss = 0.591888
Epoch 3.25: Loss = 0.684586
Epoch 3.26: Loss = 0.757629
Epoch 3.27: Loss = 0.748749
Epoch 3.28: Loss = 0.588516
Epoch 3.29: Loss = 0.686874
Epoch 3.30: Loss = 0.60257
Epoch 3.31: Loss = 0.697067
Epoch 3.32: Loss = 0.73674
Epoch 3.33: Loss = 0.613342
Epoch 3.34: Loss = 0.659958
Epoch 3.35: Loss = 0.781006
Epoch 3.36: Loss = 0.614532
Epoch 3.37: Loss = 0.679626
Epoch 3.38: Loss = 0.681442
Epoch 3.39: Loss = 0.74054
Epoch 3.40: Loss = 0.726837
Epoch 3.41: Loss = 0.578781
Epoch 3.42: Loss = 0.607895
Epoch 3.43: Loss = 0.779785
Epoch 3.44: Loss = 0.621429
Epoch 3.45: Loss = 0.742996
Epoch 3.46: Loss = 0.651184
Epoch 3.47: Loss = 0.741409
Epoch 3.48: Loss = 0.557663
Epoch 3.49: Loss = 0.649048
Epoch 3.50: Loss = 0.584213
Epoch 3.51: Loss = 0.639267
Epoch 3.52: Loss = 0.640732
Epoch 3.53: Loss = 0.743881
Epoch 3.54: Loss = 0.629913
Epoch 3.55: Loss = 0.558228
Epoch 3.56: Loss = 0.741241
Epoch 3.57: Loss = 0.592819
Epoch 3.58: Loss = 0.663055
Epoch 3.59: Loss = 0.77034
Epoch 3.60: Loss = 0.686432
Epoch 3.61: Loss = 0.75943
Epoch 3.62: Loss = 0.782822
Epoch 3.63: Loss = 0.596954
Epoch 3.64: Loss = 0.594193
Epoch 3.65: Loss = 0.719284
Epoch 3.66: Loss = 0.606674
Epoch 3.67: Loss = 0.673874
Epoch 3.68: Loss = 0.627625
Epoch 3.69: Loss = 0.668549
Epoch 3.70: Loss = 0.748581
Epoch 3.71: Loss = 0.664627
Epoch 3.72: Loss = 0.659393
Epoch 3.73: Loss = 0.71933
Epoch 3.74: Loss = 0.713425
Epoch 3.75: Loss = 0.671371
Epoch 3.76: Loss = 0.573212
Epoch 3.77: Loss = 0.714066
Epoch 3.78: Loss = 0.730576
Epoch 3.79: Loss = 0.718765
Epoch 3.80: Loss = 0.636871
Epoch 3.81: Loss = 0.659836
Epoch 3.82: Loss = 0.674881
Epoch 3.83: Loss = 0.699692
Epoch 3.84: Loss = 0.702271
Epoch 3.85: Loss = 0.621979
Epoch 3.86: Loss = 0.631607
Epoch 3.87: Loss = 0.602509
Epoch 3.88: Loss = 0.709198
Epoch 3.89: Loss = 0.661377
Epoch 3.90: Loss = 0.668961
Epoch 3.91: Loss = 0.552017
Epoch 3.92: Loss = 0.675125
Epoch 3.93: Loss = 0.776367
Epoch 3.94: Loss = 0.570084
Epoch 3.95: Loss = 0.716583
Epoch 3.96: Loss = 0.721344
Epoch 3.97: Loss = 0.678345
Epoch 3.98: Loss = 0.742691
Epoch 3.99: Loss = 0.690765
Epoch 3.100: Loss = 0.68605
Epoch 3.101: Loss = 0.575409
Epoch 3.102: Loss = 0.669586
Epoch 3.103: Loss = 0.7043
Epoch 3.104: Loss = 0.615417
Epoch 3.105: Loss = 0.66716
Epoch 3.106: Loss = 0.626068
Epoch 3.107: Loss = 0.671814
Epoch 3.108: Loss = 0.693787
Epoch 3.109: Loss = 0.691986
Epoch 3.110: Loss = 0.656372
Epoch 3.111: Loss = 0.646286
Epoch 3.112: Loss = 0.611603
Epoch 3.113: Loss = 0.626755
Epoch 3.114: Loss = 0.644363
Epoch 3.115: Loss = 0.713409
Epoch 3.116: Loss = 0.705872
Epoch 3.117: Loss = 0.651459
Epoch 3.118: Loss = 0.633087
Epoch 3.119: Loss = 0.574921
Epoch 3.120: Loss = 0.575684
TRAIN LOSS = 0.673294
TRAIN ACC = 77.9388 % (46766/60000)
Loss = 0.583435
Loss = 0.71138
Loss = 0.650436
Loss = 0.572144
Loss = 0.586731
Loss = 0.755127
Loss = 0.799988
Loss = 0.743073
Loss = 0.653915
Loss = 0.625214
Loss = 0.751297
Loss = 0.725067
Loss = 0.691986
Loss = 0.656357
Loss = 0.64502
Loss = 0.683777
Loss = 0.671326
Loss = 0.682037
Loss = 0.694412
Loss = 0.661102
TEST LOSS = 0.677191
TEST ACC = 467.659 % (7799/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.674805
Epoch 4.2: Loss = 0.605164
Epoch 4.3: Loss = 0.555908
Epoch 4.4: Loss = 0.623627
Epoch 4.5: Loss = 0.803268
Epoch 4.6: Loss = 0.673798
Epoch 4.7: Loss = 0.68512
Epoch 4.8: Loss = 0.637009
Epoch 4.9: Loss = 0.709503
Epoch 4.10: Loss = 0.586945
Epoch 4.11: Loss = 0.630951
Epoch 4.12: Loss = 0.61058
Epoch 4.13: Loss = 0.69162
Epoch 4.14: Loss = 0.726669
Epoch 4.15: Loss = 0.653137
Epoch 4.16: Loss = 0.710587
Epoch 4.17: Loss = 0.659637
Epoch 4.18: Loss = 0.769638
Epoch 4.19: Loss = 0.68187
Epoch 4.20: Loss = 0.692078
Epoch 4.21: Loss = 0.702469
Epoch 4.22: Loss = 0.890625
Epoch 4.23: Loss = 0.626785
Epoch 4.24: Loss = 0.660583
Epoch 4.25: Loss = 0.61319
Epoch 4.26: Loss = 0.643097
Epoch 4.27: Loss = 0.74614
Epoch 4.28: Loss = 0.669922
Epoch 4.29: Loss = 0.69455
Epoch 4.30: Loss = 0.726959
Epoch 4.31: Loss = 0.767639
Epoch 4.32: Loss = 0.606827
Epoch 4.33: Loss = 0.636612
Epoch 4.34: Loss = 0.665192
Epoch 4.35: Loss = 0.647324
Epoch 4.36: Loss = 0.679276
Epoch 4.37: Loss = 0.634491
Epoch 4.38: Loss = 0.661194
Epoch 4.39: Loss = 0.680695
Epoch 4.40: Loss = 0.633591
Epoch 4.41: Loss = 0.663467
Epoch 4.42: Loss = 0.650574
Epoch 4.43: Loss = 0.750015
Epoch 4.44: Loss = 0.622269
Epoch 4.45: Loss = 0.596161
Epoch 4.46: Loss = 0.782761
Epoch 4.47: Loss = 0.6362
Epoch 4.48: Loss = 0.591965
Epoch 4.49: Loss = 0.633438
Epoch 4.50: Loss = 0.670181
Epoch 4.51: Loss = 0.643463
Epoch 4.52: Loss = 0.546616
Epoch 4.53: Loss = 0.653458
Epoch 4.54: Loss = 0.64447
Epoch 4.55: Loss = 0.659897
Epoch 4.56: Loss = 0.608215
Epoch 4.57: Loss = 0.595276
Epoch 4.58: Loss = 0.600464
Epoch 4.59: Loss = 0.539597
Epoch 4.60: Loss = 0.587723
Epoch 4.61: Loss = 0.688004
Epoch 4.62: Loss = 0.653961
Epoch 4.63: Loss = 0.747818
Epoch 4.64: Loss = 0.767853
Epoch 4.65: Loss = 0.609634
Epoch 4.66: Loss = 0.638199
Epoch 4.67: Loss = 0.596039
Epoch 4.68: Loss = 0.573547
Epoch 4.69: Loss = 0.664246
Epoch 4.70: Loss = 0.607224
Epoch 4.71: Loss = 0.74295
Epoch 4.72: Loss = 0.628082
Epoch 4.73: Loss = 0.696732
Epoch 4.74: Loss = 0.637177
Epoch 4.75: Loss = 0.60643
Epoch 4.76: Loss = 0.599716
Epoch 4.77: Loss = 0.674728
Epoch 4.78: Loss = 0.664337
Epoch 4.79: Loss = 0.640518
Epoch 4.80: Loss = 0.601685
Epoch 4.81: Loss = 0.680542
Epoch 4.82: Loss = 0.552444
Epoch 4.83: Loss = 0.593262
Epoch 4.84: Loss = 0.603439
Epoch 4.85: Loss = 0.572662
Epoch 4.86: Loss = 0.692963
Epoch 4.87: Loss = 0.572937
Epoch 4.88: Loss = 0.646851
Epoch 4.89: Loss = 0.704041
Epoch 4.90: Loss = 0.684326
Epoch 4.91: Loss = 0.614746
Epoch 4.92: Loss = 0.660934
Epoch 4.93: Loss = 0.594025
Epoch 4.94: Loss = 0.64772
Epoch 4.95: Loss = 0.683411
Epoch 4.96: Loss = 0.708893
Epoch 4.97: Loss = 0.716324
Epoch 4.98: Loss = 0.784821
Epoch 4.99: Loss = 0.590103
Epoch 4.100: Loss = 0.548233
Epoch 4.101: Loss = 0.661301
Epoch 4.102: Loss = 0.524063
Epoch 4.103: Loss = 0.594788
Epoch 4.104: Loss = 0.759415
Epoch 4.105: Loss = 0.633545
Epoch 4.106: Loss = 0.681381
Epoch 4.107: Loss = 0.668381
Epoch 4.108: Loss = 0.622238
Epoch 4.109: Loss = 0.587784
Epoch 4.110: Loss = 0.699356
Epoch 4.111: Loss = 0.454681
Epoch 4.112: Loss = 0.661453
Epoch 4.113: Loss = 0.631393
Epoch 4.114: Loss = 0.530823
Epoch 4.115: Loss = 0.584854
Epoch 4.116: Loss = 0.615646
Epoch 4.117: Loss = 0.694016
Epoch 4.118: Loss = 0.597534
Epoch 4.119: Loss = 0.647125
Epoch 4.120: Loss = 0.703705
TRAIN LOSS = 0.650757
TRAIN ACC = 79.5471 % (47730/60000)
Loss = 0.58049
Loss = 0.706375
Loss = 0.624313
Loss = 0.573044
Loss = 0.57225
Loss = 0.73111
Loss = 0.803619
Loss = 0.722198
Loss = 0.645599
Loss = 0.607361
Loss = 0.749405
Loss = 0.746643
Loss = 0.674911
Loss = 0.640182
Loss = 0.624847
Loss = 0.672943
Loss = 0.653976
Loss = 0.658905
Loss = 0.685638
Loss = 0.656372
TEST LOSS = 0.666509
TEST ACC = 477.299 % (7875/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.593765
Epoch 5.2: Loss = 0.605728
Epoch 5.3: Loss = 0.656067
Epoch 5.4: Loss = 0.609741
Epoch 5.5: Loss = 0.689514
Epoch 5.6: Loss = 0.681213
Epoch 5.7: Loss = 0.545883
Epoch 5.8: Loss = 0.651855
Epoch 5.9: Loss = 0.603546
Epoch 5.10: Loss = 0.726288
Epoch 5.11: Loss = 0.566071
Epoch 5.12: Loss = 0.652924
Epoch 5.13: Loss = 0.670059
Epoch 5.14: Loss = 0.588989
Epoch 5.15: Loss = 0.768005
Epoch 5.16: Loss = 0.572144
Epoch 5.17: Loss = 0.662323
Epoch 5.18: Loss = 0.704483
Epoch 5.19: Loss = 0.726578
Epoch 5.20: Loss = 0.631561
Epoch 5.21: Loss = 0.652161
Epoch 5.22: Loss = 0.692047
Epoch 5.23: Loss = 0.604004
Epoch 5.24: Loss = 0.701904
Epoch 5.25: Loss = 0.684067
Epoch 5.26: Loss = 0.684631
Epoch 5.27: Loss = 0.605774
Epoch 5.28: Loss = 0.607666
Epoch 5.29: Loss = 0.561646
Epoch 5.30: Loss = 0.634995
Epoch 5.31: Loss = 0.68959
Epoch 5.32: Loss = 0.655594
Epoch 5.33: Loss = 0.678238
Epoch 5.34: Loss = 0.638153
Epoch 5.35: Loss = 0.70549
Epoch 5.36: Loss = 0.696091
Epoch 5.37: Loss = 0.736145
Epoch 5.38: Loss = 0.55043
Epoch 5.39: Loss = 0.721527
Epoch 5.40: Loss = 0.693069
Epoch 5.41: Loss = 0.798691
Epoch 5.42: Loss = 0.597794
Epoch 5.43: Loss = 0.631836
Epoch 5.44: Loss = 0.683792
Epoch 5.45: Loss = 0.667511
Epoch 5.46: Loss = 0.614746
Epoch 5.47: Loss = 0.547348
Epoch 5.48: Loss = 0.629349
Epoch 5.49: Loss = 0.707916
Epoch 5.50: Loss = 0.602081
Epoch 5.51: Loss = 0.69397
Epoch 5.52: Loss = 0.73912
Epoch 5.53: Loss = 0.681793
Epoch 5.54: Loss = 0.56189
Epoch 5.55: Loss = 0.641037
Epoch 5.56: Loss = 0.638336
Epoch 5.57: Loss = 0.66333
Epoch 5.58: Loss = 0.734207
Epoch 5.59: Loss = 0.61763
Epoch 5.60: Loss = 0.686676
Epoch 5.61: Loss = 0.672592
Epoch 5.62: Loss = 0.683838
Epoch 5.63: Loss = 0.558121
Epoch 5.64: Loss = 0.681793
Epoch 5.65: Loss = 0.679962
Epoch 5.66: Loss = 0.589981
Epoch 5.67: Loss = 0.759415
Epoch 5.68: Loss = 0.663635
Epoch 5.69: Loss = 0.656693
Epoch 5.70: Loss = 0.647125
Epoch 5.71: Loss = 0.611679
Epoch 5.72: Loss = 0.56041
Epoch 5.73: Loss = 0.649094
Epoch 5.74: Loss = 0.656967
Epoch 5.75: Loss = 0.565613
Epoch 5.76: Loss = 0.619003
Epoch 5.77: Loss = 0.720825
Epoch 5.78: Loss = 0.582993
Epoch 5.79: Loss = 0.561142
Epoch 5.80: Loss = 0.634293
Epoch 5.81: Loss = 0.605286
Epoch 5.82: Loss = 0.63089
Epoch 5.83: Loss = 0.527878
Epoch 5.84: Loss = 0.670654
Epoch 5.85: Loss = 0.670944
Epoch 5.86: Loss = 0.636978
Epoch 5.87: Loss = 0.75351
Epoch 5.88: Loss = 0.633865
Epoch 5.89: Loss = 0.627243
Epoch 5.90: Loss = 0.693466
Epoch 5.91: Loss = 0.644333
Epoch 5.92: Loss = 0.631134
Epoch 5.93: Loss = 0.663651
Epoch 5.94: Loss = 0.659988
Epoch 5.95: Loss = 0.642212
Epoch 5.96: Loss = 0.644852
Epoch 5.97: Loss = 0.681396
Epoch 5.98: Loss = 0.57106
Epoch 5.99: Loss = 0.76918
Epoch 5.100: Loss = 0.628586
Epoch 5.101: Loss = 0.593079
Epoch 5.102: Loss = 0.648254
Epoch 5.103: Loss = 0.7258
Epoch 5.104: Loss = 0.62674
Epoch 5.105: Loss = 0.645233
Epoch 5.106: Loss = 0.535202
Epoch 5.107: Loss = 0.714218
Epoch 5.108: Loss = 0.577896
Epoch 5.109: Loss = 0.6026
Epoch 5.110: Loss = 0.637161
Epoch 5.111: Loss = 0.705841
Epoch 5.112: Loss = 0.733902
Epoch 5.113: Loss = 0.643951
Epoch 5.114: Loss = 0.616226
Epoch 5.115: Loss = 0.555328
Epoch 5.116: Loss = 0.652954
Epoch 5.117: Loss = 0.643646
Epoch 5.118: Loss = 0.668198
Epoch 5.119: Loss = 0.68895
Epoch 5.120: Loss = 0.725952
TRAIN LOSS = 0.649368
TRAIN ACC = 80.3741 % (48227/60000)
Loss = 0.593506
Loss = 0.711166
Loss = 0.640182
Loss = 0.559387
Loss = 0.608673
Loss = 0.749557
Loss = 0.810349
Loss = 0.751556
Loss = 0.679504
Loss = 0.632278
Loss = 0.792938
Loss = 0.805359
Loss = 0.699493
Loss = 0.690002
Loss = 0.66153
Loss = 0.664978
Loss = 0.688293
Loss = 0.700119
Loss = 0.730881
Loss = 0.657333
TEST LOSS = 0.691354
TEST ACC = 482.269 % (7950/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.671768
Epoch 6.2: Loss = 0.630539
Epoch 6.3: Loss = 0.516174
Epoch 6.4: Loss = 0.668228
Epoch 6.5: Loss = 0.573471
Epoch 6.6: Loss = 0.584061
Epoch 6.7: Loss = 0.61911
Epoch 6.8: Loss = 0.626999
Epoch 6.9: Loss = 0.706635
Epoch 6.10: Loss = 0.619171
Epoch 6.11: Loss = 0.703201
Epoch 6.12: Loss = 0.576004
Epoch 6.13: Loss = 0.672806
Epoch 6.14: Loss = 0.590958
Epoch 6.15: Loss = 0.56012
Epoch 6.16: Loss = 0.665771
Epoch 6.17: Loss = 0.728287
Epoch 6.18: Loss = 0.842331
Epoch 6.19: Loss = 0.62822
Epoch 6.20: Loss = 0.644714
Epoch 6.21: Loss = 0.581116
Epoch 6.22: Loss = 0.591187
Epoch 6.23: Loss = 0.638977
Epoch 6.24: Loss = 0.625732
Epoch 6.25: Loss = 0.641266
Epoch 6.26: Loss = 0.607941
Epoch 6.27: Loss = 0.655457
Epoch 6.28: Loss = 0.629059
Epoch 6.29: Loss = 0.557877
Epoch 6.30: Loss = 0.647232
Epoch 6.31: Loss = 0.692459
Epoch 6.32: Loss = 0.586578
Epoch 6.33: Loss = 0.545197
Epoch 6.34: Loss = 0.681946
Epoch 6.35: Loss = 0.618591
Epoch 6.36: Loss = 0.62326
Epoch 6.37: Loss = 0.746613
Epoch 6.38: Loss = 0.619827
Epoch 6.39: Loss = 0.707199
Epoch 6.40: Loss = 0.663818
Epoch 6.41: Loss = 0.608978
Epoch 6.42: Loss = 0.667709
Epoch 6.43: Loss = 0.574341
Epoch 6.44: Loss = 0.663223
Epoch 6.45: Loss = 0.751129
Epoch 6.46: Loss = 0.617508
Epoch 6.47: Loss = 0.682419
Epoch 6.48: Loss = 0.626373
Epoch 6.49: Loss = 0.549805
Epoch 6.50: Loss = 0.632629
Epoch 6.51: Loss = 0.690094
Epoch 6.52: Loss = 0.728348
Epoch 6.53: Loss = 0.736023
Epoch 6.54: Loss = 0.736115
Epoch 6.55: Loss = 0.600342
Epoch 6.56: Loss = 0.592911
Epoch 6.57: Loss = 0.664795
Epoch 6.58: Loss = 0.637344
Epoch 6.59: Loss = 0.720337
Epoch 6.60: Loss = 0.705414
Epoch 6.61: Loss = 0.734955
Epoch 6.62: Loss = 0.645554
Epoch 6.63: Loss = 0.546875
Epoch 6.64: Loss = 0.640808
Epoch 6.65: Loss = 0.696609
Epoch 6.66: Loss = 0.635315
Epoch 6.67: Loss = 0.606812
Epoch 6.68: Loss = 0.690399
Epoch 6.69: Loss = 0.633728
Epoch 6.70: Loss = 0.725037
Epoch 6.71: Loss = 0.634247
Epoch 6.72: Loss = 0.653854
Epoch 6.73: Loss = 0.654785
Epoch 6.74: Loss = 0.539642
Epoch 6.75: Loss = 0.530502
Epoch 6.76: Loss = 0.626572
Epoch 6.77: Loss = 0.616806
Epoch 6.78: Loss = 0.659332
Epoch 6.79: Loss = 0.68454
Epoch 6.80: Loss = 0.810547
Epoch 6.81: Loss = 0.638397
Epoch 6.82: Loss = 0.703201
Epoch 6.83: Loss = 0.692581
Epoch 6.84: Loss = 0.65593
Epoch 6.85: Loss = 0.648987
Epoch 6.86: Loss = 0.644684
Epoch 6.87: Loss = 0.780975
Epoch 6.88: Loss = 0.6064
Epoch 6.89: Loss = 0.649139
Epoch 6.90: Loss = 0.682465
Epoch 6.91: Loss = 0.794205
Epoch 6.92: Loss = 0.66272
Epoch 6.93: Loss = 0.616379
Epoch 6.94: Loss = 0.668289
Epoch 6.95: Loss = 0.724548
Epoch 6.96: Loss = 0.721298
Epoch 6.97: Loss = 0.686844
Epoch 6.98: Loss = 0.767029
Epoch 6.99: Loss = 0.715088
Epoch 6.100: Loss = 0.548386
Epoch 6.101: Loss = 0.727631
Epoch 6.102: Loss = 0.732758
Epoch 6.103: Loss = 0.671799
Epoch 6.104: Loss = 0.654526
Epoch 6.105: Loss = 0.647369
Epoch 6.106: Loss = 0.715729
Epoch 6.107: Loss = 0.811859
Epoch 6.108: Loss = 0.609573
Epoch 6.109: Loss = 0.596085
Epoch 6.110: Loss = 0.598297
Epoch 6.111: Loss = 0.536591
Epoch 6.112: Loss = 0.624771
Epoch 6.113: Loss = 0.711273
Epoch 6.114: Loss = 0.834778
Epoch 6.115: Loss = 0.70076
Epoch 6.116: Loss = 0.674576
Epoch 6.117: Loss = 0.55304
Epoch 6.118: Loss = 0.666656
Epoch 6.119: Loss = 0.646988
Epoch 6.120: Loss = 0.702591
TRAIN LOSS = 0.655579
TRAIN ACC = 80.7602 % (48459/60000)
Loss = 0.574265
Loss = 0.705215
Loss = 0.600525
Loss = 0.547455
Loss = 0.612488
Loss = 0.755264
Loss = 0.795059
Loss = 0.707336
Loss = 0.657715
Loss = 0.614594
Loss = 0.778534
Loss = 0.790878
Loss = 0.699738
Loss = 0.667358
Loss = 0.654144
Loss = 0.662689
Loss = 0.644531
Loss = 0.682144
Loss = 0.695282
Loss = 0.660294
TEST LOSS = 0.675275
TEST ACC = 484.589 % (8011/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.570145
Epoch 7.2: Loss = 0.715134
Epoch 7.3: Loss = 0.655914
Epoch 7.4: Loss = 0.791275
Epoch 7.5: Loss = 0.672882
Epoch 7.6: Loss = 0.597366
Epoch 7.7: Loss = 0.633255
Epoch 7.8: Loss = 0.60022
Epoch 7.9: Loss = 0.595276
Epoch 7.10: Loss = 0.763229
Epoch 7.11: Loss = 0.720032
Epoch 7.12: Loss = 0.676483
Epoch 7.13: Loss = 0.656281
Epoch 7.14: Loss = 0.588943
Epoch 7.15: Loss = 0.598618
Epoch 7.16: Loss = 0.710342
Epoch 7.17: Loss = 0.544922
Epoch 7.18: Loss = 0.616776
Epoch 7.19: Loss = 0.623184
Epoch 7.20: Loss = 0.675079
Epoch 7.21: Loss = 0.594635
Epoch 7.22: Loss = 0.66655
Epoch 7.23: Loss = 0.615234
Epoch 7.24: Loss = 0.641724
Epoch 7.25: Loss = 0.645706
Epoch 7.26: Loss = 0.713669
Epoch 7.27: Loss = 0.569168
Epoch 7.28: Loss = 0.703293
Epoch 7.29: Loss = 0.717514
Epoch 7.30: Loss = 0.594879
Epoch 7.31: Loss = 0.689957
Epoch 7.32: Loss = 0.643555
Epoch 7.33: Loss = 0.567429
Epoch 7.34: Loss = 0.619873
Epoch 7.35: Loss = 0.687103
Epoch 7.36: Loss = 0.607178
Epoch 7.37: Loss = 0.741516
Epoch 7.38: Loss = 0.735641
Epoch 7.39: Loss = 0.641251
Epoch 7.40: Loss = 0.696381
Epoch 7.41: Loss = 0.716644
Epoch 7.42: Loss = 0.56636
Epoch 7.43: Loss = 0.682251
Epoch 7.44: Loss = 0.716675
Epoch 7.45: Loss = 0.685379
Epoch 7.46: Loss = 0.687912
Epoch 7.47: Loss = 0.745636
Epoch 7.48: Loss = 0.560608
Epoch 7.49: Loss = 0.639023
Epoch 7.50: Loss = 0.571121
Epoch 7.51: Loss = 0.672058
Epoch 7.52: Loss = 0.624695
Epoch 7.53: Loss = 0.60495
Epoch 7.54: Loss = 0.557922
Epoch 7.55: Loss = 0.629822
Epoch 7.56: Loss = 0.632736
Epoch 7.57: Loss = 0.730804
Epoch 7.58: Loss = 0.813919
Epoch 7.59: Loss = 0.610641
Epoch 7.60: Loss = 0.57869
Epoch 7.61: Loss = 0.506332
Epoch 7.62: Loss = 0.598923
Epoch 7.63: Loss = 0.738602
Epoch 7.64: Loss = 0.743378
Epoch 7.65: Loss = 0.622375
Epoch 7.66: Loss = 0.696457
Epoch 7.67: Loss = 0.657486
Epoch 7.68: Loss = 0.57077
Epoch 7.69: Loss = 0.659958
Epoch 7.70: Loss = 0.544785
Epoch 7.71: Loss = 0.723236
Epoch 7.72: Loss = 0.706192
Epoch 7.73: Loss = 0.568802
Epoch 7.74: Loss = 0.683609
Epoch 7.75: Loss = 0.744675
Epoch 7.76: Loss = 0.66452
Epoch 7.77: Loss = 0.715561
Epoch 7.78: Loss = 0.771652
Epoch 7.79: Loss = 0.674103
Epoch 7.80: Loss = 0.626801
Epoch 7.81: Loss = 0.794174
Epoch 7.82: Loss = 0.679977
Epoch 7.83: Loss = 0.577438
Epoch 7.84: Loss = 0.683075
Epoch 7.85: Loss = 0.65329
Epoch 7.86: Loss = 0.71933
Epoch 7.87: Loss = 0.608826
Epoch 7.88: Loss = 0.629181
Epoch 7.89: Loss = 0.702774
Epoch 7.90: Loss = 0.810043
Epoch 7.91: Loss = 0.760529
Epoch 7.92: Loss = 0.689621
Epoch 7.93: Loss = 0.766068
Epoch 7.94: Loss = 0.716705
Epoch 7.95: Loss = 0.686264
Epoch 7.96: Loss = 0.667709
Epoch 7.97: Loss = 0.630646
Epoch 7.98: Loss = 0.77916
Epoch 7.99: Loss = 0.674042
Epoch 7.100: Loss = 0.666855
Epoch 7.101: Loss = 0.75235
Epoch 7.102: Loss = 0.609451
Epoch 7.103: Loss = 0.733276
Epoch 7.104: Loss = 0.702011
Epoch 7.105: Loss = 0.679291
Epoch 7.106: Loss = 0.62915
Epoch 7.107: Loss = 0.679657
Epoch 7.108: Loss = 0.657578
Epoch 7.109: Loss = 0.479721
Epoch 7.110: Loss = 0.626373
Epoch 7.111: Loss = 0.661728
Epoch 7.112: Loss = 0.718094
Epoch 7.113: Loss = 0.689056
Epoch 7.114: Loss = 0.696472
Epoch 7.115: Loss = 0.774902
Epoch 7.116: Loss = 0.649185
Epoch 7.117: Loss = 0.665527
Epoch 7.118: Loss = 0.642242
Epoch 7.119: Loss = 0.748077
Epoch 7.120: Loss = 0.662292
TRAIN LOSS = 0.664154
TRAIN ACC = 81.1234 % (48676/60000)
Loss = 0.579193
Loss = 0.720016
Loss = 0.597626
Loss = 0.542694
Loss = 0.601578
Loss = 0.759689
Loss = 0.81073
Loss = 0.73056
Loss = 0.6689
Loss = 0.624313
Loss = 0.781464
Loss = 0.796188
Loss = 0.695724
Loss = 0.668915
Loss = 0.652008
Loss = 0.674759
Loss = 0.67305
Loss = 0.676559
Loss = 0.722778
Loss = 0.654022
TEST LOSS = 0.681538
TEST ACC = 486.758 % (8032/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.636703
Epoch 8.2: Loss = 0.61734
Epoch 8.3: Loss = 0.488007
Epoch 8.4: Loss = 0.818359
Epoch 8.5: Loss = 0.687057
Epoch 8.6: Loss = 0.725174
Epoch 8.7: Loss = 0.725845
Epoch 8.8: Loss = 0.778793
Epoch 8.9: Loss = 0.657349
Epoch 8.10: Loss = 0.637497
Epoch 8.11: Loss = 0.577393
Epoch 8.12: Loss = 0.634445
Epoch 8.13: Loss = 0.679489
Epoch 8.14: Loss = 0.614883
Epoch 8.15: Loss = 0.643387
Epoch 8.16: Loss = 0.629395
Epoch 8.17: Loss = 0.70224
Epoch 8.18: Loss = 0.722412
Epoch 8.19: Loss = 0.738525
Epoch 8.20: Loss = 0.665115
Epoch 8.21: Loss = 0.696182
Epoch 8.22: Loss = 0.72998
Epoch 8.23: Loss = 0.712509
Epoch 8.24: Loss = 0.6875
Epoch 8.25: Loss = 0.55162
Epoch 8.26: Loss = 0.575134
Epoch 8.27: Loss = 0.623032
Epoch 8.28: Loss = 0.640747
Epoch 8.29: Loss = 0.69223
Epoch 8.30: Loss = 0.600449
Epoch 8.31: Loss = 0.685059
Epoch 8.32: Loss = 0.659134
Epoch 8.33: Loss = 0.616455
Epoch 8.34: Loss = 0.784256
Epoch 8.35: Loss = 0.649963
Epoch 8.36: Loss = 0.650391
Epoch 8.37: Loss = 0.604919
Epoch 8.38: Loss = 0.576294
Epoch 8.39: Loss = 0.720184
Epoch 8.40: Loss = 0.648102
Epoch 8.41: Loss = 0.827728
Epoch 8.42: Loss = 0.494293
Epoch 8.43: Loss = 0.655869
Epoch 8.44: Loss = 0.594254
Epoch 8.45: Loss = 0.592377
Epoch 8.46: Loss = 0.76384
Epoch 8.47: Loss = 0.647385
Epoch 8.48: Loss = 0.562622
Epoch 8.49: Loss = 0.834183
Epoch 8.50: Loss = 0.760117
Epoch 8.51: Loss = 0.648453
Epoch 8.52: Loss = 0.541229
Epoch 8.53: Loss = 0.611893
Epoch 8.54: Loss = 0.798477
Epoch 8.55: Loss = 0.606964
Epoch 8.56: Loss = 0.671692
Epoch 8.57: Loss = 0.627197
Epoch 8.58: Loss = 0.588333
Epoch 8.59: Loss = 0.637985
Epoch 8.60: Loss = 0.696701
Epoch 8.61: Loss = 0.529099
Epoch 8.62: Loss = 0.61203
Epoch 8.63: Loss = 0.600632
Epoch 8.64: Loss = 0.63147
Epoch 8.65: Loss = 0.789505
Epoch 8.66: Loss = 0.539459
Epoch 8.67: Loss = 0.734909
Epoch 8.68: Loss = 0.752487
Epoch 8.69: Loss = 0.651886
Epoch 8.70: Loss = 0.625809
Epoch 8.71: Loss = 0.759964
Epoch 8.72: Loss = 0.602905
Epoch 8.73: Loss = 0.718597
Epoch 8.74: Loss = 0.724518
Epoch 8.75: Loss = 0.670654
Epoch 8.76: Loss = 0.551971
Epoch 8.77: Loss = 0.584198
Epoch 8.78: Loss = 0.744492
Epoch 8.79: Loss = 0.625229
Epoch 8.80: Loss = 0.634766
Epoch 8.81: Loss = 0.791092
Epoch 8.82: Loss = 0.73439
Epoch 8.83: Loss = 0.753006
Epoch 8.84: Loss = 0.651199
Epoch 8.85: Loss = 0.661758
Epoch 8.86: Loss = 0.58963
Epoch 8.87: Loss = 0.722992
Epoch 8.88: Loss = 0.680527
Epoch 8.89: Loss = 0.596375
Epoch 8.90: Loss = 0.764465
Epoch 8.91: Loss = 0.640305
Epoch 8.92: Loss = 0.735382
Epoch 8.93: Loss = 0.669937
Epoch 8.94: Loss = 0.586441
Epoch 8.95: Loss = 0.685501
Epoch 8.96: Loss = 0.589157
Epoch 8.97: Loss = 0.797333
Epoch 8.98: Loss = 0.658279
Epoch 8.99: Loss = 0.667206
Epoch 8.100: Loss = 0.662918
Epoch 8.101: Loss = 0.60704
Epoch 8.102: Loss = 0.590408
Epoch 8.103: Loss = 0.743484
Epoch 8.104: Loss = 0.729858
Epoch 8.105: Loss = 0.647064
Epoch 8.106: Loss = 0.565033
Epoch 8.107: Loss = 0.71312
Epoch 8.108: Loss = 0.653107
Epoch 8.109: Loss = 0.707458
Epoch 8.110: Loss = 0.734909
Epoch 8.111: Loss = 0.798676
Epoch 8.112: Loss = 0.728302
Epoch 8.113: Loss = 0.69455
Epoch 8.114: Loss = 0.65416
Epoch 8.115: Loss = 0.75119
Epoch 8.116: Loss = 0.680801
Epoch 8.117: Loss = 0.6371
Epoch 8.118: Loss = 0.684036
Epoch 8.119: Loss = 0.663574
Epoch 8.120: Loss = 0.667068
TRAIN LOSS = 0.66684
TRAIN ACC = 81.308 % (48787/60000)
Loss = 0.608688
Loss = 0.755615
Loss = 0.624985
Loss = 0.571274
Loss = 0.660446
Loss = 0.774979
Loss = 0.852158
Loss = 0.755081
Loss = 0.712784
Loss = 0.674957
Loss = 0.813431
Loss = 0.860641
Loss = 0.745041
Loss = 0.731964
Loss = 0.723785
Loss = 0.696335
Loss = 0.709961
Loss = 0.724365
Loss = 0.805283
Loss = 0.682281
TEST LOSS = 0.724203
TEST ACC = 487.869 % (8044/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.546783
Epoch 9.2: Loss = 0.67424
Epoch 9.3: Loss = 0.724655
Epoch 9.4: Loss = 0.775284
Epoch 9.5: Loss = 0.720947
Epoch 9.6: Loss = 0.634537
Epoch 9.7: Loss = 0.771545
Epoch 9.8: Loss = 0.816788
Epoch 9.9: Loss = 0.657516
Epoch 9.10: Loss = 0.626816
Epoch 9.11: Loss = 0.731201
Epoch 9.12: Loss = 0.609268
Epoch 9.13: Loss = 0.696884
Epoch 9.14: Loss = 0.600296
Epoch 9.15: Loss = 0.598541
Epoch 9.16: Loss = 0.737259
Epoch 9.17: Loss = 0.69899
Epoch 9.18: Loss = 0.647415
Epoch 9.19: Loss = 0.624435
Epoch 9.20: Loss = 0.579254
Epoch 9.21: Loss = 0.773956
Epoch 9.22: Loss = 0.561401
Epoch 9.23: Loss = 0.803772
Epoch 9.24: Loss = 0.541916
Epoch 9.25: Loss = 0.7285
Epoch 9.26: Loss = 0.867844
Epoch 9.27: Loss = 0.665131
Epoch 9.28: Loss = 0.651642
Epoch 9.29: Loss = 0.76004
Epoch 9.30: Loss = 0.624313
Epoch 9.31: Loss = 0.721268
Epoch 9.32: Loss = 0.709167
Epoch 9.33: Loss = 0.663422
Epoch 9.34: Loss = 0.698212
Epoch 9.35: Loss = 0.654327
Epoch 9.36: Loss = 0.645782
Epoch 9.37: Loss = 0.631195
Epoch 9.38: Loss = 0.520142
Epoch 9.39: Loss = 0.632797
Epoch 9.40: Loss = 0.740067
Epoch 9.41: Loss = 0.648453
Epoch 9.42: Loss = 0.870255
Epoch 9.43: Loss = 0.709885
Epoch 9.44: Loss = 0.626144
Epoch 9.45: Loss = 0.691116
Epoch 9.46: Loss = 0.757614
Epoch 9.47: Loss = 0.697876
Epoch 9.48: Loss = 0.623627
Epoch 9.49: Loss = 0.655518
Epoch 9.50: Loss = 0.671616
Epoch 9.51: Loss = 0.693939
Epoch 9.52: Loss = 0.61908
Epoch 9.53: Loss = 0.742722
Epoch 9.54: Loss = 0.742508
Epoch 9.55: Loss = 0.559296
Epoch 9.56: Loss = 0.638504
Epoch 9.57: Loss = 0.55542
Epoch 9.58: Loss = 0.646515
Epoch 9.59: Loss = 0.696335
Epoch 9.60: Loss = 0.587418
Epoch 9.61: Loss = 0.590103
Epoch 9.62: Loss = 0.618805
Epoch 9.63: Loss = 0.645859
Epoch 9.64: Loss = 0.588196
Epoch 9.65: Loss = 0.717117
Epoch 9.66: Loss = 0.667236
Epoch 9.67: Loss = 0.557571
Epoch 9.68: Loss = 0.593445
Epoch 9.69: Loss = 0.738693
Epoch 9.70: Loss = 0.612228
Epoch 9.71: Loss = 0.607254
Epoch 9.72: Loss = 0.645981
Epoch 9.73: Loss = 0.602982
Epoch 9.74: Loss = 0.666855
Epoch 9.75: Loss = 0.649673
Epoch 9.76: Loss = 0.623169
Epoch 9.77: Loss = 0.538467
Epoch 9.78: Loss = 0.695755
Epoch 9.79: Loss = 0.563309
Epoch 9.80: Loss = 0.716461
Epoch 9.81: Loss = 0.70787
Epoch 9.82: Loss = 0.674881
Epoch 9.83: Loss = 0.658691
Epoch 9.84: Loss = 0.703857
Epoch 9.85: Loss = 0.847885
Epoch 9.86: Loss = 0.618439
Epoch 9.87: Loss = 0.683395
Epoch 9.88: Loss = 0.724075
Epoch 9.89: Loss = 0.673233
Epoch 9.90: Loss = 0.579941
Epoch 9.91: Loss = 0.744904
Epoch 9.92: Loss = 0.572388
Epoch 9.93: Loss = 0.604538
Epoch 9.94: Loss = 0.527115
Epoch 9.95: Loss = 0.664139
Epoch 9.96: Loss = 0.628372
Epoch 9.97: Loss = 0.55368
Epoch 9.98: Loss = 0.583099
Epoch 9.99: Loss = 0.680359
Epoch 9.100: Loss = 0.676651
Epoch 9.101: Loss = 0.693558
Epoch 9.102: Loss = 0.683594
Epoch 9.103: Loss = 0.545593
Epoch 9.104: Loss = 0.606628
Epoch 9.105: Loss = 0.751694
Epoch 9.106: Loss = 0.562607
Epoch 9.107: Loss = 0.662598
Epoch 9.108: Loss = 0.620621
Epoch 9.109: Loss = 0.792191
Epoch 9.110: Loss = 0.674911
Epoch 9.111: Loss = 0.706543
Epoch 9.112: Loss = 0.719513
Epoch 9.113: Loss = 0.709091
Epoch 9.114: Loss = 0.636871
Epoch 9.115: Loss = 0.693253
Epoch 9.116: Loss = 0.81633
Epoch 9.117: Loss = 0.601501
Epoch 9.118: Loss = 0.673065
Epoch 9.119: Loss = 0.524368
Epoch 9.120: Loss = 0.618896
TRAIN LOSS = 0.662872
TRAIN ACC = 81.7108 % (49028/60000)
Loss = 0.599289
Loss = 0.721741
Loss = 0.585236
Loss = 0.55661
Loss = 0.632416
Loss = 0.781113
Loss = 0.847198
Loss = 0.73912
Loss = 0.68045
Loss = 0.639862
Loss = 0.804153
Loss = 0.78334
Loss = 0.732773
Loss = 0.71846
Loss = 0.688385
Loss = 0.680862
Loss = 0.653992
Loss = 0.712891
Loss = 0.762283
Loss = 0.633438
TEST LOSS = 0.69768
TEST ACC = 490.279 % (8088/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.779083
Epoch 10.2: Loss = 0.741516
Epoch 10.3: Loss = 0.664886
Epoch 10.4: Loss = 0.600861
Epoch 10.5: Loss = 0.620712
Epoch 10.6: Loss = 0.563141
Epoch 10.7: Loss = 0.774765
Epoch 10.8: Loss = 0.671127
Epoch 10.9: Loss = 0.644745
Epoch 10.10: Loss = 0.652359
Epoch 10.11: Loss = 0.634384
Epoch 10.12: Loss = 0.64682
Epoch 10.13: Loss = 0.632339
Epoch 10.14: Loss = 0.58197
Epoch 10.15: Loss = 0.582642
Epoch 10.16: Loss = 0.611862
Epoch 10.17: Loss = 0.800507
Epoch 10.18: Loss = 0.562927
Epoch 10.19: Loss = 0.715134
Epoch 10.20: Loss = 0.606705
Epoch 10.21: Loss = 0.679123
Epoch 10.22: Loss = 0.79216
Epoch 10.23: Loss = 0.559082
Epoch 10.24: Loss = 0.662201
Epoch 10.25: Loss = 0.638443
Epoch 10.26: Loss = 0.657547
Epoch 10.27: Loss = 0.621735
Epoch 10.28: Loss = 0.610138
Epoch 10.29: Loss = 0.625244
Epoch 10.30: Loss = 0.556183
Epoch 10.31: Loss = 0.654724
Epoch 10.32: Loss = 0.692444
Epoch 10.33: Loss = 0.703262
Epoch 10.34: Loss = 0.670364
Epoch 10.35: Loss = 0.629333
Epoch 10.36: Loss = 0.638535
Epoch 10.37: Loss = 0.779724
Epoch 10.38: Loss = 0.633636
Epoch 10.39: Loss = 0.798065
Epoch 10.40: Loss = 0.599274
Epoch 10.41: Loss = 0.780731
Epoch 10.42: Loss = 0.573776
Epoch 10.43: Loss = 0.730515
Epoch 10.44: Loss = 0.605194
Epoch 10.45: Loss = 0.449448
Epoch 10.46: Loss = 0.600571
Epoch 10.47: Loss = 0.761185
Epoch 10.48: Loss = 0.915482
Epoch 10.49: Loss = 0.674088
Epoch 10.50: Loss = 0.64447
Epoch 10.51: Loss = 0.632538
Epoch 10.52: Loss = 0.66214
Epoch 10.53: Loss = 0.683975
Epoch 10.54: Loss = 0.628708
Epoch 10.55: Loss = 0.720657
Epoch 10.56: Loss = 0.775146
Epoch 10.57: Loss = 0.666779
Epoch 10.58: Loss = 0.717651
Epoch 10.59: Loss = 0.654846
Epoch 10.60: Loss = 0.626999
Epoch 10.61: Loss = 0.725571
Epoch 10.62: Loss = 0.609268
Epoch 10.63: Loss = 0.638748
Epoch 10.64: Loss = 0.708984
Epoch 10.65: Loss = 0.708023
Epoch 10.66: Loss = 0.584915
Epoch 10.67: Loss = 0.662766
Epoch 10.68: Loss = 0.751709
Epoch 10.69: Loss = 0.631821
Epoch 10.70: Loss = 0.628922
Epoch 10.71: Loss = 0.733505
Epoch 10.72: Loss = 0.620728
Epoch 10.73: Loss = 0.768433
Epoch 10.74: Loss = 0.62056
Epoch 10.75: Loss = 0.741302
Epoch 10.76: Loss = 0.637512
Epoch 10.77: Loss = 0.682465
Epoch 10.78: Loss = 0.611633
Epoch 10.79: Loss = 0.558792
Epoch 10.80: Loss = 0.769363
Epoch 10.81: Loss = 0.610352
Epoch 10.82: Loss = 0.608841
Epoch 10.83: Loss = 0.671997
Epoch 10.84: Loss = 0.688492
Epoch 10.85: Loss = 0.71254
Epoch 10.86: Loss = 0.635132
Epoch 10.87: Loss = 0.746475
Epoch 10.88: Loss = 0.662598
Epoch 10.89: Loss = 0.530075
Epoch 10.90: Loss = 0.647766
Epoch 10.91: Loss = 0.735214
Epoch 10.92: Loss = 0.715546
Epoch 10.93: Loss = 0.648148
Epoch 10.94: Loss = 0.708176
Epoch 10.95: Loss = 0.563019
Epoch 10.96: Loss = 0.811569
Epoch 10.97: Loss = 0.772812
Epoch 10.98: Loss = 0.626831
Epoch 10.99: Loss = 0.647919
Epoch 10.100: Loss = 0.669525
Epoch 10.101: Loss = 0.749084
Epoch 10.102: Loss = 0.628525
Epoch 10.103: Loss = 0.707489
Epoch 10.104: Loss = 0.662552
Epoch 10.105: Loss = 0.707718
Epoch 10.106: Loss = 0.535172
Epoch 10.107: Loss = 0.647491
Epoch 10.108: Loss = 0.728897
Epoch 10.109: Loss = 0.69754
Epoch 10.110: Loss = 0.549042
Epoch 10.111: Loss = 0.661896
Epoch 10.112: Loss = 0.67067
Epoch 10.113: Loss = 0.794449
Epoch 10.114: Loss = 0.662262
Epoch 10.115: Loss = 0.751541
Epoch 10.116: Loss = 0.577499
Epoch 10.117: Loss = 0.635437
Epoch 10.118: Loss = 0.51767
Epoch 10.119: Loss = 0.697266
Epoch 10.120: Loss = 0.720032
TRAIN LOSS = 0.665894
TRAIN ACC = 81.7307 % (49040/60000)
Loss = 0.582169
Loss = 0.730728
Loss = 0.587387
Loss = 0.560455
Loss = 0.63768
Loss = 0.788116
Loss = 0.865723
Loss = 0.746994
Loss = 0.659546
Loss = 0.624161
Loss = 0.824997
Loss = 0.779022
Loss = 0.722473
Loss = 0.707321
Loss = 0.69635
Loss = 0.683777
Loss = 0.665314
Loss = 0.716827
Loss = 0.74054
Loss = 0.649597
TEST LOSS = 0.698459
TEST ACC = 490.399 % (8082/10000)

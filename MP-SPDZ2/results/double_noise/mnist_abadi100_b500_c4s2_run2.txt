Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.43697
Epoch 1.2: Loss = 2.40363
Epoch 1.3: Loss = 2.3423
Epoch 1.4: Loss = 2.30702
Epoch 1.5: Loss = 2.25914
Epoch 1.6: Loss = 2.22353
Epoch 1.7: Loss = 2.18628
Epoch 1.8: Loss = 2.15007
Epoch 1.9: Loss = 2.09525
Epoch 1.10: Loss = 2.08089
Epoch 1.11: Loss = 2.05568
Epoch 1.12: Loss = 1.99794
Epoch 1.13: Loss = 1.97411
Epoch 1.14: Loss = 1.9576
Epoch 1.15: Loss = 1.88565
Epoch 1.16: Loss = 1.84491
Epoch 1.17: Loss = 1.81224
Epoch 1.18: Loss = 1.76379
Epoch 1.19: Loss = 1.74576
Epoch 1.20: Loss = 1.72559
Epoch 1.21: Loss = 1.70143
Epoch 1.22: Loss = 1.66229
Epoch 1.23: Loss = 1.66341
Epoch 1.24: Loss = 1.59668
Epoch 1.25: Loss = 1.55959
Epoch 1.26: Loss = 1.54341
Epoch 1.27: Loss = 1.55867
Epoch 1.28: Loss = 1.49097
Epoch 1.29: Loss = 1.47815
Epoch 1.30: Loss = 1.46155
Epoch 1.31: Loss = 1.42358
Epoch 1.32: Loss = 1.39978
Epoch 1.33: Loss = 1.35077
Epoch 1.34: Loss = 1.35455
Epoch 1.35: Loss = 1.30692
Epoch 1.36: Loss = 1.27641
Epoch 1.37: Loss = 1.31306
Epoch 1.38: Loss = 1.30348
Epoch 1.39: Loss = 1.25244
Epoch 1.40: Loss = 1.28429
Epoch 1.41: Loss = 1.18442
Epoch 1.42: Loss = 1.22459
Epoch 1.43: Loss = 1.22273
Epoch 1.44: Loss = 1.11543
Epoch 1.45: Loss = 1.13478
Epoch 1.46: Loss = 1.13353
Epoch 1.47: Loss = 1.16635
Epoch 1.48: Loss = 1.09241
Epoch 1.49: Loss = 1.10371
Epoch 1.50: Loss = 1.08833
Epoch 1.51: Loss = 1.11433
Epoch 1.52: Loss = 1.05624
Epoch 1.53: Loss = 1.02748
Epoch 1.54: Loss = 1.05986
Epoch 1.55: Loss = 0.952682
Epoch 1.56: Loss = 0.981506
Epoch 1.57: Loss = 0.966461
Epoch 1.58: Loss = 0.897339
Epoch 1.59: Loss = 1.00702
Epoch 1.60: Loss = 0.961899
Epoch 1.61: Loss = 0.922638
Epoch 1.62: Loss = 0.916183
Epoch 1.63: Loss = 1.00589
Epoch 1.64: Loss = 0.907974
Epoch 1.65: Loss = 0.985779
Epoch 1.66: Loss = 0.952866
Epoch 1.67: Loss = 0.96962
Epoch 1.68: Loss = 0.863983
Epoch 1.69: Loss = 0.863739
Epoch 1.70: Loss = 0.881714
Epoch 1.71: Loss = 0.888046
Epoch 1.72: Loss = 0.910782
Epoch 1.73: Loss = 0.845474
Epoch 1.74: Loss = 0.814713
Epoch 1.75: Loss = 0.852692
Epoch 1.76: Loss = 0.829407
Epoch 1.77: Loss = 0.782913
Epoch 1.78: Loss = 0.864471
Epoch 1.79: Loss = 0.787476
Epoch 1.80: Loss = 0.777664
Epoch 1.81: Loss = 0.810089
Epoch 1.82: Loss = 0.772125
Epoch 1.83: Loss = 0.784225
Epoch 1.84: Loss = 0.77182
Epoch 1.85: Loss = 0.828186
Epoch 1.86: Loss = 0.723053
Epoch 1.87: Loss = 0.760284
Epoch 1.88: Loss = 0.720291
Epoch 1.89: Loss = 0.714737
Epoch 1.90: Loss = 0.731461
Epoch 1.91: Loss = 0.702606
Epoch 1.92: Loss = 0.689484
Epoch 1.93: Loss = 0.669449
Epoch 1.94: Loss = 0.774628
Epoch 1.95: Loss = 0.760727
Epoch 1.96: Loss = 0.722656
Epoch 1.97: Loss = 0.645599
Epoch 1.98: Loss = 0.748962
Epoch 1.99: Loss = 0.650803
Epoch 1.100: Loss = 0.676834
Epoch 1.101: Loss = 0.72908
Epoch 1.102: Loss = 0.700378
Epoch 1.103: Loss = 0.691223
Epoch 1.104: Loss = 0.665146
Epoch 1.105: Loss = 0.686356
Epoch 1.106: Loss = 0.709274
Epoch 1.107: Loss = 0.651718
Epoch 1.108: Loss = 0.701965
Epoch 1.109: Loss = 0.651764
Epoch 1.110: Loss = 0.614166
Epoch 1.111: Loss = 0.623352
Epoch 1.112: Loss = 0.66626
Epoch 1.113: Loss = 0.601318
Epoch 1.114: Loss = 0.618378
Epoch 1.115: Loss = 0.63652
Epoch 1.116: Loss = 0.674973
Epoch 1.117: Loss = 0.662796
Epoch 1.118: Loss = 0.661102
Epoch 1.119: Loss = 0.674026
Epoch 1.120: Loss = 0.650406
TRAIN LOSS = 1.14395
TRAIN ACC = 66.9449 % (40169/60000)
Loss = 0.660934
Loss = 0.676315
Loss = 0.78595
Loss = 0.741043
Loss = 0.753922
Loss = 0.680527
Loss = 0.617691
Loss = 0.781677
Loss = 0.745636
Loss = 0.683289
Loss = 0.397675
Loss = 0.531372
Loss = 0.384537
Loss = 0.581055
Loss = 0.485046
Loss = 0.472916
Loss = 0.422928
Loss = 0.264587
Loss = 0.459061
Loss = 0.716354
TEST LOSS = 0.592126
TEST ACC = 401.689 % (8246/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.591644
Epoch 2.2: Loss = 0.588257
Epoch 2.3: Loss = 0.555832
Epoch 2.4: Loss = 0.677399
Epoch 2.5: Loss = 0.627563
Epoch 2.6: Loss = 0.603287
Epoch 2.7: Loss = 0.655792
Epoch 2.8: Loss = 0.647049
Epoch 2.9: Loss = 0.576691
Epoch 2.10: Loss = 0.603638
Epoch 2.11: Loss = 0.528564
Epoch 2.12: Loss = 0.54216
Epoch 2.13: Loss = 0.665359
Epoch 2.14: Loss = 0.520081
Epoch 2.15: Loss = 0.562042
Epoch 2.16: Loss = 0.546722
Epoch 2.17: Loss = 0.621231
Epoch 2.18: Loss = 0.54184
Epoch 2.19: Loss = 0.606522
Epoch 2.20: Loss = 0.612732
Epoch 2.21: Loss = 0.517334
Epoch 2.22: Loss = 0.537262
Epoch 2.23: Loss = 0.536392
Epoch 2.24: Loss = 0.611908
Epoch 2.25: Loss = 0.629181
Epoch 2.26: Loss = 0.544739
Epoch 2.27: Loss = 0.514801
Epoch 2.28: Loss = 0.518021
Epoch 2.29: Loss = 0.57785
Epoch 2.30: Loss = 0.585037
Epoch 2.31: Loss = 0.479492
Epoch 2.32: Loss = 0.533951
Epoch 2.33: Loss = 0.592484
Epoch 2.34: Loss = 0.582687
Epoch 2.35: Loss = 0.496811
Epoch 2.36: Loss = 0.525909
Epoch 2.37: Loss = 0.534439
Epoch 2.38: Loss = 0.539368
Epoch 2.39: Loss = 0.518784
Epoch 2.40: Loss = 0.532364
Epoch 2.41: Loss = 0.556503
Epoch 2.42: Loss = 0.587952
Epoch 2.43: Loss = 0.521225
Epoch 2.44: Loss = 0.4655
Epoch 2.45: Loss = 0.528305
Epoch 2.46: Loss = 0.543564
Epoch 2.47: Loss = 0.542923
Epoch 2.48: Loss = 0.520172
Epoch 2.49: Loss = 0.58728
Epoch 2.50: Loss = 0.515717
Epoch 2.51: Loss = 0.49324
Epoch 2.52: Loss = 0.555206
Epoch 2.53: Loss = 0.593506
Epoch 2.54: Loss = 0.573456
Epoch 2.55: Loss = 0.525208
Epoch 2.56: Loss = 0.520523
Epoch 2.57: Loss = 0.551819
Epoch 2.58: Loss = 0.51799
Epoch 2.59: Loss = 0.550217
Epoch 2.60: Loss = 0.564316
Epoch 2.61: Loss = 0.559189
Epoch 2.62: Loss = 0.485153
Epoch 2.63: Loss = 0.474854
Epoch 2.64: Loss = 0.552902
Epoch 2.65: Loss = 0.514786
Epoch 2.66: Loss = 0.435104
Epoch 2.67: Loss = 0.46579
Epoch 2.68: Loss = 0.518143
Epoch 2.69: Loss = 0.511749
Epoch 2.70: Loss = 0.486359
Epoch 2.71: Loss = 0.509308
Epoch 2.72: Loss = 0.462112
Epoch 2.73: Loss = 0.500153
Epoch 2.74: Loss = 0.496475
Epoch 2.75: Loss = 0.505585
Epoch 2.76: Loss = 0.484558
Epoch 2.77: Loss = 0.522354
Epoch 2.78: Loss = 0.464127
Epoch 2.79: Loss = 0.482147
Epoch 2.80: Loss = 0.490982
Epoch 2.81: Loss = 0.589798
Epoch 2.82: Loss = 0.512161
Epoch 2.83: Loss = 0.515366
Epoch 2.84: Loss = 0.524994
Epoch 2.85: Loss = 0.460037
Epoch 2.86: Loss = 0.448944
Epoch 2.87: Loss = 0.461548
Epoch 2.88: Loss = 0.487076
Epoch 2.89: Loss = 0.518356
Epoch 2.90: Loss = 0.49057
Epoch 2.91: Loss = 0.514481
Epoch 2.92: Loss = 0.460678
Epoch 2.93: Loss = 0.556656
Epoch 2.94: Loss = 0.476059
Epoch 2.95: Loss = 0.455399
Epoch 2.96: Loss = 0.467667
Epoch 2.97: Loss = 0.471176
Epoch 2.98: Loss = 0.425735
Epoch 2.99: Loss = 0.540665
Epoch 2.100: Loss = 0.513916
Epoch 2.101: Loss = 0.468979
Epoch 2.102: Loss = 0.434052
Epoch 2.103: Loss = 0.45752
Epoch 2.104: Loss = 0.432053
Epoch 2.105: Loss = 0.455566
Epoch 2.106: Loss = 0.439651
Epoch 2.107: Loss = 0.43573
Epoch 2.108: Loss = 0.35762
Epoch 2.109: Loss = 0.431
Epoch 2.110: Loss = 0.461929
Epoch 2.111: Loss = 0.462509
Epoch 2.112: Loss = 0.524918
Epoch 2.113: Loss = 0.49704
Epoch 2.114: Loss = 0.461365
Epoch 2.115: Loss = 0.428177
Epoch 2.116: Loss = 0.475693
Epoch 2.117: Loss = 0.403458
Epoch 2.118: Loss = 0.462357
Epoch 2.119: Loss = 0.439301
Epoch 2.120: Loss = 0.513977
TRAIN LOSS = 0.520462
TRAIN ACC = 84.5459 % (50730/60000)
Loss = 0.469727
Loss = 0.521362
Loss = 0.603531
Loss = 0.574875
Loss = 0.604538
Loss = 0.490646
Loss = 0.445343
Loss = 0.621918
Loss = 0.573242
Loss = 0.52449
Loss = 0.248474
Loss = 0.374039
Loss = 0.273773
Loss = 0.418121
Loss = 0.292191
Loss = 0.335907
Loss = 0.264832
Loss = 0.118744
Loss = 0.29274
Loss = 0.556519
TEST LOSS = 0.43025
TEST ACC = 507.3 % (8736/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.424042
Epoch 3.2: Loss = 0.505905
Epoch 3.3: Loss = 0.435257
Epoch 3.4: Loss = 0.439835
Epoch 3.5: Loss = 0.481216
Epoch 3.6: Loss = 0.463226
Epoch 3.7: Loss = 0.460602
Epoch 3.8: Loss = 0.507843
Epoch 3.9: Loss = 0.488129
Epoch 3.10: Loss = 0.427505
Epoch 3.11: Loss = 0.422333
Epoch 3.12: Loss = 0.45546
Epoch 3.13: Loss = 0.449448
Epoch 3.14: Loss = 0.434814
Epoch 3.15: Loss = 0.442535
Epoch 3.16: Loss = 0.495697
Epoch 3.17: Loss = 0.421844
Epoch 3.18: Loss = 0.486603
Epoch 3.19: Loss = 0.429504
Epoch 3.20: Loss = 0.462692
Epoch 3.21: Loss = 0.431976
Epoch 3.22: Loss = 0.396866
Epoch 3.23: Loss = 0.479385
Epoch 3.24: Loss = 0.450089
Epoch 3.25: Loss = 0.476624
Epoch 3.26: Loss = 0.479568
Epoch 3.27: Loss = 0.483536
Epoch 3.28: Loss = 0.393494
Epoch 3.29: Loss = 0.429352
Epoch 3.30: Loss = 0.481384
Epoch 3.31: Loss = 0.408066
Epoch 3.32: Loss = 0.419693
Epoch 3.33: Loss = 0.422928
Epoch 3.34: Loss = 0.424728
Epoch 3.35: Loss = 0.414459
Epoch 3.36: Loss = 0.44342
Epoch 3.37: Loss = 0.499863
Epoch 3.38: Loss = 0.470123
Epoch 3.39: Loss = 0.413681
Epoch 3.40: Loss = 0.456512
Epoch 3.41: Loss = 0.48056
Epoch 3.42: Loss = 0.460815
Epoch 3.43: Loss = 0.455704
Epoch 3.44: Loss = 0.450226
Epoch 3.45: Loss = 0.364029
Epoch 3.46: Loss = 0.429199
Epoch 3.47: Loss = 0.45314
Epoch 3.48: Loss = 0.361237
Epoch 3.49: Loss = 0.55722
Epoch 3.50: Loss = 0.431824
Epoch 3.51: Loss = 0.408447
Epoch 3.52: Loss = 0.365326
Epoch 3.53: Loss = 0.384277
Epoch 3.54: Loss = 0.397568
Epoch 3.55: Loss = 0.411987
Epoch 3.56: Loss = 0.383087
Epoch 3.57: Loss = 0.430084
Epoch 3.58: Loss = 0.462845
Epoch 3.59: Loss = 0.505859
Epoch 3.60: Loss = 0.364243
Epoch 3.61: Loss = 0.502594
Epoch 3.62: Loss = 0.39502
Epoch 3.63: Loss = 0.501862
Epoch 3.64: Loss = 0.388245
Epoch 3.65: Loss = 0.419754
Epoch 3.66: Loss = 0.453354
Epoch 3.67: Loss = 0.441864
Epoch 3.68: Loss = 0.411392
Epoch 3.69: Loss = 0.386627
Epoch 3.70: Loss = 0.428009
Epoch 3.71: Loss = 0.417191
Epoch 3.72: Loss = 0.428253
Epoch 3.73: Loss = 0.404053
Epoch 3.74: Loss = 0.460968
Epoch 3.75: Loss = 0.33609
Epoch 3.76: Loss = 0.351349
Epoch 3.77: Loss = 0.455505
Epoch 3.78: Loss = 0.462875
Epoch 3.79: Loss = 0.395386
Epoch 3.80: Loss = 0.467407
Epoch 3.81: Loss = 0.460007
Epoch 3.82: Loss = 0.442764
Epoch 3.83: Loss = 0.403366
Epoch 3.84: Loss = 0.382858
Epoch 3.85: Loss = 0.423645
Epoch 3.86: Loss = 0.517487
Epoch 3.87: Loss = 0.451767
Epoch 3.88: Loss = 0.4086
Epoch 3.89: Loss = 0.462997
Epoch 3.90: Loss = 0.462494
Epoch 3.91: Loss = 0.431427
Epoch 3.92: Loss = 0.44841
Epoch 3.93: Loss = 0.415497
Epoch 3.94: Loss = 0.415421
Epoch 3.95: Loss = 0.427155
Epoch 3.96: Loss = 0.44165
Epoch 3.97: Loss = 0.419983
Epoch 3.98: Loss = 0.429642
Epoch 3.99: Loss = 0.391129
Epoch 3.100: Loss = 0.449722
Epoch 3.101: Loss = 0.441696
Epoch 3.102: Loss = 0.388397
Epoch 3.103: Loss = 0.360336
Epoch 3.104: Loss = 0.438873
Epoch 3.105: Loss = 0.456787
Epoch 3.106: Loss = 0.452759
Epoch 3.107: Loss = 0.42598
Epoch 3.108: Loss = 0.368317
Epoch 3.109: Loss = 0.44342
Epoch 3.110: Loss = 0.490158
Epoch 3.111: Loss = 0.416504
Epoch 3.112: Loss = 0.366501
Epoch 3.113: Loss = 0.497574
Epoch 3.114: Loss = 0.450912
Epoch 3.115: Loss = 0.393234
Epoch 3.116: Loss = 0.444427
Epoch 3.117: Loss = 0.365067
Epoch 3.118: Loss = 0.415298
Epoch 3.119: Loss = 0.3759
Epoch 3.120: Loss = 0.390686
TRAIN LOSS = 0.43454
TRAIN ACC = 87.0682 % (52243/60000)
Loss = 0.41568
Loss = 0.477585
Loss = 0.545029
Loss = 0.543701
Loss = 0.569977
Loss = 0.431732
Loss = 0.384537
Loss = 0.599594
Loss = 0.538818
Loss = 0.496246
Loss = 0.222717
Loss = 0.317917
Loss = 0.272156
Loss = 0.381439
Loss = 0.235046
Loss = 0.311005
Loss = 0.230331
Loss = 0.0814667
Loss = 0.237686
Loss = 0.53569
TEST LOSS = 0.391418
TEST ACC = 522.429 % (8841/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.510391
Epoch 4.2: Loss = 0.462509
Epoch 4.3: Loss = 0.44873
Epoch 4.4: Loss = 0.447327
Epoch 4.5: Loss = 0.363205
Epoch 4.6: Loss = 0.408844
Epoch 4.7: Loss = 0.383301
Epoch 4.8: Loss = 0.47673
Epoch 4.9: Loss = 0.432144
Epoch 4.10: Loss = 0.422577
Epoch 4.11: Loss = 0.39978
Epoch 4.12: Loss = 0.419235
Epoch 4.13: Loss = 0.465485
Epoch 4.14: Loss = 0.403763
Epoch 4.15: Loss = 0.461426
Epoch 4.16: Loss = 0.455551
Epoch 4.17: Loss = 0.396179
Epoch 4.18: Loss = 0.477554
Epoch 4.19: Loss = 0.416687
Epoch 4.20: Loss = 0.447098
Epoch 4.21: Loss = 0.535934
Epoch 4.22: Loss = 0.412048
Epoch 4.23: Loss = 0.397842
Epoch 4.24: Loss = 0.382919
Epoch 4.25: Loss = 0.348877
Epoch 4.26: Loss = 0.432053
Epoch 4.27: Loss = 0.375793
Epoch 4.28: Loss = 0.361404
Epoch 4.29: Loss = 0.422852
Epoch 4.30: Loss = 0.39473
Epoch 4.31: Loss = 0.430389
Epoch 4.32: Loss = 0.408859
Epoch 4.33: Loss = 0.393738
Epoch 4.34: Loss = 0.382126
Epoch 4.35: Loss = 0.434097
Epoch 4.36: Loss = 0.417892
Epoch 4.37: Loss = 0.401321
Epoch 4.38: Loss = 0.388794
Epoch 4.39: Loss = 0.387802
Epoch 4.40: Loss = 0.460114
Epoch 4.41: Loss = 0.366623
Epoch 4.42: Loss = 0.432846
Epoch 4.43: Loss = 0.380798
Epoch 4.44: Loss = 0.406479
Epoch 4.45: Loss = 0.35881
Epoch 4.46: Loss = 0.329468
Epoch 4.47: Loss = 0.407776
Epoch 4.48: Loss = 0.418533
Epoch 4.49: Loss = 0.41687
Epoch 4.50: Loss = 0.42807
Epoch 4.51: Loss = 0.35434
Epoch 4.52: Loss = 0.379532
Epoch 4.53: Loss = 0.440155
Epoch 4.54: Loss = 0.357681
Epoch 4.55: Loss = 0.356216
Epoch 4.56: Loss = 0.371033
Epoch 4.57: Loss = 0.36554
Epoch 4.58: Loss = 0.399185
Epoch 4.59: Loss = 0.432678
Epoch 4.60: Loss = 0.466263
Epoch 4.61: Loss = 0.44873
Epoch 4.62: Loss = 0.382935
Epoch 4.63: Loss = 0.416122
Epoch 4.64: Loss = 0.353821
Epoch 4.65: Loss = 0.330658
Epoch 4.66: Loss = 0.436371
Epoch 4.67: Loss = 0.355789
Epoch 4.68: Loss = 0.469727
Epoch 4.69: Loss = 0.314362
Epoch 4.70: Loss = 0.392181
Epoch 4.71: Loss = 0.393204
Epoch 4.72: Loss = 0.401169
Epoch 4.73: Loss = 0.381973
Epoch 4.74: Loss = 0.404144
Epoch 4.75: Loss = 0.441803
Epoch 4.76: Loss = 0.356125
Epoch 4.77: Loss = 0.395981
Epoch 4.78: Loss = 0.428833
Epoch 4.79: Loss = 0.411453
Epoch 4.80: Loss = 0.322479
Epoch 4.81: Loss = 0.424149
Epoch 4.82: Loss = 0.441849
Epoch 4.83: Loss = 0.412094
Epoch 4.84: Loss = 0.345047
Epoch 4.85: Loss = 0.334183
Epoch 4.86: Loss = 0.316025
Epoch 4.87: Loss = 0.381378
Epoch 4.88: Loss = 0.38588
Epoch 4.89: Loss = 0.327774
Epoch 4.90: Loss = 0.444565
Epoch 4.91: Loss = 0.409348
Epoch 4.92: Loss = 0.459961
Epoch 4.93: Loss = 0.427505
Epoch 4.94: Loss = 0.392334
Epoch 4.95: Loss = 0.409271
Epoch 4.96: Loss = 0.347961
Epoch 4.97: Loss = 0.313858
Epoch 4.98: Loss = 0.496216
Epoch 4.99: Loss = 0.399948
Epoch 4.100: Loss = 0.406235
Epoch 4.101: Loss = 0.399017
Epoch 4.102: Loss = 0.487366
Epoch 4.103: Loss = 0.304398
Epoch 4.104: Loss = 0.453781
Epoch 4.105: Loss = 0.464386
Epoch 4.106: Loss = 0.392654
Epoch 4.107: Loss = 0.510818
Epoch 4.108: Loss = 0.450119
Epoch 4.109: Loss = 0.379547
Epoch 4.110: Loss = 0.408279
Epoch 4.111: Loss = 0.34285
Epoch 4.112: Loss = 0.388336
Epoch 4.113: Loss = 0.411713
Epoch 4.114: Loss = 0.36644
Epoch 4.115: Loss = 0.458206
Epoch 4.116: Loss = 0.378265
Epoch 4.117: Loss = 0.468445
Epoch 4.118: Loss = 0.311218
Epoch 4.119: Loss = 0.454514
Epoch 4.120: Loss = 0.410446
TRAIN LOSS = 0.405746
TRAIN ACC = 88.1287 % (52879/60000)
Loss = 0.396591
Loss = 0.460083
Loss = 0.533615
Loss = 0.523605
Loss = 0.556168
Loss = 0.422745
Loss = 0.366653
Loss = 0.607437
Loss = 0.522964
Loss = 0.460815
Loss = 0.204742
Loss = 0.301208
Loss = 0.25824
Loss = 0.354416
Loss = 0.208954
Loss = 0.281067
Loss = 0.200836
Loss = 0.0678558
Loss = 0.219101
Loss = 0.52121
TEST LOSS = 0.373415
TEST ACC = 528.789 % (8920/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.347214
Epoch 5.2: Loss = 0.393463
Epoch 5.3: Loss = 0.352814
Epoch 5.4: Loss = 0.410507
Epoch 5.5: Loss = 0.306992
Epoch 5.6: Loss = 0.363098
Epoch 5.7: Loss = 0.403702
Epoch 5.8: Loss = 0.388123
Epoch 5.9: Loss = 0.349777
Epoch 5.10: Loss = 0.384094
Epoch 5.11: Loss = 0.507385
Epoch 5.12: Loss = 0.335678
Epoch 5.13: Loss = 0.426453
Epoch 5.14: Loss = 0.344452
Epoch 5.15: Loss = 0.479858
Epoch 5.16: Loss = 0.335953
Epoch 5.17: Loss = 0.348785
Epoch 5.18: Loss = 0.388931
Epoch 5.19: Loss = 0.41452
Epoch 5.20: Loss = 0.379532
Epoch 5.21: Loss = 0.413315
Epoch 5.22: Loss = 0.418365
Epoch 5.23: Loss = 0.409378
Epoch 5.24: Loss = 0.371368
Epoch 5.25: Loss = 0.375763
Epoch 5.26: Loss = 0.40094
Epoch 5.27: Loss = 0.42131
Epoch 5.28: Loss = 0.46405
Epoch 5.29: Loss = 0.357574
Epoch 5.30: Loss = 0.39328
Epoch 5.31: Loss = 0.332703
Epoch 5.32: Loss = 0.44426
Epoch 5.33: Loss = 0.369614
Epoch 5.34: Loss = 0.369003
Epoch 5.35: Loss = 0.38681
Epoch 5.36: Loss = 0.445633
Epoch 5.37: Loss = 0.51825
Epoch 5.38: Loss = 0.444916
Epoch 5.39: Loss = 0.387741
Epoch 5.40: Loss = 0.374573
Epoch 5.41: Loss = 0.365143
Epoch 5.42: Loss = 0.405899
Epoch 5.43: Loss = 0.336151
Epoch 5.44: Loss = 0.414368
Epoch 5.45: Loss = 0.335648
Epoch 5.46: Loss = 0.490448
Epoch 5.47: Loss = 0.453781
Epoch 5.48: Loss = 0.499023
Epoch 5.49: Loss = 0.383087
Epoch 5.50: Loss = 0.399734
Epoch 5.51: Loss = 0.421509
Epoch 5.52: Loss = 0.403275
Epoch 5.53: Loss = 0.382156
Epoch 5.54: Loss = 0.356628
Epoch 5.55: Loss = 0.483429
Epoch 5.56: Loss = 0.40451
Epoch 5.57: Loss = 0.329315
Epoch 5.58: Loss = 0.432327
Epoch 5.59: Loss = 0.383194
Epoch 5.60: Loss = 0.375275
Epoch 5.61: Loss = 0.431229
Epoch 5.62: Loss = 0.466202
Epoch 5.63: Loss = 0.418777
Epoch 5.64: Loss = 0.373932
Epoch 5.65: Loss = 0.38913
Epoch 5.66: Loss = 0.424728
Epoch 5.67: Loss = 0.421265
Epoch 5.68: Loss = 0.377502
Epoch 5.69: Loss = 0.374329
Epoch 5.70: Loss = 0.353516
Epoch 5.71: Loss = 0.387695
Epoch 5.72: Loss = 0.418152
Epoch 5.73: Loss = 0.346603
Epoch 5.74: Loss = 0.398285
Epoch 5.75: Loss = 0.420898
Epoch 5.76: Loss = 0.404877
Epoch 5.77: Loss = 0.372787
Epoch 5.78: Loss = 0.450256
Epoch 5.79: Loss = 0.427368
Epoch 5.80: Loss = 0.427414
Epoch 5.81: Loss = 0.444473
Epoch 5.82: Loss = 0.365036
Epoch 5.83: Loss = 0.399826
Epoch 5.84: Loss = 0.383499
Epoch 5.85: Loss = 0.413055
Epoch 5.86: Loss = 0.400055
Epoch 5.87: Loss = 0.378815
Epoch 5.88: Loss = 0.387482
Epoch 5.89: Loss = 0.322281
Epoch 5.90: Loss = 0.370758
Epoch 5.91: Loss = 0.347397
Epoch 5.92: Loss = 0.396835
Epoch 5.93: Loss = 0.353027
Epoch 5.94: Loss = 0.390594
Epoch 5.95: Loss = 0.429535
Epoch 5.96: Loss = 0.32991
Epoch 5.97: Loss = 0.392075
Epoch 5.98: Loss = 0.501236
Epoch 5.99: Loss = 0.417786
Epoch 5.100: Loss = 0.408249
Epoch 5.101: Loss = 0.434372
Epoch 5.102: Loss = 0.336105
Epoch 5.103: Loss = 0.38501
Epoch 5.104: Loss = 0.352158
Epoch 5.105: Loss = 0.352432
Epoch 5.106: Loss = 0.331436
Epoch 5.107: Loss = 0.43454
Epoch 5.108: Loss = 0.368088
Epoch 5.109: Loss = 0.342133
Epoch 5.110: Loss = 0.435211
Epoch 5.111: Loss = 0.411392
Epoch 5.112: Loss = 0.337357
Epoch 5.113: Loss = 0.424088
Epoch 5.114: Loss = 0.338379
Epoch 5.115: Loss = 0.377472
Epoch 5.116: Loss = 0.369232
Epoch 5.117: Loss = 0.38681
Epoch 5.118: Loss = 0.307892
Epoch 5.119: Loss = 0.395187
Epoch 5.120: Loss = 0.290878
TRAIN LOSS = 0.392853
TRAIN ACC = 88.591 % (53157/60000)
Loss = 0.395035
Loss = 0.450684
Loss = 0.508286
Loss = 0.524628
Loss = 0.539429
Loss = 0.398712
Loss = 0.343521
Loss = 0.593185
Loss = 0.5056
Loss = 0.465469
Loss = 0.197495
Loss = 0.270157
Loss = 0.268967
Loss = 0.355347
Loss = 0.20726
Loss = 0.276505
Loss = 0.201935
Loss = 0.0605011
Loss = 0.216873
Loss = 0.516083
TEST LOSS = 0.364783
TEST ACC = 531.569 % (8966/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.402588
Epoch 6.2: Loss = 0.425095
Epoch 6.3: Loss = 0.464828
Epoch 6.4: Loss = 0.449982
Epoch 6.5: Loss = 0.378265
Epoch 6.6: Loss = 0.312439
Epoch 6.7: Loss = 0.39389
Epoch 6.8: Loss = 0.427139
Epoch 6.9: Loss = 0.345886
Epoch 6.10: Loss = 0.426987
Epoch 6.11: Loss = 0.341782
Epoch 6.12: Loss = 0.368561
Epoch 6.13: Loss = 0.339584
Epoch 6.14: Loss = 0.329758
Epoch 6.15: Loss = 0.459167
Epoch 6.16: Loss = 0.366241
Epoch 6.17: Loss = 0.40567
Epoch 6.18: Loss = 0.341629
Epoch 6.19: Loss = 0.341614
Epoch 6.20: Loss = 0.399048
Epoch 6.21: Loss = 0.368301
Epoch 6.22: Loss = 0.36705
Epoch 6.23: Loss = 0.399506
Epoch 6.24: Loss = 0.443802
Epoch 6.25: Loss = 0.409393
Epoch 6.26: Loss = 0.365051
Epoch 6.27: Loss = 0.394867
Epoch 6.28: Loss = 0.314667
Epoch 6.29: Loss = 0.35463
Epoch 6.30: Loss = 0.326538
Epoch 6.31: Loss = 0.36264
Epoch 6.32: Loss = 0.315262
Epoch 6.33: Loss = 0.413025
Epoch 6.34: Loss = 0.360352
Epoch 6.35: Loss = 0.322632
Epoch 6.36: Loss = 0.44046
Epoch 6.37: Loss = 0.40094
Epoch 6.38: Loss = 0.313309
Epoch 6.39: Loss = 0.384583
Epoch 6.40: Loss = 0.388916
Epoch 6.41: Loss = 0.348724
Epoch 6.42: Loss = 0.357162
Epoch 6.43: Loss = 0.436142
Epoch 6.44: Loss = 0.3573
Epoch 6.45: Loss = 0.354126
Epoch 6.46: Loss = 0.367126
Epoch 6.47: Loss = 0.356262
Epoch 6.48: Loss = 0.368576
Epoch 6.49: Loss = 0.386917
Epoch 6.50: Loss = 0.447433
Epoch 6.51: Loss = 0.485703
Epoch 6.52: Loss = 0.414108
Epoch 6.53: Loss = 0.376648
Epoch 6.54: Loss = 0.410156
Epoch 6.55: Loss = 0.434113
Epoch 6.56: Loss = 0.367996
Epoch 6.57: Loss = 0.366867
Epoch 6.58: Loss = 0.399109
Epoch 6.59: Loss = 0.360046
Epoch 6.60: Loss = 0.501389
Epoch 6.61: Loss = 0.458069
Epoch 6.62: Loss = 0.486282
Epoch 6.63: Loss = 0.311493
Epoch 6.64: Loss = 0.390961
Epoch 6.65: Loss = 0.439972
Epoch 6.66: Loss = 0.377869
Epoch 6.67: Loss = 0.445099
Epoch 6.68: Loss = 0.327927
Epoch 6.69: Loss = 0.456604
Epoch 6.70: Loss = 0.365173
Epoch 6.71: Loss = 0.342117
Epoch 6.72: Loss = 0.263794
Epoch 6.73: Loss = 0.316666
Epoch 6.74: Loss = 0.353546
Epoch 6.75: Loss = 0.396484
Epoch 6.76: Loss = 0.361038
Epoch 6.77: Loss = 0.3125
Epoch 6.78: Loss = 0.392487
Epoch 6.79: Loss = 0.379974
Epoch 6.80: Loss = 0.348801
Epoch 6.81: Loss = 0.412613
Epoch 6.82: Loss = 0.302475
Epoch 6.83: Loss = 0.324707
Epoch 6.84: Loss = 0.35025
Epoch 6.85: Loss = 0.301651
Epoch 6.86: Loss = 0.455429
Epoch 6.87: Loss = 0.348755
Epoch 6.88: Loss = 0.424652
Epoch 6.89: Loss = 0.363373
Epoch 6.90: Loss = 0.310577
Epoch 6.91: Loss = 0.466736
Epoch 6.92: Loss = 0.400681
Epoch 6.93: Loss = 0.363892
Epoch 6.94: Loss = 0.430374
Epoch 6.95: Loss = 0.358078
Epoch 6.96: Loss = 0.36702
Epoch 6.97: Loss = 0.475845
Epoch 6.98: Loss = 0.421829
Epoch 6.99: Loss = 0.389465
Epoch 6.100: Loss = 0.393646
Epoch 6.101: Loss = 0.343323
Epoch 6.102: Loss = 0.384598
Epoch 6.103: Loss = 0.383591
Epoch 6.104: Loss = 0.37587
Epoch 6.105: Loss = 0.414597
Epoch 6.106: Loss = 0.381287
Epoch 6.107: Loss = 0.395996
Epoch 6.108: Loss = 0.389053
Epoch 6.109: Loss = 0.297562
Epoch 6.110: Loss = 0.416306
Epoch 6.111: Loss = 0.382782
Epoch 6.112: Loss = 0.393692
Epoch 6.113: Loss = 0.425186
Epoch 6.114: Loss = 0.396103
Epoch 6.115: Loss = 0.380112
Epoch 6.116: Loss = 0.348038
Epoch 6.117: Loss = 0.291061
Epoch 6.118: Loss = 0.3909
Epoch 6.119: Loss = 0.368805
Epoch 6.120: Loss = 0.447678
TRAIN LOSS = 0.382141
TRAIN ACC = 89.061 % (53439/60000)
Loss = 0.385864
Loss = 0.438248
Loss = 0.506683
Loss = 0.522812
Loss = 0.537811
Loss = 0.382858
Loss = 0.335022
Loss = 0.596344
Loss = 0.501389
Loss = 0.452759
Loss = 0.181152
Loss = 0.264923
Loss = 0.273193
Loss = 0.348541
Loss = 0.196808
Loss = 0.273926
Loss = 0.193344
Loss = 0.056488
Loss = 0.202545
Loss = 0.517975
TEST LOSS = 0.358434
TEST ACC = 534.389 % (8987/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.389557
Epoch 7.2: Loss = 0.491409
Epoch 7.3: Loss = 0.32106
Epoch 7.4: Loss = 0.390289
Epoch 7.5: Loss = 0.410858
Epoch 7.6: Loss = 0.318954
Epoch 7.7: Loss = 0.433609
Epoch 7.8: Loss = 0.388901
Epoch 7.9: Loss = 0.383331
Epoch 7.10: Loss = 0.358765
Epoch 7.11: Loss = 0.387146
Epoch 7.12: Loss = 0.393311
Epoch 7.13: Loss = 0.354813
Epoch 7.14: Loss = 0.341324
Epoch 7.15: Loss = 0.456177
Epoch 7.16: Loss = 0.416931
Epoch 7.17: Loss = 0.430664
Epoch 7.18: Loss = 0.386551
Epoch 7.19: Loss = 0.40416
Epoch 7.20: Loss = 0.321533
Epoch 7.21: Loss = 0.335266
Epoch 7.22: Loss = 0.364685
Epoch 7.23: Loss = 0.394135
Epoch 7.24: Loss = 0.390823
Epoch 7.25: Loss = 0.309875
Epoch 7.26: Loss = 0.388504
Epoch 7.27: Loss = 0.351532
Epoch 7.28: Loss = 0.399216
Epoch 7.29: Loss = 0.331055
Epoch 7.30: Loss = 0.451324
Epoch 7.31: Loss = 0.352478
Epoch 7.32: Loss = 0.385849
Epoch 7.33: Loss = 0.359268
Epoch 7.34: Loss = 0.372879
Epoch 7.35: Loss = 0.31987
Epoch 7.36: Loss = 0.421722
Epoch 7.37: Loss = 0.358658
Epoch 7.38: Loss = 0.502945
Epoch 7.39: Loss = 0.352554
Epoch 7.40: Loss = 0.336227
Epoch 7.41: Loss = 0.360992
Epoch 7.42: Loss = 0.404022
Epoch 7.43: Loss = 0.302277
Epoch 7.44: Loss = 0.395706
Epoch 7.45: Loss = 0.381195
Epoch 7.46: Loss = 0.367874
Epoch 7.47: Loss = 0.498779
Epoch 7.48: Loss = 0.409943
Epoch 7.49: Loss = 0.394653
Epoch 7.50: Loss = 0.356766
Epoch 7.51: Loss = 0.50766
Epoch 7.52: Loss = 0.363968
Epoch 7.53: Loss = 0.358521
Epoch 7.54: Loss = 0.349564
Epoch 7.55: Loss = 0.32843
Epoch 7.56: Loss = 0.349426
Epoch 7.57: Loss = 0.429413
Epoch 7.58: Loss = 0.355072
Epoch 7.59: Loss = 0.365875
Epoch 7.60: Loss = 0.332016
Epoch 7.61: Loss = 0.388763
Epoch 7.62: Loss = 0.400192
Epoch 7.63: Loss = 0.258713
Epoch 7.64: Loss = 0.381439
Epoch 7.65: Loss = 0.446152
Epoch 7.66: Loss = 0.356339
Epoch 7.67: Loss = 0.365891
Epoch 7.68: Loss = 0.267609
Epoch 7.69: Loss = 0.325699
Epoch 7.70: Loss = 0.329819
Epoch 7.71: Loss = 0.439835
Epoch 7.72: Loss = 0.338577
Epoch 7.73: Loss = 0.34906
Epoch 7.74: Loss = 0.395111
Epoch 7.75: Loss = 0.48558
Epoch 7.76: Loss = 0.431519
Epoch 7.77: Loss = 0.324921
Epoch 7.78: Loss = 0.444061
Epoch 7.79: Loss = 0.340851
Epoch 7.80: Loss = 0.455627
Epoch 7.81: Loss = 0.33992
Epoch 7.82: Loss = 0.446655
Epoch 7.83: Loss = 0.430328
Epoch 7.84: Loss = 0.380493
Epoch 7.85: Loss = 0.386414
Epoch 7.86: Loss = 0.37352
Epoch 7.87: Loss = 0.450058
Epoch 7.88: Loss = 0.256882
Epoch 7.89: Loss = 0.362045
Epoch 7.90: Loss = 0.268433
Epoch 7.91: Loss = 0.357941
Epoch 7.92: Loss = 0.314346
Epoch 7.93: Loss = 0.306122
Epoch 7.94: Loss = 0.446503
Epoch 7.95: Loss = 0.336792
Epoch 7.96: Loss = 0.334961
Epoch 7.97: Loss = 0.350113
Epoch 7.98: Loss = 0.358887
Epoch 7.99: Loss = 0.467239
Epoch 7.100: Loss = 0.359482
Epoch 7.101: Loss = 0.38739
Epoch 7.102: Loss = 0.422714
Epoch 7.103: Loss = 0.359924
Epoch 7.104: Loss = 0.381058
Epoch 7.105: Loss = 0.414734
Epoch 7.106: Loss = 0.436508
Epoch 7.107: Loss = 0.472214
Epoch 7.108: Loss = 0.407089
Epoch 7.109: Loss = 0.337204
Epoch 7.110: Loss = 0.388535
Epoch 7.111: Loss = 0.422699
Epoch 7.112: Loss = 0.358032
Epoch 7.113: Loss = 0.38942
Epoch 7.114: Loss = 0.411392
Epoch 7.115: Loss = 0.344818
Epoch 7.116: Loss = 0.389618
Epoch 7.117: Loss = 0.332458
Epoch 7.118: Loss = 0.331451
Epoch 7.119: Loss = 0.410355
Epoch 7.120: Loss = 0.321518
TRAIN LOSS = 0.378693
TRAIN ACC = 89.3539 % (53615/60000)
Loss = 0.373505
Loss = 0.427292
Loss = 0.518753
Loss = 0.513016
Loss = 0.533981
Loss = 0.383911
Loss = 0.333252
Loss = 0.599884
Loss = 0.508362
Loss = 0.445786
Loss = 0.182648
Loss = 0.274796
Loss = 0.283676
Loss = 0.343277
Loss = 0.194092
Loss = 0.26326
Loss = 0.181351
Loss = 0.0552521
Loss = 0.20874
Loss = 0.499237
TEST LOSS = 0.356203
TEST ACC = 536.15 % (9005/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.307556
Epoch 8.2: Loss = 0.37413
Epoch 8.3: Loss = 0.33815
Epoch 8.4: Loss = 0.397858
Epoch 8.5: Loss = 0.414261
Epoch 8.6: Loss = 0.435165
Epoch 8.7: Loss = 0.350815
Epoch 8.8: Loss = 0.332138
Epoch 8.9: Loss = 0.358505
Epoch 8.10: Loss = 0.392136
Epoch 8.11: Loss = 0.402878
Epoch 8.12: Loss = 0.293732
Epoch 8.13: Loss = 0.435989
Epoch 8.14: Loss = 0.400009
Epoch 8.15: Loss = 0.287048
Epoch 8.16: Loss = 0.316132
Epoch 8.17: Loss = 0.446381
Epoch 8.18: Loss = 0.278717
Epoch 8.19: Loss = 0.422272
Epoch 8.20: Loss = 0.524384
Epoch 8.21: Loss = 0.317947
Epoch 8.22: Loss = 0.29808
Epoch 8.23: Loss = 0.366791
Epoch 8.24: Loss = 0.387955
Epoch 8.25: Loss = 0.364609
Epoch 8.26: Loss = 0.468063
Epoch 8.27: Loss = 0.413406
Epoch 8.28: Loss = 0.369827
Epoch 8.29: Loss = 0.410538
Epoch 8.30: Loss = 0.341064
Epoch 8.31: Loss = 0.360367
Epoch 8.32: Loss = 0.306778
Epoch 8.33: Loss = 0.37439
Epoch 8.34: Loss = 0.415894
Epoch 8.35: Loss = 0.428635
Epoch 8.36: Loss = 0.38356
Epoch 8.37: Loss = 0.395767
Epoch 8.38: Loss = 0.339798
Epoch 8.39: Loss = 0.393768
Epoch 8.40: Loss = 0.432785
Epoch 8.41: Loss = 0.441681
Epoch 8.42: Loss = 0.401764
Epoch 8.43: Loss = 0.336502
Epoch 8.44: Loss = 0.348175
Epoch 8.45: Loss = 0.438004
Epoch 8.46: Loss = 0.401566
Epoch 8.47: Loss = 0.358505
Epoch 8.48: Loss = 0.361206
Epoch 8.49: Loss = 0.352234
Epoch 8.50: Loss = 0.32489
Epoch 8.51: Loss = 0.45929
Epoch 8.52: Loss = 0.346115
Epoch 8.53: Loss = 0.34343
Epoch 8.54: Loss = 0.395508
Epoch 8.55: Loss = 0.364822
Epoch 8.56: Loss = 0.384598
Epoch 8.57: Loss = 0.427551
Epoch 8.58: Loss = 0.297546
Epoch 8.59: Loss = 0.415787
Epoch 8.60: Loss = 0.338013
Epoch 8.61: Loss = 0.380981
Epoch 8.62: Loss = 0.332275
Epoch 8.63: Loss = 0.447937
Epoch 8.64: Loss = 0.472519
Epoch 8.65: Loss = 0.335861
Epoch 8.66: Loss = 0.317307
Epoch 8.67: Loss = 0.318542
Epoch 8.68: Loss = 0.320358
Epoch 8.69: Loss = 0.384354
Epoch 8.70: Loss = 0.39624
Epoch 8.71: Loss = 0.341171
Epoch 8.72: Loss = 0.275024
Epoch 8.73: Loss = 0.350052
Epoch 8.74: Loss = 0.456223
Epoch 8.75: Loss = 0.40596
Epoch 8.76: Loss = 0.422836
Epoch 8.77: Loss = 0.278687
Epoch 8.78: Loss = 0.387024
Epoch 8.79: Loss = 0.373093
Epoch 8.80: Loss = 0.411499
Epoch 8.81: Loss = 0.341309
Epoch 8.82: Loss = 0.33551
Epoch 8.83: Loss = 0.299637
Epoch 8.84: Loss = 0.452881
Epoch 8.85: Loss = 0.404205
Epoch 8.86: Loss = 0.353027
Epoch 8.87: Loss = 0.353226
Epoch 8.88: Loss = 0.421814
Epoch 8.89: Loss = 0.279984
Epoch 8.90: Loss = 0.317902
Epoch 8.91: Loss = 0.502991
Epoch 8.92: Loss = 0.343277
Epoch 8.93: Loss = 0.43573
Epoch 8.94: Loss = 0.311752
Epoch 8.95: Loss = 0.371902
Epoch 8.96: Loss = 0.376755
Epoch 8.97: Loss = 0.362015
Epoch 8.98: Loss = 0.404785
Epoch 8.99: Loss = 0.386185
Epoch 8.100: Loss = 0.364609
Epoch 8.101: Loss = 0.348541
Epoch 8.102: Loss = 0.500839
Epoch 8.103: Loss = 0.372498
Epoch 8.104: Loss = 0.390717
Epoch 8.105: Loss = 0.394043
Epoch 8.106: Loss = 0.433762
Epoch 8.107: Loss = 0.463135
Epoch 8.108: Loss = 0.317276
Epoch 8.109: Loss = 0.389847
Epoch 8.110: Loss = 0.41452
Epoch 8.111: Loss = 0.325851
Epoch 8.112: Loss = 0.415436
Epoch 8.113: Loss = 0.404251
Epoch 8.114: Loss = 0.410904
Epoch 8.115: Loss = 0.375061
Epoch 8.116: Loss = 0.335999
Epoch 8.117: Loss = 0.381561
Epoch 8.118: Loss = 0.318893
Epoch 8.119: Loss = 0.34964
Epoch 8.120: Loss = 0.319946
TRAIN LOSS = 0.375885
TRAIN ACC = 89.6042 % (53765/60000)
Loss = 0.374374
Loss = 0.439285
Loss = 0.503723
Loss = 0.516327
Loss = 0.535583
Loss = 0.366806
Loss = 0.331192
Loss = 0.585999
Loss = 0.515244
Loss = 0.455505
Loss = 0.188461
Loss = 0.264999
Loss = 0.279587
Loss = 0.323776
Loss = 0.184372
Loss = 0.258682
Loss = 0.184372
Loss = 0.0504303
Loss = 0.203262
Loss = 0.507492
TEST LOSS = 0.353474
TEST ACC = 537.65 % (9032/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.347778
Epoch 9.2: Loss = 0.389679
Epoch 9.3: Loss = 0.353012
Epoch 9.4: Loss = 0.371994
Epoch 9.5: Loss = 0.33754
Epoch 9.6: Loss = 0.318909
Epoch 9.7: Loss = 0.310059
Epoch 9.8: Loss = 0.47995
Epoch 9.9: Loss = 0.359406
Epoch 9.10: Loss = 0.360916
Epoch 9.11: Loss = 0.444183
Epoch 9.12: Loss = 0.398132
Epoch 9.13: Loss = 0.359451
Epoch 9.14: Loss = 0.310471
Epoch 9.15: Loss = 0.34964
Epoch 9.16: Loss = 0.340134
Epoch 9.17: Loss = 0.386978
Epoch 9.18: Loss = 0.439499
Epoch 9.19: Loss = 0.359024
Epoch 9.20: Loss = 0.423889
Epoch 9.21: Loss = 0.34845
Epoch 9.22: Loss = 0.375732
Epoch 9.23: Loss = 0.341248
Epoch 9.24: Loss = 0.304794
Epoch 9.25: Loss = 0.358643
Epoch 9.26: Loss = 0.442383
Epoch 9.27: Loss = 0.323715
Epoch 9.28: Loss = 0.375732
Epoch 9.29: Loss = 0.373199
Epoch 9.30: Loss = 0.492218
Epoch 9.31: Loss = 0.372864
Epoch 9.32: Loss = 0.41951
Epoch 9.33: Loss = 0.375168
Epoch 9.34: Loss = 0.429733
Epoch 9.35: Loss = 0.367691
Epoch 9.36: Loss = 0.372742
Epoch 9.37: Loss = 0.357422
Epoch 9.38: Loss = 0.419479
Epoch 9.39: Loss = 0.290237
Epoch 9.40: Loss = 0.372147
Epoch 9.41: Loss = 0.417572
Epoch 9.42: Loss = 0.416245
Epoch 9.43: Loss = 0.300354
Epoch 9.44: Loss = 0.258865
Epoch 9.45: Loss = 0.281387
Epoch 9.46: Loss = 0.349503
Epoch 9.47: Loss = 0.340973
Epoch 9.48: Loss = 0.208496
Epoch 9.49: Loss = 0.36937
Epoch 9.50: Loss = 0.408966
Epoch 9.51: Loss = 0.404205
Epoch 9.52: Loss = 0.341553
Epoch 9.53: Loss = 0.260086
Epoch 9.54: Loss = 0.415375
Epoch 9.55: Loss = 0.414993
Epoch 9.56: Loss = 0.327362
Epoch 9.57: Loss = 0.473862
Epoch 9.58: Loss = 0.395248
Epoch 9.59: Loss = 0.330429
Epoch 9.60: Loss = 0.436218
Epoch 9.61: Loss = 0.341171
Epoch 9.62: Loss = 0.420685
Epoch 9.63: Loss = 0.294067
Epoch 9.64: Loss = 0.397018
Epoch 9.65: Loss = 0.43074
Epoch 9.66: Loss = 0.400452
Epoch 9.67: Loss = 0.342209
Epoch 9.68: Loss = 0.426498
Epoch 9.69: Loss = 0.340439
Epoch 9.70: Loss = 0.442627
Epoch 9.71: Loss = 0.375473
Epoch 9.72: Loss = 0.427139
Epoch 9.73: Loss = 0.41713
Epoch 9.74: Loss = 0.367355
Epoch 9.75: Loss = 0.368896
Epoch 9.76: Loss = 0.356888
Epoch 9.77: Loss = 0.328766
Epoch 9.78: Loss = 0.322708
Epoch 9.79: Loss = 0.320435
Epoch 9.80: Loss = 0.454163
Epoch 9.81: Loss = 0.355545
Epoch 9.82: Loss = 0.443878
Epoch 9.83: Loss = 0.356766
Epoch 9.84: Loss = 0.39386
Epoch 9.85: Loss = 0.320572
Epoch 9.86: Loss = 0.333588
Epoch 9.87: Loss = 0.416702
Epoch 9.88: Loss = 0.379959
Epoch 9.89: Loss = 0.299438
Epoch 9.90: Loss = 0.385605
Epoch 9.91: Loss = 0.315735
Epoch 9.92: Loss = 0.322449
Epoch 9.93: Loss = 0.364441
Epoch 9.94: Loss = 0.377625
Epoch 9.95: Loss = 0.309464
Epoch 9.96: Loss = 0.410721
Epoch 9.97: Loss = 0.322983
Epoch 9.98: Loss = 0.335831
Epoch 9.99: Loss = 0.471497
Epoch 9.100: Loss = 0.3414
Epoch 9.101: Loss = 0.384796
Epoch 9.102: Loss = 0.408401
Epoch 9.103: Loss = 0.302963
Epoch 9.104: Loss = 0.396484
Epoch 9.105: Loss = 0.396164
Epoch 9.106: Loss = 0.311966
Epoch 9.107: Loss = 0.344193
Epoch 9.108: Loss = 0.375305
Epoch 9.109: Loss = 0.410309
Epoch 9.110: Loss = 0.343155
Epoch 9.111: Loss = 0.29747
Epoch 9.112: Loss = 0.34967
Epoch 9.113: Loss = 0.378815
Epoch 9.114: Loss = 0.38179
Epoch 9.115: Loss = 0.429092
Epoch 9.116: Loss = 0.391815
Epoch 9.117: Loss = 0.417877
Epoch 9.118: Loss = 0.419998
Epoch 9.119: Loss = 0.355637
Epoch 9.120: Loss = 0.476349
TRAIN LOSS = 0.370911
TRAIN ACC = 89.8331 % (53903/60000)
Loss = 0.363434
Loss = 0.431778
Loss = 0.501419
Loss = 0.514771
Loss = 0.531662
Loss = 0.363693
Loss = 0.334793
Loss = 0.58992
Loss = 0.524323
Loss = 0.435638
Loss = 0.183823
Loss = 0.270187
Loss = 0.28656
Loss = 0.329529
Loss = 0.17926
Loss = 0.270172
Loss = 0.183212
Loss = 0.0531006
Loss = 0.196213
Loss = 0.5056
TEST LOSS = 0.352454
TEST ACC = 539.029 % (9018/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.328827
Epoch 10.2: Loss = 0.435471
Epoch 10.3: Loss = 0.388016
Epoch 10.4: Loss = 0.390244
Epoch 10.5: Loss = 0.319824
Epoch 10.6: Loss = 0.417191
Epoch 10.7: Loss = 0.375214
Epoch 10.8: Loss = 0.467697
Epoch 10.9: Loss = 0.376328
Epoch 10.10: Loss = 0.39679
Epoch 10.11: Loss = 0.414948
Epoch 10.12: Loss = 0.294693
Epoch 10.13: Loss = 0.339081
Epoch 10.14: Loss = 0.353775
Epoch 10.15: Loss = 0.300781
Epoch 10.16: Loss = 0.468552
Epoch 10.17: Loss = 0.411316
Epoch 10.18: Loss = 0.502426
Epoch 10.19: Loss = 0.325516
Epoch 10.20: Loss = 0.399231
Epoch 10.21: Loss = 0.315048
Epoch 10.22: Loss = 0.328033
Epoch 10.23: Loss = 0.371857
Epoch 10.24: Loss = 0.403427
Epoch 10.25: Loss = 0.391037
Epoch 10.26: Loss = 0.364441
Epoch 10.27: Loss = 0.388977
Epoch 10.28: Loss = 0.305542
Epoch 10.29: Loss = 0.326141
Epoch 10.30: Loss = 0.344254
Epoch 10.31: Loss = 0.363129
Epoch 10.32: Loss = 0.317688
Epoch 10.33: Loss = 0.435196
Epoch 10.34: Loss = 0.44577
Epoch 10.35: Loss = 0.400513
Epoch 10.36: Loss = 0.373093
Epoch 10.37: Loss = 0.338577
Epoch 10.38: Loss = 0.283371
Epoch 10.39: Loss = 0.509171
Epoch 10.40: Loss = 0.344162
Epoch 10.41: Loss = 0.463181
Epoch 10.42: Loss = 0.48259
Epoch 10.43: Loss = 0.41449
Epoch 10.44: Loss = 0.274673
Epoch 10.45: Loss = 0.330063
Epoch 10.46: Loss = 0.350586
Epoch 10.47: Loss = 0.361618
Epoch 10.48: Loss = 0.338791
Epoch 10.49: Loss = 0.30188
Epoch 10.50: Loss = 0.363419
Epoch 10.51: Loss = 0.380844
Epoch 10.52: Loss = 0.366104
Epoch 10.53: Loss = 0.436661
Epoch 10.54: Loss = 0.3302
Epoch 10.55: Loss = 0.322968
Epoch 10.56: Loss = 0.358047
Epoch 10.57: Loss = 0.377014
Epoch 10.58: Loss = 0.44902
Epoch 10.59: Loss = 0.343521
Epoch 10.60: Loss = 0.341064
Epoch 10.61: Loss = 0.355179
Epoch 10.62: Loss = 0.40004
Epoch 10.63: Loss = 0.340881
Epoch 10.64: Loss = 0.362259
Epoch 10.65: Loss = 0.4048
Epoch 10.66: Loss = 0.390991
Epoch 10.67: Loss = 0.306107
Epoch 10.68: Loss = 0.349991
Epoch 10.69: Loss = 0.306183
Epoch 10.70: Loss = 0.368042
Epoch 10.71: Loss = 0.320724
Epoch 10.72: Loss = 0.338318
Epoch 10.73: Loss = 0.367218
Epoch 10.74: Loss = 0.366821
Epoch 10.75: Loss = 0.297928
Epoch 10.76: Loss = 0.395081
Epoch 10.77: Loss = 0.372757
Epoch 10.78: Loss = 0.328979
Epoch 10.79: Loss = 0.303085
Epoch 10.80: Loss = 0.421417
Epoch 10.81: Loss = 0.333618
Epoch 10.82: Loss = 0.461044
Epoch 10.83: Loss = 0.374084
Epoch 10.84: Loss = 0.375778
Epoch 10.85: Loss = 0.380051
Epoch 10.86: Loss = 0.222916
Epoch 10.87: Loss = 0.295181
Epoch 10.88: Loss = 0.272079
Epoch 10.89: Loss = 0.351105
Epoch 10.90: Loss = 0.314682
Epoch 10.91: Loss = 0.396667
Epoch 10.92: Loss = 0.439804
Epoch 10.93: Loss = 0.335526
Epoch 10.94: Loss = 0.283783
Epoch 10.95: Loss = 0.357666
Epoch 10.96: Loss = 0.312683
Epoch 10.97: Loss = 0.311508
Epoch 10.98: Loss = 0.308044
Epoch 10.99: Loss = 0.424286
Epoch 10.100: Loss = 0.434799
Epoch 10.101: Loss = 0.361221
Epoch 10.102: Loss = 0.323135
Epoch 10.103: Loss = 0.34819
Epoch 10.104: Loss = 0.35527
Epoch 10.105: Loss = 0.355408
Epoch 10.106: Loss = 0.401962
Epoch 10.107: Loss = 0.391602
Epoch 10.108: Loss = 0.403641
Epoch 10.109: Loss = 0.367493
Epoch 10.110: Loss = 0.368607
Epoch 10.111: Loss = 0.37999
Epoch 10.112: Loss = 0.393341
Epoch 10.113: Loss = 0.387146
Epoch 10.114: Loss = 0.360977
Epoch 10.115: Loss = 0.389557
Epoch 10.116: Loss = 0.417252
Epoch 10.117: Loss = 0.314941
Epoch 10.118: Loss = 0.330673
Epoch 10.119: Loss = 0.377457
Epoch 10.120: Loss = 0.300079
TRAIN LOSS = 0.365433
TRAIN ACC = 90.097 % (54060/60000)
Loss = 0.348328
Loss = 0.408768
Loss = 0.495224
Loss = 0.501709
Loss = 0.510925
Loss = 0.35675
Loss = 0.316818
Loss = 0.588852
Loss = 0.502945
Loss = 0.435593
Loss = 0.175812
Loss = 0.264893
Loss = 0.294128
Loss = 0.327271
Loss = 0.170212
Loss = 0.265259
Loss = 0.171509
Loss = 0.0457153
Loss = 0.199799
Loss = 0.486862
TEST LOSS = 0.343368
TEST ACC = 540.599 % (9067/10000)

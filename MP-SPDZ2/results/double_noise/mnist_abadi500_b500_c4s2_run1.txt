Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.32198
Epoch 1.2: Loss = 2.26041
Epoch 1.3: Loss = 2.22134
Epoch 1.4: Loss = 2.16399
Epoch 1.5: Loss = 2.12598
Epoch 1.6: Loss = 2.10492
Epoch 1.7: Loss = 2.01653
Epoch 1.8: Loss = 2.02614
Epoch 1.9: Loss = 1.98444
Epoch 1.10: Loss = 1.90471
Epoch 1.11: Loss = 1.85936
Epoch 1.12: Loss = 1.84569
Epoch 1.13: Loss = 1.82208
Epoch 1.14: Loss = 1.78673
Epoch 1.15: Loss = 1.73688
Epoch 1.16: Loss = 1.70804
Epoch 1.17: Loss = 1.62463
Epoch 1.18: Loss = 1.66951
Epoch 1.19: Loss = 1.61105
Epoch 1.20: Loss = 1.59268
Epoch 1.21: Loss = 1.55138
Epoch 1.22: Loss = 1.49303
Epoch 1.23: Loss = 1.45012
Epoch 1.24: Loss = 1.51086
Epoch 1.25: Loss = 1.45734
Epoch 1.26: Loss = 1.43524
Epoch 1.27: Loss = 1.33257
Epoch 1.28: Loss = 1.36421
Epoch 1.29: Loss = 1.30653
Epoch 1.30: Loss = 1.30978
Epoch 1.31: Loss = 1.27483
Epoch 1.32: Loss = 1.28128
Epoch 1.33: Loss = 1.26237
Epoch 1.34: Loss = 1.21397
Epoch 1.35: Loss = 1.16264
Epoch 1.36: Loss = 1.26982
Epoch 1.37: Loss = 1.19205
Epoch 1.38: Loss = 1.15134
Epoch 1.39: Loss = 1.15231
Epoch 1.40: Loss = 1.11165
Epoch 1.41: Loss = 1.08
Epoch 1.42: Loss = 1.07407
Epoch 1.43: Loss = 1.0206
Epoch 1.44: Loss = 1.00957
Epoch 1.45: Loss = 1.07399
Epoch 1.46: Loss = 1.00928
Epoch 1.47: Loss = 1.02635
Epoch 1.48: Loss = 1.01035
Epoch 1.49: Loss = 0.984314
Epoch 1.50: Loss = 0.967239
Epoch 1.51: Loss = 0.979568
Epoch 1.52: Loss = 0.928436
Epoch 1.53: Loss = 0.948868
Epoch 1.54: Loss = 0.877808
Epoch 1.55: Loss = 0.965271
Epoch 1.56: Loss = 0.896042
Epoch 1.57: Loss = 0.828186
Epoch 1.58: Loss = 0.918121
Epoch 1.59: Loss = 0.881134
Epoch 1.60: Loss = 0.837936
Epoch 1.61: Loss = 0.836395
Epoch 1.62: Loss = 0.79924
Epoch 1.63: Loss = 0.804947
Epoch 1.64: Loss = 0.850616
Epoch 1.65: Loss = 0.850296
Epoch 1.66: Loss = 0.789978
Epoch 1.67: Loss = 0.825089
Epoch 1.68: Loss = 0.821243
Epoch 1.69: Loss = 0.821136
Epoch 1.70: Loss = 0.782318
Epoch 1.71: Loss = 0.768997
Epoch 1.72: Loss = 0.731644
Epoch 1.73: Loss = 0.734741
Epoch 1.74: Loss = 0.69722
Epoch 1.75: Loss = 0.727783
Epoch 1.76: Loss = 0.760529
Epoch 1.77: Loss = 0.662491
Epoch 1.78: Loss = 0.727402
Epoch 1.79: Loss = 0.786606
Epoch 1.80: Loss = 0.728165
Epoch 1.81: Loss = 0.712494
Epoch 1.82: Loss = 0.666092
Epoch 1.83: Loss = 0.602097
Epoch 1.84: Loss = 0.665573
Epoch 1.85: Loss = 0.677933
Epoch 1.86: Loss = 0.618805
Epoch 1.87: Loss = 0.782593
Epoch 1.88: Loss = 0.644104
Epoch 1.89: Loss = 0.584763
Epoch 1.90: Loss = 0.6427
Epoch 1.91: Loss = 0.611038
Epoch 1.92: Loss = 0.645737
Epoch 1.93: Loss = 0.697571
Epoch 1.94: Loss = 0.652451
Epoch 1.95: Loss = 0.61879
Epoch 1.96: Loss = 0.644653
Epoch 1.97: Loss = 0.660492
Epoch 1.98: Loss = 0.629974
Epoch 1.99: Loss = 0.614868
Epoch 1.100: Loss = 0.664688
Epoch 1.101: Loss = 0.629517
Epoch 1.102: Loss = 0.642029
Epoch 1.103: Loss = 0.587753
Epoch 1.104: Loss = 0.58429
Epoch 1.105: Loss = 0.577286
Epoch 1.106: Loss = 0.586105
Epoch 1.107: Loss = 0.590302
Epoch 1.108: Loss = 0.593094
Epoch 1.109: Loss = 0.598251
Epoch 1.110: Loss = 0.588684
Epoch 1.111: Loss = 0.560318
Epoch 1.112: Loss = 0.588379
Epoch 1.113: Loss = 0.595261
Epoch 1.114: Loss = 0.557053
Epoch 1.115: Loss = 0.560715
Epoch 1.116: Loss = 0.541
Epoch 1.117: Loss = 0.531219
Epoch 1.118: Loss = 0.568314
Epoch 1.119: Loss = 0.565399
Epoch 1.120: Loss = 0.5746
TRAIN LOSS = 1.03462
TRAIN ACC = 72.9752 % (43787/60000)
Loss = 0.587982
Loss = 0.602554
Loss = 0.735504
Loss = 0.660446
Loss = 0.691742
Loss = 0.599487
Loss = 0.561386
Loss = 0.729065
Loss = 0.675262
Loss = 0.63913
Loss = 0.316727
Loss = 0.4785
Loss = 0.362106
Loss = 0.538254
Loss = 0.422836
Loss = 0.431107
Loss = 0.408813
Loss = 0.231522
Loss = 0.400055
Loss = 0.672302
TEST LOSS = 0.537239
TEST ACC = 437.869 % (8519/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.572891
Epoch 2.2: Loss = 0.656601
Epoch 2.3: Loss = 0.527008
Epoch 2.4: Loss = 0.621613
Epoch 2.5: Loss = 0.52037
Epoch 2.6: Loss = 0.633545
Epoch 2.7: Loss = 0.575333
Epoch 2.8: Loss = 0.519821
Epoch 2.9: Loss = 0.548538
Epoch 2.10: Loss = 0.544998
Epoch 2.11: Loss = 0.575287
Epoch 2.12: Loss = 0.504562
Epoch 2.13: Loss = 0.50708
Epoch 2.14: Loss = 0.559616
Epoch 2.15: Loss = 0.545868
Epoch 2.16: Loss = 0.490845
Epoch 2.17: Loss = 0.554565
Epoch 2.18: Loss = 0.542831
Epoch 2.19: Loss = 0.537033
Epoch 2.20: Loss = 0.561249
Epoch 2.21: Loss = 0.542267
Epoch 2.22: Loss = 0.535553
Epoch 2.23: Loss = 0.560349
Epoch 2.24: Loss = 0.522324
Epoch 2.25: Loss = 0.473969
Epoch 2.26: Loss = 0.533417
Epoch 2.27: Loss = 0.524948
Epoch 2.28: Loss = 0.451797
Epoch 2.29: Loss = 0.463745
Epoch 2.30: Loss = 0.506073
Epoch 2.31: Loss = 0.513123
Epoch 2.32: Loss = 0.473846
Epoch 2.33: Loss = 0.5298
Epoch 2.34: Loss = 0.499893
Epoch 2.35: Loss = 0.508759
Epoch 2.36: Loss = 0.501465
Epoch 2.37: Loss = 0.589066
Epoch 2.38: Loss = 0.487457
Epoch 2.39: Loss = 0.512833
Epoch 2.40: Loss = 0.488098
Epoch 2.41: Loss = 0.44838
Epoch 2.42: Loss = 0.557266
Epoch 2.43: Loss = 0.580795
Epoch 2.44: Loss = 0.483215
Epoch 2.45: Loss = 0.504089
Epoch 2.46: Loss = 0.478912
Epoch 2.47: Loss = 0.449112
Epoch 2.48: Loss = 0.479248
Epoch 2.49: Loss = 0.478302
Epoch 2.50: Loss = 0.45694
Epoch 2.51: Loss = 0.456375
Epoch 2.52: Loss = 0.472412
Epoch 2.53: Loss = 0.549545
Epoch 2.54: Loss = 0.516647
Epoch 2.55: Loss = 0.426773
Epoch 2.56: Loss = 0.411118
Epoch 2.57: Loss = 0.463898
Epoch 2.58: Loss = 0.430817
Epoch 2.59: Loss = 0.433182
Epoch 2.60: Loss = 0.422348
Epoch 2.61: Loss = 0.473251
Epoch 2.62: Loss = 0.438187
Epoch 2.63: Loss = 0.454224
Epoch 2.64: Loss = 0.476776
Epoch 2.65: Loss = 0.482666
Epoch 2.66: Loss = 0.487274
Epoch 2.67: Loss = 0.545914
Epoch 2.68: Loss = 0.473938
Epoch 2.69: Loss = 0.451996
Epoch 2.70: Loss = 0.454193
Epoch 2.71: Loss = 0.467621
Epoch 2.72: Loss = 0.407974
Epoch 2.73: Loss = 0.424301
Epoch 2.74: Loss = 0.481201
Epoch 2.75: Loss = 0.515259
Epoch 2.76: Loss = 0.448563
Epoch 2.77: Loss = 0.539017
Epoch 2.78: Loss = 0.450058
Epoch 2.79: Loss = 0.421219
Epoch 2.80: Loss = 0.474747
Epoch 2.81: Loss = 0.460297
Epoch 2.82: Loss = 0.434769
Epoch 2.83: Loss = 0.464722
Epoch 2.84: Loss = 0.41272
Epoch 2.85: Loss = 0.437775
Epoch 2.86: Loss = 0.448822
Epoch 2.87: Loss = 0.458344
Epoch 2.88: Loss = 0.47142
Epoch 2.89: Loss = 0.506989
Epoch 2.90: Loss = 0.443985
Epoch 2.91: Loss = 0.454178
Epoch 2.92: Loss = 0.475128
Epoch 2.93: Loss = 0.434219
Epoch 2.94: Loss = 0.509842
Epoch 2.95: Loss = 0.467133
Epoch 2.96: Loss = 0.496796
Epoch 2.97: Loss = 0.479553
Epoch 2.98: Loss = 0.449799
Epoch 2.99: Loss = 0.450256
Epoch 2.100: Loss = 0.49234
Epoch 2.101: Loss = 0.462265
Epoch 2.102: Loss = 0.437286
Epoch 2.103: Loss = 0.386002
Epoch 2.104: Loss = 0.510025
Epoch 2.105: Loss = 0.485458
Epoch 2.106: Loss = 0.518295
Epoch 2.107: Loss = 0.391632
Epoch 2.108: Loss = 0.422592
Epoch 2.109: Loss = 0.43837
Epoch 2.110: Loss = 0.515244
Epoch 2.111: Loss = 0.400864
Epoch 2.112: Loss = 0.426727
Epoch 2.113: Loss = 0.523972
Epoch 2.114: Loss = 0.412567
Epoch 2.115: Loss = 0.398758
Epoch 2.116: Loss = 0.4198
Epoch 2.117: Loss = 0.366135
Epoch 2.118: Loss = 0.401779
Epoch 2.119: Loss = 0.404999
Epoch 2.120: Loss = 0.454346
TRAIN LOSS = 0.485107
TRAIN ACC = 85.8704 % (51525/60000)
Loss = 0.432526
Loss = 0.483292
Loss = 0.606979
Loss = 0.545517
Loss = 0.574905
Loss = 0.452698
Loss = 0.426941
Loss = 0.631256
Loss = 0.555603
Loss = 0.523712
Loss = 0.203415
Loss = 0.336395
Loss = 0.294449
Loss = 0.404907
Loss = 0.276108
Loss = 0.319977
Loss = 0.274902
Loss = 0.107574
Loss = 0.273849
Loss = 0.562592
TEST LOSS = 0.41438
TEST ACC = 515.25 % (8800/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.481964
Epoch 3.2: Loss = 0.488449
Epoch 3.3: Loss = 0.505203
Epoch 3.4: Loss = 0.42186
Epoch 3.5: Loss = 0.491135
Epoch 3.6: Loss = 0.449936
Epoch 3.7: Loss = 0.470352
Epoch 3.8: Loss = 0.389465
Epoch 3.9: Loss = 0.37883
Epoch 3.10: Loss = 0.360641
Epoch 3.11: Loss = 0.394836
Epoch 3.12: Loss = 0.472092
Epoch 3.13: Loss = 0.445465
Epoch 3.14: Loss = 0.397476
Epoch 3.15: Loss = 0.500946
Epoch 3.16: Loss = 0.465546
Epoch 3.17: Loss = 0.460205
Epoch 3.18: Loss = 0.442245
Epoch 3.19: Loss = 0.423843
Epoch 3.20: Loss = 0.438461
Epoch 3.21: Loss = 0.393097
Epoch 3.22: Loss = 0.394241
Epoch 3.23: Loss = 0.450562
Epoch 3.24: Loss = 0.48024
Epoch 3.25: Loss = 0.374237
Epoch 3.26: Loss = 0.455292
Epoch 3.27: Loss = 0.362167
Epoch 3.28: Loss = 0.418762
Epoch 3.29: Loss = 0.447418
Epoch 3.30: Loss = 0.393387
Epoch 3.31: Loss = 0.417465
Epoch 3.32: Loss = 0.490601
Epoch 3.33: Loss = 0.436493
Epoch 3.34: Loss = 0.481476
Epoch 3.35: Loss = 0.436859
Epoch 3.36: Loss = 0.460312
Epoch 3.37: Loss = 0.466965
Epoch 3.38: Loss = 0.475464
Epoch 3.39: Loss = 0.431808
Epoch 3.40: Loss = 0.409775
Epoch 3.41: Loss = 0.384369
Epoch 3.42: Loss = 0.406204
Epoch 3.43: Loss = 0.441818
Epoch 3.44: Loss = 0.368713
Epoch 3.45: Loss = 0.456451
Epoch 3.46: Loss = 0.427002
Epoch 3.47: Loss = 0.359848
Epoch 3.48: Loss = 0.385635
Epoch 3.49: Loss = 0.465378
Epoch 3.50: Loss = 0.400604
Epoch 3.51: Loss = 0.475266
Epoch 3.52: Loss = 0.410049
Epoch 3.53: Loss = 0.470123
Epoch 3.54: Loss = 0.395844
Epoch 3.55: Loss = 0.389587
Epoch 3.56: Loss = 0.491989

Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.44559
Epoch 1.2: Loss = 2.38759
Epoch 1.3: Loss = 2.32472
Epoch 1.4: Loss = 2.26595
Epoch 1.5: Loss = 2.21616
Epoch 1.6: Loss = 2.16435
Epoch 1.7: Loss = 2.13208
Epoch 1.8: Loss = 2.10536
Epoch 1.9: Loss = 2.05585
Epoch 1.10: Loss = 1.99036
Epoch 1.11: Loss = 2.01807
Epoch 1.12: Loss = 1.98508
Epoch 1.13: Loss = 1.95872
Epoch 1.14: Loss = 1.91046
Epoch 1.15: Loss = 1.86584
Epoch 1.16: Loss = 1.84499
Epoch 1.17: Loss = 1.8367
Epoch 1.18: Loss = 1.77377
Epoch 1.19: Loss = 1.75012
Epoch 1.20: Loss = 1.7029
Epoch 1.21: Loss = 1.61838
Epoch 1.22: Loss = 1.64487
Epoch 1.23: Loss = 1.62175
Epoch 1.24: Loss = 1.59164
Epoch 1.25: Loss = 1.51999
Epoch 1.26: Loss = 1.54195
Epoch 1.27: Loss = 1.53473
Epoch 1.28: Loss = 1.46709
Epoch 1.29: Loss = 1.49631
Epoch 1.30: Loss = 1.40384
Epoch 1.31: Loss = 1.34886
Epoch 1.32: Loss = 1.3744
Epoch 1.33: Loss = 1.35245
Epoch 1.34: Loss = 1.35391
Epoch 1.35: Loss = 1.33124
Epoch 1.36: Loss = 1.27989
Epoch 1.37: Loss = 1.27548
Epoch 1.38: Loss = 1.27281
Epoch 1.39: Loss = 1.18163
Epoch 1.40: Loss = 1.20892
Epoch 1.41: Loss = 1.19206
Epoch 1.42: Loss = 1.17618
Epoch 1.43: Loss = 1.13969
Epoch 1.44: Loss = 1.12976
Epoch 1.45: Loss = 1.12845
Epoch 1.46: Loss = 1.1456
Epoch 1.47: Loss = 1.06291
Epoch 1.48: Loss = 1.08104
Epoch 1.49: Loss = 1.01816
Epoch 1.50: Loss = 1.075
Epoch 1.51: Loss = 1.05547
Epoch 1.52: Loss = 1.04756
Epoch 1.53: Loss = 1.05298
Epoch 1.54: Loss = 0.934021
Epoch 1.55: Loss = 0.913223
Epoch 1.56: Loss = 0.942703
Epoch 1.57: Loss = 0.911621
Epoch 1.58: Loss = 0.908936
Epoch 1.59: Loss = 0.927902
Epoch 1.60: Loss = 0.923035
Epoch 1.61: Loss = 0.933029
Epoch 1.62: Loss = 0.886444
Epoch 1.63: Loss = 0.866547
Epoch 1.64: Loss = 0.855362
Epoch 1.65: Loss = 0.838974
Epoch 1.66: Loss = 0.84256
Epoch 1.67: Loss = 0.867508
Epoch 1.68: Loss = 0.838318
Epoch 1.69: Loss = 0.747833
Epoch 1.70: Loss = 0.835541
Epoch 1.71: Loss = 0.797394
Epoch 1.72: Loss = 0.831192
Epoch 1.73: Loss = 0.792953
Epoch 1.74: Loss = 0.842804
Epoch 1.75: Loss = 0.792297
Epoch 1.76: Loss = 0.779968
Epoch 1.77: Loss = 0.803757
Epoch 1.78: Loss = 0.749329
Epoch 1.79: Loss = 0.804626
Epoch 1.80: Loss = 0.756592
Epoch 1.81: Loss = 0.742462
Epoch 1.82: Loss = 0.772461
Epoch 1.83: Loss = 0.747635
Epoch 1.84: Loss = 0.713684
Epoch 1.85: Loss = 0.728882
Epoch 1.86: Loss = 0.731705
Epoch 1.87: Loss = 0.687637
Epoch 1.88: Loss = 0.763397
Epoch 1.89: Loss = 0.661148
Epoch 1.90: Loss = 0.704147
Epoch 1.91: Loss = 0.701004
Epoch 1.92: Loss = 0.759415
Epoch 1.93: Loss = 0.726318
Epoch 1.94: Loss = 0.663651
Epoch 1.95: Loss = 0.652618
Epoch 1.96: Loss = 0.687759
Epoch 1.97: Loss = 0.654846
Epoch 1.98: Loss = 0.656372
Epoch 1.99: Loss = 0.604263
Epoch 1.100: Loss = 0.627411
Epoch 1.101: Loss = 0.734436
Epoch 1.102: Loss = 0.659286
Epoch 1.103: Loss = 0.665909
Epoch 1.104: Loss = 0.666031
Epoch 1.105: Loss = 0.687012
Epoch 1.106: Loss = 0.670563
Epoch 1.107: Loss = 0.590591
Epoch 1.108: Loss = 0.623932
Epoch 1.109: Loss = 0.635681
Epoch 1.110: Loss = 0.592865
Epoch 1.111: Loss = 0.631195
Epoch 1.112: Loss = 0.620697
Epoch 1.113: Loss = 0.633179
Epoch 1.114: Loss = 0.68782
Epoch 1.115: Loss = 0.696198
Epoch 1.116: Loss = 0.640091
Epoch 1.117: Loss = 0.549713
Epoch 1.118: Loss = 0.608429
Epoch 1.119: Loss = 0.590485
Epoch 1.120: Loss = 0.544006
TRAIN LOSS = 1.10918
TRAIN ACC = 68.129 % (40879/60000)
Loss = 0.606354
Loss = 0.634918
Loss = 0.744247
Loss = 0.702881
Loss = 0.717255
Loss = 0.635651
Loss = 0.597198
Loss = 0.749191
Loss = 0.714264
Loss = 0.66745
Loss = 0.387009
Loss = 0.491516
Loss = 0.40152
Loss = 0.577377
Loss = 0.466156
Loss = 0.430649
Loss = 0.448212
Loss = 0.26358
Loss = 0.441101
Loss = 0.707047
TEST LOSS = 0.569179
TEST ACC = 408.789 % (8407/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.574371
Epoch 2.2: Loss = 0.538666
Epoch 2.3: Loss = 0.604584
Epoch 2.4: Loss = 0.599121
Epoch 2.5: Loss = 0.574646
Epoch 2.6: Loss = 0.538452
Epoch 2.7: Loss = 0.498169
Epoch 2.8: Loss = 0.541214
Epoch 2.9: Loss = 0.608154
Epoch 2.10: Loss = 0.607468
Epoch 2.11: Loss = 0.587418
Epoch 2.12: Loss = 0.576355
Epoch 2.13: Loss = 0.547943
Epoch 2.14: Loss = 0.552979
Epoch 2.15: Loss = 0.520889
Epoch 2.16: Loss = 0.616043
Epoch 2.17: Loss = 0.538834
Epoch 2.18: Loss = 0.533447
Epoch 2.19: Loss = 0.550522
Epoch 2.20: Loss = 0.588211
Epoch 2.21: Loss = 0.55864
Epoch 2.22: Loss = 0.576126
Epoch 2.23: Loss = 0.577911
Epoch 2.24: Loss = 0.533813
Epoch 2.25: Loss = 0.534332
Epoch 2.26: Loss = 0.582352
Epoch 2.27: Loss = 0.55954
Epoch 2.28: Loss = 0.563461
Epoch 2.29: Loss = 0.555756
Epoch 2.30: Loss = 0.599121
Epoch 2.31: Loss = 0.599182
Epoch 2.32: Loss = 0.527939
Epoch 2.33: Loss = 0.564255
Epoch 2.34: Loss = 0.535141
Epoch 2.35: Loss = 0.554291
Epoch 2.36: Loss = 0.559372
Epoch 2.37: Loss = 0.487915
Epoch 2.38: Loss = 0.511261
Epoch 2.39: Loss = 0.539337
Epoch 2.40: Loss = 0.573166
Epoch 2.41: Loss = 0.561417
Epoch 2.42: Loss = 0.505859
Epoch 2.43: Loss = 0.552734
Epoch 2.44: Loss = 0.493439
Epoch 2.45: Loss = 0.492081
Epoch 2.46: Loss = 0.450729
Epoch 2.47: Loss = 0.538742
Epoch 2.48: Loss = 0.498917
Epoch 2.49: Loss = 0.449921
Epoch 2.50: Loss = 0.524063
Epoch 2.51: Loss = 0.467972
Epoch 2.52: Loss = 0.497101
Epoch 2.53: Loss = 0.489349
Epoch 2.54: Loss = 0.524857
Epoch 2.55: Loss = 0.447235
Epoch 2.56: Loss = 0.511368
Epoch 2.57: Loss = 0.517136
Epoch 2.58: Loss = 0.521286
Epoch 2.59: Loss = 0.459778
Epoch 2.60: Loss = 0.5961
Epoch 2.61: Loss = 0.513168
Epoch 2.62: Loss = 0.45697
Epoch 2.63: Loss = 0.502869
Epoch 2.64: Loss = 0.479935
Epoch 2.65: Loss = 0.509384
Epoch 2.66: Loss = 0.525391
Epoch 2.67: Loss = 0.506226
Epoch 2.68: Loss = 0.555038
Epoch 2.69: Loss = 0.441696
Epoch 2.70: Loss = 0.493362
Epoch 2.71: Loss = 0.493073
Epoch 2.72: Loss = 0.475998
Epoch 2.73: Loss = 0.517029
Epoch 2.74: Loss = 0.463898
Epoch 2.75: Loss = 0.51442
Epoch 2.76: Loss = 0.518585
Epoch 2.77: Loss = 0.43837
Epoch 2.78: Loss = 0.507019
Epoch 2.79: Loss = 0.447495
Epoch 2.80: Loss = 0.447113
Epoch 2.81: Loss = 0.488297
Epoch 2.82: Loss = 0.520493
Epoch 2.83: Loss = 0.482224
Epoch 2.84: Loss = 0.563232
Epoch 2.85: Loss = 0.427399
Epoch 2.86: Loss = 0.464981
Epoch 2.87: Loss = 0.54126
Epoch 2.88: Loss = 0.569992
Epoch 2.89: Loss = 0.499161
Epoch 2.90: Loss = 0.429733
Epoch 2.91: Loss = 0.433441
Epoch 2.92: Loss = 0.479675
Epoch 2.93: Loss = 0.431305
Epoch 2.94: Loss = 0.459137
Epoch 2.95: Loss = 0.475723
Epoch 2.96: Loss = 0.431808
Epoch 2.97: Loss = 0.443542
Epoch 2.98: Loss = 0.461349
Epoch 2.99: Loss = 0.442551
Epoch 2.100: Loss = 0.364029
Epoch 2.101: Loss = 0.433258
Epoch 2.102: Loss = 0.39212
Epoch 2.103: Loss = 0.454468
Epoch 2.104: Loss = 0.42749
Epoch 2.105: Loss = 0.512329
Epoch 2.106: Loss = 0.450836
Epoch 2.107: Loss = 0.415894
Epoch 2.108: Loss = 0.429703
Epoch 2.109: Loss = 0.474823
Epoch 2.110: Loss = 0.413055
Epoch 2.111: Loss = 0.427826
Epoch 2.112: Loss = 0.414734
Epoch 2.113: Loss = 0.419617
Epoch 2.114: Loss = 0.407974
Epoch 2.115: Loss = 0.497971
Epoch 2.116: Loss = 0.405884
Epoch 2.117: Loss = 0.416641
Epoch 2.118: Loss = 0.475815
Epoch 2.119: Loss = 0.448593
Epoch 2.120: Loss = 0.44574
TRAIN LOSS = 0.503952
TRAIN ACC = 85.2737 % (51167/60000)
Loss = 0.437485
Loss = 0.506088
Loss = 0.596497
Loss = 0.56427
Loss = 0.592758
Loss = 0.47052
Loss = 0.441742
Loss = 0.622833
Loss = 0.576141
Loss = 0.533188
Loss = 0.240891
Loss = 0.357529
Loss = 0.300354
Loss = 0.41301
Loss = 0.29541
Loss = 0.309723
Loss = 0.289566
Loss = 0.127731
Loss = 0.290634
Loss = 0.573669
TEST LOSS = 0.427002
TEST ACC = 511.668 % (8749/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.398529
Epoch 3.2: Loss = 0.459213
Epoch 3.3: Loss = 0.562332
Epoch 3.4: Loss = 0.460037
Epoch 3.5: Loss = 0.465698
Epoch 3.6: Loss = 0.441437
Epoch 3.7: Loss = 0.447693
Epoch 3.8: Loss = 0.411575
Epoch 3.9: Loss = 0.454895
Epoch 3.10: Loss = 0.489258
Epoch 3.11: Loss = 0.440796
Epoch 3.12: Loss = 0.452484
Epoch 3.13: Loss = 0.450806
Epoch 3.14: Loss = 0.427307
Epoch 3.15: Loss = 0.456619
Epoch 3.16: Loss = 0.461273
Epoch 3.17: Loss = 0.395172
Epoch 3.18: Loss = 0.478683
Epoch 3.19: Loss = 0.508331
Epoch 3.20: Loss = 0.427994
Epoch 3.21: Loss = 0.395905
Epoch 3.22: Loss = 0.490784
Epoch 3.23: Loss = 0.431946
Epoch 3.24: Loss = 0.364212
Epoch 3.25: Loss = 0.497574
Epoch 3.26: Loss = 0.502579
Epoch 3.27: Loss = 0.458542
Epoch 3.28: Loss = 0.47876
Epoch 3.29: Loss = 0.426682
Epoch 3.30: Loss = 0.480896
Epoch 3.31: Loss = 0.377213
Epoch 3.32: Loss = 0.394531
Epoch 3.33: Loss = 0.43335
Epoch 3.34: Loss = 0.441681
Epoch 3.35: Loss = 0.42894
Epoch 3.36: Loss = 0.454971
Epoch 3.37: Loss = 0.384964
Epoch 3.38: Loss = 0.440689
Epoch 3.39: Loss = 0.426422
Epoch 3.40: Loss = 0.400604
Epoch 3.41: Loss = 0.405594
Epoch 3.42: Loss = 0.513016
Epoch 3.43: Loss = 0.442841
Epoch 3.44: Loss = 0.422668
Epoch 3.45: Loss = 0.400284
Epoch 3.46: Loss = 0.38205
Epoch 3.47: Loss = 0.424072
Epoch 3.48: Loss = 0.431061
Epoch 3.49: Loss = 0.476608
Epoch 3.50: Loss = 0.42897
Epoch 3.51: Loss = 0.393127
Epoch 3.52: Loss = 0.34906
Epoch 3.53: Loss = 0.46904
Epoch 3.54: Loss = 0.420349
Epoch 3.55: Loss = 0.433487
Epoch 3.56: Loss = 0.476517
Epoch 3.57: Loss = 0.49588
Epoch 3.58: Loss = 0.418579
Epoch 3.59: Loss = 0.428192
Epoch 3.60: Loss = 0.341919
Epoch 3.61: Loss = 0.441879
Epoch 3.62: Loss = 0.463898
Epoch 3.63: Loss = 0.426025
Epoch 3.64: Loss = 0.444962
Epoch 3.65: Loss = 0.418961
Epoch 3.66: Loss = 0.357071
Epoch 3.67: Loss = 0.37735
Epoch 3.68: Loss = 0.480743
Epoch 3.69: Loss = 0.380829
Epoch 3.70: Loss = 0.388031
Epoch 3.71: Loss = 0.397705
Epoch 3.72: Loss = 0.339294
Epoch 3.73: Loss = 0.427887
Epoch 3.74: Loss = 0.405548
Epoch 3.75: Loss = 0.442856
Epoch 3.76: Loss = 0.472763
Epoch 3.77: Loss = 0.386078
Epoch 3.78: Loss = 0.400696
Epoch 3.79: Loss = 0.364944
Epoch 3.80: Loss = 0.359024
Epoch 3.81: Loss = 0.328415
Epoch 3.82: Loss = 0.401749
Epoch 3.83: Loss = 0.441589
Epoch 3.84: Loss = 0.369156
Epoch 3.85: Loss = 0.448624
Epoch 3.86: Loss = 0.392151
Epoch 3.87: Loss = 0.438721
Epoch 3.88: Loss = 0.435593
Epoch 3.89: Loss = 0.392838
Epoch 3.90: Loss = 0.470749
Epoch 3.91: Loss = 0.415115
Epoch 3.92: Loss = 0.43222
Epoch 3.93: Loss = 0.378815
Epoch 3.94: Loss = 0.428894
Epoch 3.95: Loss = 0.383881
Epoch 3.96: Loss = 0.453674
Epoch 3.97: Loss = 0.380676
Epoch 3.98: Loss = 0.404526
Epoch 3.99: Loss = 0.398071
Epoch 3.100: Loss = 0.407761
Epoch 3.101: Loss = 0.417847
Epoch 3.102: Loss = 0.524506
Epoch 3.103: Loss = 0.409134
Epoch 3.104: Loss = 0.37738
Epoch 3.105: Loss = 0.39328
Epoch 3.106: Loss = 0.334961
Epoch 3.107: Loss = 0.433792
Epoch 3.108: Loss = 0.407578
Epoch 3.109: Loss = 0.418091
Epoch 3.110: Loss = 0.366455
Epoch 3.111: Loss = 0.379364
Epoch 3.112: Loss = 0.413788
Epoch 3.113: Loss = 0.327515
Epoch 3.114: Loss = 0.376312
Epoch 3.115: Loss = 0.366898
Epoch 3.116: Loss = 0.506668
Epoch 3.117: Loss = 0.372955
Epoch 3.118: Loss = 0.360641
Epoch 3.119: Loss = 0.369781
Epoch 3.120: Loss = 0.363678
TRAIN LOSS = 0.422134
TRAIN ACC = 87.5565 % (52536/60000)
Loss = 0.380066
Loss = 0.4543
Loss = 0.539307
Loss = 0.52449
Loss = 0.552185
Loss = 0.419876
Loss = 0.381546
Loss = 0.595871
Loss = 0.535706
Loss = 0.486984
Loss = 0.199005
Loss = 0.311035
Loss = 0.280167
Loss = 0.361847
Loss = 0.240997
Loss = 0.279037
Loss = 0.240494
Loss = 0.0879517
Loss = 0.246094
Loss = 0.536957
TEST LOSS = 0.382696
TEST ACC = 525.359 % (8873/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.498383
Epoch 4.2: Loss = 0.375763
Epoch 4.3: Loss = 0.452713
Epoch 4.4: Loss = 0.413849
Epoch 4.5: Loss = 0.3918
Epoch 4.6: Loss = 0.410202
Epoch 4.7: Loss = 0.414291
Epoch 4.8: Loss = 0.452698
Epoch 4.9: Loss = 0.394867
Epoch 4.10: Loss = 0.379578
Epoch 4.11: Loss = 0.506546
Epoch 4.12: Loss = 0.492905
Epoch 4.13: Loss = 0.472809
Epoch 4.14: Loss = 0.404877
Epoch 4.15: Loss = 0.362381
Epoch 4.16: Loss = 0.316025
Epoch 4.17: Loss = 0.390015
Epoch 4.18: Loss = 0.340286
Epoch 4.19: Loss = 0.387421
Epoch 4.20: Loss = 0.32193
Epoch 4.21: Loss = 0.378403
Epoch 4.22: Loss = 0.481201
Epoch 4.23: Loss = 0.312836
Epoch 4.24: Loss = 0.373886
Epoch 4.25: Loss = 0.377457
Epoch 4.26: Loss = 0.473389
Epoch 4.27: Loss = 0.422928
Epoch 4.28: Loss = 0.406296
Epoch 4.29: Loss = 0.413574
Epoch 4.30: Loss = 0.41626
Epoch 4.31: Loss = 0.416779
Epoch 4.32: Loss = 0.370483
Epoch 4.33: Loss = 0.313858
Epoch 4.34: Loss = 0.420258
Epoch 4.35: Loss = 0.302444
Epoch 4.36: Loss = 0.452332
Epoch 4.37: Loss = 0.397949
Epoch 4.38: Loss = 0.322922
Epoch 4.39: Loss = 0.358994
Epoch 4.40: Loss = 0.415695
Epoch 4.41: Loss = 0.406479
Epoch 4.42: Loss = 0.338623
Epoch 4.43: Loss = 0.358932
Epoch 4.44: Loss = 0.351456
Epoch 4.45: Loss = 0.365433
Epoch 4.46: Loss = 0.345505
Epoch 4.47: Loss = 0.360992
Epoch 4.48: Loss = 0.343262
Epoch 4.49: Loss = 0.413193
Epoch 4.50: Loss = 0.340103
Epoch 4.51: Loss = 0.428085
Epoch 4.52: Loss = 0.331985
Epoch 4.53: Loss = 0.315155
Epoch 4.54: Loss = 0.310043
Epoch 4.55: Loss = 0.391876
Epoch 4.56: Loss = 0.364807
Epoch 4.57: Loss = 0.396286
Epoch 4.58: Loss = 0.403183
Epoch 4.59: Loss = 0.405884
Epoch 4.60: Loss = 0.432022
Epoch 4.61: Loss = 0.317352
Epoch 4.62: Loss = 0.376221
Epoch 4.63: Loss = 0.355377
Epoch 4.64: Loss = 0.433121
Epoch 4.65: Loss = 0.34639
Epoch 4.66: Loss = 0.402176
Epoch 4.67: Loss = 0.380829
Epoch 4.68: Loss = 0.465927
Epoch 4.69: Loss = 0.41568
Epoch 4.70: Loss = 0.416489
Epoch 4.71: Loss = 0.379913
Epoch 4.72: Loss = 0.465485
Epoch 4.73: Loss = 0.420105
Epoch 4.74: Loss = 0.394165
Epoch 4.75: Loss = 0.359634
Epoch 4.76: Loss = 0.359344
Epoch 4.77: Loss = 0.457825
Epoch 4.78: Loss = 0.401886
Epoch 4.79: Loss = 0.366455
Epoch 4.80: Loss = 0.412811
Epoch 4.81: Loss = 0.42778
Epoch 4.82: Loss = 0.303574
Epoch 4.83: Loss = 0.37709
Epoch 4.84: Loss = 0.33934
Epoch 4.85: Loss = 0.447281
Epoch 4.86: Loss = 0.394531
Epoch 4.87: Loss = 0.427078
Epoch 4.88: Loss = 0.332153
Epoch 4.89: Loss = 0.414169
Epoch 4.90: Loss = 0.40239
Epoch 4.91: Loss = 0.420029
Epoch 4.92: Loss = 0.438034
Epoch 4.93: Loss = 0.303909
Epoch 4.94: Loss = 0.370407
Epoch 4.95: Loss = 0.425995
Epoch 4.96: Loss = 0.415421
Epoch 4.97: Loss = 0.298355
Epoch 4.98: Loss = 0.391495
Epoch 4.99: Loss = 0.447662
Epoch 4.100: Loss = 0.302811
Epoch 4.101: Loss = 0.492477
Epoch 4.102: Loss = 0.39592
Epoch 4.103: Loss = 0.410233
Epoch 4.104: Loss = 0.368805
Epoch 4.105: Loss = 0.432922
Epoch 4.106: Loss = 0.411911
Epoch 4.107: Loss = 0.429398
Epoch 4.108: Loss = 0.386963
Epoch 4.109: Loss = 0.340881
Epoch 4.110: Loss = 0.343048
Epoch 4.111: Loss = 0.440033
Epoch 4.112: Loss = 0.335449
Epoch 4.113: Loss = 0.3582
Epoch 4.114: Loss = 0.476593
Epoch 4.115: Loss = 0.371704
Epoch 4.116: Loss = 0.347763
Epoch 4.117: Loss = 0.322479
Epoch 4.118: Loss = 0.367783
Epoch 4.119: Loss = 0.422012
Epoch 4.120: Loss = 0.342468
TRAIN LOSS = 0.390121
TRAIN ACC = 88.5834 % (53152/60000)
Loss = 0.352234
Loss = 0.441147
Loss = 0.51564
Loss = 0.503799
Loss = 0.530884
Loss = 0.391571
Loss = 0.353516
Loss = 0.573212
Loss = 0.508484
Loss = 0.456253
Loss = 0.180298
Loss = 0.300751
Loss = 0.273117
Loss = 0.332367
Loss = 0.218109
Loss = 0.262375
Loss = 0.214737
Loss = 0.0678558
Loss = 0.222549
Loss = 0.516388
TEST LOSS = 0.360764
TEST ACC = 531.519 % (8941/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.370193
Epoch 5.2: Loss = 0.320129
Epoch 5.3: Loss = 0.402588
Epoch 5.4: Loss = 0.328186
Epoch 5.5: Loss = 0.452332
Epoch 5.6: Loss = 0.314423
Epoch 5.7: Loss = 0.411255
Epoch 5.8: Loss = 0.337448
Epoch 5.9: Loss = 0.518921
Epoch 5.10: Loss = 0.294601
Epoch 5.11: Loss = 0.38739
Epoch 5.12: Loss = 0.41507
Epoch 5.13: Loss = 0.36647
Epoch 5.14: Loss = 0.403366
Epoch 5.15: Loss = 0.374115
Epoch 5.16: Loss = 0.438385
Epoch 5.17: Loss = 0.382965
Epoch 5.18: Loss = 0.315353
Epoch 5.19: Loss = 0.40741
Epoch 5.20: Loss = 0.295044
Epoch 5.21: Loss = 0.375717
Epoch 5.22: Loss = 0.27504
Epoch 5.23: Loss = 0.35466
Epoch 5.24: Loss = 0.401184
Epoch 5.25: Loss = 0.377701
Epoch 5.26: Loss = 0.345398
Epoch 5.27: Loss = 0.329346
Epoch 5.28: Loss = 0.417557
Epoch 5.29: Loss = 0.316238
Epoch 5.30: Loss = 0.34465
Epoch 5.31: Loss = 0.369766
Epoch 5.32: Loss = 0.399551
Epoch 5.33: Loss = 0.347931
Epoch 5.34: Loss = 0.372238
Epoch 5.35: Loss = 0.459778
Epoch 5.36: Loss = 0.415268
Epoch 5.37: Loss = 0.357468
Epoch 5.38: Loss = 0.40509
Epoch 5.39: Loss = 0.361679
Epoch 5.40: Loss = 0.359619
Epoch 5.41: Loss = 0.312973
Epoch 5.42: Loss = 0.451416
Epoch 5.43: Loss = 0.371323
Epoch 5.44: Loss = 0.491653
Epoch 5.45: Loss = 0.404068
Epoch 5.46: Loss = 0.304062
Epoch 5.47: Loss = 0.293045
Epoch 5.48: Loss = 0.315338
Epoch 5.49: Loss = 0.334808
Epoch 5.50: Loss = 0.371353
Epoch 5.51: Loss = 0.338623
Epoch 5.52: Loss = 0.324875
Epoch 5.53: Loss = 0.368393
Epoch 5.54: Loss = 0.408356
Epoch 5.55: Loss = 0.403427
Epoch 5.56: Loss = 0.349136
Epoch 5.57: Loss = 0.456131
Epoch 5.58: Loss = 0.356064
Epoch 5.59: Loss = 0.375839
Epoch 5.60: Loss = 0.341507
Epoch 5.61: Loss = 0.429321
Epoch 5.62: Loss = 0.329971
Epoch 5.63: Loss = 0.322006
Epoch 5.64: Loss = 0.333984
Epoch 5.65: Loss = 0.316315
Epoch 5.66: Loss = 0.415878
Epoch 5.67: Loss = 0.399719
Epoch 5.68: Loss = 0.378433
Epoch 5.69: Loss = 0.393188
Epoch 5.70: Loss = 0.398727
Epoch 5.71: Loss = 0.362335
Epoch 5.72: Loss = 0.27858
Epoch 5.73: Loss = 0.394318
Epoch 5.74: Loss = 0.300949
Epoch 5.75: Loss = 0.393463
Epoch 5.76: Loss = 0.473541
Epoch 5.77: Loss = 0.384628
Epoch 5.78: Loss = 0.477539
Epoch 5.79: Loss = 0.421692
Epoch 5.80: Loss = 0.469131
Epoch 5.81: Loss = 0.391037
Epoch 5.82: Loss = 0.408661
Epoch 5.83: Loss = 0.389511
Epoch 5.84: Loss = 0.333069
Epoch 5.85: Loss = 0.336624
Epoch 5.86: Loss = 0.324493
Epoch 5.87: Loss = 0.407562
Epoch 5.88: Loss = 0.388382
Epoch 5.89: Loss = 0.314041
Epoch 5.90: Loss = 0.38649
Epoch 5.91: Loss = 0.431473
Epoch 5.92: Loss = 0.40654
Epoch 5.93: Loss = 0.432358
Epoch 5.94: Loss = 0.39595
Epoch 5.95: Loss = 0.359665
Epoch 5.96: Loss = 0.341934
Epoch 5.97: Loss = 0.381821
Epoch 5.98: Loss = 0.276886
Epoch 5.99: Loss = 0.276459
Epoch 5.100: Loss = 0.366089
Epoch 5.101: Loss = 0.424118
Epoch 5.102: Loss = 0.35527
Epoch 5.103: Loss = 0.442688
Epoch 5.104: Loss = 0.326813
Epoch 5.105: Loss = 0.357819
Epoch 5.106: Loss = 0.347687
Epoch 5.107: Loss = 0.361801
Epoch 5.108: Loss = 0.365005
Epoch 5.109: Loss = 0.315948
Epoch 5.110: Loss = 0.289581
Epoch 5.111: Loss = 0.436798
Epoch 5.112: Loss = 0.283646
Epoch 5.113: Loss = 0.514206
Epoch 5.114: Loss = 0.473206
Epoch 5.115: Loss = 0.452789
Epoch 5.116: Loss = 0.33049
Epoch 5.117: Loss = 0.35791
Epoch 5.118: Loss = 0.340057
Epoch 5.119: Loss = 0.402939
Epoch 5.120: Loss = 0.38707
TRAIN LOSS = 0.374023
TRAIN ACC = 89.2334 % (53542/60000)
Loss = 0.338318
Loss = 0.431946
Loss = 0.499771
Loss = 0.494873
Loss = 0.523941
Loss = 0.375656
Loss = 0.339554
Loss = 0.571411
Loss = 0.508163
Loss = 0.452728
Loss = 0.174469
Loss = 0.27858
Loss = 0.282455
Loss = 0.323853
Loss = 0.196884
Loss = 0.265289
Loss = 0.198593
Loss = 0.0600128
Loss = 0.217087
Loss = 0.500259
TEST LOSS = 0.351692
TEST ACC = 535.419 % (8990/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.36821
Epoch 6.2: Loss = 0.406708
Epoch 6.3: Loss = 0.376465
Epoch 6.4: Loss = 0.344055
Epoch 6.5: Loss = 0.36499
Epoch 6.6: Loss = 0.399475
Epoch 6.7: Loss = 0.383606
Epoch 6.8: Loss = 0.378128
Epoch 6.9: Loss = 0.368439
Epoch 6.10: Loss = 0.327225
Epoch 6.11: Loss = 0.280548
Epoch 6.12: Loss = 0.383835
Epoch 6.13: Loss = 0.317139
Epoch 6.14: Loss = 0.424377
Epoch 6.15: Loss = 0.368835
Epoch 6.16: Loss = 0.427872
Epoch 6.17: Loss = 0.367676
Epoch 6.18: Loss = 0.367905
Epoch 6.19: Loss = 0.321655
Epoch 6.20: Loss = 0.382324
Epoch 6.21: Loss = 0.345078
Epoch 6.22: Loss = 0.437286
Epoch 6.23: Loss = 0.389435
Epoch 6.24: Loss = 0.33902
Epoch 6.25: Loss = 0.366257
Epoch 6.26: Loss = 0.383392
Epoch 6.27: Loss = 0.370819
Epoch 6.28: Loss = 0.355133
Epoch 6.29: Loss = 0.4021
Epoch 6.30: Loss = 0.348114
Epoch 6.31: Loss = 0.335953
Epoch 6.32: Loss = 0.374939
Epoch 6.33: Loss = 0.394272
Epoch 6.34: Loss = 0.330444
Epoch 6.35: Loss = 0.320297
Epoch 6.36: Loss = 0.409836
Epoch 6.37: Loss = 0.332077
Epoch 6.38: Loss = 0.413315
Epoch 6.39: Loss = 0.380753
Epoch 6.40: Loss = 0.431122
Epoch 6.41: Loss = 0.45517
Epoch 6.42: Loss = 0.475372
Epoch 6.43: Loss = 0.296829
Epoch 6.44: Loss = 0.397476
Epoch 6.45: Loss = 0.365753
Epoch 6.46: Loss = 0.31369
Epoch 6.47: Loss = 0.298615
Epoch 6.48: Loss = 0.353302
Epoch 6.49: Loss = 0.407684
Epoch 6.50: Loss = 0.356445
Epoch 6.51: Loss = 0.424088
Epoch 6.52: Loss = 0.356354
Epoch 6.53: Loss = 0.374985
Epoch 6.54: Loss = 0.342072
Epoch 6.55: Loss = 0.303726
Epoch 6.56: Loss = 0.356537
Epoch 6.57: Loss = 0.284576
Epoch 6.58: Loss = 0.363358
Epoch 6.59: Loss = 0.362
Epoch 6.60: Loss = 0.390503
Epoch 6.61: Loss = 0.370636
Epoch 6.62: Loss = 0.313782
Epoch 6.63: Loss = 0.480011
Epoch 6.64: Loss = 0.378448
Epoch 6.65: Loss = 0.400955
Epoch 6.66: Loss = 0.407379
Epoch 6.67: Loss = 0.312851
Epoch 6.68: Loss = 0.34166
Epoch 6.69: Loss = 0.33876
Epoch 6.70: Loss = 0.287613
Epoch 6.71: Loss = 0.349243
Epoch 6.72: Loss = 0.361328
Epoch 6.73: Loss = 0.444733
Epoch 6.74: Loss = 0.437347
Epoch 6.75: Loss = 0.357147
Epoch 6.76: Loss = 0.326828
Epoch 6.77: Loss = 0.311234
Epoch 6.78: Loss = 0.417526
Epoch 6.79: Loss = 0.319977
Epoch 6.80: Loss = 0.328201
Epoch 6.81: Loss = 0.339096
Epoch 6.82: Loss = 0.395493
Epoch 6.83: Loss = 0.322891
Epoch 6.84: Loss = 0.273926
Epoch 6.85: Loss = 0.321503
Epoch 6.86: Loss = 0.245117
Epoch 6.87: Loss = 0.371017
Epoch 6.88: Loss = 0.351059
Epoch 6.89: Loss = 0.28392
Epoch 6.90: Loss = 0.339844
Epoch 6.91: Loss = 0.412354
Epoch 6.92: Loss = 0.336212
Epoch 6.93: Loss = 0.455765
Epoch 6.94: Loss = 0.33284
Epoch 6.95: Loss = 0.360199
Epoch 6.96: Loss = 0.342941
Epoch 6.97: Loss = 0.449875
Epoch 6.98: Loss = 0.382095
Epoch 6.99: Loss = 0.383743
Epoch 6.100: Loss = 0.389801
Epoch 6.101: Loss = 0.295868
Epoch 6.102: Loss = 0.379761
Epoch 6.103: Loss = 0.353363
Epoch 6.104: Loss = 0.309738
Epoch 6.105: Loss = 0.338867
Epoch 6.106: Loss = 0.289261
Epoch 6.107: Loss = 0.285431
Epoch 6.108: Loss = 0.348541
Epoch 6.109: Loss = 0.393341
Epoch 6.110: Loss = 0.316437
Epoch 6.111: Loss = 0.321564
Epoch 6.112: Loss = 0.392807
Epoch 6.113: Loss = 0.395721
Epoch 6.114: Loss = 0.363205
Epoch 6.115: Loss = 0.378738
Epoch 6.116: Loss = 0.428726
Epoch 6.117: Loss = 0.348526
Epoch 6.118: Loss = 0.377777
Epoch 6.119: Loss = 0.399078
Epoch 6.120: Loss = 0.411392
TRAIN LOSS = 0.363342
TRAIN ACC = 89.6591 % (53798/60000)
Loss = 0.326065
Loss = 0.429611
Loss = 0.485916
Loss = 0.483536
Loss = 0.514801
Loss = 0.371841
Loss = 0.327042
Loss = 0.563385
Loss = 0.497391
Loss = 0.439484
Loss = 0.16301
Loss = 0.27356
Loss = 0.283997
Loss = 0.312256
Loss = 0.185455
Loss = 0.257553
Loss = 0.183136
Loss = 0.0523529
Loss = 0.21283
Loss = 0.490677
TEST LOSS = 0.342695
TEST ACC = 537.979 % (9046/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.347916
Epoch 7.2: Loss = 0.426819
Epoch 7.3: Loss = 0.260056
Epoch 7.4: Loss = 0.406952
Epoch 7.5: Loss = 0.311264
Epoch 7.6: Loss = 0.337402
Epoch 7.7: Loss = 0.312103
Epoch 7.8: Loss = 0.385971
Epoch 7.9: Loss = 0.36705
Epoch 7.10: Loss = 0.344864
Epoch 7.11: Loss = 0.391022
Epoch 7.12: Loss = 0.400284
Epoch 7.13: Loss = 0.332886
Epoch 7.14: Loss = 0.346191
Epoch 7.15: Loss = 0.384171
Epoch 7.16: Loss = 0.425659
Epoch 7.17: Loss = 0.359833
Epoch 7.18: Loss = 0.329437
Epoch 7.19: Loss = 0.457108
Epoch 7.20: Loss = 0.263489
Epoch 7.21: Loss = 0.407639
Epoch 7.22: Loss = 0.433868
Epoch 7.23: Loss = 0.308868
Epoch 7.24: Loss = 0.443359
Epoch 7.25: Loss = 0.298401
Epoch 7.26: Loss = 0.376144
Epoch 7.27: Loss = 0.2724
Epoch 7.28: Loss = 0.344742
Epoch 7.29: Loss = 0.402847
Epoch 7.30: Loss = 0.386826
Epoch 7.31: Loss = 0.348557
Epoch 7.32: Loss = 0.401321
Epoch 7.33: Loss = 0.337662
Epoch 7.34: Loss = 0.389191
Epoch 7.35: Loss = 0.347168
Epoch 7.36: Loss = 0.327209
Epoch 7.37: Loss = 0.310501
Epoch 7.38: Loss = 0.362762
Epoch 7.39: Loss = 0.32782
Epoch 7.40: Loss = 0.403152
Epoch 7.41: Loss = 0.414093
Epoch 7.42: Loss = 0.361404
Epoch 7.43: Loss = 0.360184
Epoch 7.44: Loss = 0.247086
Epoch 7.45: Loss = 0.284683
Epoch 7.46: Loss = 0.421173
Epoch 7.47: Loss = 0.370377
Epoch 7.48: Loss = 0.3125
Epoch 7.49: Loss = 0.32164
Epoch 7.50: Loss = 0.372208
Epoch 7.51: Loss = 0.258911
Epoch 7.52: Loss = 0.40834
Epoch 7.53: Loss = 0.447113
Epoch 7.54: Loss = 0.402603
Epoch 7.55: Loss = 0.351746
Epoch 7.56: Loss = 0.411377
Epoch 7.57: Loss = 0.354828
Epoch 7.58: Loss = 0.292343
Epoch 7.59: Loss = 0.319366
Epoch 7.60: Loss = 0.429062
Epoch 7.61: Loss = 0.34668
Epoch 7.62: Loss = 0.331146
Epoch 7.63: Loss = 0.337265
Epoch 7.64: Loss = 0.435272
Epoch 7.65: Loss = 0.392181
Epoch 7.66: Loss = 0.303864
Epoch 7.67: Loss = 0.359177
Epoch 7.68: Loss = 0.390656
Epoch 7.69: Loss = 0.361664
Epoch 7.70: Loss = 0.348816
Epoch 7.71: Loss = 0.393814
Epoch 7.72: Loss = 0.302841
Epoch 7.73: Loss = 0.413712
Epoch 7.74: Loss = 0.395554
Epoch 7.75: Loss = 0.356308
Epoch 7.76: Loss = 0.400543
Epoch 7.77: Loss = 0.242752
Epoch 7.78: Loss = 0.361969
Epoch 7.79: Loss = 0.299805
Epoch 7.80: Loss = 0.284317
Epoch 7.81: Loss = 0.288452
Epoch 7.82: Loss = 0.304169
Epoch 7.83: Loss = 0.424393
Epoch 7.84: Loss = 0.288376
Epoch 7.85: Loss = 0.270508
Epoch 7.86: Loss = 0.305817
Epoch 7.87: Loss = 0.348373
Epoch 7.88: Loss = 0.352509
Epoch 7.89: Loss = 0.263596
Epoch 7.90: Loss = 0.39122
Epoch 7.91: Loss = 0.330856
Epoch 7.92: Loss = 0.370636
Epoch 7.93: Loss = 0.430069
Epoch 7.94: Loss = 0.329315
Epoch 7.95: Loss = 0.389282
Epoch 7.96: Loss = 0.298447
Epoch 7.97: Loss = 0.37471
Epoch 7.98: Loss = 0.294647
Epoch 7.99: Loss = 0.313416
Epoch 7.100: Loss = 0.421631
Epoch 7.101: Loss = 0.344315
Epoch 7.102: Loss = 0.30014
Epoch 7.103: Loss = 0.31398
Epoch 7.104: Loss = 0.365021
Epoch 7.105: Loss = 0.348511
Epoch 7.106: Loss = 0.341385
Epoch 7.107: Loss = 0.360718
Epoch 7.108: Loss = 0.280655
Epoch 7.109: Loss = 0.361359
Epoch 7.110: Loss = 0.355789
Epoch 7.111: Loss = 0.421219
Epoch 7.112: Loss = 0.351578
Epoch 7.113: Loss = 0.347061
Epoch 7.114: Loss = 0.44194
Epoch 7.115: Loss = 0.339966
Epoch 7.116: Loss = 0.363708
Epoch 7.117: Loss = 0.369827
Epoch 7.118: Loss = 0.380692
Epoch 7.119: Loss = 0.232391
Epoch 7.120: Loss = 0.493515
TRAIN LOSS = 0.35437
TRAIN ACC = 90.0269 % (54019/60000)
Loss = 0.315323
Loss = 0.419998
Loss = 0.478333
Loss = 0.478058
Loss = 0.507233
Loss = 0.356415
Loss = 0.320526
Loss = 0.554779
Loss = 0.48822
Loss = 0.430161
Loss = 0.147034
Loss = 0.266907
Loss = 0.290955
Loss = 0.298325
Loss = 0.177567
Loss = 0.251144
Loss = 0.173233
Loss = 0.0469666
Loss = 0.202881
Loss = 0.484055
TEST LOSS = 0.334405
TEST ACC = 540.189 % (9061/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.356171
Epoch 8.2: Loss = 0.382675
Epoch 8.3: Loss = 0.35051
Epoch 8.4: Loss = 0.325119
Epoch 8.5: Loss = 0.354156
Epoch 8.6: Loss = 0.386047
Epoch 8.7: Loss = 0.366928
Epoch 8.8: Loss = 0.296997
Epoch 8.9: Loss = 0.398987
Epoch 8.10: Loss = 0.373062
Epoch 8.11: Loss = 0.269791
Epoch 8.12: Loss = 0.391159
Epoch 8.13: Loss = 0.40416
Epoch 8.14: Loss = 0.301987
Epoch 8.15: Loss = 0.330811
Epoch 8.16: Loss = 0.362717
Epoch 8.17: Loss = 0.305038
Epoch 8.18: Loss = 0.285797
Epoch 8.19: Loss = 0.382599
Epoch 8.20: Loss = 0.383652
Epoch 8.21: Loss = 0.301178
Epoch 8.22: Loss = 0.282974
Epoch 8.23: Loss = 0.367889
Epoch 8.24: Loss = 0.364883
Epoch 8.25: Loss = 0.324768
Epoch 8.26: Loss = 0.373077
Epoch 8.27: Loss = 0.342041
Epoch 8.28: Loss = 0.304276
Epoch 8.29: Loss = 0.327469
Epoch 8.30: Loss = 0.405762
Epoch 8.31: Loss = 0.274292
Epoch 8.32: Loss = 0.340866
Epoch 8.33: Loss = 0.345291
Epoch 8.34: Loss = 0.41185
Epoch 8.35: Loss = 0.355988
Epoch 8.36: Loss = 0.508774
Epoch 8.37: Loss = 0.365738
Epoch 8.38: Loss = 0.368484
Epoch 8.39: Loss = 0.380096
Epoch 8.40: Loss = 0.366989
Epoch 8.41: Loss = 0.350647
Epoch 8.42: Loss = 0.47699
Epoch 8.43: Loss = 0.312241
Epoch 8.44: Loss = 0.239792
Epoch 8.45: Loss = 0.262924
Epoch 8.46: Loss = 0.365906
Epoch 8.47: Loss = 0.354187
Epoch 8.48: Loss = 0.325668
Epoch 8.49: Loss = 0.312485
Epoch 8.50: Loss = 0.34024
Epoch 8.51: Loss = 0.343307
Epoch 8.52: Loss = 0.354248
Epoch 8.53: Loss = 0.36322
Epoch 8.54: Loss = 0.347153
Epoch 8.55: Loss = 0.318146
Epoch 8.56: Loss = 0.3134
Epoch 8.57: Loss = 0.358444
Epoch 8.58: Loss = 0.238998
Epoch 8.59: Loss = 0.35141
Epoch 8.60: Loss = 0.348068
Epoch 8.61: Loss = 0.34082
Epoch 8.62: Loss = 0.342743
Epoch 8.63: Loss = 0.380356
Epoch 8.64: Loss = 0.379013
Epoch 8.65: Loss = 0.383972
Epoch 8.66: Loss = 0.347305
Epoch 8.67: Loss = 0.24855
Epoch 8.68: Loss = 0.369812
Epoch 8.69: Loss = 0.41571
Epoch 8.70: Loss = 0.297501
Epoch 8.71: Loss = 0.293762
Epoch 8.72: Loss = 0.394531
Epoch 8.73: Loss = 0.356705
Epoch 8.74: Loss = 0.370773
Epoch 8.75: Loss = 0.332443
Epoch 8.76: Loss = 0.378189
Epoch 8.77: Loss = 0.309235
Epoch 8.78: Loss = 0.348404
Epoch 8.79: Loss = 0.332367
Epoch 8.80: Loss = 0.373367
Epoch 8.81: Loss = 0.353424
Epoch 8.82: Loss = 0.258255
Epoch 8.83: Loss = 0.314667
Epoch 8.84: Loss = 0.330002
Epoch 8.85: Loss = 0.300522
Epoch 8.86: Loss = 0.321564
Epoch 8.87: Loss = 0.266724
Epoch 8.88: Loss = 0.352158
Epoch 8.89: Loss = 0.342514
Epoch 8.90: Loss = 0.330032
Epoch 8.91: Loss = 0.313629
Epoch 8.92: Loss = 0.46051
Epoch 8.93: Loss = 0.355988
Epoch 8.94: Loss = 0.31958
Epoch 8.95: Loss = 0.384155
Epoch 8.96: Loss = 0.34079
Epoch 8.97: Loss = 0.357147
Epoch 8.98: Loss = 0.35672
Epoch 8.99: Loss = 0.333496
Epoch 8.100: Loss = 0.355209
Epoch 8.101: Loss = 0.377487
Epoch 8.102: Loss = 0.301498
Epoch 8.103: Loss = 0.277939
Epoch 8.104: Loss = 0.420242
Epoch 8.105: Loss = 0.489624
Epoch 8.106: Loss = 0.434402
Epoch 8.107: Loss = 0.339203
Epoch 8.108: Loss = 0.34317
Epoch 8.109: Loss = 0.369812
Epoch 8.110: Loss = 0.346436
Epoch 8.111: Loss = 0.272079
Epoch 8.112: Loss = 0.340652
Epoch 8.113: Loss = 0.348236
Epoch 8.114: Loss = 0.314468
Epoch 8.115: Loss = 0.301956
Epoch 8.116: Loss = 0.376022
Epoch 8.117: Loss = 0.343811
Epoch 8.118: Loss = 0.350815
Epoch 8.119: Loss = 0.368927
Epoch 8.120: Loss = 0.314438
TRAIN LOSS = 0.346451
TRAIN ACC = 90.3732 % (54226/60000)
Loss = 0.30629
Loss = 0.411819
Loss = 0.471222
Loss = 0.475449
Loss = 0.504532
Loss = 0.342621
Loss = 0.310638
Loss = 0.556107
Loss = 0.484085
Loss = 0.426514
Loss = 0.14151
Loss = 0.26004
Loss = 0.287048
Loss = 0.292603
Loss = 0.164932
Loss = 0.252701
Loss = 0.164368
Loss = 0.0421295
Loss = 0.20282
Loss = 0.463577
TEST LOSS = 0.32805
TEST ACC = 542.259 % (9088/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.340179
Epoch 9.2: Loss = 0.340424
Epoch 9.3: Loss = 0.363708
Epoch 9.4: Loss = 0.295334
Epoch 9.5: Loss = 0.38446
Epoch 9.6: Loss = 0.445251
Epoch 9.7: Loss = 0.337982
Epoch 9.8: Loss = 0.275909
Epoch 9.9: Loss = 0.370468
Epoch 9.10: Loss = 0.346405
Epoch 9.11: Loss = 0.328064
Epoch 9.12: Loss = 0.247879
Epoch 9.13: Loss = 0.327866
Epoch 9.14: Loss = 0.356461
Epoch 9.15: Loss = 0.329636
Epoch 9.16: Loss = 0.350388
Epoch 9.17: Loss = 0.347015
Epoch 9.18: Loss = 0.295532
Epoch 9.19: Loss = 0.337204
Epoch 9.20: Loss = 0.388657
Epoch 9.21: Loss = 0.313263
Epoch 9.22: Loss = 0.315582
Epoch 9.23: Loss = 0.317108
Epoch 9.24: Loss = 0.454468
Epoch 9.25: Loss = 0.364212
Epoch 9.26: Loss = 0.29097
Epoch 9.27: Loss = 0.270233
Epoch 9.28: Loss = 0.344452
Epoch 9.29: Loss = 0.348358
Epoch 9.30: Loss = 0.364563
Epoch 9.31: Loss = 0.370239
Epoch 9.32: Loss = 0.402786
Epoch 9.33: Loss = 0.274429
Epoch 9.34: Loss = 0.310486
Epoch 9.35: Loss = 0.354538
Epoch 9.36: Loss = 0.35141
Epoch 9.37: Loss = 0.362045
Epoch 9.38: Loss = 0.287033
Epoch 9.39: Loss = 0.303955
Epoch 9.40: Loss = 0.378433
Epoch 9.41: Loss = 0.319672
Epoch 9.42: Loss = 0.273819
Epoch 9.43: Loss = 0.351318
Epoch 9.44: Loss = 0.305099
Epoch 9.45: Loss = 0.316635
Epoch 9.46: Loss = 0.3396
Epoch 9.47: Loss = 0.345337
Epoch 9.48: Loss = 0.351074
Epoch 9.49: Loss = 0.326416
Epoch 9.50: Loss = 0.430145
Epoch 9.51: Loss = 0.345352
Epoch 9.52: Loss = 0.334976
Epoch 9.53: Loss = 0.450424
Epoch 9.54: Loss = 0.345276
Epoch 9.55: Loss = 0.36145
Epoch 9.56: Loss = 0.348633
Epoch 9.57: Loss = 0.333832
Epoch 9.58: Loss = 0.401001
Epoch 9.59: Loss = 0.336136
Epoch 9.60: Loss = 0.378601
Epoch 9.61: Loss = 0.389145
Epoch 9.62: Loss = 0.353592
Epoch 9.63: Loss = 0.320801
Epoch 9.64: Loss = 0.390762
Epoch 9.65: Loss = 0.371414
Epoch 9.66: Loss = 0.328796
Epoch 9.67: Loss = 0.31517
Epoch 9.68: Loss = 0.273178
Epoch 9.69: Loss = 0.28096
Epoch 9.70: Loss = 0.366302
Epoch 9.71: Loss = 0.327469
Epoch 9.72: Loss = 0.286652
Epoch 9.73: Loss = 0.32019
Epoch 9.74: Loss = 0.408218
Epoch 9.75: Loss = 0.30452
Epoch 9.76: Loss = 0.363937
Epoch 9.77: Loss = 0.412598
Epoch 9.78: Loss = 0.308655
Epoch 9.79: Loss = 0.345474
Epoch 9.80: Loss = 0.347809
Epoch 9.81: Loss = 0.301773
Epoch 9.82: Loss = 0.392242
Epoch 9.83: Loss = 0.420914
Epoch 9.84: Loss = 0.329239
Epoch 9.85: Loss = 0.33812
Epoch 9.86: Loss = 0.314636
Epoch 9.87: Loss = 0.351486
Epoch 9.88: Loss = 0.315811
Epoch 9.89: Loss = 0.361099
Epoch 9.90: Loss = 0.354263
Epoch 9.91: Loss = 0.329956
Epoch 9.92: Loss = 0.271515
Epoch 9.93: Loss = 0.354904
Epoch 9.94: Loss = 0.30658
Epoch 9.95: Loss = 0.250244
Epoch 9.96: Loss = 0.341614
Epoch 9.97: Loss = 0.29921
Epoch 9.98: Loss = 0.493835
Epoch 9.99: Loss = 0.378479
Epoch 9.100: Loss = 0.345276
Epoch 9.101: Loss = 0.336639
Epoch 9.102: Loss = 0.353729
Epoch 9.103: Loss = 0.403275
Epoch 9.104: Loss = 0.305267
Epoch 9.105: Loss = 0.358429
Epoch 9.106: Loss = 0.306091
Epoch 9.107: Loss = 0.373749
Epoch 9.108: Loss = 0.294388
Epoch 9.109: Loss = 0.30838
Epoch 9.110: Loss = 0.320435
Epoch 9.111: Loss = 0.483871
Epoch 9.112: Loss = 0.390564
Epoch 9.113: Loss = 0.372925
Epoch 9.114: Loss = 0.282959
Epoch 9.115: Loss = 0.283234
Epoch 9.116: Loss = 0.333084
Epoch 9.117: Loss = 0.274475
Epoch 9.118: Loss = 0.24794
Epoch 9.119: Loss = 0.269272
Epoch 9.120: Loss = 0.316406
TRAIN LOSS = 0.34053
TRAIN ACC = 90.6158 % (54372/60000)
Loss = 0.307068
Loss = 0.406754
Loss = 0.461502
Loss = 0.472443
Loss = 0.495193
Loss = 0.33786
Loss = 0.301849
Loss = 0.555267
Loss = 0.477768
Loss = 0.41748
Loss = 0.139908
Loss = 0.252197
Loss = 0.289978
Loss = 0.291412
Loss = 0.161865
Loss = 0.250916
Loss = 0.162216
Loss = 0.0400543
Loss = 0.201233
Loss = 0.458191
TEST LOSS = 0.324058
TEST ACC = 543.719 % (9124/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.246964
Epoch 10.2: Loss = 0.267334
Epoch 10.3: Loss = 0.279587
Epoch 10.4: Loss = 0.352829
Epoch 10.5: Loss = 0.346283
Epoch 10.6: Loss = 0.314178
Epoch 10.7: Loss = 0.23407
Epoch 10.8: Loss = 0.397507
Epoch 10.9: Loss = 0.332947
Epoch 10.10: Loss = 0.3396
Epoch 10.11: Loss = 0.275787
Epoch 10.12: Loss = 0.217621
Epoch 10.13: Loss = 0.30336
Epoch 10.14: Loss = 0.425537
Epoch 10.15: Loss = 0.361145
Epoch 10.16: Loss = 0.295319
Epoch 10.17: Loss = 0.26123
Epoch 10.18: Loss = 0.343842
Epoch 10.19: Loss = 0.343811
Epoch 10.20: Loss = 0.282303
Epoch 10.21: Loss = 0.29451
Epoch 10.22: Loss = 0.353027
Epoch 10.23: Loss = 0.38356
Epoch 10.24: Loss = 0.326294
Epoch 10.25: Loss = 0.328903
Epoch 10.26: Loss = 0.419128
Epoch 10.27: Loss = 0.31813
Epoch 10.28: Loss = 0.3125
Epoch 10.29: Loss = 0.39003
Epoch 10.30: Loss = 0.314896
Epoch 10.31: Loss = 0.348328
Epoch 10.32: Loss = 0.455505
Epoch 10.33: Loss = 0.308289
Epoch 10.34: Loss = 0.334305
Epoch 10.35: Loss = 0.318466
Epoch 10.36: Loss = 0.398911
Epoch 10.37: Loss = 0.417816
Epoch 10.38: Loss = 0.397339
Epoch 10.39: Loss = 0.360458
Epoch 10.40: Loss = 0.375107
Epoch 10.41: Loss = 0.259338
Epoch 10.42: Loss = 0.293106
Epoch 10.43: Loss = 0.44989
Epoch 10.44: Loss = 0.393311
Epoch 10.45: Loss = 0.341431
Epoch 10.46: Loss = 0.355225
Epoch 10.47: Loss = 0.279587
Epoch 10.48: Loss = 0.394943
Epoch 10.49: Loss = 0.297836
Epoch 10.50: Loss = 0.291107
Epoch 10.51: Loss = 0.370605
Epoch 10.52: Loss = 0.428909
Epoch 10.53: Loss = 0.341049
Epoch 10.54: Loss = 0.336273
Epoch 10.55: Loss = 0.252899
Epoch 10.56: Loss = 0.325348
Epoch 10.57: Loss = 0.374283
Epoch 10.58: Loss = 0.331024
Epoch 10.59: Loss = 0.37706
Epoch 10.60: Loss = 0.355591
Epoch 10.61: Loss = 0.37674
Epoch 10.62: Loss = 0.362671
Epoch 10.63: Loss = 0.435577
Epoch 10.64: Loss = 0.344009
Epoch 10.65: Loss = 0.27774
Epoch 10.66: Loss = 0.259171
Epoch 10.67: Loss = 0.323792
Epoch 10.68: Loss = 0.349274
Epoch 10.69: Loss = 0.360535
Epoch 10.70: Loss = 0.30925
Epoch 10.71: Loss = 0.290848
Epoch 10.72: Loss = 0.243668
Epoch 10.73: Loss = 0.241058
Epoch 10.74: Loss = 0.297012
Epoch 10.75: Loss = 0.285843
Epoch 10.76: Loss = 0.341599
Epoch 10.77: Loss = 0.300415
Epoch 10.78: Loss = 0.312408
Epoch 10.79: Loss = 0.392456
Epoch 10.80: Loss = 0.341568
Epoch 10.81: Loss = 0.344894
Epoch 10.82: Loss = 0.323257
Epoch 10.83: Loss = 0.373093
Epoch 10.84: Loss = 0.416702
Epoch 10.85: Loss = 0.428192
Epoch 10.86: Loss = 0.405121
Epoch 10.87: Loss = 0.450989
Epoch 10.88: Loss = 0.351166
Epoch 10.89: Loss = 0.310608
Epoch 10.90: Loss = 0.41774
Epoch 10.91: Loss = 0.397858
Epoch 10.92: Loss = 0.314529
Epoch 10.93: Loss = 0.358353
Epoch 10.94: Loss = 0.368362
Epoch 10.95: Loss = 0.28595
Epoch 10.96: Loss = 0.303772
Epoch 10.97: Loss = 0.286713
Epoch 10.98: Loss = 0.347122
Epoch 10.99: Loss = 0.350922
Epoch 10.100: Loss = 0.324249
Epoch 10.101: Loss = 0.352692
Epoch 10.102: Loss = 0.238861
Epoch 10.103: Loss = 0.294037
Epoch 10.104: Loss = 0.294083
Epoch 10.105: Loss = 0.333298
Epoch 10.106: Loss = 0.389954
Epoch 10.107: Loss = 0.275467
Epoch 10.108: Loss = 0.296524
Epoch 10.109: Loss = 0.258896
Epoch 10.110: Loss = 0.248657
Epoch 10.111: Loss = 0.359299
Epoch 10.112: Loss = 0.337906
Epoch 10.113: Loss = 0.32402
Epoch 10.114: Loss = 0.336792
Epoch 10.115: Loss = 0.359543
Epoch 10.116: Loss = 0.336121
Epoch 10.117: Loss = 0.255051
Epoch 10.118: Loss = 0.354965
Epoch 10.119: Loss = 0.391083
Epoch 10.120: Loss = 0.286911
TRAIN LOSS = 0.334625
TRAIN ACC = 90.8478 % (54511/60000)
Loss = 0.300171
Loss = 0.400299
Loss = 0.452728
Loss = 0.467865
Loss = 0.496689
Loss = 0.328583
Loss = 0.293701
Loss = 0.54776
Loss = 0.471863
Loss = 0.414185
Loss = 0.137253
Loss = 0.256943
Loss = 0.286163
Loss = 0.281387
Loss = 0.157135
Loss = 0.24353
Loss = 0.151459
Loss = 0.0381165
Loss = 0.200302
Loss = 0.444504
TEST LOSS = 0.318532
TEST ACC = 545.11 % (9144/10000)

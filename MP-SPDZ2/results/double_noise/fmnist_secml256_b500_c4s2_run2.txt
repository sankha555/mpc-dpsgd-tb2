Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.42087
Epoch 1.2: Loss = 2.31604
Epoch 1.3: Loss = 2.22035
Epoch 1.4: Loss = 2.20345
Epoch 1.5: Loss = 2.1172
Epoch 1.6: Loss = 2.05998
Epoch 1.7: Loss = 2.0074
Epoch 1.8: Loss = 1.97723
Epoch 1.9: Loss = 1.90822
Epoch 1.10: Loss = 1.84218
Epoch 1.11: Loss = 1.80934
Epoch 1.12: Loss = 1.76381
Epoch 1.13: Loss = 1.70103
Epoch 1.14: Loss = 1.67821
Epoch 1.15: Loss = 1.62804
Epoch 1.16: Loss = 1.59299
Epoch 1.17: Loss = 1.55745
Epoch 1.18: Loss = 1.51616
Epoch 1.19: Loss = 1.46727
Epoch 1.20: Loss = 1.43497
Epoch 1.21: Loss = 1.43042
Epoch 1.22: Loss = 1.37645
Epoch 1.23: Loss = 1.3541
Epoch 1.24: Loss = 1.34909
Epoch 1.25: Loss = 1.27277
Epoch 1.26: Loss = 1.31805
Epoch 1.27: Loss = 1.26138
Epoch 1.28: Loss = 1.22548
Epoch 1.29: Loss = 1.25671
Epoch 1.30: Loss = 1.14598
Epoch 1.31: Loss = 1.21521
Epoch 1.32: Loss = 1.11081
Epoch 1.33: Loss = 1.14085
Epoch 1.34: Loss = 1.11815
Epoch 1.35: Loss = 1.14784
Epoch 1.36: Loss = 1.08916
Epoch 1.37: Loss = 1.10239
Epoch 1.38: Loss = 1.11041
Epoch 1.39: Loss = 1.03098
Epoch 1.40: Loss = 1.04361
Epoch 1.41: Loss = 1.05876
Epoch 1.42: Loss = 0.987717
Epoch 1.43: Loss = 1.05379
Epoch 1.44: Loss = 0.975037
Epoch 1.45: Loss = 0.974808
Epoch 1.46: Loss = 1.00218
Epoch 1.47: Loss = 0.980499
Epoch 1.48: Loss = 0.905548
Epoch 1.49: Loss = 1.0507
Epoch 1.50: Loss = 1.03903
Epoch 1.51: Loss = 0.950424
Epoch 1.52: Loss = 0.975037
Epoch 1.53: Loss = 0.903061
Epoch 1.54: Loss = 0.919128
Epoch 1.55: Loss = 0.932785
Epoch 1.56: Loss = 0.917664
Epoch 1.57: Loss = 0.894684
Epoch 1.58: Loss = 0.835312
Epoch 1.59: Loss = 0.863297
Epoch 1.60: Loss = 0.945847
Epoch 1.61: Loss = 0.90123
Epoch 1.62: Loss = 0.867966
Epoch 1.63: Loss = 0.866089
Epoch 1.64: Loss = 0.879257
Epoch 1.65: Loss = 0.841507
Epoch 1.66: Loss = 0.885788
Epoch 1.67: Loss = 0.78389
Epoch 1.68: Loss = 0.855637
Epoch 1.69: Loss = 0.796707
Epoch 1.70: Loss = 0.792831
Epoch 1.71: Loss = 0.837067
Epoch 1.72: Loss = 0.84465
Epoch 1.73: Loss = 0.849152
Epoch 1.74: Loss = 0.856201
Epoch 1.75: Loss = 0.846008
Epoch 1.76: Loss = 0.765289
Epoch 1.77: Loss = 0.824753
Epoch 1.78: Loss = 0.82753
Epoch 1.79: Loss = 0.820557
Epoch 1.80: Loss = 0.793045
Epoch 1.81: Loss = 0.846634
Epoch 1.82: Loss = 0.736389
Epoch 1.83: Loss = 0.836502
Epoch 1.84: Loss = 0.799759
Epoch 1.85: Loss = 0.8228
Epoch 1.86: Loss = 0.762131
Epoch 1.87: Loss = 0.814362
Epoch 1.88: Loss = 0.823563
Epoch 1.89: Loss = 0.776337
Epoch 1.90: Loss = 0.798141
Epoch 1.91: Loss = 0.727875
Epoch 1.92: Loss = 0.75885
Epoch 1.93: Loss = 0.749298
Epoch 1.94: Loss = 0.755417
Epoch 1.95: Loss = 0.779068
Epoch 1.96: Loss = 0.754684
Epoch 1.97: Loss = 0.817215
Epoch 1.98: Loss = 0.818924
Epoch 1.99: Loss = 0.853958
Epoch 1.100: Loss = 0.743378
Epoch 1.101: Loss = 0.78598
Epoch 1.102: Loss = 0.736816
Epoch 1.103: Loss = 0.825058
Epoch 1.104: Loss = 0.806396
Epoch 1.105: Loss = 0.724365
Epoch 1.106: Loss = 0.762024
Epoch 1.107: Loss = 0.831314
Epoch 1.108: Loss = 0.711197
Epoch 1.109: Loss = 0.786392
Epoch 1.110: Loss = 0.708069
Epoch 1.111: Loss = 0.74382
Epoch 1.112: Loss = 0.713821
Epoch 1.113: Loss = 0.681015
Epoch 1.114: Loss = 0.790695
Epoch 1.115: Loss = 0.737473
Epoch 1.116: Loss = 0.769379
Epoch 1.117: Loss = 0.744049
Epoch 1.118: Loss = 0.742447
Epoch 1.119: Loss = 0.778641
Epoch 1.120: Loss = 0.706543
TRAIN LOSS = 1.06737
TRAIN ACC = 64.5798 % (38750/60000)
Loss = 0.658661
Loss = 0.772461
Loss = 0.798355
Loss = 0.685959
Loss = 0.68013
Loss = 0.845428
Loss = 0.842484
Loss = 0.800491
Loss = 0.727249
Loss = 0.683289
Loss = 0.784103
Loss = 0.743744
Loss = 0.756226
Loss = 0.762573
Loss = 0.726776
Loss = 0.795517
Loss = 0.689331
Loss = 0.759781
Loss = 0.771622
Loss = 0.757507
TEST LOSS = 0.752084
TEST ACC = 387.5 % (7308/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.703293
Epoch 2.2: Loss = 0.762909
Epoch 2.3: Loss = 0.775055
Epoch 2.4: Loss = 0.770676
Epoch 2.5: Loss = 0.689514
Epoch 2.6: Loss = 0.700378
Epoch 2.7: Loss = 0.767822
Epoch 2.8: Loss = 0.746979
Epoch 2.9: Loss = 0.743332
Epoch 2.10: Loss = 0.745346
Epoch 2.11: Loss = 0.848373
Epoch 2.12: Loss = 0.71405
Epoch 2.13: Loss = 0.655075
Epoch 2.14: Loss = 0.683533
Epoch 2.15: Loss = 0.761398
Epoch 2.16: Loss = 0.841644
Epoch 2.17: Loss = 0.737274
Epoch 2.18: Loss = 0.649139
Epoch 2.19: Loss = 0.706741
Epoch 2.20: Loss = 0.755722
Epoch 2.21: Loss = 0.732269
Epoch 2.22: Loss = 0.660675
Epoch 2.23: Loss = 0.656799
Epoch 2.24: Loss = 0.705246
Epoch 2.25: Loss = 0.71582
Epoch 2.26: Loss = 0.715637
Epoch 2.27: Loss = 0.657593
Epoch 2.28: Loss = 0.734665
Epoch 2.29: Loss = 0.707687
Epoch 2.30: Loss = 0.76004
Epoch 2.31: Loss = 0.658417
Epoch 2.32: Loss = 0.667587
Epoch 2.33: Loss = 0.679306
Epoch 2.34: Loss = 0.720718
Epoch 2.35: Loss = 0.730072
Epoch 2.36: Loss = 0.625778
Epoch 2.37: Loss = 0.684021
Epoch 2.38: Loss = 0.638855
Epoch 2.39: Loss = 0.678131
Epoch 2.40: Loss = 0.698837
Epoch 2.41: Loss = 0.68364
Epoch 2.42: Loss = 0.781631
Epoch 2.43: Loss = 0.79039
Epoch 2.44: Loss = 0.693039
Epoch 2.45: Loss = 0.74823
Epoch 2.46: Loss = 0.665894
Epoch 2.47: Loss = 0.677353
Epoch 2.48: Loss = 0.736984
Epoch 2.49: Loss = 0.665619
Epoch 2.50: Loss = 0.667389
Epoch 2.51: Loss = 0.676956
Epoch 2.52: Loss = 0.679733
Epoch 2.53: Loss = 0.715195
Epoch 2.54: Loss = 0.661072
Epoch 2.55: Loss = 0.657379
Epoch 2.56: Loss = 0.625488
Epoch 2.57: Loss = 0.635086
Epoch 2.58: Loss = 0.745697
Epoch 2.59: Loss = 0.750778
Epoch 2.60: Loss = 0.712204
Epoch 2.61: Loss = 0.653412
Epoch 2.62: Loss = 0.59404
Epoch 2.63: Loss = 0.757416
Epoch 2.64: Loss = 0.694992
Epoch 2.65: Loss = 0.661194
Epoch 2.66: Loss = 0.664307
Epoch 2.67: Loss = 0.736954
Epoch 2.68: Loss = 0.646774
Epoch 2.69: Loss = 0.632111
Epoch 2.70: Loss = 0.759705
Epoch 2.71: Loss = 0.697052
Epoch 2.72: Loss = 0.687576
Epoch 2.73: Loss = 0.633316
Epoch 2.74: Loss = 0.69281
Epoch 2.75: Loss = 0.64296
Epoch 2.76: Loss = 0.64827
Epoch 2.77: Loss = 0.679352
Epoch 2.78: Loss = 0.677155
Epoch 2.79: Loss = 0.798813
Epoch 2.80: Loss = 0.642731
Epoch 2.81: Loss = 0.663345
Epoch 2.82: Loss = 0.645004
Epoch 2.83: Loss = 0.793427
Epoch 2.84: Loss = 0.705612
Epoch 2.85: Loss = 0.617554
Epoch 2.86: Loss = 0.703735
Epoch 2.87: Loss = 0.83844
Epoch 2.88: Loss = 0.684509
Epoch 2.89: Loss = 0.67775
Epoch 2.90: Loss = 0.683167
Epoch 2.91: Loss = 0.680023
Epoch 2.92: Loss = 0.69664
Epoch 2.93: Loss = 0.613922
Epoch 2.94: Loss = 0.624268
Epoch 2.95: Loss = 0.649612
Epoch 2.96: Loss = 0.659668
Epoch 2.97: Loss = 0.705643
Epoch 2.98: Loss = 0.686081
Epoch 2.99: Loss = 0.640884
Epoch 2.100: Loss = 0.664658
Epoch 2.101: Loss = 0.700378
Epoch 2.102: Loss = 0.657379
Epoch 2.103: Loss = 0.676849
Epoch 2.104: Loss = 0.657333
Epoch 2.105: Loss = 0.621719
Epoch 2.106: Loss = 0.694748
Epoch 2.107: Loss = 0.704681
Epoch 2.108: Loss = 0.722107
Epoch 2.109: Loss = 0.584122
Epoch 2.110: Loss = 0.622009
Epoch 2.111: Loss = 0.784622
Epoch 2.112: Loss = 0.560608
Epoch 2.113: Loss = 0.609787
Epoch 2.114: Loss = 0.716064
Epoch 2.115: Loss = 0.775269
Epoch 2.116: Loss = 0.686691
Epoch 2.117: Loss = 0.730362
Epoch 2.118: Loss = 0.662369
Epoch 2.119: Loss = 0.611435
Epoch 2.120: Loss = 0.609161
TRAIN LOSS = 0.693176
TRAIN ACC = 76.3733 % (45826/60000)
Loss = 0.593765
Loss = 0.717407
Loss = 0.696457
Loss = 0.598984
Loss = 0.609222
Loss = 0.770523
Loss = 0.796844
Loss = 0.720428
Loss = 0.65976
Loss = 0.619965
Loss = 0.735886
Loss = 0.716431
Loss = 0.693176
Loss = 0.675995
Loss = 0.667389
Loss = 0.718567
Loss = 0.626923
Loss = 0.693039
Loss = 0.711441
Loss = 0.686493
TEST LOSS = 0.685435
TEST ACC = 458.26 % (7688/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.650925
Epoch 3.2: Loss = 0.682083
Epoch 3.3: Loss = 0.578094
Epoch 3.4: Loss = 0.629333
Epoch 3.5: Loss = 0.720688
Epoch 3.6: Loss = 0.678329
Epoch 3.7: Loss = 0.639084
Epoch 3.8: Loss = 0.61731
Epoch 3.9: Loss = 0.636078
Epoch 3.10: Loss = 0.696121
Epoch 3.11: Loss = 0.582596
Epoch 3.12: Loss = 0.676285
Epoch 3.13: Loss = 0.759003
Epoch 3.14: Loss = 0.628235
Epoch 3.15: Loss = 0.679169
Epoch 3.16: Loss = 0.604462
Epoch 3.17: Loss = 0.654755
Epoch 3.18: Loss = 0.592728
Epoch 3.19: Loss = 0.678864
Epoch 3.20: Loss = 0.703552
Epoch 3.21: Loss = 0.740021
Epoch 3.22: Loss = 0.709198
Epoch 3.23: Loss = 0.660141
Epoch 3.24: Loss = 0.683594
Epoch 3.25: Loss = 0.541473
Epoch 3.26: Loss = 0.569702
Epoch 3.27: Loss = 0.660049
Epoch 3.28: Loss = 0.587402
Epoch 3.29: Loss = 0.681534
Epoch 3.30: Loss = 0.709595
Epoch 3.31: Loss = 0.571838
Epoch 3.32: Loss = 0.697388
Epoch 3.33: Loss = 0.734451
Epoch 3.34: Loss = 0.566406
Epoch 3.35: Loss = 0.622269
Epoch 3.36: Loss = 0.661194
Epoch 3.37: Loss = 0.634949
Epoch 3.38: Loss = 0.648132
Epoch 3.39: Loss = 0.619736
Epoch 3.40: Loss = 0.59935
Epoch 3.41: Loss = 0.674164
Epoch 3.42: Loss = 0.65036
Epoch 3.43: Loss = 0.645355
Epoch 3.44: Loss = 0.796265
Epoch 3.45: Loss = 0.739594
Epoch 3.46: Loss = 0.740341
Epoch 3.47: Loss = 0.6698
Epoch 3.48: Loss = 0.643326
Epoch 3.49: Loss = 0.612396
Epoch 3.50: Loss = 0.534332
Epoch 3.51: Loss = 0.771484
Epoch 3.52: Loss = 0.748413
Epoch 3.53: Loss = 0.662735
Epoch 3.54: Loss = 0.693375
Epoch 3.55: Loss = 0.73735
Epoch 3.56: Loss = 0.673691
Epoch 3.57: Loss = 0.692566
Epoch 3.58: Loss = 0.799164
Epoch 3.59: Loss = 0.727859
Epoch 3.60: Loss = 0.700424
Epoch 3.61: Loss = 0.761734
Epoch 3.62: Loss = 0.627914
Epoch 3.63: Loss = 0.584808
Epoch 3.64: Loss = 0.733459
Epoch 3.65: Loss = 0.630936
Epoch 3.66: Loss = 0.659805
Epoch 3.67: Loss = 0.662048
Epoch 3.68: Loss = 0.610474
Epoch 3.69: Loss = 0.699036
Epoch 3.70: Loss = 0.672806
Epoch 3.71: Loss = 0.642303
Epoch 3.72: Loss = 0.746658
Epoch 3.73: Loss = 0.685272
Epoch 3.74: Loss = 0.807968
Epoch 3.75: Loss = 0.721191
Epoch 3.76: Loss = 0.602753
Epoch 3.77: Loss = 0.643433
Epoch 3.78: Loss = 0.608017
Epoch 3.79: Loss = 0.686768
Epoch 3.80: Loss = 0.654297
Epoch 3.81: Loss = 0.707443
Epoch 3.82: Loss = 0.617447
Epoch 3.83: Loss = 0.732956
Epoch 3.84: Loss = 0.614029
Epoch 3.85: Loss = 0.737427
Epoch 3.86: Loss = 0.67691
Epoch 3.87: Loss = 0.656281
Epoch 3.88: Loss = 0.597824
Epoch 3.89: Loss = 0.707932
Epoch 3.90: Loss = 0.756317
Epoch 3.91: Loss = 0.629395
Epoch 3.92: Loss = 0.711334
Epoch 3.93: Loss = 0.5569
Epoch 3.94: Loss = 0.650085
Epoch 3.95: Loss = 0.694168
Epoch 3.96: Loss = 0.66951
Epoch 3.97: Loss = 0.608643
Epoch 3.98: Loss = 0.663162
Epoch 3.99: Loss = 0.682236
Epoch 3.100: Loss = 0.598648
Epoch 3.101: Loss = 0.63887
Epoch 3.102: Loss = 0.682343
Epoch 3.103: Loss = 0.73497
Epoch 3.104: Loss = 0.598007
Epoch 3.105: Loss = 0.658432
Epoch 3.106: Loss = 0.736633
Epoch 3.107: Loss = 0.712143
Epoch 3.108: Loss = 0.785263
Epoch 3.109: Loss = 0.589432
Epoch 3.110: Loss = 0.647873
Epoch 3.111: Loss = 0.667557
Epoch 3.112: Loss = 0.597916
Epoch 3.113: Loss = 0.800293
Epoch 3.114: Loss = 0.633133
Epoch 3.115: Loss = 0.640915
Epoch 3.116: Loss = 0.537704
Epoch 3.117: Loss = 0.751572
Epoch 3.118: Loss = 0.661377
Epoch 3.119: Loss = 0.607071
Epoch 3.120: Loss = 0.667877
TRAIN LOSS = 0.666351
TRAIN ACC = 78.4378 % (47065/60000)
Loss = 0.585464
Loss = 0.69136
Loss = 0.67363
Loss = 0.570572
Loss = 0.615753
Loss = 0.7603
Loss = 0.803268
Loss = 0.738998
Loss = 0.668655
Loss = 0.607605
Loss = 0.757385
Loss = 0.735641
Loss = 0.680206
Loss = 0.679932
Loss = 0.660828
Loss = 0.701508
Loss = 0.63588
Loss = 0.698029
Loss = 0.722427
Loss = 0.68869
TEST LOSS = 0.683806
TEST ACC = 470.65 % (7852/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.634628
Epoch 4.2: Loss = 0.590698
Epoch 4.3: Loss = 0.734375
Epoch 4.4: Loss = 0.523132
Epoch 4.5: Loss = 0.591644
Epoch 4.6: Loss = 0.714996
Epoch 4.7: Loss = 0.716949
Epoch 4.8: Loss = 0.550278
Epoch 4.9: Loss = 0.702148
Epoch 4.10: Loss = 0.618088
Epoch 4.11: Loss = 0.649734
Epoch 4.12: Loss = 0.766235
Epoch 4.13: Loss = 0.707214
Epoch 4.14: Loss = 0.629684
Epoch 4.15: Loss = 0.69841
Epoch 4.16: Loss = 0.711349
Epoch 4.17: Loss = 0.639099
Epoch 4.18: Loss = 0.70105
Epoch 4.19: Loss = 0.720474
Epoch 4.20: Loss = 0.693054
Epoch 4.21: Loss = 0.662567
Epoch 4.22: Loss = 0.549454
Epoch 4.23: Loss = 0.606445
Epoch 4.24: Loss = 0.646515
Epoch 4.25: Loss = 0.602463
Epoch 4.26: Loss = 0.685577
Epoch 4.27: Loss = 0.652191
Epoch 4.28: Loss = 0.700592
Epoch 4.29: Loss = 0.719116
Epoch 4.30: Loss = 0.610626
Epoch 4.31: Loss = 0.619858
Epoch 4.32: Loss = 0.61171
Epoch 4.33: Loss = 0.667328
Epoch 4.34: Loss = 0.691956
Epoch 4.35: Loss = 0.614044
Epoch 4.36: Loss = 0.649231
Epoch 4.37: Loss = 0.626419
Epoch 4.38: Loss = 0.635376
Epoch 4.39: Loss = 0.668777
Epoch 4.40: Loss = 0.538986
Epoch 4.41: Loss = 0.586746
Epoch 4.42: Loss = 0.668137
Epoch 4.43: Loss = 0.686554
Epoch 4.44: Loss = 0.578125
Epoch 4.45: Loss = 0.757614
Epoch 4.46: Loss = 0.662216
Epoch 4.47: Loss = 0.70462
Epoch 4.48: Loss = 0.591049
Epoch 4.49: Loss = 0.587296
Epoch 4.50: Loss = 0.772583
Epoch 4.51: Loss = 0.748291
Epoch 4.52: Loss = 0.5979
Epoch 4.53: Loss = 0.624222
Epoch 4.54: Loss = 0.690857
Epoch 4.55: Loss = 0.592957
Epoch 4.56: Loss = 0.610382
Epoch 4.57: Loss = 0.728104
Epoch 4.58: Loss = 0.757523
Epoch 4.59: Loss = 0.615906
Epoch 4.60: Loss = 0.641708
Epoch 4.61: Loss = 0.660873
Epoch 4.62: Loss = 0.680756
Epoch 4.63: Loss = 0.672333
Epoch 4.64: Loss = 0.565491
Epoch 4.65: Loss = 0.643188
Epoch 4.66: Loss = 0.770248
Epoch 4.67: Loss = 0.606216
Epoch 4.68: Loss = 0.64827
Epoch 4.69: Loss = 0.656357
Epoch 4.70: Loss = 0.733017
Epoch 4.71: Loss = 0.619156
Epoch 4.72: Loss = 0.714523
Epoch 4.73: Loss = 0.737518
Epoch 4.74: Loss = 0.661377
Epoch 4.75: Loss = 0.647995
Epoch 4.76: Loss = 0.692413
Epoch 4.77: Loss = 0.657928
Epoch 4.78: Loss = 0.733017
Epoch 4.79: Loss = 0.645721
Epoch 4.80: Loss = 0.616043
Epoch 4.81: Loss = 0.632385
Epoch 4.82: Loss = 0.596985
Epoch 4.83: Loss = 0.608398
Epoch 4.84: Loss = 0.774429
Epoch 4.85: Loss = 0.671143
Epoch 4.86: Loss = 0.719681
Epoch 4.87: Loss = 0.640991
Epoch 4.88: Loss = 0.645569
Epoch 4.89: Loss = 0.662598
Epoch 4.90: Loss = 0.601944
Epoch 4.91: Loss = 0.567078
Epoch 4.92: Loss = 0.692032
Epoch 4.93: Loss = 0.591782
Epoch 4.94: Loss = 0.705093
Epoch 4.95: Loss = 0.747711
Epoch 4.96: Loss = 0.713242
Epoch 4.97: Loss = 0.511505
Epoch 4.98: Loss = 0.71843
Epoch 4.99: Loss = 0.698563
Epoch 4.100: Loss = 0.570953
Epoch 4.101: Loss = 0.664993
Epoch 4.102: Loss = 0.604156
Epoch 4.103: Loss = 0.661163
Epoch 4.104: Loss = 0.765427
Epoch 4.105: Loss = 0.539993
Epoch 4.106: Loss = 0.777298
Epoch 4.107: Loss = 0.68663
Epoch 4.108: Loss = 0.723953
Epoch 4.109: Loss = 0.672989
Epoch 4.110: Loss = 0.69809
Epoch 4.111: Loss = 0.627487
Epoch 4.112: Loss = 0.64003
Epoch 4.113: Loss = 0.539261
Epoch 4.114: Loss = 0.681488
Epoch 4.115: Loss = 0.65155
Epoch 4.116: Loss = 0.686142
Epoch 4.117: Loss = 0.692657
Epoch 4.118: Loss = 0.664856
Epoch 4.119: Loss = 0.615692
Epoch 4.120: Loss = 0.656448
TRAIN LOSS = 0.657837
TRAIN ACC = 79.5059 % (47706/60000)
Loss = 0.57869
Loss = 0.717392
Loss = 0.671738
Loss = 0.567703
Loss = 0.647415
Loss = 0.776886
Loss = 0.812485
Loss = 0.732559
Loss = 0.685181
Loss = 0.63649
Loss = 0.787735
Loss = 0.752655
Loss = 0.709839
Loss = 0.658798
Loss = 0.69136
Loss = 0.709488
Loss = 0.635284
Loss = 0.723709
Loss = 0.72464
Loss = 0.706314
TEST LOSS = 0.696318
TEST ACC = 477.06 % (7897/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.586411
Epoch 5.2: Loss = 0.646637
Epoch 5.3: Loss = 0.686432
Epoch 5.4: Loss = 0.545776
Epoch 5.5: Loss = 0.635834
Epoch 5.6: Loss = 0.698914
Epoch 5.7: Loss = 0.595932
Epoch 5.8: Loss = 0.655334
Epoch 5.9: Loss = 0.685928
Epoch 5.10: Loss = 0.563248
Epoch 5.11: Loss = 0.630005
Epoch 5.12: Loss = 0.673706
Epoch 5.13: Loss = 0.710068
Epoch 5.14: Loss = 0.568878
Epoch 5.15: Loss = 0.581604
Epoch 5.16: Loss = 0.760559
Epoch 5.17: Loss = 0.706802
Epoch 5.18: Loss = 0.638657
Epoch 5.19: Loss = 0.731857
Epoch 5.20: Loss = 0.703293
Epoch 5.21: Loss = 0.632492
Epoch 5.22: Loss = 0.693756
Epoch 5.23: Loss = 0.62973
Epoch 5.24: Loss = 0.753265
Epoch 5.25: Loss = 0.624084
Epoch 5.26: Loss = 0.57692
Epoch 5.27: Loss = 0.73259
Epoch 5.28: Loss = 0.655762
Epoch 5.29: Loss = 0.725952
Epoch 5.30: Loss = 0.722076
Epoch 5.31: Loss = 0.667984
Epoch 5.32: Loss = 0.615265
Epoch 5.33: Loss = 0.643158
Epoch 5.34: Loss = 0.652618
Epoch 5.35: Loss = 0.704147
Epoch 5.36: Loss = 0.776657
Epoch 5.37: Loss = 0.612335
Epoch 5.38: Loss = 0.551437
Epoch 5.39: Loss = 0.642914
Epoch 5.40: Loss = 0.574661
Epoch 5.41: Loss = 0.808823
Epoch 5.42: Loss = 0.559616
Epoch 5.43: Loss = 0.571564
Epoch 5.44: Loss = 0.661591
Epoch 5.45: Loss = 0.714767
Epoch 5.46: Loss = 0.619339
Epoch 5.47: Loss = 0.52388
Epoch 5.48: Loss = 0.645706
Epoch 5.49: Loss = 0.702377
Epoch 5.50: Loss = 0.600006
Epoch 5.51: Loss = 0.755127
Epoch 5.52: Loss = 0.620224
Epoch 5.53: Loss = 0.724976
Epoch 5.54: Loss = 0.609863
Epoch 5.55: Loss = 0.613419
Epoch 5.56: Loss = 0.647263
Epoch 5.57: Loss = 0.654892
Epoch 5.58: Loss = 0.84404
Epoch 5.59: Loss = 0.653107
Epoch 5.60: Loss = 0.647736
Epoch 5.61: Loss = 0.576965
Epoch 5.62: Loss = 0.628891
Epoch 5.63: Loss = 0.656815
Epoch 5.64: Loss = 0.642609
Epoch 5.65: Loss = 0.570038
Epoch 5.66: Loss = 0.617859
Epoch 5.67: Loss = 0.724747
Epoch 5.68: Loss = 0.757248
Epoch 5.69: Loss = 0.670029
Epoch 5.70: Loss = 0.618698
Epoch 5.71: Loss = 0.683563
Epoch 5.72: Loss = 0.661133
Epoch 5.73: Loss = 0.650558
Epoch 5.74: Loss = 0.723801
Epoch 5.75: Loss = 0.657791
Epoch 5.76: Loss = 0.658234
Epoch 5.77: Loss = 0.590836
Epoch 5.78: Loss = 0.608887
Epoch 5.79: Loss = 0.617401
Epoch 5.80: Loss = 0.726318
Epoch 5.81: Loss = 0.67485
Epoch 5.82: Loss = 0.702133
Epoch 5.83: Loss = 0.606628
Epoch 5.84: Loss = 0.677551
Epoch 5.85: Loss = 0.700577
Epoch 5.86: Loss = 0.670395
Epoch 5.87: Loss = 0.734024
Epoch 5.88: Loss = 0.744598
Epoch 5.89: Loss = 0.625183
Epoch 5.90: Loss = 0.63002
Epoch 5.91: Loss = 0.546646
Epoch 5.92: Loss = 0.675476
Epoch 5.93: Loss = 0.698792
Epoch 5.94: Loss = 0.663773
Epoch 5.95: Loss = 0.56842
Epoch 5.96: Loss = 0.671494
Epoch 5.97: Loss = 0.631302
Epoch 5.98: Loss = 0.568054
Epoch 5.99: Loss = 0.64563
Epoch 5.100: Loss = 0.507187
Epoch 5.101: Loss = 0.660873
Epoch 5.102: Loss = 0.736649
Epoch 5.103: Loss = 0.745255
Epoch 5.104: Loss = 0.643631
Epoch 5.105: Loss = 0.585068
Epoch 5.106: Loss = 0.675354
Epoch 5.107: Loss = 0.68959
Epoch 5.108: Loss = 0.632019
Epoch 5.109: Loss = 0.715668
Epoch 5.110: Loss = 0.681885
Epoch 5.111: Loss = 0.678757
Epoch 5.112: Loss = 0.525864
Epoch 5.113: Loss = 0.569199
Epoch 5.114: Loss = 0.645676
Epoch 5.115: Loss = 0.707855
Epoch 5.116: Loss = 0.669891
Epoch 5.117: Loss = 0.617935
Epoch 5.118: Loss = 0.580505
Epoch 5.119: Loss = 0.760376
Epoch 5.120: Loss = 0.694031
TRAIN LOSS = 0.654785
TRAIN ACC = 80.3696 % (48224/60000)
Loss = 0.556915
Loss = 0.677658
Loss = 0.629471
Loss = 0.560806
Loss = 0.61261
Loss = 0.739975
Loss = 0.78334
Loss = 0.702713
Loss = 0.67012
Loss = 0.611923
Loss = 0.776382
Loss = 0.751007
Loss = 0.700806
Loss = 0.64122
Loss = 0.662064
Loss = 0.679932
Loss = 0.645279
Loss = 0.694138
Loss = 0.704163
Loss = 0.696823
TEST LOSS = 0.674867
TEST ACC = 482.239 % (7984/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.559662
Epoch 6.2: Loss = 0.661728
Epoch 6.3: Loss = 0.615189
Epoch 6.4: Loss = 0.618668
Epoch 6.5: Loss = 0.687164
Epoch 6.6: Loss = 0.680618
Epoch 6.7: Loss = 0.509888
Epoch 6.8: Loss = 0.561432
Epoch 6.9: Loss = 0.689484
Epoch 6.10: Loss = 0.800629
Epoch 6.11: Loss = 0.693161
Epoch 6.12: Loss = 0.651794
Epoch 6.13: Loss = 0.504501
Epoch 6.14: Loss = 0.649292
Epoch 6.15: Loss = 0.761734
Epoch 6.16: Loss = 0.591431
Epoch 6.17: Loss = 0.577682
Epoch 6.18: Loss = 0.59053
Epoch 6.19: Loss = 0.654221
Epoch 6.20: Loss = 0.78569
Epoch 6.21: Loss = 0.573471
Epoch 6.22: Loss = 0.670807
Epoch 6.23: Loss = 0.758896
Epoch 6.24: Loss = 0.739029
Epoch 6.25: Loss = 0.558334
Epoch 6.26: Loss = 0.709122
Epoch 6.27: Loss = 0.644455
Epoch 6.28: Loss = 0.726563
Epoch 6.29: Loss = 0.659912
Epoch 6.30: Loss = 0.720444
Epoch 6.31: Loss = 0.641373
Epoch 6.32: Loss = 0.564026
Epoch 6.33: Loss = 0.669525
Epoch 6.34: Loss = 0.653687
Epoch 6.35: Loss = 0.614548
Epoch 6.36: Loss = 0.715988
Epoch 6.37: Loss = 0.765091
Epoch 6.38: Loss = 0.640793
Epoch 6.39: Loss = 0.574799
Epoch 6.40: Loss = 0.606674
Epoch 6.41: Loss = 0.709717
Epoch 6.42: Loss = 0.71228
Epoch 6.43: Loss = 0.681564
Epoch 6.44: Loss = 0.511841
Epoch 6.45: Loss = 0.524368
Epoch 6.46: Loss = 0.708664
Epoch 6.47: Loss = 0.712555
Epoch 6.48: Loss = 0.696274
Epoch 6.49: Loss = 0.556549
Epoch 6.50: Loss = 0.639557
Epoch 6.51: Loss = 0.667999
Epoch 6.52: Loss = 0.743546
Epoch 6.53: Loss = 0.742477
Epoch 6.54: Loss = 0.65686
Epoch 6.55: Loss = 0.602142
Epoch 6.56: Loss = 0.60643
Epoch 6.57: Loss = 0.600769
Epoch 6.58: Loss = 0.751862
Epoch 6.59: Loss = 0.719559
Epoch 6.60: Loss = 0.586243
Epoch 6.61: Loss = 0.759659
Epoch 6.62: Loss = 0.705017
Epoch 6.63: Loss = 0.697052
Epoch 6.64: Loss = 0.67662
Epoch 6.65: Loss = 0.616913
Epoch 6.66: Loss = 0.668655
Epoch 6.67: Loss = 0.628387
Epoch 6.68: Loss = 0.738373
Epoch 6.69: Loss = 0.590668
Epoch 6.70: Loss = 0.599609
Epoch 6.71: Loss = 0.728958
Epoch 6.72: Loss = 0.610809
Epoch 6.73: Loss = 0.539322
Epoch 6.74: Loss = 0.550247
Epoch 6.75: Loss = 0.604156
Epoch 6.76: Loss = 0.696243
Epoch 6.77: Loss = 0.75592
Epoch 6.78: Loss = 0.72467
Epoch 6.79: Loss = 0.679459
Epoch 6.80: Loss = 0.555313
Epoch 6.81: Loss = 0.550201
Epoch 6.82: Loss = 0.749298
Epoch 6.83: Loss = 0.692642
Epoch 6.84: Loss = 0.640167
Epoch 6.85: Loss = 0.787521
Epoch 6.86: Loss = 0.562897
Epoch 6.87: Loss = 0.725311
Epoch 6.88: Loss = 0.719666
Epoch 6.89: Loss = 0.664017
Epoch 6.90: Loss = 0.67836
Epoch 6.91: Loss = 0.667633
Epoch 6.92: Loss = 0.594147
Epoch 6.93: Loss = 0.539948
Epoch 6.94: Loss = 0.582474
Epoch 6.95: Loss = 0.777466
Epoch 6.96: Loss = 0.628479
Epoch 6.97: Loss = 0.678116
Epoch 6.98: Loss = 0.744644
Epoch 6.99: Loss = 0.671402
Epoch 6.100: Loss = 0.675919
Epoch 6.101: Loss = 0.619202
Epoch 6.102: Loss = 0.61525
Epoch 6.103: Loss = 0.626465
Epoch 6.104: Loss = 0.744324
Epoch 6.105: Loss = 0.680862
Epoch 6.106: Loss = 0.628479
Epoch 6.107: Loss = 0.602066
Epoch 6.108: Loss = 0.740463
Epoch 6.109: Loss = 0.591232
Epoch 6.110: Loss = 0.678238
Epoch 6.111: Loss = 0.543091
Epoch 6.112: Loss = 0.566879
Epoch 6.113: Loss = 0.631042
Epoch 6.114: Loss = 0.584351
Epoch 6.115: Loss = 0.564011
Epoch 6.116: Loss = 0.663239
Epoch 6.117: Loss = 0.617966
Epoch 6.118: Loss = 0.61322
Epoch 6.119: Loss = 0.656601
Epoch 6.120: Loss = 0.635284
TRAIN LOSS = 0.652023
TRAIN ACC = 80.7831 % (48472/60000)
Loss = 0.578445
Loss = 0.701706
Loss = 0.654892
Loss = 0.562103
Loss = 0.650848
Loss = 0.768051
Loss = 0.803757
Loss = 0.743896
Loss = 0.705582
Loss = 0.634186
Loss = 0.829971
Loss = 0.815872
Loss = 0.725128
Loss = 0.695969
Loss = 0.696396
Loss = 0.685974
Loss = 0.686569
Loss = 0.732117
Loss = 0.71463
Loss = 0.717209
TEST LOSS = 0.705165
TEST ACC = 484.72 % (8020/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.648605
Epoch 7.2: Loss = 0.654404
Epoch 7.3: Loss = 0.617813
Epoch 7.4: Loss = 0.647476
Epoch 7.5: Loss = 0.854767
Epoch 7.6: Loss = 0.657242
Epoch 7.7: Loss = 0.657196
Epoch 7.8: Loss = 0.659073
Epoch 7.9: Loss = 0.727036
Epoch 7.10: Loss = 0.800064
Epoch 7.11: Loss = 0.745636
Epoch 7.12: Loss = 0.618057
Epoch 7.13: Loss = 0.572311
Epoch 7.14: Loss = 0.53511
Epoch 7.15: Loss = 0.572952
Epoch 7.16: Loss = 0.82872
Epoch 7.17: Loss = 0.722
Epoch 7.18: Loss = 0.570496
Epoch 7.19: Loss = 0.582855
Epoch 7.20: Loss = 0.677094
Epoch 7.21: Loss = 0.679169
Epoch 7.22: Loss = 0.713913
Epoch 7.23: Loss = 0.716858
Epoch 7.24: Loss = 0.771133
Epoch 7.25: Loss = 0.633606
Epoch 7.26: Loss = 0.627777
Epoch 7.27: Loss = 0.71405
Epoch 7.28: Loss = 0.587448
Epoch 7.29: Loss = 0.787292
Epoch 7.30: Loss = 0.827271
Epoch 7.31: Loss = 0.737335
Epoch 7.32: Loss = 0.744064
Epoch 7.33: Loss = 0.685364
Epoch 7.34: Loss = 0.799271
Epoch 7.35: Loss = 0.673538
Epoch 7.36: Loss = 0.645233
Epoch 7.37: Loss = 0.756989
Epoch 7.38: Loss = 0.686234
Epoch 7.39: Loss = 0.699722
Epoch 7.40: Loss = 0.625778
Epoch 7.41: Loss = 0.602005
Epoch 7.42: Loss = 0.736069
Epoch 7.43: Loss = 0.751511
Epoch 7.44: Loss = 0.64888
Epoch 7.45: Loss = 0.690598
Epoch 7.46: Loss = 0.561737
Epoch 7.47: Loss = 0.779495
Epoch 7.48: Loss = 0.602509
Epoch 7.49: Loss = 0.577133
Epoch 7.50: Loss = 0.664841
Epoch 7.51: Loss = 0.731659
Epoch 7.52: Loss = 0.510651
Epoch 7.53: Loss = 0.631516
Epoch 7.54: Loss = 0.662796
Epoch 7.55: Loss = 0.72171
Epoch 7.56: Loss = 0.710968
Epoch 7.57: Loss = 0.600677
Epoch 7.58: Loss = 0.705521
Epoch 7.59: Loss = 0.607895
Epoch 7.60: Loss = 0.759491
Epoch 7.61: Loss = 0.615067
Epoch 7.62: Loss = 0.682739
Epoch 7.63: Loss = 0.703873
Epoch 7.64: Loss = 0.567139
Epoch 7.65: Loss = 0.743942
Epoch 7.66: Loss = 0.715805
Epoch 7.67: Loss = 0.65686
Epoch 7.68: Loss = 0.705231
Epoch 7.69: Loss = 0.628876
Epoch 7.70: Loss = 0.626694
Epoch 7.71: Loss = 0.640823
Epoch 7.72: Loss = 0.62558
Epoch 7.73: Loss = 0.647064
Epoch 7.74: Loss = 0.64917
Epoch 7.75: Loss = 0.833328
Epoch 7.76: Loss = 0.658463
Epoch 7.77: Loss = 0.562271
Epoch 7.78: Loss = 0.592804
Epoch 7.79: Loss = 0.691238
Epoch 7.80: Loss = 0.565521
Epoch 7.81: Loss = 0.620483
Epoch 7.82: Loss = 0.651596
Epoch 7.83: Loss = 0.629684
Epoch 7.84: Loss = 0.616608
Epoch 7.85: Loss = 0.583191
Epoch 7.86: Loss = 0.79216
Epoch 7.87: Loss = 0.676697
Epoch 7.88: Loss = 0.640198
Epoch 7.89: Loss = 0.639328
Epoch 7.90: Loss = 0.649109
Epoch 7.91: Loss = 0.662033
Epoch 7.92: Loss = 0.633896
Epoch 7.93: Loss = 0.646469
Epoch 7.94: Loss = 0.692932
Epoch 7.95: Loss = 0.780548
Epoch 7.96: Loss = 0.459167
Epoch 7.97: Loss = 0.699631
Epoch 7.98: Loss = 0.726898
Epoch 7.99: Loss = 0.703888
Epoch 7.100: Loss = 0.767441
Epoch 7.101: Loss = 0.676971
Epoch 7.102: Loss = 0.667969
Epoch 7.103: Loss = 0.643082
Epoch 7.104: Loss = 0.722382
Epoch 7.105: Loss = 0.699203
Epoch 7.106: Loss = 0.599289
Epoch 7.107: Loss = 0.612381
Epoch 7.108: Loss = 0.769257
Epoch 7.109: Loss = 0.633728
Epoch 7.110: Loss = 0.624557
Epoch 7.111: Loss = 0.639648
Epoch 7.112: Loss = 0.692184
Epoch 7.113: Loss = 0.695023
Epoch 7.114: Loss = 0.731812
Epoch 7.115: Loss = 0.744232
Epoch 7.116: Loss = 0.563889
Epoch 7.117: Loss = 0.683426
Epoch 7.118: Loss = 0.753601
Epoch 7.119: Loss = 0.595306
Epoch 7.120: Loss = 0.592529
TRAIN LOSS = 0.67038
TRAIN ACC = 81.2698 % (48764/60000)
Loss = 0.61499
Loss = 0.720978
Loss = 0.68103
Loss = 0.588501
Loss = 0.670227
Loss = 0.798584
Loss = 0.851288
Loss = 0.757401
Loss = 0.710403
Loss = 0.652374
Loss = 0.88356
Loss = 0.830978
Loss = 0.749161
Loss = 0.710281
Loss = 0.742615
Loss = 0.725037
Loss = 0.716492
Loss = 0.776382
Loss = 0.745483
Loss = 0.732925
TEST LOSS = 0.732934
TEST ACC = 487.639 % (8050/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.693954
Epoch 8.2: Loss = 0.673828
Epoch 8.3: Loss = 0.535141
Epoch 8.4: Loss = 0.575378
Epoch 8.5: Loss = 0.563004
Epoch 8.6: Loss = 0.718643
Epoch 8.7: Loss = 0.758224
Epoch 8.8: Loss = 0.784271
Epoch 8.9: Loss = 0.662949
Epoch 8.10: Loss = 0.750656
Epoch 8.11: Loss = 0.689392
Epoch 8.12: Loss = 0.779282
Epoch 8.13: Loss = 0.666565
Epoch 8.14: Loss = 0.736465
Epoch 8.15: Loss = 0.713104
Epoch 8.16: Loss = 0.788391
Epoch 8.17: Loss = 0.607269
Epoch 8.18: Loss = 0.666367
Epoch 8.19: Loss = 0.771469
Epoch 8.20: Loss = 0.72731
Epoch 8.21: Loss = 0.786636
Epoch 8.22: Loss = 0.812576
Epoch 8.23: Loss = 0.637848
Epoch 8.24: Loss = 0.622589
Epoch 8.25: Loss = 0.712372
Epoch 8.26: Loss = 0.809616
Epoch 8.27: Loss = 0.702698
Epoch 8.28: Loss = 0.604858
Epoch 8.29: Loss = 0.642746
Epoch 8.30: Loss = 0.684662
Epoch 8.31: Loss = 0.628571
Epoch 8.32: Loss = 0.712158
Epoch 8.33: Loss = 0.660553
Epoch 8.34: Loss = 0.607925
Epoch 8.35: Loss = 0.607437
Epoch 8.36: Loss = 0.751511
Epoch 8.37: Loss = 0.654556
Epoch 8.38: Loss = 0.712692
Epoch 8.39: Loss = 0.658112
Epoch 8.40: Loss = 0.731308
Epoch 8.41: Loss = 0.693893
Epoch 8.42: Loss = 0.662277
Epoch 8.43: Loss = 0.84761
Epoch 8.44: Loss = 0.636047
Epoch 8.45: Loss = 0.722702
Epoch 8.46: Loss = 0.652878
Epoch 8.47: Loss = 0.672623
Epoch 8.48: Loss = 0.749603
Epoch 8.49: Loss = 0.618073
Epoch 8.50: Loss = 0.67514
Epoch 8.51: Loss = 0.735779
Epoch 8.52: Loss = 0.756088
Epoch 8.53: Loss = 0.611359
Epoch 8.54: Loss = 0.638382
Epoch 8.55: Loss = 0.558884
Epoch 8.56: Loss = 0.730835
Epoch 8.57: Loss = 0.697937
Epoch 8.58: Loss = 0.608398
Epoch 8.59: Loss = 0.784592
Epoch 8.60: Loss = 0.556717
Epoch 8.61: Loss = 0.905334
Epoch 8.62: Loss = 0.711243
Epoch 8.63: Loss = 0.68158
Epoch 8.64: Loss = 0.663483
Epoch 8.65: Loss = 0.677902
Epoch 8.66: Loss = 0.749496
Epoch 8.67: Loss = 0.622025
Epoch 8.68: Loss = 0.793442
Epoch 8.69: Loss = 0.626999
Epoch 8.70: Loss = 0.685669
Epoch 8.71: Loss = 0.725784
Epoch 8.72: Loss = 0.691376
Epoch 8.73: Loss = 0.672165
Epoch 8.74: Loss = 0.610672
Epoch 8.75: Loss = 0.54808
Epoch 8.76: Loss = 0.656265
Epoch 8.77: Loss = 0.795212
Epoch 8.78: Loss = 0.636002
Epoch 8.79: Loss = 0.626923
Epoch 8.80: Loss = 0.692398
Epoch 8.81: Loss = 0.720764
Epoch 8.82: Loss = 0.688599
Epoch 8.83: Loss = 0.720505
Epoch 8.84: Loss = 0.773422
Epoch 8.85: Loss = 0.750656
Epoch 8.86: Loss = 0.62532
Epoch 8.87: Loss = 0.672974
Epoch 8.88: Loss = 0.623474
Epoch 8.89: Loss = 0.66539
Epoch 8.90: Loss = 0.776581
Epoch 8.91: Loss = 0.849457
Epoch 8.92: Loss = 0.648132
Epoch 8.93: Loss = 0.731445
Epoch 8.94: Loss = 0.775833
Epoch 8.95: Loss = 0.620438
Epoch 8.96: Loss = 0.605225
Epoch 8.97: Loss = 0.616806
Epoch 8.98: Loss = 0.639923
Epoch 8.99: Loss = 0.689316
Epoch 8.100: Loss = 0.789383
Epoch 8.101: Loss = 0.672409
Epoch 8.102: Loss = 0.652176
Epoch 8.103: Loss = 0.777786
Epoch 8.104: Loss = 0.594162
Epoch 8.105: Loss = 0.635132
Epoch 8.106: Loss = 0.649536
Epoch 8.107: Loss = 0.540756
Epoch 8.108: Loss = 0.612732
Epoch 8.109: Loss = 0.65773
Epoch 8.110: Loss = 0.715729
Epoch 8.111: Loss = 0.679474
Epoch 8.112: Loss = 0.694702
Epoch 8.113: Loss = 0.681305
Epoch 8.114: Loss = 0.602509
Epoch 8.115: Loss = 0.652924
Epoch 8.116: Loss = 0.76709
Epoch 8.117: Loss = 0.649323
Epoch 8.118: Loss = 0.709808
Epoch 8.119: Loss = 0.735077
Epoch 8.120: Loss = 0.598557
TRAIN LOSS = 0.68541
TRAIN ACC = 81.5552 % (48935/60000)
Loss = 0.621628
Loss = 0.714157
Loss = 0.67572
Loss = 0.573349
Loss = 0.639603
Loss = 0.798553
Loss = 0.854095
Loss = 0.75946
Loss = 0.689987
Loss = 0.629547
Loss = 0.89267
Loss = 0.805618
Loss = 0.738724
Loss = 0.701904
Loss = 0.702499
Loss = 0.727493
Loss = 0.737244
Loss = 0.740219
Loss = 0.735016
Loss = 0.742538
TEST LOSS = 0.724001
TEST ACC = 489.349 % (8062/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.650696
Epoch 9.2: Loss = 0.761826
Epoch 9.3: Loss = 0.69603
Epoch 9.4: Loss = 0.72345
Epoch 9.5: Loss = 0.66156
Epoch 9.6: Loss = 0.651215
Epoch 9.7: Loss = 0.703934
Epoch 9.8: Loss = 0.588898
Epoch 9.9: Loss = 0.717773
Epoch 9.10: Loss = 0.862549
Epoch 9.11: Loss = 0.7117
Epoch 9.12: Loss = 0.656158
Epoch 9.13: Loss = 0.835739
Epoch 9.14: Loss = 0.740311
Epoch 9.15: Loss = 0.802002
Epoch 9.16: Loss = 0.683884
Epoch 9.17: Loss = 0.671646
Epoch 9.18: Loss = 0.608322
Epoch 9.19: Loss = 0.615143
Epoch 9.20: Loss = 0.718857
Epoch 9.21: Loss = 0.686737
Epoch 9.22: Loss = 0.730423
Epoch 9.23: Loss = 0.718552
Epoch 9.24: Loss = 0.69075
Epoch 9.25: Loss = 0.688995
Epoch 9.26: Loss = 0.534576
Epoch 9.27: Loss = 0.695831
Epoch 9.28: Loss = 0.722687
Epoch 9.29: Loss = 0.650421
Epoch 9.30: Loss = 0.728729
Epoch 9.31: Loss = 0.751328
Epoch 9.32: Loss = 0.59137
Epoch 9.33: Loss = 0.698517
Epoch 9.34: Loss = 0.740707
Epoch 9.35: Loss = 0.851059
Epoch 9.36: Loss = 0.672348
Epoch 9.37: Loss = 0.704453
Epoch 9.38: Loss = 0.752289
Epoch 9.39: Loss = 0.579773
Epoch 9.40: Loss = 1.00441
Epoch 9.41: Loss = 0.689865
Epoch 9.42: Loss = 0.53389
Epoch 9.43: Loss = 0.872391
Epoch 9.44: Loss = 0.729401
Epoch 9.45: Loss = 0.603821
Epoch 9.46: Loss = 0.588211
Epoch 9.47: Loss = 0.600113
Epoch 9.48: Loss = 0.781204
Epoch 9.49: Loss = 0.646225
Epoch 9.50: Loss = 0.69014
Epoch 9.51: Loss = 0.691544
Epoch 9.52: Loss = 0.634171
Epoch 9.53: Loss = 0.725555
Epoch 9.54: Loss = 0.691666
Epoch 9.55: Loss = 0.609131
Epoch 9.56: Loss = 0.712357
Epoch 9.57: Loss = 0.606964
Epoch 9.58: Loss = 0.774857
Epoch 9.59: Loss = 0.733917
Epoch 9.60: Loss = 0.592773
Epoch 9.61: Loss = 0.618347
Epoch 9.62: Loss = 0.558029
Epoch 9.63: Loss = 0.700562
Epoch 9.64: Loss = 0.747299
Epoch 9.65: Loss = 0.635361
Epoch 9.66: Loss = 0.795731
Epoch 9.67: Loss = 0.680359
Epoch 9.68: Loss = 0.655991
Epoch 9.69: Loss = 0.622757
Epoch 9.70: Loss = 0.716064
Epoch 9.71: Loss = 0.66095
Epoch 9.72: Loss = 0.674988
Epoch 9.73: Loss = 0.770203
Epoch 9.74: Loss = 0.66658
Epoch 9.75: Loss = 0.713043
Epoch 9.76: Loss = 0.583038
Epoch 9.77: Loss = 0.648987
Epoch 9.78: Loss = 0.642136
Epoch 9.79: Loss = 0.610931
Epoch 9.80: Loss = 0.629532
Epoch 9.81: Loss = 0.710587
Epoch 9.82: Loss = 0.68277
Epoch 9.83: Loss = 0.534149
Epoch 9.84: Loss = 0.638046
Epoch 9.85: Loss = 0.609589
Epoch 9.86: Loss = 0.755753
Epoch 9.87: Loss = 0.626297
Epoch 9.88: Loss = 0.605637
Epoch 9.89: Loss = 0.711533
Epoch 9.90: Loss = 0.683334
Epoch 9.91: Loss = 0.673767
Epoch 9.92: Loss = 0.557861
Epoch 9.93: Loss = 0.782562
Epoch 9.94: Loss = 0.77739
Epoch 9.95: Loss = 0.665863
Epoch 9.96: Loss = 0.62178
Epoch 9.97: Loss = 0.67746
Epoch 9.98: Loss = 0.577896
Epoch 9.99: Loss = 0.690063
Epoch 9.100: Loss = 0.601791
Epoch 9.101: Loss = 0.756393
Epoch 9.102: Loss = 0.464111
Epoch 9.103: Loss = 0.646606
Epoch 9.104: Loss = 0.583527
Epoch 9.105: Loss = 0.747559
Epoch 9.106: Loss = 0.654358
Epoch 9.107: Loss = 0.648773
Epoch 9.108: Loss = 0.694321
Epoch 9.109: Loss = 0.653961
Epoch 9.110: Loss = 0.772003
Epoch 9.111: Loss = 0.637909
Epoch 9.112: Loss = 0.703156
Epoch 9.113: Loss = 0.743195
Epoch 9.114: Loss = 0.508652
Epoch 9.115: Loss = 0.651932
Epoch 9.116: Loss = 0.700226
Epoch 9.117: Loss = 0.738205
Epoch 9.118: Loss = 0.917023
Epoch 9.119: Loss = 0.628403
Epoch 9.120: Loss = 0.747665
TRAIN LOSS = 0.682327
TRAIN ACC = 81.6437 % (48989/60000)
Loss = 0.61763
Loss = 0.700089
Loss = 0.649002
Loss = 0.588348
Loss = 0.644699
Loss = 0.769882
Loss = 0.852936
Loss = 0.731049
Loss = 0.6922
Loss = 0.64061
Loss = 0.858063
Loss = 0.74736
Loss = 0.748871
Loss = 0.68689
Loss = 0.685104
Loss = 0.743958
Loss = 0.702301
Loss = 0.74086
Loss = 0.684708
Loss = 0.729782
TEST LOSS = 0.710717
TEST ACC = 489.89 % (8054/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.596512
Epoch 10.2: Loss = 0.714539
Epoch 10.3: Loss = 0.684235
Epoch 10.4: Loss = 0.660507
Epoch 10.5: Loss = 0.818985
Epoch 10.6: Loss = 43.522
Epoch 10.7: Loss = 29.2138
Epoch 10.8: Loss = 16.0043
Epoch 10.9: Loss = 9.68173
Epoch 10.10: Loss = 8.38881
Epoch 10.11: Loss = 6.4977
Epoch 10.12: Loss = 5.0119
Epoch 10.13: Loss = 4.28844
Epoch 10.14: Loss = 5.81218
Epoch 10.15: Loss = 5.27435
Epoch 10.16: Loss = 4.37318
Epoch 10.17: Loss = 5.44977
Epoch 10.18: Loss = 4.21222
Epoch 10.19: Loss = 5.18286
Epoch 10.20: Loss = 3.65123
Epoch 10.21: Loss = 3.25584
Epoch 10.22: Loss = 2.84909
Epoch 10.23: Loss = 2.87282
Epoch 10.24: Loss = 3.96629
Epoch 10.25: Loss = 2.95456
Epoch 10.26: Loss = 3.2561
Epoch 10.27: Loss = 2.5638
Epoch 10.28: Loss = 2.57312
Epoch 10.29: Loss = 2.59244
Epoch 10.30: Loss = 2.53725
Epoch 10.31: Loss = 2.47531
Epoch 10.32: Loss = 1.90152
Epoch 10.33: Loss = 1.63658
Epoch 10.34: Loss = 1.41248
Epoch 10.35: Loss = 1.7171
Epoch 10.36: Loss = 1.4767
Epoch 10.37: Loss = 1.29585
Epoch 10.38: Loss = 1.05582
Epoch 10.39: Loss = 1.08749
Epoch 10.40: Loss = 0.853073
Epoch 10.41: Loss = 0.831177
Epoch 10.42: Loss = 0.798477
Epoch 10.43: Loss = 0.896027
Epoch 10.44: Loss = 0.802582
Epoch 10.45: Loss = 0.606735
Epoch 10.46: Loss = 0.600967
Epoch 10.47: Loss = 0.679474
Epoch 10.48: Loss = 0.682938
Epoch 10.49: Loss = 0.716049
Epoch 10.50: Loss = 0.779297
Epoch 10.51: Loss = 0.69072
Epoch 10.52: Loss = 0.652512
Epoch 10.53: Loss = 0.670288
Epoch 10.54: Loss = 0.704071
Epoch 10.55: Loss = 0.58226
Epoch 10.56: Loss = 0.646851
Epoch 10.57: Loss = 0.630585
Epoch 10.58: Loss = 0.723145
Epoch 10.59: Loss = 0.657166
Epoch 10.60: Loss = 0.698807
Epoch 10.61: Loss = 0.694565
Epoch 10.62: Loss = 0.61911
Epoch 10.63: Loss = 0.593582
Epoch 10.64: Loss = 0.507263
Epoch 10.65: Loss = 0.654678
Epoch 10.66: Loss = 0.787888
Epoch 10.67: Loss = 0.673492
Epoch 10.68: Loss = 0.45195
Epoch 10.69: Loss = 0.734467
Epoch 10.70: Loss = 0.599503
Epoch 10.71: Loss = 0.727386
Epoch 10.72: Loss = 0.60141
Epoch 10.73: Loss = 0.643509
Epoch 10.74: Loss = 0.68782
Epoch 10.75: Loss = 0.674789
Epoch 10.76: Loss = 0.618057
Epoch 10.77: Loss = 0.615768
Epoch 10.78: Loss = 0.735291
Epoch 10.79: Loss = 0.730331
Epoch 10.80: Loss = 0.714951
Epoch 10.81: Loss = 0.5905
Epoch 10.82: Loss = 0.528152
Epoch 10.83: Loss = 0.765167
Epoch 10.84: Loss = 0.699356
Epoch 10.85: Loss = 0.603439
Epoch 10.86: Loss = 0.624878
Epoch 10.87: Loss = 0.652161
Epoch 10.88: Loss = 0.617584
Epoch 10.89: Loss = 0.632675
Epoch 10.90: Loss = 0.619919
Epoch 10.91: Loss = 0.708969
Epoch 10.92: Loss = 0.6577
Epoch 10.93: Loss = 0.67955
Epoch 10.94: Loss = 0.524811
Epoch 10.95: Loss = 0.667572
Epoch 10.96: Loss = 0.658615
Epoch 10.97: Loss = 0.702896
Epoch 10.98: Loss = 0.664932
Epoch 10.99: Loss = 0.666733
Epoch 10.100: Loss = 0.744064
Epoch 10.101: Loss = 0.7388
Epoch 10.102: Loss = 0.602325
Epoch 10.103: Loss = 0.705231
Epoch 10.104: Loss = 0.709976
Epoch 10.105: Loss = 0.657547
Epoch 10.106: Loss = 0.601532
Epoch 10.107: Loss = 0.821091
Epoch 10.108: Loss = 0.600586
Epoch 10.109: Loss = 0.721069
Epoch 10.110: Loss = 0.566284
Epoch 10.111: Loss = 0.756851
Epoch 10.112: Loss = 0.523819
Epoch 10.113: Loss = 0.692169
Epoch 10.114: Loss = 0.626617
Epoch 10.115: Loss = 0.517731
Epoch 10.116: Loss = 0.609726
Epoch 10.117: Loss = 0.715958
Epoch 10.118: Loss = 0.682602
Epoch 10.119: Loss = 0.606995
Epoch 10.120: Loss = 0.586624
TRAIN LOSS = 2.14621
TRAIN ACC = 77.8748 % (46727/60000)
Loss = 0.599106
Loss = 0.6884
Loss = 0.667252
Loss = 0.559326
Loss = 0.649689
Loss = 0.795868
Loss = 0.879471
Loss = 0.723862
Loss = 0.695465
Loss = 0.629669
Loss = 0.846893
Loss = 0.804184
Loss = 0.746262
Loss = 0.673004
Loss = 0.700287
Loss = 0.720505
Loss = 0.706161
Loss = 0.761124
Loss = 0.7659
Loss = 0.689514
TEST LOSS = 0.715097
TEST ACC = 467.27 % (8059/10000)

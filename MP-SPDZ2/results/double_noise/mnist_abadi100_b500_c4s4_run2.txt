Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.43002
Epoch 1.2: Loss = 2.37703
Epoch 1.3: Loss = 2.33366
Epoch 1.4: Loss = 2.28932
Epoch 1.5: Loss = 2.26819
Epoch 1.6: Loss = 2.23334
Epoch 1.7: Loss = 2.18723
Epoch 1.8: Loss = 2.15271
Epoch 1.9: Loss = 2.10634
Epoch 1.10: Loss = 2.05806
Epoch 1.11: Loss = 2.02965
Epoch 1.12: Loss = 1.98767
Epoch 1.13: Loss = 1.93216
Epoch 1.14: Loss = 1.93205
Epoch 1.15: Loss = 1.90218
Epoch 1.16: Loss = 1.89856
Epoch 1.17: Loss = 1.84676
Epoch 1.18: Loss = 1.77933
Epoch 1.19: Loss = 1.7541
Epoch 1.20: Loss = 1.75267
Epoch 1.21: Loss = 1.70078
Epoch 1.22: Loss = 1.66251
Epoch 1.23: Loss = 1.67188
Epoch 1.24: Loss = 1.62497
Epoch 1.25: Loss = 1.57506
Epoch 1.26: Loss = 1.54541
Epoch 1.27: Loss = 1.51067
Epoch 1.28: Loss = 1.47864
Epoch 1.29: Loss = 1.51971
Epoch 1.30: Loss = 1.4552
Epoch 1.31: Loss = 1.43552
Epoch 1.32: Loss = 1.34538
Epoch 1.33: Loss = 1.34102
Epoch 1.34: Loss = 1.37564
Epoch 1.35: Loss = 1.29297
Epoch 1.36: Loss = 1.30536
Epoch 1.37: Loss = 1.27649
Epoch 1.38: Loss = 1.21825
Epoch 1.39: Loss = 1.17442
Epoch 1.40: Loss = 1.19768
Epoch 1.41: Loss = 1.18486
Epoch 1.42: Loss = 1.21815
Epoch 1.43: Loss = 1.16743
Epoch 1.44: Loss = 1.19345
Epoch 1.45: Loss = 1.12241
Epoch 1.46: Loss = 1.10648
Epoch 1.47: Loss = 1.0793
Epoch 1.48: Loss = 1.12766
Epoch 1.49: Loss = 1.0517
Epoch 1.50: Loss = 1.04881
Epoch 1.51: Loss = 0.994125
Epoch 1.52: Loss = 0.983582
Epoch 1.53: Loss = 0.98555
Epoch 1.54: Loss = 0.937256
Epoch 1.55: Loss = 0.940475
Epoch 1.56: Loss = 0.97493
Epoch 1.57: Loss = 0.930527
Epoch 1.58: Loss = 1.00056
Epoch 1.59: Loss = 0.912369
Epoch 1.60: Loss = 0.882263
Epoch 1.61: Loss = 0.867157
Epoch 1.62: Loss = 0.940491
Epoch 1.63: Loss = 0.875198
Epoch 1.64: Loss = 0.863815
Epoch 1.65: Loss = 0.912933
Epoch 1.66: Loss = 0.870972
Epoch 1.67: Loss = 0.901413
Epoch 1.68: Loss = 0.893616
Epoch 1.69: Loss = 0.825119
Epoch 1.70: Loss = 0.782364
Epoch 1.71: Loss = 0.819702
Epoch 1.72: Loss = 0.821152
Epoch 1.73: Loss = 0.803085
Epoch 1.74: Loss = 0.80864
Epoch 1.75: Loss = 0.827103
Epoch 1.76: Loss = 0.853485
Epoch 1.77: Loss = 0.84668
Epoch 1.78: Loss = 0.789246
Epoch 1.79: Loss = 0.736969
Epoch 1.80: Loss = 0.810593
Epoch 1.81: Loss = 0.733353
Epoch 1.82: Loss = 0.759018
Epoch 1.83: Loss = 0.755783
Epoch 1.84: Loss = 0.716797
Epoch 1.85: Loss = 0.751953
Epoch 1.86: Loss = 0.773727
Epoch 1.87: Loss = 0.688904
Epoch 1.88: Loss = 0.756943
Epoch 1.89: Loss = 0.708817
Epoch 1.90: Loss = 0.739105
Epoch 1.91: Loss = 0.705658
Epoch 1.92: Loss = 0.653519
Epoch 1.93: Loss = 0.664001
Epoch 1.94: Loss = 0.730164
Epoch 1.95: Loss = 0.695816
Epoch 1.96: Loss = 0.666428
Epoch 1.97: Loss = 0.69809
Epoch 1.98: Loss = 0.685257
Epoch 1.99: Loss = 0.603745
Epoch 1.100: Loss = 0.666626
Epoch 1.101: Loss = 0.6642
Epoch 1.102: Loss = 0.71756
Epoch 1.103: Loss = 0.721634
Epoch 1.104: Loss = 0.692856
Epoch 1.105: Loss = 0.649307
Epoch 1.106: Loss = 0.624313
Epoch 1.107: Loss = 0.626175
Epoch 1.108: Loss = 0.637833
Epoch 1.109: Loss = 0.636917
Epoch 1.110: Loss = 0.631851
Epoch 1.111: Loss = 0.643784
Epoch 1.112: Loss = 0.63266
Epoch 1.113: Loss = 0.620575
Epoch 1.114: Loss = 0.706406
Epoch 1.115: Loss = 0.6633
Epoch 1.116: Loss = 0.666855
Epoch 1.117: Loss = 0.635544
Epoch 1.118: Loss = 0.60556
Epoch 1.119: Loss = 0.577744
Epoch 1.120: Loss = 0.630142
TRAIN LOSS = 1.12326
TRAIN ACC = 68.8736 % (41326/60000)
Loss = 0.623505
Loss = 0.695816
Loss = 0.759659
Loss = 0.727188
Loss = 0.750702
Loss = 0.650894
Loss = 0.656555
Loss = 0.796051
Loss = 0.751312
Loss = 0.710907
Loss = 0.372223
Loss = 0.517853
Loss = 0.403641
Loss = 0.596848
Loss = 0.47319
Loss = 0.47319
Loss = 0.48587
Loss = 0.256485
Loss = 0.434265
Loss = 0.715591
TEST LOSS = 0.592587
TEST ACC = 413.26 % (8349/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.613586
Epoch 2.2: Loss = 0.704605
Epoch 2.3: Loss = 0.659027
Epoch 2.4: Loss = 0.64679
Epoch 2.5: Loss = 0.518829
Epoch 2.6: Loss = 0.614777
Epoch 2.7: Loss = 0.614319
Epoch 2.8: Loss = 0.593338
Epoch 2.9: Loss = 0.625626
Epoch 2.10: Loss = 0.611633
Epoch 2.11: Loss = 0.576218
Epoch 2.12: Loss = 0.594284
Epoch 2.13: Loss = 0.609894
Epoch 2.14: Loss = 0.591644
Epoch 2.15: Loss = 0.62262
Epoch 2.16: Loss = 0.620865
Epoch 2.17: Loss = 0.579437
Epoch 2.18: Loss = 0.634125
Epoch 2.19: Loss = 0.500748
Epoch 2.20: Loss = 0.597382
Epoch 2.21: Loss = 0.502014
Epoch 2.22: Loss = 0.557556
Epoch 2.23: Loss = 0.625336
Epoch 2.24: Loss = 0.567032
Epoch 2.25: Loss = 0.569504
Epoch 2.26: Loss = 0.604004
Epoch 2.27: Loss = 0.571167
Epoch 2.28: Loss = 0.55365
Epoch 2.29: Loss = 0.57608
Epoch 2.30: Loss = 0.571274
Epoch 2.31: Loss = 0.553482
Epoch 2.32: Loss = 0.570267
Epoch 2.33: Loss = 0.621674
Epoch 2.34: Loss = 0.523254
Epoch 2.35: Loss = 0.542908
Epoch 2.36: Loss = 0.580444
Epoch 2.37: Loss = 0.526199
Epoch 2.38: Loss = 0.545959
Epoch 2.39: Loss = 0.577606
Epoch 2.40: Loss = 0.556519
Epoch 2.41: Loss = 0.547745
Epoch 2.42: Loss = 0.528061
Epoch 2.43: Loss = 0.579346
Epoch 2.44: Loss = 0.554703
Epoch 2.45: Loss = 0.522629
Epoch 2.46: Loss = 0.555496
Epoch 2.47: Loss = 0.549744
Epoch 2.48: Loss = 0.482361
Epoch 2.49: Loss = 0.566971
Epoch 2.50: Loss = 0.514511
Epoch 2.51: Loss = 0.554291
Epoch 2.52: Loss = 0.484619
Epoch 2.53: Loss = 0.551117
Epoch 2.54: Loss = 0.530472
Epoch 2.55: Loss = 0.45105
Epoch 2.56: Loss = 0.501617
Epoch 2.57: Loss = 0.490753
Epoch 2.58: Loss = 0.546478
Epoch 2.59: Loss = 0.544357
Epoch 2.60: Loss = 0.511429
Epoch 2.61: Loss = 0.531815
Epoch 2.62: Loss = 0.454178
Epoch 2.63: Loss = 0.466187
Epoch 2.64: Loss = 0.477219
Epoch 2.65: Loss = 0.554276
Epoch 2.66: Loss = 0.511475
Epoch 2.67: Loss = 0.468445
Epoch 2.68: Loss = 0.447067
Epoch 2.69: Loss = 0.506317
Epoch 2.70: Loss = 0.548004
Epoch 2.71: Loss = 0.527466
Epoch 2.72: Loss = 0.433792
Epoch 2.73: Loss = 0.524963
Epoch 2.74: Loss = 0.471039
Epoch 2.75: Loss = 0.514481
Epoch 2.76: Loss = 0.499222
Epoch 2.77: Loss = 0.531982
Epoch 2.78: Loss = 0.546921
Epoch 2.79: Loss = 0.458725
Epoch 2.80: Loss = 0.47081
Epoch 2.81: Loss = 0.538345
Epoch 2.82: Loss = 0.51152
Epoch 2.83: Loss = 0.61618
Epoch 2.84: Loss = 0.527466
Epoch 2.85: Loss = 0.427658
Epoch 2.86: Loss = 0.528809
Epoch 2.87: Loss = 0.545044
Epoch 2.88: Loss = 0.549454
Epoch 2.89: Loss = 0.447403
Epoch 2.90: Loss = 0.51889
Epoch 2.91: Loss = 0.524063
Epoch 2.92: Loss = 0.499481
Epoch 2.93: Loss = 0.506134
Epoch 2.94: Loss = 0.569366
Epoch 2.95: Loss = 0.438263
Epoch 2.96: Loss = 0.506256
Epoch 2.97: Loss = 0.528366
Epoch 2.98: Loss = 0.479797
Epoch 2.99: Loss = 0.464783
Epoch 2.100: Loss = 0.478821
Epoch 2.101: Loss = 0.480042
Epoch 2.102: Loss = 0.529129
Epoch 2.103: Loss = 0.545441
Epoch 2.104: Loss = 0.514359
Epoch 2.105: Loss = 0.496674
Epoch 2.106: Loss = 0.42897
Epoch 2.107: Loss = 0.515701
Epoch 2.108: Loss = 0.472778
Epoch 2.109: Loss = 0.457825
Epoch 2.110: Loss = 0.421249
Epoch 2.111: Loss = 0.491135
Epoch 2.112: Loss = 0.516907
Epoch 2.113: Loss = 0.552917
Epoch 2.114: Loss = 0.588242
Epoch 2.115: Loss = 0.503143
Epoch 2.116: Loss = 0.469116
Epoch 2.117: Loss = 0.482178
Epoch 2.118: Loss = 0.462769
Epoch 2.119: Loss = 0.504883
Epoch 2.120: Loss = 0.408127
TRAIN LOSS = 0.533234
TRAIN ACC = 84.1812 % (50511/60000)
Loss = 0.453827
Loss = 0.540482
Loss = 0.612701
Loss = 0.579315
Loss = 0.621811
Loss = 0.484848
Loss = 0.498718
Loss = 0.662338
Loss = 0.611313
Loss = 0.566284
Loss = 0.248322
Loss = 0.382858
Loss = 0.320496
Loss = 0.460495
Loss = 0.338562
Loss = 0.375061
Loss = 0.328644
Loss = 0.127136
Loss = 0.301682
Loss = 0.567047
TEST LOSS = 0.454097
TEST ACC = 505.109 % (8613/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.539871
Epoch 3.2: Loss = 0.493332
Epoch 3.3: Loss = 0.415131
Epoch 3.4: Loss = 0.521896
Epoch 3.5: Loss = 0.456543
Epoch 3.6: Loss = 0.547333
Epoch 3.7: Loss = 0.434036
Epoch 3.8: Loss = 0.521469
Epoch 3.9: Loss = 0.500732
Epoch 3.10: Loss = 0.44989
Epoch 3.11: Loss = 0.472733
Epoch 3.12: Loss = 0.438873
Epoch 3.13: Loss = 0.486221
Epoch 3.14: Loss = 0.460953
Epoch 3.15: Loss = 0.509186
Epoch 3.16: Loss = 0.530457
Epoch 3.17: Loss = 0.475891
Epoch 3.18: Loss = 0.48941
Epoch 3.19: Loss = 0.458328
Epoch 3.20: Loss = 0.409775
Epoch 3.21: Loss = 0.473557
Epoch 3.22: Loss = 0.415665
Epoch 3.23: Loss = 0.485413
Epoch 3.24: Loss = 0.567719
Epoch 3.25: Loss = 0.469299
Epoch 3.26: Loss = 0.456711
Epoch 3.27: Loss = 0.469269
Epoch 3.28: Loss = 0.547806
Epoch 3.29: Loss = 0.416687
Epoch 3.30: Loss = 0.46637
Epoch 3.31: Loss = 0.460419
Epoch 3.32: Loss = 0.436508
Epoch 3.33: Loss = 0.467651
Epoch 3.34: Loss = 0.461914
Epoch 3.35: Loss = 0.503464
Epoch 3.36: Loss = 0.441055
Epoch 3.37: Loss = 0.523865
Epoch 3.38: Loss = 0.450241
Epoch 3.39: Loss = 0.494308
Epoch 3.40: Loss = 0.439774
Epoch 3.41: Loss = 0.483307
Epoch 3.42: Loss = 0.478134
Epoch 3.43: Loss = 0.430588
Epoch 3.44: Loss = 0.454208
Epoch 3.45: Loss = 0.492722
Epoch 3.46: Loss = 0.370636
Epoch 3.47: Loss = 0.422119
Epoch 3.48: Loss = 0.436951
Epoch 3.49: Loss = 0.467133
Epoch 3.50: Loss = 0.503296
Epoch 3.51: Loss = 0.497086
Epoch 3.52: Loss = 0.464325
Epoch 3.53: Loss = 0.499252
Epoch 3.54: Loss = 0.445175
Epoch 3.55: Loss = 0.471878
Epoch 3.56: Loss = 0.486298
Epoch 3.57: Loss = 0.515564
Epoch 3.58: Loss = 0.50592
Epoch 3.59: Loss = 0.473618
Epoch 3.60: Loss = 0.395569
Epoch 3.61: Loss = 0.476227
Epoch 3.62: Loss = 0.432663
Epoch 3.63: Loss = 0.42514
Epoch 3.64: Loss = 0.461868
Epoch 3.65: Loss = 0.458298
Epoch 3.66: Loss = 0.478302
Epoch 3.67: Loss = 0.422913
Epoch 3.68: Loss = 0.448776
Epoch 3.69: Loss = 0.443069
Epoch 3.70: Loss = 0.526123
Epoch 3.71: Loss = 0.541794
Epoch 3.72: Loss = 0.408218
Epoch 3.73: Loss = 0.519699
Epoch 3.74: Loss = 0.559387
Epoch 3.75: Loss = 0.473831
Epoch 3.76: Loss = 0.483475
Epoch 3.77: Loss = 0.457962
Epoch 3.78: Loss = 0.459869
Epoch 3.79: Loss = 0.546814
Epoch 3.80: Loss = 0.476242
Epoch 3.81: Loss = 0.509323
Epoch 3.82: Loss = 0.524124
Epoch 3.83: Loss = 0.453201
Epoch 3.84: Loss = 0.466629
Epoch 3.85: Loss = 0.471985
Epoch 3.86: Loss = 0.439835
Epoch 3.87: Loss = 0.424011
Epoch 3.88: Loss = 0.441864
Epoch 3.89: Loss = 0.445267
Epoch 3.90: Loss = 0.562805
Epoch 3.91: Loss = 0.428513
Epoch 3.92: Loss = 0.411667
Epoch 3.93: Loss = 0.478149
Epoch 3.94: Loss = 0.440292
Epoch 3.95: Loss = 0.431427
Epoch 3.96: Loss = 0.493576
Epoch 3.97: Loss = 0.393707
Epoch 3.98: Loss = 0.503845
Epoch 3.99: Loss = 0.386703
Epoch 3.100: Loss = 0.486862
Epoch 3.101: Loss = 0.398178
Epoch 3.102: Loss = 0.389221
Epoch 3.103: Loss = 0.468704
Epoch 3.104: Loss = 0.454346
Epoch 3.105: Loss = 0.470734
Epoch 3.106: Loss = 0.543549
Epoch 3.107: Loss = 0.559052
Epoch 3.108: Loss = 0.408722
Epoch 3.109: Loss = 0.471573
Epoch 3.110: Loss = 0.442261
Epoch 3.111: Loss = 0.453415
Epoch 3.112: Loss = 0.455765
Epoch 3.113: Loss = 0.446899
Epoch 3.114: Loss = 0.548264
Epoch 3.115: Loss = 0.51503
Epoch 3.116: Loss = 0.411331
Epoch 3.117: Loss = 0.462646
Epoch 3.118: Loss = 0.513428
Epoch 3.119: Loss = 0.361206
Epoch 3.120: Loss = 0.432236
TRAIN LOSS = 0.468781
TRAIN ACC = 85.7773 % (51468/60000)
Loss = 0.388123
Loss = 0.521362
Loss = 0.597977
Loss = 0.566818
Loss = 0.6035
Loss = 0.444962
Loss = 0.475189
Loss = 0.656113
Loss = 0.581818
Loss = 0.536163
Loss = 0.23378
Loss = 0.360535
Loss = 0.349274
Loss = 0.430328
Loss = 0.278168
Loss = 0.345871
Loss = 0.296158
Loss = 0.103973
Loss = 0.279449
Loss = 0.570892
TEST LOSS = 0.431023
TEST ACC = 514.679 % (8714/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.497452
Epoch 4.2: Loss = 0.468811
Epoch 4.3: Loss = 0.502075
Epoch 4.4: Loss = 0.461487
Epoch 4.5: Loss = 0.53894
Epoch 4.6: Loss = 0.543304
Epoch 4.7: Loss = 0.448532
Epoch 4.8: Loss = 0.388779
Epoch 4.9: Loss = 0.469376
Epoch 4.10: Loss = 0.569748
Epoch 4.11: Loss = 0.474503
Epoch 4.12: Loss = 0.337097
Epoch 4.13: Loss = 0.458878
Epoch 4.14: Loss = 0.428497
Epoch 4.15: Loss = 0.5746
Epoch 4.16: Loss = 0.458023
Epoch 4.17: Loss = 0.418274
Epoch 4.18: Loss = 0.444382
Epoch 4.19: Loss = 0.546432
Epoch 4.20: Loss = 0.50116
Epoch 4.21: Loss = 0.473831
Epoch 4.22: Loss = 0.451157
Epoch 4.23: Loss = 0.411087
Epoch 4.24: Loss = 0.426773
Epoch 4.25: Loss = 0.475082
Epoch 4.26: Loss = 0.42598
Epoch 4.27: Loss = 0.462601
Epoch 4.28: Loss = 0.435577
Epoch 4.29: Loss = 0.435181
Epoch 4.30: Loss = 0.458817
Epoch 4.31: Loss = 0.403564
Epoch 4.32: Loss = 0.507553
Epoch 4.33: Loss = 0.504745
Epoch 4.34: Loss = 0.422195
Epoch 4.35: Loss = 0.599777
Epoch 4.36: Loss = 0.415054
Epoch 4.37: Loss = 0.411057
Epoch 4.38: Loss = 0.434448
Epoch 4.39: Loss = 0.464645
Epoch 4.40: Loss = 0.427856
Epoch 4.41: Loss = 0.462708
Epoch 4.42: Loss = 0.508911
Epoch 4.43: Loss = 0.424713
Epoch 4.44: Loss = 0.373306
Epoch 4.45: Loss = 0.487045
Epoch 4.46: Loss = 0.328827
Epoch 4.47: Loss = 0.387833
Epoch 4.48: Loss = 0.515106
Epoch 4.49: Loss = 0.474091
Epoch 4.50: Loss = 0.480301
Epoch 4.51: Loss = 0.482193
Epoch 4.52: Loss = 0.437927
Epoch 4.53: Loss = 0.459824
Epoch 4.54: Loss = 0.453125
Epoch 4.55: Loss = 0.554916
Epoch 4.56: Loss = 0.429306
Epoch 4.57: Loss = 0.366623
Epoch 4.58: Loss = 0.469894
Epoch 4.59: Loss = 0.299393
Epoch 4.60: Loss = 0.379181
Epoch 4.61: Loss = 0.392578
Epoch 4.62: Loss = 0.53511
Epoch 4.63: Loss = 0.384705
Epoch 4.64: Loss = 0.431
Epoch 4.65: Loss = 0.538651
Epoch 4.66: Loss = 0.400085
Epoch 4.67: Loss = 0.394135
Epoch 4.68: Loss = 0.552597
Epoch 4.69: Loss = 0.516464
Epoch 4.70: Loss = 0.404465
Epoch 4.71: Loss = 0.405472
Epoch 4.72: Loss = 0.463623
Epoch 4.73: Loss = 0.426865
Epoch 4.74: Loss = 0.36116
Epoch 4.75: Loss = 0.458145
Epoch 4.76: Loss = 0.430923
Epoch 4.77: Loss = 0.427444
Epoch 4.78: Loss = 0.384003
Epoch 4.79: Loss = 0.416504
Epoch 4.80: Loss = 0.559235
Epoch 4.81: Loss = 0.387527
Epoch 4.82: Loss = 0.368744
Epoch 4.83: Loss = 0.401047
Epoch 4.84: Loss = 0.495041
Epoch 4.85: Loss = 0.416962
Epoch 4.86: Loss = 0.393204
Epoch 4.87: Loss = 0.470367
Epoch 4.88: Loss = 0.471481
Epoch 4.89: Loss = 0.484695
Epoch 4.90: Loss = 0.452698
Epoch 4.91: Loss = 0.404343
Epoch 4.92: Loss = 0.443665
Epoch 4.93: Loss = 0.427734
Epoch 4.94: Loss = 0.416321
Epoch 4.95: Loss = 0.381409
Epoch 4.96: Loss = 0.587875
Epoch 4.97: Loss = 0.505005
Epoch 4.98: Loss = 0.393768
Epoch 4.99: Loss = 0.440048
Epoch 4.100: Loss = 0.376022
Epoch 4.101: Loss = 0.419586
Epoch 4.102: Loss = 0.413345
Epoch 4.103: Loss = 0.451721
Epoch 4.104: Loss = 0.385437
Epoch 4.105: Loss = 0.44548
Epoch 4.106: Loss = 0.378632
Epoch 4.107: Loss = 0.497681
Epoch 4.108: Loss = 0.486816
Epoch 4.109: Loss = 0.458939
Epoch 4.110: Loss = 0.426361
Epoch 4.111: Loss = 0.519257
Epoch 4.112: Loss = 0.43779
Epoch 4.113: Loss = 0.439621
Epoch 4.114: Loss = 0.383469
Epoch 4.115: Loss = 0.475159
Epoch 4.116: Loss = 0.417908
Epoch 4.117: Loss = 0.40683
Epoch 4.118: Loss = 0.494659
Epoch 4.119: Loss = 0.491653
Epoch 4.120: Loss = 0.451782
TRAIN LOSS = 0.448624
TRAIN ACC = 86.5921 % (51958/60000)
Loss = 0.372177
Loss = 0.487137
Loss = 0.541016
Loss = 0.546585
Loss = 0.614182
Loss = 0.416687
Loss = 0.450821
Loss = 0.645493
Loss = 0.580429
Loss = 0.506271
Loss = 0.208481
Loss = 0.350632
Loss = 0.344833
Loss = 0.400162
Loss = 0.248856
Loss = 0.313309
Loss = 0.281998
Loss = 0.0901794
Loss = 0.246628
Loss = 0.556793
TEST LOSS = 0.410133
TEST ACC = 519.579 % (8784/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.500916
Epoch 5.2: Loss = 0.437408
Epoch 5.3: Loss = 0.493118
Epoch 5.4: Loss = 0.450943
Epoch 5.5: Loss = 0.544617
Epoch 5.6: Loss = 0.380905
Epoch 5.7: Loss = 0.490005
Epoch 5.8: Loss = 0.387451
Epoch 5.9: Loss = 0.416702
Epoch 5.10: Loss = 0.420441
Epoch 5.11: Loss = 0.45015
Epoch 5.12: Loss = 0.428741
Epoch 5.13: Loss = 0.449661
Epoch 5.14: Loss = 0.536865
Epoch 5.15: Loss = 0.448639
Epoch 5.16: Loss = 0.440002
Epoch 5.17: Loss = 0.441849
Epoch 5.18: Loss = 0.369736
Epoch 5.19: Loss = 0.405533
Epoch 5.20: Loss = 0.45636
Epoch 5.21: Loss = 0.451843
Epoch 5.22: Loss = 0.471542
Epoch 5.23: Loss = 0.392273
Epoch 5.24: Loss = 0.40361
Epoch 5.25: Loss = 0.378387
Epoch 5.26: Loss = 0.443253
Epoch 5.27: Loss = 0.351761
Epoch 5.28: Loss = 0.449249
Epoch 5.29: Loss = 0.444717
Epoch 5.30: Loss = 0.459625
Epoch 5.31: Loss = 0.397568
Epoch 5.32: Loss = 0.497849
Epoch 5.33: Loss = 0.478226
Epoch 5.34: Loss = 0.402191
Epoch 5.35: Loss = 0.445801
Epoch 5.36: Loss = 0.368042
Epoch 5.37: Loss = 0.399475
Epoch 5.38: Loss = 0.411163
Epoch 5.39: Loss = 0.461395
Epoch 5.40: Loss = 0.578201
Epoch 5.41: Loss = 0.451599
Epoch 5.42: Loss = 0.348907
Epoch 5.43: Loss = 0.396973
Epoch 5.44: Loss = 0.414688
Epoch 5.45: Loss = 0.409286
Epoch 5.46: Loss = 0.40123
Epoch 5.47: Loss = 0.455795
Epoch 5.48: Loss = 0.466187
Epoch 5.49: Loss = 0.538864
Epoch 5.50: Loss = 0.406601
Epoch 5.51: Loss = 0.476578
Epoch 5.52: Loss = 0.485306
Epoch 5.53: Loss = 0.399658
Epoch 5.54: Loss = 0.460602
Epoch 5.55: Loss = 0.485977
Epoch 5.56: Loss = 0.466431
Epoch 5.57: Loss = 0.462631
Epoch 5.58: Loss = 0.503586
Epoch 5.59: Loss = 0.452271
Epoch 5.60: Loss = 0.473587
Epoch 5.61: Loss = 0.494415
Epoch 5.62: Loss = 0.464951
Epoch 5.63: Loss = 0.425613
Epoch 5.64: Loss = 0.464722
Epoch 5.65: Loss = 0.503647
Epoch 5.66: Loss = 0.531494
Epoch 5.67: Loss = 0.456497
Epoch 5.68: Loss = 0.393112
Epoch 5.69: Loss = 0.458023
Epoch 5.70: Loss = 0.471832
Epoch 5.71: Loss = 0.459961
Epoch 5.72: Loss = 0.392105
Epoch 5.73: Loss = 0.424805
Epoch 5.74: Loss = 0.419846
Epoch 5.75: Loss = 0.415131
Epoch 5.76: Loss = 0.415619
Epoch 5.77: Loss = 0.416443
Epoch 5.78: Loss = 0.366821
Epoch 5.79: Loss = 0.464722
Epoch 5.80: Loss = 0.547867
Epoch 5.81: Loss = 0.482086
Epoch 5.82: Loss = 0.427521
Epoch 5.83: Loss = 0.366959
Epoch 5.84: Loss = 0.402008
Epoch 5.85: Loss = 0.389267
Epoch 5.86: Loss = 0.365082
Epoch 5.87: Loss = 0.545319
Epoch 5.88: Loss = 0.419052
Epoch 5.89: Loss = 0.451279
Epoch 5.90: Loss = 0.406815
Epoch 5.91: Loss = 0.348618
Epoch 5.92: Loss = 0.450485
Epoch 5.93: Loss = 0.450104
Epoch 5.94: Loss = 0.46492
Epoch 5.95: Loss = 0.465378
Epoch 5.96: Loss = 0.460052
Epoch 5.97: Loss = 0.370346
Epoch 5.98: Loss = 0.444351
Epoch 5.99: Loss = 0.433762
Epoch 5.100: Loss = 0.391113
Epoch 5.101: Loss = 0.45137
Epoch 5.102: Loss = 0.426468
Epoch 5.103: Loss = 0.433273
Epoch 5.104: Loss = 0.357681
Epoch 5.105: Loss = 0.493408
Epoch 5.106: Loss = 0.479279
Epoch 5.107: Loss = 0.503647
Epoch 5.108: Loss = 0.489639
Epoch 5.109: Loss = 0.409485
Epoch 5.110: Loss = 0.407333
Epoch 5.111: Loss = 0.495728
Epoch 5.112: Loss = 0.425491
Epoch 5.113: Loss = 0.520554
Epoch 5.114: Loss = 0.415253
Epoch 5.115: Loss = 0.448776
Epoch 5.116: Loss = 0.518982
Epoch 5.117: Loss = 0.400406
Epoch 5.118: Loss = 0.418518
Epoch 5.119: Loss = 0.428513
Epoch 5.120: Loss = 0.405945
TRAIN LOSS = 0.442032
TRAIN ACC = 86.9476 % (52171/60000)
Loss = 0.378616
Loss = 0.500839
Loss = 0.580338
Loss = 0.562698
Loss = 0.639236
Loss = 0.392136
Loss = 0.437271
Loss = 0.683914
Loss = 0.618393
Loss = 0.515854
Loss = 0.216904
Loss = 0.340759
Loss = 0.38443
Loss = 0.397232
Loss = 0.222443
Loss = 0.294174
Loss = 0.251633
Loss = 0.0818329
Loss = 0.247269
Loss = 0.603653
TEST LOSS = 0.417481
TEST ACC = 521.709 % (8804/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.344696
Epoch 6.2: Loss = 0.402908
Epoch 6.3: Loss = 0.357376
Epoch 6.4: Loss = 0.365891
Epoch 6.5: Loss = 0.427704
Epoch 6.6: Loss = 0.453583
Epoch 6.7: Loss = 0.423691
Epoch 6.8: Loss = 0.435806
Epoch 6.9: Loss = 0.479309
Epoch 6.10: Loss = 0.370956
Epoch 6.11: Loss = 0.514984
Epoch 6.12: Loss = 0.367935
Epoch 6.13: Loss = 0.480804
Epoch 6.14: Loss = 0.420959
Epoch 6.15: Loss = 0.454208
Epoch 6.16: Loss = 0.454926
Epoch 6.17: Loss = 0.467911
Epoch 6.18: Loss = 0.429199
Epoch 6.19: Loss = 0.52887
Epoch 6.20: Loss = 0.563599
Epoch 6.21: Loss = 0.399185
Epoch 6.22: Loss = 0.429672
Epoch 6.23: Loss = 0.446289
Epoch 6.24: Loss = 0.403336
Epoch 6.25: Loss = 0.41861
Epoch 6.26: Loss = 0.432587
Epoch 6.27: Loss = 0.381561
Epoch 6.28: Loss = 0.448318
Epoch 6.29: Loss = 0.465942
Epoch 6.30: Loss = 0.522705
Epoch 6.31: Loss = 0.400864
Epoch 6.32: Loss = 0.420746
Epoch 6.33: Loss = 0.438995
Epoch 6.34: Loss = 0.470367
Epoch 6.35: Loss = 0.333572
Epoch 6.36: Loss = 0.436325
Epoch 6.37: Loss = 0.504211
Epoch 6.38: Loss = 0.404419
Epoch 6.39: Loss = 0.44455
Epoch 6.40: Loss = 0.359024
Epoch 6.41: Loss = 0.393555
Epoch 6.42: Loss = 0.490204
Epoch 6.43: Loss = 0.492691
Epoch 6.44: Loss = 0.437515
Epoch 6.45: Loss = 0.505035
Epoch 6.46: Loss = 0.464645
Epoch 6.47: Loss = 0.445999
Epoch 6.48: Loss = 0.424545
Epoch 6.49: Loss = 0.528793
Epoch 6.50: Loss = 0.503723
Epoch 6.51: Loss = 0.492325
Epoch 6.52: Loss = 0.450668
Epoch 6.53: Loss = 0.482834
Epoch 6.54: Loss = 0.469009
Epoch 6.55: Loss = 0.430603
Epoch 6.56: Loss = 0.382446
Epoch 6.57: Loss = 0.443176
Epoch 6.58: Loss = 0.346054
Epoch 6.59: Loss = 0.401245
Epoch 6.60: Loss = 0.49234
Epoch 6.61: Loss = 0.439392
Epoch 6.62: Loss = 0.418716
Epoch 6.63: Loss = 0.41713
Epoch 6.64: Loss = 0.505554
Epoch 6.65: Loss = 0.359741
Epoch 6.66: Loss = 0.529327
Epoch 6.67: Loss = 0.462952
Epoch 6.68: Loss = 0.483383
Epoch 6.69: Loss = 0.490433
Epoch 6.70: Loss = 0.370804
Epoch 6.71: Loss = 0.44986
Epoch 6.72: Loss = 0.440094
Epoch 6.73: Loss = 0.460098
Epoch 6.74: Loss = 0.403259
Epoch 6.75: Loss = 0.510071
Epoch 6.76: Loss = 0.523804
Epoch 6.77: Loss = 0.46524
Epoch 6.78: Loss = 0.431915
Epoch 6.79: Loss = 0.40773
Epoch 6.80: Loss = 0.455078
Epoch 6.81: Loss = 0.476791
Epoch 6.82: Loss = 0.315369
Epoch 6.83: Loss = 0.432861
Epoch 6.84: Loss = 0.429413
Epoch 6.85: Loss = 0.494812
Epoch 6.86: Loss = 0.401566
Epoch 6.87: Loss = 0.632858
Epoch 6.88: Loss = 0.486435
Epoch 6.89: Loss = 0.517288
Epoch 6.90: Loss = 0.429443
Epoch 6.91: Loss = 0.479095
Epoch 6.92: Loss = 0.572952
Epoch 6.93: Loss = 0.342087
Epoch 6.94: Loss = 0.373001
Epoch 6.95: Loss = 0.394562
Epoch 6.96: Loss = 0.504181
Epoch 6.97: Loss = 0.448807
Epoch 6.98: Loss = 0.457947
Epoch 6.99: Loss = 0.489288
Epoch 6.100: Loss = 0.457611
Epoch 6.101: Loss = 0.511032
Epoch 6.102: Loss = 0.441162
Epoch 6.103: Loss = 0.389389
Epoch 6.104: Loss = 0.487106
Epoch 6.105: Loss = 0.52243
Epoch 6.106: Loss = 0.454422
Epoch 6.107: Loss = 0.409378
Epoch 6.108: Loss = 0.452759
Epoch 6.109: Loss = 0.615021
Epoch 6.110: Loss = 0.438446
Epoch 6.111: Loss = 0.31311
Epoch 6.112: Loss = 0.456909
Epoch 6.113: Loss = 0.412064
Epoch 6.114: Loss = 0.40419
Epoch 6.115: Loss = 0.435974
Epoch 6.116: Loss = 0.544189
Epoch 6.117: Loss = 0.475952
Epoch 6.118: Loss = 0.408783
Epoch 6.119: Loss = 0.494507
Epoch 6.120: Loss = 0.506577
TRAIN LOSS = 0.447647
TRAIN ACC = 87.1902 % (52317/60000)
Loss = 0.359421
Loss = 0.50206
Loss = 0.595078
Loss = 0.564178
Loss = 0.635529
Loss = 0.406067
Loss = 0.423035
Loss = 0.724274
Loss = 0.620178
Loss = 0.526138
Loss = 0.207153
Loss = 0.326782
Loss = 0.376999
Loss = 0.396851
Loss = 0.241516
Loss = 0.307419
Loss = 0.271942
Loss = 0.0888519
Loss = 0.268448
Loss = 0.613815
TEST LOSS = 0.422787
TEST ACC = 523.169 % (8818/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.396454
Epoch 7.2: Loss = 0.457047
Epoch 7.3: Loss = 0.521072
Epoch 7.4: Loss = 0.395065
Epoch 7.5: Loss = 0.441666
Epoch 7.6: Loss = 0.413818
Epoch 7.7: Loss = 0.470825
Epoch 7.8: Loss = 0.42366
Epoch 7.9: Loss = 0.508484
Epoch 7.10: Loss = 0.585754
Epoch 7.11: Loss = 0.449066
Epoch 7.12: Loss = 0.454193
Epoch 7.13: Loss = 0.427994
Epoch 7.14: Loss = 0.529465
Epoch 7.15: Loss = 0.494827
Epoch 7.16: Loss = 0.42009
Epoch 7.17: Loss = 0.456696
Epoch 7.18: Loss = 0.322891
Epoch 7.19: Loss = 0.332703
Epoch 7.20: Loss = 0.441101
Epoch 7.21: Loss = 0.50473
Epoch 7.22: Loss = 0.353653
Epoch 7.23: Loss = 0.449097
Epoch 7.24: Loss = 0.428909
Epoch 7.25: Loss = 0.405838
Epoch 7.26: Loss = 0.381577
Epoch 7.27: Loss = 0.426956
Epoch 7.28: Loss = 0.62265
Epoch 7.29: Loss = 0.438797
Epoch 7.30: Loss = 0.360092
Epoch 7.31: Loss = 0.464447
Epoch 7.32: Loss = 0.376129
Epoch 7.33: Loss = 0.401581
Epoch 7.34: Loss = 0.380768
Epoch 7.35: Loss = 0.419312
Epoch 7.36: Loss = 0.434586
Epoch 7.37: Loss = 0.383255
Epoch 7.38: Loss = 0.394119
Epoch 7.39: Loss = 0.406799
Epoch 7.40: Loss = 0.442459
Epoch 7.41: Loss = 0.430771
Epoch 7.42: Loss = 0.449463
Epoch 7.43: Loss = 0.41835
Epoch 7.44: Loss = 0.418671
Epoch 7.45: Loss = 0.360229
Epoch 7.46: Loss = 0.534515
Epoch 7.47: Loss = 0.41423
Epoch 7.48: Loss = 0.441833
Epoch 7.49: Loss = 0.447433
Epoch 7.50: Loss = 0.498093
Epoch 7.51: Loss = 0.48291
Epoch 7.52: Loss = 0.415115
Epoch 7.53: Loss = 0.46521
Epoch 7.54: Loss = 0.520996
Epoch 7.55: Loss = 0.405823
Epoch 7.56: Loss = 0.437042
Epoch 7.57: Loss = 0.503067
Epoch 7.58: Loss = 0.385178
Epoch 7.59: Loss = 0.441681
Epoch 7.60: Loss = 0.498383
Epoch 7.61: Loss = 0.50824
Epoch 7.62: Loss = 0.536179
Epoch 7.63: Loss = 0.466736
Epoch 7.64: Loss = 0.446091
Epoch 7.65: Loss = 0.474182
Epoch 7.66: Loss = 0.56366
Epoch 7.67: Loss = 0.396484
Epoch 7.68: Loss = 0.427887
Epoch 7.69: Loss = 0.48204
Epoch 7.70: Loss = 0.489868
Epoch 7.71: Loss = 0.418533
Epoch 7.72: Loss = 0.464081
Epoch 7.73: Loss = 0.500412
Epoch 7.74: Loss = 0.443253
Epoch 7.75: Loss = 0.403671
Epoch 7.76: Loss = 0.397644
Epoch 7.77: Loss = 0.491699
Epoch 7.78: Loss = 0.46402
Epoch 7.79: Loss = 0.552063
Epoch 7.80: Loss = 0.470764
Epoch 7.81: Loss = 0.466461
Epoch 7.82: Loss = 0.41629
Epoch 7.83: Loss = 0.429688
Epoch 7.84: Loss = 0.462692
Epoch 7.85: Loss = 0.424377
Epoch 7.86: Loss = 0.409241
Epoch 7.87: Loss = 0.473724
Epoch 7.88: Loss = 0.427261
Epoch 7.89: Loss = 0.570587
Epoch 7.90: Loss = 0.459457
Epoch 7.91: Loss = 0.535812
Epoch 7.92: Loss = 0.495789
Epoch 7.93: Loss = 0.440643
Epoch 7.94: Loss = 0.540985
Epoch 7.95: Loss = 0.378662
Epoch 7.96: Loss = 0.459335
Epoch 7.97: Loss = 0.48027
Epoch 7.98: Loss = 0.503693
Epoch 7.99: Loss = 0.513
Epoch 7.100: Loss = 0.416977
Epoch 7.101: Loss = 0.417618
Epoch 7.102: Loss = 0.350113
Epoch 7.103: Loss = 0.454758
Epoch 7.104: Loss = 0.491486
Epoch 7.105: Loss = 0.417877
Epoch 7.106: Loss = 0.433319
Epoch 7.107: Loss = 0.4077
Epoch 7.108: Loss = 0.499664
Epoch 7.109: Loss = 0.412918
Epoch 7.110: Loss = 0.48793
Epoch 7.111: Loss = 0.373444
Epoch 7.112: Loss = 0.504654
Epoch 7.113: Loss = 0.456558
Epoch 7.114: Loss = 0.487854
Epoch 7.115: Loss = 0.444702
Epoch 7.116: Loss = 0.518158
Epoch 7.117: Loss = 0.495392
Epoch 7.118: Loss = 0.535629
Epoch 7.119: Loss = 0.480988
Epoch 7.120: Loss = 0.380524
TRAIN LOSS = 0.451172
TRAIN ACC = 87.413 % (52450/60000)
Loss = 0.364319
Loss = 0.550903
Loss = 0.601532
Loss = 0.584396
Loss = 0.627014
Loss = 0.418671
Loss = 0.451721
Loss = 0.706436
Loss = 0.628372
Loss = 0.542633
Loss = 0.210663
Loss = 0.316467
Loss = 0.415024
Loss = 0.395477
Loss = 0.241913
Loss = 0.318085
Loss = 0.242844
Loss = 0.072464
Loss = 0.268616
Loss = 0.579514
TEST LOSS = 0.426853
TEST ACC = 524.5 % (8812/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.430649
Epoch 8.2: Loss = 0.492554
Epoch 8.3: Loss = 0.570221
Epoch 8.4: Loss = 0.432922
Epoch 8.5: Loss = 0.441132
Epoch 8.6: Loss = 0.44812
Epoch 8.7: Loss = 0.472
Epoch 8.8: Loss = 0.477066
Epoch 8.9: Loss = 0.370529
Epoch 8.10: Loss = 0.600342
Epoch 8.11: Loss = 0.475235
Epoch 8.12: Loss = 0.399307
Epoch 8.13: Loss = 0.427246
Epoch 8.14: Loss = 0.558914
Epoch 8.15: Loss = 0.467102
Epoch 8.16: Loss = 0.334915
Epoch 8.17: Loss = 0.430191
Epoch 8.18: Loss = 0.345337
Epoch 8.19: Loss = 0.491623
Epoch 8.20: Loss = 0.457352
Epoch 8.21: Loss = 0.438385
Epoch 8.22: Loss = 0.528641
Epoch 8.23: Loss = 0.410538
Epoch 8.24: Loss = 0.410889
Epoch 8.25: Loss = 0.458694
Epoch 8.26: Loss = 0.442993
Epoch 8.27: Loss = 0.509644
Epoch 8.28: Loss = 0.406937
Epoch 8.29: Loss = 0.522003
Epoch 8.30: Loss = 0.448074
Epoch 8.31: Loss = 0.471527
Epoch 8.32: Loss = 0.492813
Epoch 8.33: Loss = 0.421158
Epoch 8.34: Loss = 0.552307
Epoch 8.35: Loss = 0.410843
Epoch 8.36: Loss = 0.447754
Epoch 8.37: Loss = 0.326645
Epoch 8.38: Loss = 0.475525
Epoch 8.39: Loss = 0.401917
Epoch 8.40: Loss = 0.557755
Epoch 8.41: Loss = 0.351654
Epoch 8.42: Loss = 0.373627
Epoch 8.43: Loss = 0.411224
Epoch 8.44: Loss = 0.471008
Epoch 8.45: Loss = 0.513779
Epoch 8.46: Loss = 0.436081
Epoch 8.47: Loss = 0.433289
Epoch 8.48: Loss = 0.455994
Epoch 8.49: Loss = 0.413757
Epoch 8.50: Loss = 0.490662
Epoch 8.51: Loss = 0.450928
Epoch 8.52: Loss = 0.468933
Epoch 8.53: Loss = 0.53241
Epoch 8.54: Loss = 0.461166
Epoch 8.55: Loss = 0.435669
Epoch 8.56: Loss = 0.566513
Epoch 8.57: Loss = 0.468536
Epoch 8.58: Loss = 0.458374
Epoch 8.59: Loss = 0.470993
Epoch 8.60: Loss = 0.452042
Epoch 8.61: Loss = 0.489944
Epoch 8.62: Loss = 0.365356
Epoch 8.63: Loss = 0.424438
Epoch 8.64: Loss = 0.452942
Epoch 8.65: Loss = 0.502106
Epoch 8.66: Loss = 0.472183
Epoch 8.67: Loss = 0.451965
Epoch 8.68: Loss = 0.465973
Epoch 8.69: Loss = 0.496109
Epoch 8.70: Loss = 0.505295
Epoch 8.71: Loss = 0.420609
Epoch 8.72: Loss = 0.456635
Epoch 8.73: Loss = 0.553741
Epoch 8.74: Loss = 0.477112
Epoch 8.75: Loss = 0.542068
Epoch 8.76: Loss = 0.382156
Epoch 8.77: Loss = 0.348328
Epoch 8.78: Loss = 0.513321
Epoch 8.79: Loss = 0.529526
Epoch 8.80: Loss = 0.46315
Epoch 8.81: Loss = 0.52388
Epoch 8.82: Loss = 0.4543
Epoch 8.83: Loss = 0.432755
Epoch 8.84: Loss = 0.450638
Epoch 8.85: Loss = 0.4879
Epoch 8.86: Loss = 0.453491
Epoch 8.87: Loss = 0.561249
Epoch 8.88: Loss = 0.372223
Epoch 8.89: Loss = 0.376312
Epoch 8.90: Loss = 0.399811
Epoch 8.91: Loss = 0.396729
Epoch 8.92: Loss = 0.471146
Epoch 8.93: Loss = 0.435974
Epoch 8.94: Loss = 0.415344
Epoch 8.95: Loss = 0.505585
Epoch 8.96: Loss = 0.318756
Epoch 8.97: Loss = 0.47966
Epoch 8.98: Loss = 0.466385
Epoch 8.99: Loss = 0.40033
Epoch 8.100: Loss = 0.45723
Epoch 8.101: Loss = 0.350891
Epoch 8.102: Loss = 0.460358
Epoch 8.103: Loss = 0.493057
Epoch 8.104: Loss = 0.572678
Epoch 8.105: Loss = 0.473648
Epoch 8.106: Loss = 0.471039
Epoch 8.107: Loss = 0.413513
Epoch 8.108: Loss = 0.320511
Epoch 8.109: Loss = 0.565506
Epoch 8.110: Loss = 0.626816
Epoch 8.111: Loss = 0.534897
Epoch 8.112: Loss = 0.418732
Epoch 8.113: Loss = 0.495087
Epoch 8.114: Loss = 0.568848
Epoch 8.115: Loss = 0.470398
Epoch 8.116: Loss = 0.460236
Epoch 8.117: Loss = 0.461807
Epoch 8.118: Loss = 0.380875
Epoch 8.119: Loss = 0.547379
Epoch 8.120: Loss = 0.418289
TRAIN LOSS = 0.458755
TRAIN ACC = 87.3093 % (52388/60000)
Loss = 0.373535
Loss = 0.567307
Loss = 0.614349
Loss = 0.594101
Loss = 0.637299
Loss = 0.421432
Loss = 0.467468
Loss = 0.737045
Loss = 0.631256
Loss = 0.571274
Loss = 0.203445
Loss = 0.409409
Loss = 0.415558
Loss = 0.385208
Loss = 0.235535
Loss = 0.317444
Loss = 0.291962
Loss = 0.0639343
Loss = 0.296906
Loss = 0.603394
TEST LOSS = 0.441893
TEST ACC = 523.878 % (8810/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.494202
Epoch 9.2: Loss = 0.483612
Epoch 9.3: Loss = 0.363022
Epoch 9.4: Loss = 0.412582
Epoch 9.5: Loss = 0.404434
Epoch 9.6: Loss = 0.474136
Epoch 9.7: Loss = 0.459427
Epoch 9.8: Loss = 0.458099
Epoch 9.9: Loss = 0.456573
Epoch 9.10: Loss = 0.465652
Epoch 9.11: Loss = 0.484177
Epoch 9.12: Loss = 0.575333
Epoch 9.13: Loss = 0.465683
Epoch 9.14: Loss = 0.462891
Epoch 9.15: Loss = 0.434998
Epoch 9.16: Loss = 0.422806
Epoch 9.17: Loss = 0.425644
Epoch 9.18: Loss = 0.396103
Epoch 9.19: Loss = 0.481628
Epoch 9.20: Loss = 0.404129
Epoch 9.21: Loss = 0.494156
Epoch 9.22: Loss = 0.466217
Epoch 9.23: Loss = 0.496506
Epoch 9.24: Loss = 0.381088
Epoch 9.25: Loss = 0.337265
Epoch 9.26: Loss = 0.419891
Epoch 9.27: Loss = 0.533447
Epoch 9.28: Loss = 0.385864
Epoch 9.29: Loss = 0.501709
Epoch 9.30: Loss = 0.448669
Epoch 9.31: Loss = 0.438293
Epoch 9.32: Loss = 0.469543
Epoch 9.33: Loss = 0.536057
Epoch 9.34: Loss = 0.465652
Epoch 9.35: Loss = 0.532913
Epoch 9.36: Loss = 0.496063
Epoch 9.37: Loss = 0.373184
Epoch 9.38: Loss = 0.495087
Epoch 9.39: Loss = 0.488098
Epoch 9.40: Loss = 0.554169
Epoch 9.41: Loss = 0.435043
Epoch 9.42: Loss = 0.466217
Epoch 9.43: Loss = 0.466705
Epoch 9.44: Loss = 0.481506
Epoch 9.45: Loss = 0.441895
Epoch 9.46: Loss = 0.518097
Epoch 9.47: Loss = 0.46286
Epoch 9.48: Loss = 0.553207
Epoch 9.49: Loss = 0.367065
Epoch 9.50: Loss = 0.612747
Epoch 9.51: Loss = 0.402069
Epoch 9.52: Loss = 0.540497
Epoch 9.53: Loss = 0.44574
Epoch 9.54: Loss = 0.438095
Epoch 9.55: Loss = 0.618835
Epoch 9.56: Loss = 0.459915
Epoch 9.57: Loss = 0.406998
Epoch 9.58: Loss = 0.504913
Epoch 9.59: Loss = 0.438538
Epoch 9.60: Loss = 0.43428
Epoch 9.61: Loss = 0.425415
Epoch 9.62: Loss = 0.43634
Epoch 9.63: Loss = 0.535736
Epoch 9.64: Loss = 0.472046
Epoch 9.65: Loss = 0.509354
Epoch 9.66: Loss = 0.392715
Epoch 9.67: Loss = 0.475403
Epoch 9.68: Loss = 0.506516
Epoch 9.69: Loss = 0.380173
Epoch 9.70: Loss = 0.463013
Epoch 9.71: Loss = 0.523468
Epoch 9.72: Loss = 0.469955
Epoch 9.73: Loss = 0.513229
Epoch 9.74: Loss = 0.55896
Epoch 9.75: Loss = 0.458206
Epoch 9.76: Loss = 0.448471
Epoch 9.77: Loss = 0.40712
Epoch 9.78: Loss = 0.412048
Epoch 9.79: Loss = 0.4664
Epoch 9.80: Loss = 0.459183
Epoch 9.81: Loss = 0.458786
Epoch 9.82: Loss = 0.476669
Epoch 9.83: Loss = 0.499298
Epoch 9.84: Loss = 0.508408
Epoch 9.85: Loss = 0.382629
Epoch 9.86: Loss = 0.548004
Epoch 9.87: Loss = 0.48732
Epoch 9.88: Loss = 0.541016
Epoch 9.89: Loss = 0.4263
Epoch 9.90: Loss = 0.362457
Epoch 9.91: Loss = 0.426941
Epoch 9.92: Loss = 0.505508
Epoch 9.93: Loss = 0.443481
Epoch 9.94: Loss = 0.41394
Epoch 9.95: Loss = 0.522964
Epoch 9.96: Loss = 0.542068
Epoch 9.97: Loss = 0.600006
Epoch 9.98: Loss = 0.568024
Epoch 9.99: Loss = 0.516861
Epoch 9.100: Loss = 0.615356
Epoch 9.101: Loss = 0.453171
Epoch 9.102: Loss = 0.484116
Epoch 9.103: Loss = 0.556259
Epoch 9.104: Loss = 0.503738
Epoch 9.105: Loss = 0.457184
Epoch 9.106: Loss = 0.391434
Epoch 9.107: Loss = 0.436035
Epoch 9.108: Loss = 0.457947
Epoch 9.109: Loss = 0.482346
Epoch 9.110: Loss = 0.435684
Epoch 9.111: Loss = 0.406311
Epoch 9.112: Loss = 0.58519
Epoch 9.113: Loss = 0.503845
Epoch 9.114: Loss = 0.449402
Epoch 9.115: Loss = 0.385361
Epoch 9.116: Loss = 0.496689
Epoch 9.117: Loss = 0.455627
Epoch 9.118: Loss = 0.481644
Epoch 9.119: Loss = 0.464859
Epoch 9.120: Loss = 0.583664
TRAIN LOSS = 0.470551
TRAIN ACC = 87.4527 % (52474/60000)
Loss = 0.399628
Loss = 0.580185
Loss = 0.622559
Loss = 0.625473
Loss = 0.646103
Loss = 0.416306
Loss = 0.472748
Loss = 0.7668
Loss = 0.633316
Loss = 0.589493
Loss = 0.209076
Loss = 0.408493
Loss = 0.40625
Loss = 0.405167
Loss = 0.224945
Loss = 0.327454
Loss = 0.266724
Loss = 0.0753326
Loss = 0.288895
Loss = 0.613983
TEST LOSS = 0.448946
TEST ACC = 524.739 % (8798/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.482391
Epoch 10.2: Loss = 0.447189
Epoch 10.3: Loss = 0.407837
Epoch 10.4: Loss = 0.45163
Epoch 10.5: Loss = 0.483292
Epoch 10.6: Loss = 0.600876
Epoch 10.7: Loss = 0.512878
Epoch 10.8: Loss = 0.43219
Epoch 10.9: Loss = 0.456253
Epoch 10.10: Loss = 0.46344
Epoch 10.11: Loss = 0.586761
Epoch 10.12: Loss = 0.411453
Epoch 10.13: Loss = 0.453003
Epoch 10.14: Loss = 0.507599
Epoch 10.15: Loss = 0.601089
Epoch 10.16: Loss = 0.4375
Epoch 10.17: Loss = 0.414459
Epoch 10.18: Loss = 0.473633
Epoch 10.19: Loss = 0.357544
Epoch 10.20: Loss = 0.517654
Epoch 10.21: Loss = 0.349197
Epoch 10.22: Loss = 0.618668
Epoch 10.23: Loss = 0.537125
Epoch 10.24: Loss = 0.476379
Epoch 10.25: Loss = 0.472504
Epoch 10.26: Loss = 0.459625
Epoch 10.27: Loss = 0.563919
Epoch 10.28: Loss = 0.595901
Epoch 10.29: Loss = 0.44136
Epoch 10.30: Loss = 0.553635
Epoch 10.31: Loss = 0.471344
Epoch 10.32: Loss = 0.490784
Epoch 10.33: Loss = 0.546112
Epoch 10.34: Loss = 0.40007
Epoch 10.35: Loss = 0.522385
Epoch 10.36: Loss = 0.453735
Epoch 10.37: Loss = 0.554886
Epoch 10.38: Loss = 0.561249
Epoch 10.39: Loss = 0.533432
Epoch 10.40: Loss = 0.488831
Epoch 10.41: Loss = 0.466553
Epoch 10.42: Loss = 0.532928
Epoch 10.43: Loss = 0.437988
Epoch 10.44: Loss = 0.5354
Epoch 10.45: Loss = 0.458405
Epoch 10.46: Loss = 0.541229
Epoch 10.47: Loss = 0.506836
Epoch 10.48: Loss = 0.447433
Epoch 10.49: Loss = 0.422028
Epoch 10.50: Loss = 0.491913
Epoch 10.51: Loss = 0.457748
Epoch 10.52: Loss = 0.435837
Epoch 10.53: Loss = 0.523926
Epoch 10.54: Loss = 0.468658
Epoch 10.55: Loss = 0.529236
Epoch 10.56: Loss = 0.434235
Epoch 10.57: Loss = 0.499237
Epoch 10.58: Loss = 0.504791
Epoch 10.59: Loss = 0.449188
Epoch 10.60: Loss = 0.434692
Epoch 10.61: Loss = 0.576355
Epoch 10.62: Loss = 0.524567
Epoch 10.63: Loss = 0.463242
Epoch 10.64: Loss = 0.509781
Epoch 10.65: Loss = 0.4776
Epoch 10.66: Loss = 0.414581
Epoch 10.67: Loss = 0.525726
Epoch 10.68: Loss = 0.502365
Epoch 10.69: Loss = 0.480804
Epoch 10.70: Loss = 0.354355
Epoch 10.71: Loss = 0.511627
Epoch 10.72: Loss = 0.356003
Epoch 10.73: Loss = 0.52655
Epoch 10.74: Loss = 0.482788
Epoch 10.75: Loss = 0.377502
Epoch 10.76: Loss = 0.454651
Epoch 10.77: Loss = 0.493622
Epoch 10.78: Loss = 0.450943
Epoch 10.79: Loss = 0.404968
Epoch 10.80: Loss = 0.466644
Epoch 10.81: Loss = 0.43248
Epoch 10.82: Loss = 0.368774
Epoch 10.83: Loss = 0.531265
Epoch 10.84: Loss = 0.488083
Epoch 10.85: Loss = 0.416672
Epoch 10.86: Loss = 0.53804
Epoch 10.87: Loss = 0.383911
Epoch 10.88: Loss = 0.491669
Epoch 10.89: Loss = 0.38765
Epoch 10.90: Loss = 0.509384
Epoch 10.91: Loss = 0.449814
Epoch 10.92: Loss = 0.365784
Epoch 10.93: Loss = 0.397888
Epoch 10.94: Loss = 0.41098
Epoch 10.95: Loss = 0.480927
Epoch 10.96: Loss = 0.379913
Epoch 10.97: Loss = 0.522278
Epoch 10.98: Loss = 0.521179
Epoch 10.99: Loss = 0.360672
Epoch 10.100: Loss = 0.450623
Epoch 10.101: Loss = 0.441803
Epoch 10.102: Loss = 0.329605
Epoch 10.103: Loss = 0.454941
Epoch 10.104: Loss = 0.553329
Epoch 10.105: Loss = 0.467941
Epoch 10.106: Loss = 0.376984
Epoch 10.107: Loss = 0.549576
Epoch 10.108: Loss = 0.359131
Epoch 10.109: Loss = 0.664734
Epoch 10.110: Loss = 0.422318
Epoch 10.111: Loss = 0.465958
Epoch 10.112: Loss = 0.568176
Epoch 10.113: Loss = 0.429092
Epoch 10.114: Loss = 0.42363
Epoch 10.115: Loss = 0.548584
Epoch 10.116: Loss = 0.504868
Epoch 10.117: Loss = 0.435745
Epoch 10.118: Loss = 0.578629
Epoch 10.119: Loss = 0.493362
Epoch 10.120: Loss = 0.508545
TRAIN LOSS = 0.474899
TRAIN ACC = 87.5244 % (52517/60000)
Loss = 0.389969
Loss = 0.552429
Loss = 0.609314
Loss = 0.600403
Loss = 0.649994
Loss = 0.394104
Loss = 0.455017
Loss = 0.784851
Loss = 0.656677
Loss = 0.590927
Loss = 0.212326
Loss = 0.37944
Loss = 0.386658
Loss = 0.413406
Loss = 0.219543
Loss = 0.337769
Loss = 0.26619
Loss = 0.0753479
Loss = 0.247177
Loss = 0.583038
TEST LOSS = 0.440229
TEST ACC = 525.169 % (8841/10000)

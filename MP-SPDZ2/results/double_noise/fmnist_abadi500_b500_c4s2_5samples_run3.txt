Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.41391
Epoch 1.2: Loss = 2.30519
Epoch 1.3: Loss = 2.21715
Epoch 1.4: Loss = 2.1601
Epoch 1.5: Loss = 2.08736
Epoch 1.6: Loss = 1.99622
Epoch 1.7: Loss = 1.95313
Epoch 1.8: Loss = 1.87529
Epoch 1.9: Loss = 1.80795
Epoch 1.10: Loss = 1.71213
Epoch 1.11: Loss = 1.72514
Epoch 1.12: Loss = 1.71552
Epoch 1.13: Loss = 1.59996
Epoch 1.14: Loss = 1.58435
Epoch 1.15: Loss = 1.52742
Epoch 1.16: Loss = 1.52034
Epoch 1.17: Loss = 1.44168
Epoch 1.18: Loss = 1.46658
Epoch 1.19: Loss = 1.42426
Epoch 1.20: Loss = 1.37492
Epoch 1.21: Loss = 1.3597
Epoch 1.22: Loss = 1.34113
Epoch 1.23: Loss = 1.25528
Epoch 1.24: Loss = 1.27411
Epoch 1.25: Loss = 1.23503
Epoch 1.26: Loss = 1.2567
Epoch 1.27: Loss = 1.209
Epoch 1.28: Loss = 1.15388
Epoch 1.29: Loss = 1.15199
Epoch 1.30: Loss = 1.16252
Epoch 1.31: Loss = 1.14183
Epoch 1.32: Loss = 1.12326
Epoch 1.33: Loss = 1.112
Epoch 1.34: Loss = 1.08711
Epoch 1.35: Loss = 1.03683
Epoch 1.36: Loss = 1.08842
Epoch 1.37: Loss = 0.994949
Epoch 1.38: Loss = 1.06612
Epoch 1.39: Loss = 1.02594
Epoch 1.40: Loss = 1.01945
Epoch 1.41: Loss = 1.01631
Epoch 1.42: Loss = 1.00844
Epoch 1.43: Loss = 1.00288
Epoch 1.44: Loss = 0.969223
Epoch 1.45: Loss = 1.00629
Epoch 1.46: Loss = 0.968094
Epoch 1.47: Loss = 0.962997
Epoch 1.48: Loss = 0.97641
Epoch 1.49: Loss = 1.00444
Epoch 1.50: Loss = 0.899765
Epoch 1.51: Loss = 0.844772
Epoch 1.52: Loss = 0.882202
Epoch 1.53: Loss = 0.937332
Epoch 1.54: Loss = 0.958572
Epoch 1.55: Loss = 0.965302
Epoch 1.56: Loss = 1.00154
Epoch 1.57: Loss = 0.887619
Epoch 1.58: Loss = 0.840988
Epoch 1.59: Loss = 0.88353
Epoch 1.60: Loss = 0.964447
Epoch 1.61: Loss = 0.85463
Epoch 1.62: Loss = 0.912567
Epoch 1.63: Loss = 0.923752
Epoch 1.64: Loss = 0.853241
Epoch 1.65: Loss = 0.847946
Epoch 1.66: Loss = 0.888824
Epoch 1.67: Loss = 0.862808
Epoch 1.68: Loss = 0.833664
Epoch 1.69: Loss = 0.888565
Epoch 1.70: Loss = 0.799652
Epoch 1.71: Loss = 0.876526
Epoch 1.72: Loss = 0.885971
Epoch 1.73: Loss = 0.877686
Epoch 1.74: Loss = 0.816849
Epoch 1.75: Loss = 0.829742
Epoch 1.76: Loss = 0.875473
Epoch 1.77: Loss = 0.830582
Epoch 1.78: Loss = 0.835114
Epoch 1.79: Loss = 0.797501
Epoch 1.80: Loss = 0.782944
Epoch 1.81: Loss = 0.774963
Epoch 1.82: Loss = 0.830322
Epoch 1.83: Loss = 0.803146
Epoch 1.84: Loss = 0.832428
Epoch 1.85: Loss = 0.800674
Epoch 1.86: Loss = 0.846481
Epoch 1.87: Loss = 0.772583
Epoch 1.88: Loss = 0.814499
Epoch 1.89: Loss = 0.772705
Epoch 1.90: Loss = 0.793442
Epoch 1.91: Loss = 0.754059
Epoch 1.92: Loss = 0.746506
Epoch 1.93: Loss = 0.747269
Epoch 1.94: Loss = 0.880753
Epoch 1.95: Loss = 0.769424
Epoch 1.96: Loss = 0.859329
Epoch 1.97: Loss = 0.719864
Epoch 1.98: Loss = 0.78656
Epoch 1.99: Loss = 0.718811
Epoch 1.100: Loss = 0.722198
Epoch 1.101: Loss = 0.706573
Epoch 1.102: Loss = 0.748322
Epoch 1.103: Loss = 0.72583
Epoch 1.104: Loss = 0.719772
Epoch 1.105: Loss = 0.721603
Epoch 1.106: Loss = 0.726822
Epoch 1.107: Loss = 0.772629
Epoch 1.108: Loss = 0.68692
Epoch 1.109: Loss = 0.731873
Epoch 1.110: Loss = 0.728043
Epoch 1.111: Loss = 0.753525
Epoch 1.112: Loss = 0.764252
Epoch 1.113: Loss = 0.756012
Epoch 1.114: Loss = 0.748962
Epoch 1.115: Loss = 0.733978
Epoch 1.116: Loss = 0.756454
Epoch 1.117: Loss = 0.695618
Epoch 1.118: Loss = 0.744949
Epoch 1.119: Loss = 0.611954
Epoch 1.120: Loss = 0.702606
TRAIN LOSS = 1.04454
TRAIN ACC = 65.8371 % (39504/60000)
Loss = 0.670502
Loss = 0.812744
Loss = 0.757141
Loss = 0.671921
Loss = 0.670715
Loss = 0.824173
Loss = 0.841278
Loss = 0.792999
Loss = 0.723038
Loss = 0.689499
Loss = 0.79097
Loss = 0.7435
Loss = 0.765396
Loss = 0.75882
Loss = 0.739517
Loss = 0.802383
Loss = 0.705078
Loss = 0.758789
Loss = 0.779205
Loss = 0.731445
TEST LOSS = 0.751455
TEST ACC = 395.039 % (7331/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.751556
Epoch 2.2: Loss = 0.725708
Epoch 2.3: Loss = 0.764069
Epoch 2.4: Loss = 0.785156
Epoch 2.5: Loss = 0.736176
Epoch 2.6: Loss = 0.745697
Epoch 2.7: Loss = 0.788498
Epoch 2.8: Loss = 0.780945
Epoch 2.9: Loss = 0.759552
Epoch 2.10: Loss = 0.706161
Epoch 2.11: Loss = 0.720673
Epoch 2.12: Loss = 0.783417
Epoch 2.13: Loss = 0.671646
Epoch 2.14: Loss = 0.696671
Epoch 2.15: Loss = 0.671768
Epoch 2.16: Loss = 0.720856
Epoch 2.17: Loss = 0.698059
Epoch 2.18: Loss = 0.642181
Epoch 2.19: Loss = 0.761124
Epoch 2.20: Loss = 0.760757
Epoch 2.21: Loss = 0.688461
Epoch 2.22: Loss = 0.76265
Epoch 2.23: Loss = 0.667282
Epoch 2.24: Loss = 0.702179
Epoch 2.25: Loss = 0.754059
Epoch 2.26: Loss = 0.646576
Epoch 2.27: Loss = 0.698395
Epoch 2.28: Loss = 0.659943
Epoch 2.29: Loss = 0.67688
Epoch 2.30: Loss = 0.621613
Epoch 2.31: Loss = 0.739639
Epoch 2.32: Loss = 0.686981
Epoch 2.33: Loss = 0.730499
Epoch 2.34: Loss = 0.672424
Epoch 2.35: Loss = 0.745712
Epoch 2.36: Loss = 0.658707
Epoch 2.37: Loss = 0.753082
Epoch 2.38: Loss = 0.653961
Epoch 2.39: Loss = 0.719376
Epoch 2.40: Loss = 0.703171
Epoch 2.41: Loss = 0.637466
Epoch 2.42: Loss = 0.653091
Epoch 2.43: Loss = 0.737656
Epoch 2.44: Loss = 0.692032
Epoch 2.45: Loss = 0.67662
Epoch 2.46: Loss = 0.643036
Epoch 2.47: Loss = 0.69249
Epoch 2.48: Loss = 0.766052
Epoch 2.49: Loss = 0.619675
Epoch 2.50: Loss = 0.732712
Epoch 2.51: Loss = 0.677246
Epoch 2.52: Loss = 0.681641
Epoch 2.53: Loss = 0.660263
Epoch 2.54: Loss = 0.709259
Epoch 2.55: Loss = 0.725388
Epoch 2.56: Loss = 0.688354
Epoch 2.57: Loss = 0.725861
Epoch 2.58: Loss = 0.650558
Epoch 2.59: Loss = 0.677338
Epoch 2.60: Loss = 0.668549
Epoch 2.61: Loss = 0.671143
Epoch 2.62: Loss = 0.770645
Epoch 2.63: Loss = 0.657028
Epoch 2.64: Loss = 0.722015
Epoch 2.65: Loss = 0.711517
Epoch 2.66: Loss = 0.653519
Epoch 2.67: Loss = 0.630432
Epoch 2.68: Loss = 0.623199
Epoch 2.69: Loss = 0.589417
Epoch 2.70: Loss = 0.642517
Epoch 2.71: Loss = 0.669159
Epoch 2.72: Loss = 0.682449
Epoch 2.73: Loss = 0.668213
Epoch 2.74: Loss = 0.732483
Epoch 2.75: Loss = 0.694336
Epoch 2.76: Loss = 0.650986
Epoch 2.77: Loss = 0.6604
Epoch 2.78: Loss = 0.741104
Epoch 2.79: Loss = 0.664307
Epoch 2.80: Loss = 0.610016
Epoch 2.81: Loss = 0.719009
Epoch 2.82: Loss = 0.623489
Epoch 2.83: Loss = 0.652939
Epoch 2.84: Loss = 0.654602
Epoch 2.85: Loss = 0.647568
Epoch 2.86: Loss = 0.622864
Epoch 2.87: Loss = 0.662506
Epoch 2.88: Loss = 0.714737
Epoch 2.89: Loss = 0.650589
Epoch 2.90: Loss = 0.647171
Epoch 2.91: Loss = 0.672241
Epoch 2.92: Loss = 0.72142
Epoch 2.93: Loss = 0.690186
Epoch 2.94: Loss = 0.615677
Epoch 2.95: Loss = 0.672729
Epoch 2.96: Loss = 0.650726
Epoch 2.97: Loss = 0.68219
Epoch 2.98: Loss = 0.649155
Epoch 2.99: Loss = 0.705627
Epoch 2.100: Loss = 0.702637
Epoch 2.101: Loss = 0.691772
Epoch 2.102: Loss = 0.623138
Epoch 2.103: Loss = 0.70372
Epoch 2.104: Loss = 0.696869
Epoch 2.105: Loss = 0.686646
Epoch 2.106: Loss = 0.706696
Epoch 2.107: Loss = 0.740692
Epoch 2.108: Loss = 0.650513
Epoch 2.109: Loss = 0.722656
Epoch 2.110: Loss = 0.65416
Epoch 2.111: Loss = 0.77977
Epoch 2.112: Loss = 0.653229
Epoch 2.113: Loss = 0.645584
Epoch 2.114: Loss = 0.787094
Epoch 2.115: Loss = 0.664047
Epoch 2.116: Loss = 0.697861
Epoch 2.117: Loss = 0.642075
Epoch 2.118: Loss = 0.698181
Epoch 2.119: Loss = 0.685974
Epoch 2.120: Loss = 0.743927
TRAIN LOSS = 0.691727
TRAIN ACC = 76.4313 % (45861/60000)
Loss = 0.60701
Loss = 0.738266
Loss = 0.656006
Loss = 0.600204
Loss = 0.592896
Loss = 0.757553
Loss = 0.789978
Loss = 0.734283
Loss = 0.653961
Loss = 0.610565
Loss = 0.737244
Loss = 0.709213
Loss = 0.675919
Loss = 0.682877
Loss = 0.668472
Loss = 0.724548
Loss = 0.607468
Loss = 0.703033
Loss = 0.714981
Loss = 0.655685
TEST LOSS = 0.681008
TEST ACC = 458.609 % (7664/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.621094
Epoch 3.2: Loss = 0.623276
Epoch 3.3: Loss = 0.789566
Epoch 3.4: Loss = 0.7612
Epoch 3.5: Loss = 0.758728
Epoch 3.6: Loss = 0.591843
Epoch 3.7: Loss = 0.624741
Epoch 3.8: Loss = 0.706802
Epoch 3.9: Loss = 0.660141
Epoch 3.10: Loss = 0.718918
Epoch 3.11: Loss = 0.651886
Epoch 3.12: Loss = 0.777802
Epoch 3.13: Loss = 0.643127
Epoch 3.14: Loss = 0.612305
Epoch 3.15: Loss = 0.645477
Epoch 3.16: Loss = 0.67131
Epoch 3.17: Loss = 0.68927
Epoch 3.18: Loss = 0.629196
Epoch 3.19: Loss = 0.635864
Epoch 3.20: Loss = 0.608368
Epoch 3.21: Loss = 0.730682
Epoch 3.22: Loss = 0.673065
Epoch 3.23: Loss = 0.633209
Epoch 3.24: Loss = 0.716064
Epoch 3.25: Loss = 0.752197
Epoch 3.26: Loss = 0.685089
Epoch 3.27: Loss = 0.694382
Epoch 3.28: Loss = 0.674484
Epoch 3.29: Loss = 0.711029
Epoch 3.30: Loss = 0.660202
Epoch 3.31: Loss = 0.718765
Epoch 3.32: Loss = 0.57515
Epoch 3.33: Loss = 0.629822
Epoch 3.34: Loss = 0.618057
Epoch 3.35: Loss = 0.598618
Epoch 3.36: Loss = 0.727325
Epoch 3.37: Loss = 0.736252
Epoch 3.38: Loss = 0.738266
Epoch 3.39: Loss = 0.650635
Epoch 3.40: Loss = 0.652817
Epoch 3.41: Loss = 0.701859
Epoch 3.42: Loss = 0.608597
Epoch 3.43: Loss = 0.573456
Epoch 3.44: Loss = 0.695419
Epoch 3.45: Loss = 0.662262
Epoch 3.46: Loss = 0.613937
Epoch 3.47: Loss = 0.605865
Epoch 3.48: Loss = 0.591431
Epoch 3.49: Loss = 0.555313
Epoch 3.50: Loss = 0.628555
Epoch 3.51: Loss = 0.648727
Epoch 3.52: Loss = 0.655792
Epoch 3.53: Loss = 0.661514
Epoch 3.54: Loss = 0.620621
Epoch 3.55: Loss = 0.593994
Epoch 3.56: Loss = 0.551376
Epoch 3.57: Loss = 0.688751
Epoch 3.58: Loss = 0.634903
Epoch 3.59: Loss = 0.600372
Epoch 3.60: Loss = 0.650116
Epoch 3.61: Loss = 0.649597
Epoch 3.62: Loss = 0.654739
Epoch 3.63: Loss = 0.612732
Epoch 3.64: Loss = 0.624557
Epoch 3.65: Loss = 0.710526
Epoch 3.66: Loss = 0.644135
Epoch 3.67: Loss = 0.673477
Epoch 3.68: Loss = 0.689972
Epoch 3.69: Loss = 0.764267
Epoch 3.70: Loss = 0.630951
Epoch 3.71: Loss = 0.667267
Epoch 3.72: Loss = 0.635651
Epoch 3.73: Loss = 0.633163
Epoch 3.74: Loss = 0.670456
Epoch 3.75: Loss = 0.622711
Epoch 3.76: Loss = 0.696243
Epoch 3.77: Loss = 0.675415
Epoch 3.78: Loss = 0.707535
Epoch 3.79: Loss = 0.689209
Epoch 3.80: Loss = 0.626724
Epoch 3.81: Loss = 0.655334
Epoch 3.82: Loss = 0.506149
Epoch 3.83: Loss = 0.679688
Epoch 3.84: Loss = 0.699188
Epoch 3.85: Loss = 0.643585
Epoch 3.86: Loss = 0.782089
Epoch 3.87: Loss = 0.661789
Epoch 3.88: Loss = 0.663956
Epoch 3.89: Loss = 0.555374
Epoch 3.90: Loss = 0.668655
Epoch 3.91: Loss = 0.666992
Epoch 3.92: Loss = 0.625122
Epoch 3.93: Loss = 0.562988
Epoch 3.94: Loss = 0.688202
Epoch 3.95: Loss = 0.658279
Epoch 3.96: Loss = 0.698288
Epoch 3.97: Loss = 0.736969
Epoch 3.98: Loss = 0.581299
Epoch 3.99: Loss = 0.673096
Epoch 3.100: Loss = 0.573639
Epoch 3.101: Loss = 0.655991
Epoch 3.102: Loss = 0.73381
Epoch 3.103: Loss = 0.551361
Epoch 3.104: Loss = 0.534744
Epoch 3.105: Loss = 0.612762
Epoch 3.106: Loss = 0.595093
Epoch 3.107: Loss = 0.701035
Epoch 3.108: Loss = 0.668167
Epoch 3.109: Loss = 0.647232
Epoch 3.110: Loss = 0.603897
Epoch 3.111: Loss = 0.63385
Epoch 3.112: Loss = 0.626846
Epoch 3.113: Loss = 0.648422
Epoch 3.114: Loss = 0.648376
Epoch 3.115: Loss = 0.622437
Epoch 3.116: Loss = 0.773621
Epoch 3.117: Loss = 0.679764
Epoch 3.118: Loss = 0.608551
Epoch 3.119: Loss = 0.710144
Epoch 3.120: Loss = 0.635696
TRAIN LOSS = 0.655777
TRAIN ACC = 78.2394 % (46945/60000)
Loss = 0.598541
Loss = 0.704285
Loss = 0.651703
Loss = 0.603806
Loss = 0.589325
Loss = 0.737732
Loss = 0.78775
Loss = 0.701187
Loss = 0.623093
Loss = 0.592972
Loss = 0.748047
Loss = 0.733795
Loss = 0.657486
Loss = 0.669632
Loss = 0.653122
Loss = 0.676819
Loss = 0.613266
Loss = 0.686523
Loss = 0.70636
Loss = 0.644516
TEST LOSS = 0.668998
TEST ACC = 469.449 % (7805/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.688126
Epoch 4.2: Loss = 0.613892
Epoch 4.3: Loss = 0.722031
Epoch 4.4: Loss = 0.697678
Epoch 4.5: Loss = 0.781219
Epoch 4.6: Loss = 0.639236
Epoch 4.7: Loss = 0.683395
Epoch 4.8: Loss = 0.73056
Epoch 4.9: Loss = 0.679642
Epoch 4.10: Loss = 0.723618
Epoch 4.11: Loss = 0.667847
Epoch 4.12: Loss = 0.557083
Epoch 4.13: Loss = 0.636108
Epoch 4.14: Loss = 0.742966
Epoch 4.15: Loss = 0.623428
Epoch 4.16: Loss = 0.620865
Epoch 4.17: Loss = 0.658829
Epoch 4.18: Loss = 0.649307
Epoch 4.19: Loss = 0.658203
Epoch 4.20: Loss = 0.692184
Epoch 4.21: Loss = 0.675446
Epoch 4.22: Loss = 0.575821
Epoch 4.23: Loss = 0.595459
Epoch 4.24: Loss = 0.606186
Epoch 4.25: Loss = 0.671844
Epoch 4.26: Loss = 0.560135
Epoch 4.27: Loss = 0.57901
Epoch 4.28: Loss = 0.589172
Epoch 4.29: Loss = 0.606369
Epoch 4.30: Loss = 0.668793
Epoch 4.31: Loss = 0.704224
Epoch 4.32: Loss = 0.738968
Epoch 4.33: Loss = 0.626114
Epoch 4.34: Loss = 0.63913
Epoch 4.35: Loss = 0.795197
Epoch 4.36: Loss = 0.518082
Epoch 4.37: Loss = 0.56218
Epoch 4.38: Loss = 0.611404
Epoch 4.39: Loss = 0.6577
Epoch 4.40: Loss = 0.620117
Epoch 4.41: Loss = 0.682953
Epoch 4.42: Loss = 0.759232
Epoch 4.43: Loss = 0.667999
Epoch 4.44: Loss = 0.721542
Epoch 4.45: Loss = 0.68309
Epoch 4.46: Loss = 0.593765
Epoch 4.47: Loss = 0.529083
Epoch 4.48: Loss = 0.626373
Epoch 4.49: Loss = 0.616394
Epoch 4.50: Loss = 0.694534
Epoch 4.51: Loss = 0.635864
Epoch 4.52: Loss = 0.717209
Epoch 4.53: Loss = 0.520569
Epoch 4.54: Loss = 0.604263
Epoch 4.55: Loss = 0.598389
Epoch 4.56: Loss = 0.533737
Epoch 4.57: Loss = 0.616425
Epoch 4.58: Loss = 0.620529
Epoch 4.59: Loss = 0.704346
Epoch 4.60: Loss = 0.714661
Epoch 4.61: Loss = 0.648239
Epoch 4.62: Loss = 0.621323
Epoch 4.63: Loss = 0.609222
Epoch 4.64: Loss = 0.738678
Epoch 4.65: Loss = 0.523026
Epoch 4.66: Loss = 0.575836
Epoch 4.67: Loss = 0.586166
Epoch 4.68: Loss = 0.680023
Epoch 4.69: Loss = 0.674316
Epoch 4.70: Loss = 0.674622
Epoch 4.71: Loss = 0.617416
Epoch 4.72: Loss = 0.683609
Epoch 4.73: Loss = 0.616638
Epoch 4.74: Loss = 0.588242
Epoch 4.75: Loss = 0.592209
Epoch 4.76: Loss = 0.686096
Epoch 4.77: Loss = 0.552261
Epoch 4.78: Loss = 0.60498
Epoch 4.79: Loss = 0.567993
Epoch 4.80: Loss = 0.701706
Epoch 4.81: Loss = 0.728256
Epoch 4.82: Loss = 0.621841
Epoch 4.83: Loss = 0.668365
Epoch 4.84: Loss = 0.583282
Epoch 4.85: Loss = 0.632233
Epoch 4.86: Loss = 0.691666
Epoch 4.87: Loss = 0.626968
Epoch 4.88: Loss = 0.625885
Epoch 4.89: Loss = 0.700974
Epoch 4.90: Loss = 0.598007
Epoch 4.91: Loss = 0.577499
Epoch 4.92: Loss = 0.582169
Epoch 4.93: Loss = 0.620041
Epoch 4.94: Loss = 0.680786
Epoch 4.95: Loss = 0.699524
Epoch 4.96: Loss = 0.690475
Epoch 4.97: Loss = 0.60759
Epoch 4.98: Loss = 0.608368
Epoch 4.99: Loss = 0.577728
Epoch 4.100: Loss = 0.579971
Epoch 4.101: Loss = 0.590012
Epoch 4.102: Loss = 0.658554
Epoch 4.103: Loss = 0.625107
Epoch 4.104: Loss = 0.624847
Epoch 4.105: Loss = 0.708588
Epoch 4.106: Loss = 0.525772
Epoch 4.107: Loss = 0.664017
Epoch 4.108: Loss = 0.659973
Epoch 4.109: Loss = 0.574997
Epoch 4.110: Loss = 0.676285
Epoch 4.111: Loss = 0.650467
Epoch 4.112: Loss = 0.541397
Epoch 4.113: Loss = 0.682571
Epoch 4.114: Loss = 0.648544
Epoch 4.115: Loss = 0.558304
Epoch 4.116: Loss = 0.700623
Epoch 4.117: Loss = 0.670868
Epoch 4.118: Loss = 0.614258
Epoch 4.119: Loss = 0.603546
Epoch 4.120: Loss = 0.680695
TRAIN LOSS = 0.640106
TRAIN ACC = 79.5685 % (47743/60000)
Loss = 0.580002
Loss = 0.695572
Loss = 0.654236
Loss = 0.570343
Loss = 0.605865
Loss = 0.738724
Loss = 0.785889
Loss = 0.680466
Loss = 0.618073
Loss = 0.564728
Loss = 0.760162
Loss = 0.726364
Loss = 0.645233
Loss = 0.662094
Loss = 0.621933
Loss = 0.678909
Loss = 0.604965
Loss = 0.689163
Loss = 0.695877
Loss = 0.639252
TEST LOSS = 0.660892
TEST ACC = 477.429 % (7912/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.681274
Epoch 5.2: Loss = 0.622147
Epoch 5.3: Loss = 0.741196
Epoch 5.4: Loss = 0.606308
Epoch 5.5: Loss = 0.609467
Epoch 5.6: Loss = 0.675308
Epoch 5.7: Loss = 0.642014
Epoch 5.8: Loss = 0.612701
Epoch 5.9: Loss = 0.505478
Epoch 5.10: Loss = 0.603348
Epoch 5.11: Loss = 0.620056
Epoch 5.12: Loss = 0.597153
Epoch 5.13: Loss = 0.564911
Epoch 5.14: Loss = 0.618683
Epoch 5.15: Loss = 0.647949
Epoch 5.16: Loss = 0.661301
Epoch 5.17: Loss = 0.619049
Epoch 5.18: Loss = 0.719437
Epoch 5.19: Loss = 0.508331
Epoch 5.20: Loss = 0.662399
Epoch 5.21: Loss = 0.535873
Epoch 5.22: Loss = 0.737671
Epoch 5.23: Loss = 0.629944
Epoch 5.24: Loss = 0.774139
Epoch 5.25: Loss = 0.690155
Epoch 5.26: Loss = 0.589951
Epoch 5.27: Loss = 0.753006
Epoch 5.28: Loss = 0.578445
Epoch 5.29: Loss = 0.663589
Epoch 5.30: Loss = 0.565735
Epoch 5.31: Loss = 0.684601
Epoch 5.32: Loss = 0.729645
Epoch 5.33: Loss = 0.611435
Epoch 5.34: Loss = 0.666565
Epoch 5.35: Loss = 0.516006
Epoch 5.36: Loss = 0.622559
Epoch 5.37: Loss = 0.633362
Epoch 5.38: Loss = 0.693863
Epoch 5.39: Loss = 0.655655
Epoch 5.40: Loss = 0.649246
Epoch 5.41: Loss = 0.618118
Epoch 5.42: Loss = 0.637787
Epoch 5.43: Loss = 0.532333
Epoch 5.44: Loss = 0.677109
Epoch 5.45: Loss = 0.617523
Epoch 5.46: Loss = 0.567139
Epoch 5.47: Loss = 0.650711
Epoch 5.48: Loss = 0.523087
Epoch 5.49: Loss = 0.622879
Epoch 5.50: Loss = 0.653885
Epoch 5.51: Loss = 0.601379
Epoch 5.52: Loss = 0.609695
Epoch 5.53: Loss = 0.660049
Epoch 5.54: Loss = 0.605042
Epoch 5.55: Loss = 0.603882
Epoch 5.56: Loss = 0.669678
Epoch 5.57: Loss = 0.724731
Epoch 5.58: Loss = 0.741882
Epoch 5.59: Loss = 0.627243
Epoch 5.60: Loss = 0.612152
Epoch 5.61: Loss = 0.685654
Epoch 5.62: Loss = 0.55365
Epoch 5.63: Loss = 0.747833
Epoch 5.64: Loss = 0.68924
Epoch 5.65: Loss = 0.696838
Epoch 5.66: Loss = 0.605026
Epoch 5.67: Loss = 0.551788
Epoch 5.68: Loss = 0.736572
Epoch 5.69: Loss = 0.575577
Epoch 5.70: Loss = 0.613235
Epoch 5.71: Loss = 0.583786
Epoch 5.72: Loss = 0.622818
Epoch 5.73: Loss = 0.594131
Epoch 5.74: Loss = 0.548676
Epoch 5.75: Loss = 0.511078
Epoch 5.76: Loss = 0.685837
Epoch 5.77: Loss = 0.757446
Epoch 5.78: Loss = 0.566818
Epoch 5.79: Loss = 0.573288
Epoch 5.80: Loss = 0.563736
Epoch 5.81: Loss = 0.657684
Epoch 5.82: Loss = 0.677994
Epoch 5.83: Loss = 0.65712
Epoch 5.84: Loss = 0.522964
Epoch 5.85: Loss = 0.650589
Epoch 5.86: Loss = 0.626541
Epoch 5.87: Loss = 0.547028
Epoch 5.88: Loss = 0.725082
Epoch 5.89: Loss = 0.759018
Epoch 5.90: Loss = 0.699738
Epoch 5.91: Loss = 0.670074
Epoch 5.92: Loss = 0.623566
Epoch 5.93: Loss = 0.695816
Epoch 5.94: Loss = 0.556061
Epoch 5.95: Loss = 0.660034
Epoch 5.96: Loss = 0.637634
Epoch 5.97: Loss = 0.602692
Epoch 5.98: Loss = 0.612061
Epoch 5.99: Loss = 0.651169
Epoch 5.100: Loss = 0.647339
Epoch 5.101: Loss = 0.694138
Epoch 5.102: Loss = 0.728058
Epoch 5.103: Loss = 0.61322
Epoch 5.104: Loss = 0.676559
Epoch 5.105: Loss = 0.600891
Epoch 5.106: Loss = 0.625916
Epoch 5.107: Loss = 0.613449
Epoch 5.108: Loss = 0.681412
Epoch 5.109: Loss = 0.594742
Epoch 5.110: Loss = 0.578613
Epoch 5.111: Loss = 0.665878
Epoch 5.112: Loss = 0.66037
Epoch 5.113: Loss = 0.669617
Epoch 5.114: Loss = 0.577698
Epoch 5.115: Loss = 0.623917
Epoch 5.116: Loss = 0.663254
Epoch 5.117: Loss = 0.63562
Epoch 5.118: Loss = 0.548965
Epoch 5.119: Loss = 0.716415
Epoch 5.120: Loss = 0.65123
TRAIN LOSS = 0.635178
TRAIN ACC = 80.3177 % (48193/60000)
Loss = 0.561829
Loss = 0.704849
Loss = 0.64801
Loss = 0.55983
Loss = 0.600601
Loss = 0.748016
Loss = 0.800247
Loss = 0.683212
Loss = 0.613708
Loss = 0.582153
Loss = 0.774887
Loss = 0.731873
Loss = 0.669327
Loss = 0.672119
Loss = 0.61705
Loss = 0.677612
Loss = 0.604889
Loss = 0.711395
Loss = 0.691803
Loss = 0.6465
TEST LOSS = 0.664995
TEST ACC = 481.929 % (7953/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.617889
Epoch 6.2: Loss = 0.638672
Epoch 6.3: Loss = 0.77121
Epoch 6.4: Loss = 0.687195
Epoch 6.5: Loss = 0.635269
Epoch 6.6: Loss = 0.69754
Epoch 6.7: Loss = 0.635544
Epoch 6.8: Loss = 0.759918
Epoch 6.9: Loss = 0.531647
Epoch 6.10: Loss = 0.707123
Epoch 6.11: Loss = 0.737061
Epoch 6.12: Loss = 0.668549
Epoch 6.13: Loss = 0.54924
Epoch 6.14: Loss = 0.576309
Epoch 6.15: Loss = 0.685944
Epoch 6.16: Loss = 0.591202
Epoch 6.17: Loss = 0.624741
Epoch 6.18: Loss = 0.57724
Epoch 6.19: Loss = 0.606522
Epoch 6.20: Loss = 0.626892
Epoch 6.21: Loss = 0.564453
Epoch 6.22: Loss = 0.612808
Epoch 6.23: Loss = 0.708191
Epoch 6.24: Loss = 0.666855
Epoch 6.25: Loss = 0.547073
Epoch 6.26: Loss = 0.579117
Epoch 6.27: Loss = 0.650513
Epoch 6.28: Loss = 0.669022
Epoch 6.29: Loss = 0.589035
Epoch 6.30: Loss = 0.646759
Epoch 6.31: Loss = 0.609543
Epoch 6.32: Loss = 0.758682
Epoch 6.33: Loss = 0.597748
Epoch 6.34: Loss = 0.573746
Epoch 6.35: Loss = 0.603973
Epoch 6.36: Loss = 0.65538
Epoch 6.37: Loss = 0.596558
Epoch 6.38: Loss = 0.636688
Epoch 6.39: Loss = 0.666199
Epoch 6.40: Loss = 0.816605
Epoch 6.41: Loss = 0.615738
Epoch 6.42: Loss = 0.517105
Epoch 6.43: Loss = 0.666473
Epoch 6.44: Loss = 0.752136
Epoch 6.45: Loss = 0.574432
Epoch 6.46: Loss = 0.562347
Epoch 6.47: Loss = 0.567612
Epoch 6.48: Loss = 0.62236
Epoch 6.49: Loss = 0.645248
Epoch 6.50: Loss = 0.665558
Epoch 6.51: Loss = 0.649231
Epoch 6.52: Loss = 0.607224
Epoch 6.53: Loss = 0.650406
Epoch 6.54: Loss = 0.632309
Epoch 6.55: Loss = 0.630753
Epoch 6.56: Loss = 0.716003
Epoch 6.57: Loss = 0.637985
Epoch 6.58: Loss = 0.662216
Epoch 6.59: Loss = 0.717438
Epoch 6.60: Loss = 0.606812
Epoch 6.61: Loss = 0.720276
Epoch 6.62: Loss = 0.590576
Epoch 6.63: Loss = 0.654419
Epoch 6.64: Loss = 0.61351
Epoch 6.65: Loss = 0.539673
Epoch 6.66: Loss = 0.731583
Epoch 6.67: Loss = 0.57811
Epoch 6.68: Loss = 0.618286
Epoch 6.69: Loss = 0.650848
Epoch 6.70: Loss = 0.742813
Epoch 6.71: Loss = 0.659103
Epoch 6.72: Loss = 0.697998
Epoch 6.73: Loss = 0.550766
Epoch 6.74: Loss = 0.573181
Epoch 6.75: Loss = 0.592072
Epoch 6.76: Loss = 0.556442
Epoch 6.77: Loss = 0.607452
Epoch 6.78: Loss = 0.66954
Epoch 6.79: Loss = 0.647186
Epoch 6.80: Loss = 0.694672
Epoch 6.81: Loss = 0.606277
Epoch 6.82: Loss = 0.602661
Epoch 6.83: Loss = 0.65741
Epoch 6.84: Loss = 0.658859
Epoch 6.85: Loss = 0.58223
Epoch 6.86: Loss = 0.696014
Epoch 6.87: Loss = 0.633255
Epoch 6.88: Loss = 0.657669
Epoch 6.89: Loss = 0.613739
Epoch 6.90: Loss = 0.7883
Epoch 6.91: Loss = 0.715042
Epoch 6.92: Loss = 0.71286
Epoch 6.93: Loss = 0.681335
Epoch 6.94: Loss = 0.665497
Epoch 6.95: Loss = 0.688889
Epoch 6.96: Loss = 0.556931
Epoch 6.97: Loss = 0.543076
Epoch 6.98: Loss = 0.576843
Epoch 6.99: Loss = 0.542679
Epoch 6.100: Loss = 0.753983
Epoch 6.101: Loss = 0.700516
Epoch 6.102: Loss = 0.585785
Epoch 6.103: Loss = 0.751389
Epoch 6.104: Loss = 0.611099
Epoch 6.105: Loss = 0.561493
Epoch 6.106: Loss = 0.735626
Epoch 6.107: Loss = 0.65303
Epoch 6.108: Loss = 0.621262
Epoch 6.109: Loss = 0.648544
Epoch 6.110: Loss = 0.656342
Epoch 6.111: Loss = 0.627182
Epoch 6.112: Loss = 0.490234
Epoch 6.113: Loss = 0.558533
Epoch 6.114: Loss = 0.703888
Epoch 6.115: Loss = 0.679535
Epoch 6.116: Loss = 0.68576
Epoch 6.117: Loss = 0.54628
Epoch 6.118: Loss = 0.646103
Epoch 6.119: Loss = 0.7146
Epoch 6.120: Loss = 0.751251
TRAIN LOSS = 0.641052
TRAIN ACC = 80.6488 % (48391/60000)
Loss = 0.587402
Loss = 0.69931
Loss = 0.652695
Loss = 0.572113
Loss = 0.594833
Loss = 0.768997
Loss = 0.818802
Loss = 0.701141
Loss = 0.613602
Loss = 0.615524
Loss = 0.80043
Loss = 0.722504
Loss = 0.657867
Loss = 0.658707
Loss = 0.608139
Loss = 0.677368
Loss = 0.60791
Loss = 0.704422
Loss = 0.697296
Loss = 0.663712
TEST LOSS = 0.671139
TEST ACC = 483.91 % (7959/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.557388
Epoch 7.2: Loss = 0.657608
Epoch 7.3: Loss = 0.677902
Epoch 7.4: Loss = 0.64444
Epoch 7.5: Loss = 0.828125
Epoch 7.6: Loss = 0.710388
Epoch 7.7: Loss = 0.60405
Epoch 7.8: Loss = 0.663864
Epoch 7.9: Loss = 0.535675
Epoch 7.10: Loss = 0.62648
Epoch 7.11: Loss = 0.595032
Epoch 7.12: Loss = 0.634003
Epoch 7.13: Loss = 0.652054
Epoch 7.14: Loss = 0.539307
Epoch 7.15: Loss = 0.625397
Epoch 7.16: Loss = 0.630234
Epoch 7.17: Loss = 0.5849
Epoch 7.18: Loss = 0.619446
Epoch 7.19: Loss = 0.500854
Epoch 7.20: Loss = 0.556656
Epoch 7.21: Loss = 0.750992
Epoch 7.22: Loss = 0.648407
Epoch 7.23: Loss = 0.599319
Epoch 7.24: Loss = 0.624695
Epoch 7.25: Loss = 0.59552
Epoch 7.26: Loss = 0.646393
Epoch 7.27: Loss = 0.654236
Epoch 7.28: Loss = 0.625931
Epoch 7.29: Loss = 0.615372
Epoch 7.30: Loss = 0.626953
Epoch 7.31: Loss = 0.588058
Epoch 7.32: Loss = 0.696457
Epoch 7.33: Loss = 0.716782
Epoch 7.34: Loss = 0.641571
Epoch 7.35: Loss = 0.69664
Epoch 7.36: Loss = 0.710587
Epoch 7.37: Loss = 0.727722
Epoch 7.38: Loss = 0.671844
Epoch 7.39: Loss = 0.661407
Epoch 7.40: Loss = 0.682953
Epoch 7.41: Loss = 0.550262
Epoch 7.42: Loss = 0.624451
Epoch 7.43: Loss = 0.600876
Epoch 7.44: Loss = 0.739349
Epoch 7.45: Loss = 0.527161
Epoch 7.46: Loss = 0.680573
Epoch 7.47: Loss = 0.699326
Epoch 7.48: Loss = 0.652802
Epoch 7.49: Loss = 0.622574
Epoch 7.50: Loss = 0.734879
Epoch 7.51: Loss = 0.746338
Epoch 7.52: Loss = 0.705322
Epoch 7.53: Loss = 0.515701
Epoch 7.54: Loss = 0.628159
Epoch 7.55: Loss = 0.662842
Epoch 7.56: Loss = 0.620087
Epoch 7.57: Loss = 0.60228
Epoch 7.58: Loss = 0.634293
Epoch 7.59: Loss = 0.675568
Epoch 7.60: Loss = 0.5905
Epoch 7.61: Loss = 0.759995
Epoch 7.62: Loss = 0.612518
Epoch 7.63: Loss = 0.604691
Epoch 7.64: Loss = 0.554306
Epoch 7.65: Loss = 0.649292
Epoch 7.66: Loss = 0.542542
Epoch 7.67: Loss = 0.686264
Epoch 7.68: Loss = 0.794769
Epoch 7.69: Loss = 0.733521
Epoch 7.70: Loss = 0.671539
Epoch 7.71: Loss = 0.729401
Epoch 7.72: Loss = 0.622086
Epoch 7.73: Loss = 0.635132
Epoch 7.74: Loss = 0.723831
Epoch 7.75: Loss = 0.683945
Epoch 7.76: Loss = 0.69841
Epoch 7.77: Loss = 0.611801
Epoch 7.78: Loss = 0.499161
Epoch 7.79: Loss = 0.631149
Epoch 7.80: Loss = 0.686493
Epoch 7.81: Loss = 0.647415
Epoch 7.82: Loss = 0.608673
Epoch 7.83: Loss = 0.612625
Epoch 7.84: Loss = 0.632751
Epoch 7.85: Loss = 0.700363
Epoch 7.86: Loss = 0.698517
Epoch 7.87: Loss = 0.671509
Epoch 7.88: Loss = 0.606277
Epoch 7.89: Loss = 0.688782
Epoch 7.90: Loss = 0.633224
Epoch 7.91: Loss = 0.651306
Epoch 7.92: Loss = 0.69133
Epoch 7.93: Loss = 0.617828
Epoch 7.94: Loss = 0.671906
Epoch 7.95: Loss = 0.610016
Epoch 7.96: Loss = 0.657852
Epoch 7.97: Loss = 0.793671
Epoch 7.98: Loss = 0.740463
Epoch 7.99: Loss = 0.698135
Epoch 7.100: Loss = 0.640076
Epoch 7.101: Loss = 0.817337
Epoch 7.102: Loss = 0.774323
Epoch 7.103: Loss = 0.592346
Epoch 7.104: Loss = 0.611877
Epoch 7.105: Loss = 0.785599
Epoch 7.106: Loss = 0.637527
Epoch 7.107: Loss = 0.717804
Epoch 7.108: Loss = 0.707169
Epoch 7.109: Loss = 0.585205
Epoch 7.110: Loss = 0.704025
Epoch 7.111: Loss = 0.550247
Epoch 7.112: Loss = 0.647186
Epoch 7.113: Loss = 0.57962
Epoch 7.114: Loss = 0.761566
Epoch 7.115: Loss = 0.626511
Epoch 7.116: Loss = 0.800339
Epoch 7.117: Loss = 0.683731
Epoch 7.118: Loss = 0.676315
Epoch 7.119: Loss = 0.707809
Epoch 7.120: Loss = 0.663315
TRAIN LOSS = 0.653946
TRAIN ACC = 80.9753 % (48588/60000)
Loss = 0.607803
Loss = 0.727722
Loss = 0.668869
Loss = 0.589539
Loss = 0.602951
Loss = 0.779312
Loss = 0.845825
Loss = 0.719696
Loss = 0.676178
Loss = 0.585724
Loss = 0.803177
Loss = 0.740891
Loss = 0.691101
Loss = 0.677155
Loss = 0.64328
Loss = 0.67514
Loss = 0.651932
Loss = 0.73497
Loss = 0.714035
Loss = 0.668365
TEST LOSS = 0.690183
TEST ACC = 485.88 % (8005/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.547424
Epoch 8.2: Loss = 0.591583
Epoch 8.3: Loss = 0.693237
Epoch 8.4: Loss = 0.666656
Epoch 8.5: Loss = 0.740097
Epoch 8.6: Loss = 0.610275
Epoch 8.7: Loss = 0.720032
Epoch 8.8: Loss = 0.544769
Epoch 8.9: Loss = 0.632996
Epoch 8.10: Loss = 0.623871
Epoch 8.11: Loss = 0.596939
Epoch 8.12: Loss = 0.688675
Epoch 8.13: Loss = 0.654709
Epoch 8.14: Loss = 0.607285
Epoch 8.15: Loss = 0.696381
Epoch 8.16: Loss = 0.795624
Epoch 8.17: Loss = 0.729416
Epoch 8.18: Loss = 0.737869
Epoch 8.19: Loss = 0.614319
Epoch 8.20: Loss = 0.682739
Epoch 8.21: Loss = 0.618546
Epoch 8.22: Loss = 0.636261
Epoch 8.23: Loss = 0.78212
Epoch 8.24: Loss = 0.639725
Epoch 8.25: Loss = 0.485168
Epoch 8.26: Loss = 0.582275
Epoch 8.27: Loss = 0.677124
Epoch 8.28: Loss = 0.709946
Epoch 8.29: Loss = 0.782837
Epoch 8.30: Loss = 0.832031
Epoch 8.31: Loss = 0.609985
Epoch 8.32: Loss = 0.624786
Epoch 8.33: Loss = 0.660538
Epoch 8.34: Loss = 0.785416
Epoch 8.35: Loss = 0.593292
Epoch 8.36: Loss = 0.601013
Epoch 8.37: Loss = 0.665726
Epoch 8.38: Loss = 0.737869
Epoch 8.39: Loss = 0.695694
Epoch 8.40: Loss = 0.685776
Epoch 8.41: Loss = 0.687698
Epoch 8.42: Loss = 0.611511
Epoch 8.43: Loss = 0.610504
Epoch 8.44: Loss = 0.65274
Epoch 8.45: Loss = 0.676605
Epoch 8.46: Loss = 0.684814
Epoch 8.47: Loss = 0.677124
Epoch 8.48: Loss = 0.646973
Epoch 8.49: Loss = 0.768097
Epoch 8.50: Loss = 0.51236
Epoch 8.51: Loss = 0.705185
Epoch 8.52: Loss = 0.545807
Epoch 8.53: Loss = 0.650146
Epoch 8.54: Loss = 0.690979
Epoch 8.55: Loss = 0.560425
Epoch 8.56: Loss = 0.578735
Epoch 8.57: Loss = 0.74382
Epoch 8.58: Loss = 0.585541
Epoch 8.59: Loss = 0.600082
Epoch 8.60: Loss = 0.704041
Epoch 8.61: Loss = 0.668533
Epoch 8.62: Loss = 0.647751
Epoch 8.63: Loss = 0.575989
Epoch 8.64: Loss = 0.609009
Epoch 8.65: Loss = 0.69075
Epoch 8.66: Loss = 0.532852
Epoch 8.67: Loss = 0.600708
Epoch 8.68: Loss = 0.640335
Epoch 8.69: Loss = 0.744003
Epoch 8.70: Loss = 0.592987
Epoch 8.71: Loss = 0.614136
Epoch 8.72: Loss = 0.63269
Epoch 8.73: Loss = 0.613541
Epoch 8.74: Loss = 0.662323
Epoch 8.75: Loss = 0.801315
Epoch 8.76: Loss = 0.590881
Epoch 8.77: Loss = 0.687378
Epoch 8.78: Loss = 0.710022
Epoch 8.79: Loss = 0.609604
Epoch 8.80: Loss = 0.709763
Epoch 8.81: Loss = 0.517746
Epoch 8.82: Loss = 0.682541
Epoch 8.83: Loss = 0.682281
Epoch 8.84: Loss = 0.687958
Epoch 8.85: Loss = 0.663498
Epoch 8.86: Loss = 0.743713
Epoch 8.87: Loss = 0.656189
Epoch 8.88: Loss = 0.759171
Epoch 8.89: Loss = 0.631485
Epoch 8.90: Loss = 0.503052
Epoch 8.91: Loss = 0.69046
Epoch 8.92: Loss = 0.589172
Epoch 8.93: Loss = 0.617355
Epoch 8.94: Loss = 0.634567
Epoch 8.95: Loss = 0.65274
Epoch 8.96: Loss = 0.697891
Epoch 8.97: Loss = 0.655243
Epoch 8.98: Loss = 0.725754
Epoch 8.99: Loss = 0.758759
Epoch 8.100: Loss = 0.718842
Epoch 8.101: Loss = 0.685043
Epoch 8.102: Loss = 0.617371
Epoch 8.103: Loss = 0.592331
Epoch 8.104: Loss = 0.607529
Epoch 8.105: Loss = 0.645584
Epoch 8.106: Loss = 0.86525
Epoch 8.107: Loss = 0.652405
Epoch 8.108: Loss = 0.667099
Epoch 8.109: Loss = 0.653885
Epoch 8.110: Loss = 0.643097
Epoch 8.111: Loss = 0.680344
Epoch 8.112: Loss = 0.575409
Epoch 8.113: Loss = 0.634003
Epoch 8.114: Loss = 0.702896
Epoch 8.115: Loss = 0.703613
Epoch 8.116: Loss = 0.566818
Epoch 8.117: Loss = 0.589783
Epoch 8.118: Loss = 0.634933
Epoch 8.119: Loss = 0.711655
Epoch 8.120: Loss = 0.690613
TRAIN LOSS = 0.656662
TRAIN ACC = 80.9433 % (48568/60000)
Loss = 0.60791
Loss = 0.723404
Loss = 0.669189
Loss = 0.579086
Loss = 0.62323
Loss = 0.775803
Loss = 0.813873
Loss = 0.69812
Loss = 0.656693
Loss = 0.579605
Loss = 0.808502
Loss = 0.765549
Loss = 0.724838
Loss = 0.697769
Loss = 0.652924
Loss = 0.690262
Loss = 0.666016
Loss = 0.712326
Loss = 0.691055
Loss = 0.685425
TEST LOSS = 0.691079
TEST ACC = 485.68 % (7984/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.57074
Epoch 9.2: Loss = 0.678452
Epoch 9.3: Loss = 0.606674
Epoch 9.4: Loss = 0.643616
Epoch 9.5: Loss = 0.655319
Epoch 9.6: Loss = 0.617264
Epoch 9.7: Loss = 0.712723
Epoch 9.8: Loss = 0.707352
Epoch 9.9: Loss = 0.758102
Epoch 9.10: Loss = 0.61618
Epoch 9.11: Loss = 0.649261
Epoch 9.12: Loss = 0.709915
Epoch 9.13: Loss = 0.794998
Epoch 9.14: Loss = 0.697769
Epoch 9.15: Loss = 0.651428
Epoch 9.16: Loss = 0.685791
Epoch 9.17: Loss = 0.599976
Epoch 9.18: Loss = 0.845291
Epoch 9.19: Loss = 0.694
Epoch 9.20: Loss = 0.567825
Epoch 9.21: Loss = 0.710098
Epoch 9.22: Loss = 0.648987
Epoch 9.23: Loss = 0.617737
Epoch 9.24: Loss = 0.569077
Epoch 9.25: Loss = 0.694443
Epoch 9.26: Loss = 0.822601
Epoch 9.27: Loss = 0.571533
Epoch 9.28: Loss = 0.742844
Epoch 9.29: Loss = 0.731949
Epoch 9.30: Loss = 0.732651
Epoch 9.31: Loss = 0.747864
Epoch 9.32: Loss = 0.647217
Epoch 9.33: Loss = 0.592712
Epoch 9.34: Loss = 0.555511
Epoch 9.35: Loss = 0.582092
Epoch 9.36: Loss = 0.645203
Epoch 9.37: Loss = 0.678497
Epoch 9.38: Loss = 0.675507
Epoch 9.39: Loss = 0.744232
Epoch 9.40: Loss = 0.697617
Epoch 9.41: Loss = 0.812317
Epoch 9.42: Loss = 0.676453
Epoch 9.43: Loss = 0.569672
Epoch 9.44: Loss = 0.757675
Epoch 9.45: Loss = 0.602097
Epoch 9.46: Loss = 0.717789
Epoch 9.47: Loss = 0.65744
Epoch 9.48: Loss = 0.611908
Epoch 9.49: Loss = 0.653442
Epoch 9.50: Loss = 0.562531
Epoch 9.51: Loss = 0.810486
Epoch 9.52: Loss = 0.59201
Epoch 9.53: Loss = 0.699066
Epoch 9.54: Loss = 0.591476
Epoch 9.55: Loss = 0.536392
Epoch 9.56: Loss = 0.682953
Epoch 9.57: Loss = 0.578369
Epoch 9.58: Loss = 0.567459
Epoch 9.59: Loss = 0.637634
Epoch 9.60: Loss = 0.667236
Epoch 9.61: Loss = 0.612747
Epoch 9.62: Loss = 0.655014
Epoch 9.63: Loss = 0.617996
Epoch 9.64: Loss = 0.760712
Epoch 9.65: Loss = 0.774338
Epoch 9.66: Loss = 0.634583
Epoch 9.67: Loss = 0.68161
Epoch 9.68: Loss = 0.702347
Epoch 9.69: Loss = 0.681961
Epoch 9.70: Loss = 0.597519
Epoch 9.71: Loss = 0.807541
Epoch 9.72: Loss = 0.625565
Epoch 9.73: Loss = 0.642365
Epoch 9.74: Loss = 0.525009
Epoch 9.75: Loss = 0.678162
Epoch 9.76: Loss = 0.635376
Epoch 9.77: Loss = 0.616699
Epoch 9.78: Loss = 0.615082
Epoch 9.79: Loss = 0.62616
Epoch 9.80: Loss = 0.819717
Epoch 9.81: Loss = 0.614456
Epoch 9.82: Loss = 0.605087
Epoch 9.83: Loss = 0.758316
Epoch 9.84: Loss = 0.639374
Epoch 9.85: Loss = 0.693619
Epoch 9.86: Loss = 0.76889
Epoch 9.87: Loss = 0.747772
Epoch 9.88: Loss = 0.693024
Epoch 9.89: Loss = 0.702026
Epoch 9.90: Loss = 0.611008
Epoch 9.91: Loss = 0.727234
Epoch 9.92: Loss = 0.690079
Epoch 9.93: Loss = 0.711853
Epoch 9.94: Loss = 0.628281
Epoch 9.95: Loss = 0.67012
Epoch 9.96: Loss = 0.619171
Epoch 9.97: Loss = 0.614502
Epoch 9.98: Loss = 0.689621
Epoch 9.99: Loss = 0.580109
Epoch 9.100: Loss = 0.603043
Epoch 9.101: Loss = 0.73204
Epoch 9.102: Loss = 0.519073
Epoch 9.103: Loss = 0.618347
Epoch 9.104: Loss = 0.698975
Epoch 9.105: Loss = 0.715851
Epoch 9.106: Loss = 0.627808
Epoch 9.107: Loss = 0.680679
Epoch 9.108: Loss = 0.679306
Epoch 9.109: Loss = 0.680313
Epoch 9.110: Loss = 0.65506
Epoch 9.111: Loss = 0.7379
Epoch 9.112: Loss = 0.646805
Epoch 9.113: Loss = 0.707001
Epoch 9.114: Loss = 0.786926
Epoch 9.115: Loss = 0.727707
Epoch 9.116: Loss = 0.663086
Epoch 9.117: Loss = 0.589951
Epoch 9.118: Loss = 0.633163
Epoch 9.119: Loss = 0.685822
Epoch 9.120: Loss = 0.576538
TRAIN LOSS = 0.665817
TRAIN ACC = 81.0287 % (48619/60000)
Loss = 0.608261
Loss = 0.717834
Loss = 0.684143
Loss = 0.588654
Loss = 0.629318
Loss = 0.770279
Loss = 0.872208
Loss = 0.706421
Loss = 0.71228
Loss = 0.582062
Loss = 0.840256
Loss = 0.772888
Loss = 0.726212
Loss = 0.720642
Loss = 0.653
Loss = 0.705383
Loss = 0.696106
Loss = 0.74736
Loss = 0.687271
Loss = 0.732529
TEST LOSS = 0.707655
TEST ACC = 486.189 % (7990/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.672394
Epoch 10.2: Loss = 0.685379
Epoch 10.3: Loss = 0.570282
Epoch 10.4: Loss = 0.609253
Epoch 10.5: Loss = 0.683731
Epoch 10.6: Loss = 0.588455
Epoch 10.7: Loss = 0.632767
Epoch 10.8: Loss = 0.70871
Epoch 10.9: Loss = 0.659317
Epoch 10.10: Loss = 0.698975
Epoch 10.11: Loss = 0.642471
Epoch 10.12: Loss = 0.758575
Epoch 10.13: Loss = 0.606796
Epoch 10.14: Loss = 0.7565
Epoch 10.15: Loss = 0.593292
Epoch 10.16: Loss = 0.60257
Epoch 10.17: Loss = 0.684555
Epoch 10.18: Loss = 0.672989
Epoch 10.19: Loss = 0.622421
Epoch 10.20: Loss = 0.645294
Epoch 10.21: Loss = 0.635956
Epoch 10.22: Loss = 0.710846
Epoch 10.23: Loss = 0.795486
Epoch 10.24: Loss = 0.628098
Epoch 10.25: Loss = 0.613083
Epoch 10.26: Loss = 0.621475
Epoch 10.27: Loss = 0.637054
Epoch 10.28: Loss = 0.734665
Epoch 10.29: Loss = 0.61853
Epoch 10.30: Loss = 0.649658
Epoch 10.31: Loss = 0.740341
Epoch 10.32: Loss = 0.666534
Epoch 10.33: Loss = 0.647095
Epoch 10.34: Loss = 0.696548
Epoch 10.35: Loss = 0.628464
Epoch 10.36: Loss = 0.665939
Epoch 10.37: Loss = 0.597
Epoch 10.38: Loss = 0.656952
Epoch 10.39: Loss = 0.735184
Epoch 10.40: Loss = 0.750015
Epoch 10.41: Loss = 0.617172
Epoch 10.42: Loss = 0.636917
Epoch 10.43: Loss = 0.68248
Epoch 10.44: Loss = 0.584457
Epoch 10.45: Loss = 0.631149
Epoch 10.46: Loss = 0.641541
Epoch 10.47: Loss = 0.726837
Epoch 10.48: Loss = 0.669296
Epoch 10.49: Loss = 0.617493
Epoch 10.50: Loss = 0.679672
Epoch 10.51: Loss = 0.789825
Epoch 10.52: Loss = 0.505585
Epoch 10.53: Loss = 0.725571
Epoch 10.54: Loss = 0.744644
Epoch 10.55: Loss = 0.721985
Epoch 10.56: Loss = 0.53624
Epoch 10.57: Loss = 0.804871
Epoch 10.58: Loss = 0.653137
Epoch 10.59: Loss = 0.680771
Epoch 10.60: Loss = 0.662659
Epoch 10.61: Loss = 0.699768
Epoch 10.62: Loss = 0.630905
Epoch 10.63: Loss = 0.648056
Epoch 10.64: Loss = 0.620316
Epoch 10.65: Loss = 0.719467
Epoch 10.66: Loss = 0.650665
Epoch 10.67: Loss = 0.770584
Epoch 10.68: Loss = 0.545105
Epoch 10.69: Loss = 0.630951
Epoch 10.70: Loss = 0.756149
Epoch 10.71: Loss = 0.73996
Epoch 10.72: Loss = 0.710449
Epoch 10.73: Loss = 0.681366
Epoch 10.74: Loss = 0.890472
Epoch 10.75: Loss = 0.719742
Epoch 10.76: Loss = 0.542297
Epoch 10.77: Loss = 0.759674
Epoch 10.78: Loss = 0.721542
Epoch 10.79: Loss = 0.622452
Epoch 10.80: Loss = 0.646469
Epoch 10.81: Loss = 0.67482
Epoch 10.82: Loss = 0.648254
Epoch 10.83: Loss = 0.707825
Epoch 10.84: Loss = 0.656235
Epoch 10.85: Loss = 0.649689
Epoch 10.86: Loss = 0.684814
Epoch 10.87: Loss = 0.656067
Epoch 10.88: Loss = 0.568787
Epoch 10.89: Loss = 0.663025
Epoch 10.90: Loss = 0.629166
Epoch 10.91: Loss = 0.677643
Epoch 10.92: Loss = 0.645706
Epoch 10.93: Loss = 0.601013
Epoch 10.94: Loss = 0.781097
Epoch 10.95: Loss = 0.703873
Epoch 10.96: Loss = 0.625259
Epoch 10.97: Loss = 0.759186
Epoch 10.98: Loss = 0.593338
Epoch 10.99: Loss = 0.700378
Epoch 10.100: Loss = 0.72525
Epoch 10.101: Loss = 0.696594
Epoch 10.102: Loss = 0.631851
Epoch 10.103: Loss = 0.619232
Epoch 10.104: Loss = 0.646423
Epoch 10.105: Loss = 0.705994
Epoch 10.106: Loss = 0.734207
Epoch 10.107: Loss = 0.676254
Epoch 10.108: Loss = 0.708923
Epoch 10.109: Loss = 0.779282
Epoch 10.110: Loss = 0.685013
Epoch 10.111: Loss = 0.584717
Epoch 10.112: Loss = 0.645203
Epoch 10.113: Loss = 0.792877
Epoch 10.114: Loss = 0.587143
Epoch 10.115: Loss = 0.604279
Epoch 10.116: Loss = 0.649353
Epoch 10.117: Loss = 0.712128
Epoch 10.118: Loss = 0.790161
Epoch 10.119: Loss = 0.605103
Epoch 10.120: Loss = 0.666977
TRAIN LOSS = 0.670151
TRAIN ACC = 81.2408 % (48746/60000)
Loss = 0.609665
Loss = 0.744614
Loss = 0.698593
Loss = 0.600433
Loss = 0.658203
Loss = 0.825577
Loss = 0.874435
Loss = 0.739029
Loss = 0.714767
Loss = 0.595978
Loss = 0.864395
Loss = 0.809677
Loss = 0.73848
Loss = 0.71666
Loss = 0.658905
Loss = 0.717682
Loss = 0.693253
Loss = 0.771515
Loss = 0.719849
Loss = 0.723007
TEST LOSS = 0.723736
TEST ACC = 487.459 % (7974/10000)

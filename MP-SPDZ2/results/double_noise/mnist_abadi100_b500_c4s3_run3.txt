Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.43246
Epoch 1.2: Loss = 2.36099
Epoch 1.3: Loss = 2.29755
Epoch 1.4: Loss = 2.24875
Epoch 1.5: Loss = 2.22633
Epoch 1.6: Loss = 2.20987
Epoch 1.7: Loss = 2.13773
Epoch 1.8: Loss = 2.11765
Epoch 1.9: Loss = 2.04054
Epoch 1.10: Loss = 2.02963
Epoch 1.11: Loss = 1.97833
Epoch 1.12: Loss = 1.98259
Epoch 1.13: Loss = 1.91826
Epoch 1.14: Loss = 1.88217
Epoch 1.15: Loss = 1.85315
Epoch 1.16: Loss = 1.82504
Epoch 1.17: Loss = 1.81653
Epoch 1.18: Loss = 1.78722
Epoch 1.19: Loss = 1.73103
Epoch 1.20: Loss = 1.71257
Epoch 1.21: Loss = 1.71344
Epoch 1.22: Loss = 1.61746
Epoch 1.23: Loss = 1.62518
Epoch 1.24: Loss = 1.61375
Epoch 1.25: Loss = 1.56966
Epoch 1.26: Loss = 1.56424
Epoch 1.27: Loss = 1.52545
Epoch 1.28: Loss = 1.51663
Epoch 1.29: Loss = 1.46419
Epoch 1.30: Loss = 1.4146
Epoch 1.31: Loss = 1.42291
Epoch 1.32: Loss = 1.41083
Epoch 1.33: Loss = 1.38257
Epoch 1.34: Loss = 1.36461
Epoch 1.35: Loss = 1.31305
Epoch 1.36: Loss = 1.34499
Epoch 1.37: Loss = 1.35889
Epoch 1.38: Loss = 1.27216
Epoch 1.39: Loss = 1.27559
Epoch 1.40: Loss = 1.20201
Epoch 1.41: Loss = 1.21652
Epoch 1.42: Loss = 1.1855
Epoch 1.43: Loss = 1.16606
Epoch 1.44: Loss = 1.20724
Epoch 1.45: Loss = 1.16185
Epoch 1.46: Loss = 1.1443
Epoch 1.47: Loss = 1.11592
Epoch 1.48: Loss = 1.13629
Epoch 1.49: Loss = 1.07057
Epoch 1.50: Loss = 1.04994
Epoch 1.51: Loss = 1.06918
Epoch 1.52: Loss = 0.979507
Epoch 1.53: Loss = 1.03371
Epoch 1.54: Loss = 0.989243
Epoch 1.55: Loss = 1.043
Epoch 1.56: Loss = 0.993866
Epoch 1.57: Loss = 1.02611
Epoch 1.58: Loss = 1.03593
Epoch 1.59: Loss = 0.984116
Epoch 1.60: Loss = 0.951447
Epoch 1.61: Loss = 0.902771
Epoch 1.62: Loss = 0.919388
Epoch 1.63: Loss = 0.912842
Epoch 1.64: Loss = 0.924957
Epoch 1.65: Loss = 0.868912
Epoch 1.66: Loss = 0.953522
Epoch 1.67: Loss = 0.884018
Epoch 1.68: Loss = 0.860458
Epoch 1.69: Loss = 0.840652
Epoch 1.70: Loss = 0.871994
Epoch 1.71: Loss = 0.92009
Epoch 1.72: Loss = 0.822601
Epoch 1.73: Loss = 0.791992
Epoch 1.74: Loss = 0.789368
Epoch 1.75: Loss = 0.850296
Epoch 1.76: Loss = 0.785568
Epoch 1.77: Loss = 0.863113
Epoch 1.78: Loss = 0.78212
Epoch 1.79: Loss = 0.806274
Epoch 1.80: Loss = 0.790054
Epoch 1.81: Loss = 0.760757
Epoch 1.82: Loss = 0.782501
Epoch 1.83: Loss = 0.737427
Epoch 1.84: Loss = 0.74556
Epoch 1.85: Loss = 0.70079
Epoch 1.86: Loss = 0.757507
Epoch 1.87: Loss = 0.709473
Epoch 1.88: Loss = 0.732971
Epoch 1.89: Loss = 0.727676
Epoch 1.90: Loss = 0.68335
Epoch 1.91: Loss = 0.699417
Epoch 1.92: Loss = 0.715729
Epoch 1.93: Loss = 0.68161
Epoch 1.94: Loss = 0.704437
Epoch 1.95: Loss = 0.683426
Epoch 1.96: Loss = 0.682617
Epoch 1.97: Loss = 0.753052
Epoch 1.98: Loss = 0.74939
Epoch 1.99: Loss = 0.651733
Epoch 1.100: Loss = 0.653244
Epoch 1.101: Loss = 0.703888
Epoch 1.102: Loss = 0.671951
Epoch 1.103: Loss = 0.691788
Epoch 1.104: Loss = 0.627731
Epoch 1.105: Loss = 0.66069
Epoch 1.106: Loss = 0.691315
Epoch 1.107: Loss = 0.626938
Epoch 1.108: Loss = 0.678818
Epoch 1.109: Loss = 0.662796
Epoch 1.110: Loss = 0.619873
Epoch 1.111: Loss = 0.673615
Epoch 1.112: Loss = 0.621918
Epoch 1.113: Loss = 0.609314
Epoch 1.114: Loss = 0.626404
Epoch 1.115: Loss = 0.620224
Epoch 1.116: Loss = 0.698151
Epoch 1.117: Loss = 0.581177
Epoch 1.118: Loss = 0.662796
Epoch 1.119: Loss = 0.611252
Epoch 1.120: Loss = 0.581039
TRAIN LOSS = 1.12915
TRAIN ACC = 67.9001 % (40741/60000)
Loss = 0.633286
Loss = 0.674957
Loss = 0.76973
Loss = 0.712784
Loss = 0.759583
Loss = 0.667542
Loss = 0.615112
Loss = 0.773254
Loss = 0.745453
Loss = 0.710617
Loss = 0.377533
Loss = 0.56694
Loss = 0.395248
Loss = 0.581192
Loss = 0.477219
Loss = 0.47493
Loss = 0.441956
Loss = 0.264526
Loss = 0.431412
Loss = 0.690063
TEST LOSS = 0.588167
TEST ACC = 407.41 % (8310/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.636963
Epoch 2.2: Loss = 0.60968
Epoch 2.3: Loss = 0.614532
Epoch 2.4: Loss = 0.554214
Epoch 2.5: Loss = 0.616714
Epoch 2.6: Loss = 0.57016
Epoch 2.7: Loss = 0.589569
Epoch 2.8: Loss = 0.69072
Epoch 2.9: Loss = 0.614349
Epoch 2.10: Loss = 0.558487
Epoch 2.11: Loss = 0.553543
Epoch 2.12: Loss = 0.646118
Epoch 2.13: Loss = 0.570709
Epoch 2.14: Loss = 0.596024
Epoch 2.15: Loss = 0.491913
Epoch 2.16: Loss = 0.590958
Epoch 2.17: Loss = 0.575287
Epoch 2.18: Loss = 0.580978
Epoch 2.19: Loss = 0.564789
Epoch 2.20: Loss = 0.536179
Epoch 2.21: Loss = 0.568542
Epoch 2.22: Loss = 0.572159
Epoch 2.23: Loss = 0.575912
Epoch 2.24: Loss = 0.565598
Epoch 2.25: Loss = 0.599319
Epoch 2.26: Loss = 0.558502
Epoch 2.27: Loss = 0.544464
Epoch 2.28: Loss = 0.487366
Epoch 2.29: Loss = 0.524506
Epoch 2.30: Loss = 0.527832
Epoch 2.31: Loss = 0.559723
Epoch 2.32: Loss = 0.562866
Epoch 2.33: Loss = 0.55719
Epoch 2.34: Loss = 0.620193
Epoch 2.35: Loss = 0.609329
Epoch 2.36: Loss = 0.490509
Epoch 2.37: Loss = 0.567627
Epoch 2.38: Loss = 0.556366
Epoch 2.39: Loss = 0.566849
Epoch 2.40: Loss = 0.533127
Epoch 2.41: Loss = 0.530014
Epoch 2.42: Loss = 0.533569
Epoch 2.43: Loss = 0.510132
Epoch 2.44: Loss = 0.484726
Epoch 2.45: Loss = 0.529099
Epoch 2.46: Loss = 0.500793
Epoch 2.47: Loss = 0.535599
Epoch 2.48: Loss = 0.554825
Epoch 2.49: Loss = 0.515427
Epoch 2.50: Loss = 0.570816
Epoch 2.51: Loss = 0.481369
Epoch 2.52: Loss = 0.47435
Epoch 2.53: Loss = 0.556046
Epoch 2.54: Loss = 0.478638
Epoch 2.55: Loss = 0.475113
Epoch 2.56: Loss = 0.519897
Epoch 2.57: Loss = 0.510559
Epoch 2.58: Loss = 0.556747
Epoch 2.59: Loss = 0.484787
Epoch 2.60: Loss = 0.584579
Epoch 2.61: Loss = 0.498795
Epoch 2.62: Loss = 0.522339
Epoch 2.63: Loss = 0.454315
Epoch 2.64: Loss = 0.520538
Epoch 2.65: Loss = 0.54187
Epoch 2.66: Loss = 0.578873
Epoch 2.67: Loss = 0.48381
Epoch 2.68: Loss = 0.491867
Epoch 2.69: Loss = 0.490845
Epoch 2.70: Loss = 0.500153
Epoch 2.71: Loss = 0.441971
Epoch 2.72: Loss = 0.538116
Epoch 2.73: Loss = 0.5289
Epoch 2.74: Loss = 0.479538
Epoch 2.75: Loss = 0.465149
Epoch 2.76: Loss = 0.487152
Epoch 2.77: Loss = 0.512192
Epoch 2.78: Loss = 0.493286
Epoch 2.79: Loss = 0.488007
Epoch 2.80: Loss = 0.45816
Epoch 2.81: Loss = 0.486969
Epoch 2.82: Loss = 0.502655
Epoch 2.83: Loss = 0.443268
Epoch 2.84: Loss = 0.487778
Epoch 2.85: Loss = 0.455643
Epoch 2.86: Loss = 0.582413
Epoch 2.87: Loss = 0.561966
Epoch 2.88: Loss = 0.491196
Epoch 2.89: Loss = 0.499435
Epoch 2.90: Loss = 0.422089
Epoch 2.91: Loss = 0.502487
Epoch 2.92: Loss = 0.473877
Epoch 2.93: Loss = 0.469727
Epoch 2.94: Loss = 0.462112
Epoch 2.95: Loss = 0.537582
Epoch 2.96: Loss = 0.577698
Epoch 2.97: Loss = 0.521072
Epoch 2.98: Loss = 0.469315
Epoch 2.99: Loss = 0.420227
Epoch 2.100: Loss = 0.443802
Epoch 2.101: Loss = 0.493881
Epoch 2.102: Loss = 0.439957
Epoch 2.103: Loss = 0.468094
Epoch 2.104: Loss = 0.429916
Epoch 2.105: Loss = 0.449005
Epoch 2.106: Loss = 0.441299
Epoch 2.107: Loss = 0.475067
Epoch 2.108: Loss = 0.488129
Epoch 2.109: Loss = 0.531921
Epoch 2.110: Loss = 0.401611
Epoch 2.111: Loss = 0.501419
Epoch 2.112: Loss = 0.43576
Epoch 2.113: Loss = 0.450089
Epoch 2.114: Loss = 0.488708
Epoch 2.115: Loss = 0.412445
Epoch 2.116: Loss = 0.445511
Epoch 2.117: Loss = 0.425339
Epoch 2.118: Loss = 0.474213
Epoch 2.119: Loss = 0.46666
Epoch 2.120: Loss = 0.450134
TRAIN LOSS = 0.518784
TRAIN ACC = 84.5154 % (50711/60000)
Loss = 0.461624
Loss = 0.537369
Loss = 0.598083
Loss = 0.563248
Loss = 0.616898
Loss = 0.494171
Loss = 0.437607
Loss = 0.623138
Loss = 0.573975
Loss = 0.543991
Loss = 0.244949
Loss = 0.401352
Loss = 0.321884
Loss = 0.422409
Loss = 0.306244
Loss = 0.349945
Loss = 0.301849
Loss = 0.130432
Loss = 0.272827
Loss = 0.561234
TEST LOSS = 0.438161
TEST ACC = 507.109 % (8711/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.53479
Epoch 3.2: Loss = 0.488739
Epoch 3.3: Loss = 0.474991
Epoch 3.4: Loss = 0.425812
Epoch 3.5: Loss = 0.459229
Epoch 3.6: Loss = 0.439743
Epoch 3.7: Loss = 0.520813
Epoch 3.8: Loss = 0.543259
Epoch 3.9: Loss = 0.450424
Epoch 3.10: Loss = 0.361496
Epoch 3.11: Loss = 0.430878
Epoch 3.12: Loss = 0.525665
Epoch 3.13: Loss = 0.460037
Epoch 3.14: Loss = 0.449982
Epoch 3.15: Loss = 0.380615
Epoch 3.16: Loss = 0.325134
Epoch 3.17: Loss = 0.419067
Epoch 3.18: Loss = 0.546417
Epoch 3.19: Loss = 0.384109
Epoch 3.20: Loss = 0.454575
Epoch 3.21: Loss = 0.382706
Epoch 3.22: Loss = 0.392853
Epoch 3.23: Loss = 0.476868
Epoch 3.24: Loss = 0.534912
Epoch 3.25: Loss = 0.461105
Epoch 3.26: Loss = 0.409302
Epoch 3.27: Loss = 0.423141
Epoch 3.28: Loss = 0.362915
Epoch 3.29: Loss = 0.409424
Epoch 3.30: Loss = 0.403809
Epoch 3.31: Loss = 0.499969
Epoch 3.32: Loss = 0.453796
Epoch 3.33: Loss = 0.52742
Epoch 3.34: Loss = 0.451782
Epoch 3.35: Loss = 0.38266
Epoch 3.36: Loss = 0.40329
Epoch 3.37: Loss = 0.490631
Epoch 3.38: Loss = 0.475983
Epoch 3.39: Loss = 0.468262
Epoch 3.40: Loss = 0.441086
Epoch 3.41: Loss = 0.456772
Epoch 3.42: Loss = 0.431107
Epoch 3.43: Loss = 0.502594
Epoch 3.44: Loss = 0.385864
Epoch 3.45: Loss = 0.394257
Epoch 3.46: Loss = 0.360336
Epoch 3.47: Loss = 0.438965
Epoch 3.48: Loss = 0.414658
Epoch 3.49: Loss = 0.410995
Epoch 3.50: Loss = 0.438538
Epoch 3.51: Loss = 0.485229
Epoch 3.52: Loss = 0.509903
Epoch 3.53: Loss = 0.489899
Epoch 3.54: Loss = 0.374939
Epoch 3.55: Loss = 0.495728
Epoch 3.56: Loss = 0.477356
Epoch 3.57: Loss = 0.514542
Epoch 3.58: Loss = 0.463852
Epoch 3.59: Loss = 0.407837
Epoch 3.60: Loss = 0.376007
Epoch 3.61: Loss = 0.478882
Epoch 3.62: Loss = 0.41333
Epoch 3.63: Loss = 0.402252
Epoch 3.64: Loss = 0.417755
Epoch 3.65: Loss = 0.48967
Epoch 3.66: Loss = 0.50885
Epoch 3.67: Loss = 0.494385
Epoch 3.68: Loss = 0.357681
Epoch 3.69: Loss = 0.447861
Epoch 3.70: Loss = 0.383942
Epoch 3.71: Loss = 0.388153
Epoch 3.72: Loss = 0.425049
Epoch 3.73: Loss = 0.484863
Epoch 3.74: Loss = 0.435287
Epoch 3.75: Loss = 0.445618
Epoch 3.76: Loss = 0.463409
Epoch 3.77: Loss = 0.427765
Epoch 3.78: Loss = 0.378891
Epoch 3.79: Loss = 0.408325
Epoch 3.80: Loss = 0.457535
Epoch 3.81: Loss = 0.427109
Epoch 3.82: Loss = 0.448441
Epoch 3.83: Loss = 0.469482
Epoch 3.84: Loss = 0.412048
Epoch 3.85: Loss = 0.382538
Epoch 3.86: Loss = 0.477844
Epoch 3.87: Loss = 0.405609
Epoch 3.88: Loss = 0.427383
Epoch 3.89: Loss = 0.430817
Epoch 3.90: Loss = 0.391235
Epoch 3.91: Loss = 0.490372
Epoch 3.92: Loss = 0.481995
Epoch 3.93: Loss = 0.465775
Epoch 3.94: Loss = 0.407211
Epoch 3.95: Loss = 0.525467
Epoch 3.96: Loss = 0.420135
Epoch 3.97: Loss = 0.454239
Epoch 3.98: Loss = 0.470459
Epoch 3.99: Loss = 0.530807
Epoch 3.100: Loss = 0.437225
Epoch 3.101: Loss = 0.367691
Epoch 3.102: Loss = 0.494308
Epoch 3.103: Loss = 0.481476
Epoch 3.104: Loss = 0.395233
Epoch 3.105: Loss = 0.423538
Epoch 3.106: Loss = 0.487701
Epoch 3.107: Loss = 0.399094
Epoch 3.108: Loss = 0.475876
Epoch 3.109: Loss = 0.455154
Epoch 3.110: Loss = 0.413727
Epoch 3.111: Loss = 0.406021
Epoch 3.112: Loss = 0.418137
Epoch 3.113: Loss = 0.405167
Epoch 3.114: Loss = 0.404099
Epoch 3.115: Loss = 0.491806
Epoch 3.116: Loss = 0.494186
Epoch 3.117: Loss = 0.467957
Epoch 3.118: Loss = 0.48439
Epoch 3.119: Loss = 0.484528
Epoch 3.120: Loss = 0.446121
TRAIN LOSS = 0.444321
TRAIN ACC = 86.7447 % (52049/60000)
Loss = 0.398224
Loss = 0.495499
Loss = 0.547333
Loss = 0.512955
Loss = 0.58522
Loss = 0.432648
Loss = 0.377747
Loss = 0.605682
Loss = 0.553558
Loss = 0.494583
Loss = 0.207382
Loss = 0.340668
Loss = 0.284943
Loss = 0.380768
Loss = 0.275803
Loss = 0.336243
Loss = 0.258545
Loss = 0.0899811
Loss = 0.240158
Loss = 0.519547
TEST LOSS = 0.396874
TEST ACC = 520.49 % (8825/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.439972
Epoch 4.2: Loss = 0.403809
Epoch 4.3: Loss = 0.390747
Epoch 4.4: Loss = 0.387924
Epoch 4.5: Loss = 0.395889
Epoch 4.6: Loss = 0.409073
Epoch 4.7: Loss = 0.404312
Epoch 4.8: Loss = 0.48967
Epoch 4.9: Loss = 0.349121
Epoch 4.10: Loss = 0.473831
Epoch 4.11: Loss = 0.410233
Epoch 4.12: Loss = 0.432129
Epoch 4.13: Loss = 0.406113
Epoch 4.14: Loss = 0.376022
Epoch 4.15: Loss = 0.540848
Epoch 4.16: Loss = 0.392029
Epoch 4.17: Loss = 0.453888
Epoch 4.18: Loss = 0.412842
Epoch 4.19: Loss = 0.38797
Epoch 4.20: Loss = 0.402512
Epoch 4.21: Loss = 0.353165
Epoch 4.22: Loss = 0.419128
Epoch 4.23: Loss = 0.519485
Epoch 4.24: Loss = 0.383713
Epoch 4.25: Loss = 0.479004
Epoch 4.26: Loss = 0.41716
Epoch 4.27: Loss = 0.304153
Epoch 4.28: Loss = 0.390335
Epoch 4.29: Loss = 0.497147
Epoch 4.30: Loss = 0.357101
Epoch 4.31: Loss = 0.415131
Epoch 4.32: Loss = 0.42157
Epoch 4.33: Loss = 0.376343
Epoch 4.34: Loss = 0.453003
Epoch 4.35: Loss = 0.401321
Epoch 4.36: Loss = 0.485001
Epoch 4.37: Loss = 0.374603
Epoch 4.38: Loss = 0.440536
Epoch 4.39: Loss = 0.418457
Epoch 4.40: Loss = 0.414536
Epoch 4.41: Loss = 0.463181
Epoch 4.42: Loss = 0.394409
Epoch 4.43: Loss = 0.410553
Epoch 4.44: Loss = 0.421295
Epoch 4.45: Loss = 0.485107
Epoch 4.46: Loss = 0.326523
Epoch 4.47: Loss = 0.480499
Epoch 4.48: Loss = 0.403946
Epoch 4.49: Loss = 0.433212
Epoch 4.50: Loss = 0.47435
Epoch 4.51: Loss = 0.388718
Epoch 4.52: Loss = 0.38205
Epoch 4.53: Loss = 0.490036
Epoch 4.54: Loss = 0.418823
Epoch 4.55: Loss = 0.434021
Epoch 4.56: Loss = 0.454987
Epoch 4.57: Loss = 0.447983
Epoch 4.58: Loss = 0.416809
Epoch 4.59: Loss = 0.393799
Epoch 4.60: Loss = 0.44812
Epoch 4.61: Loss = 0.364059
Epoch 4.62: Loss = 0.4431
Epoch 4.63: Loss = 0.404068
Epoch 4.64: Loss = 0.41394
Epoch 4.65: Loss = 0.371368
Epoch 4.66: Loss = 0.400757
Epoch 4.67: Loss = 0.415649
Epoch 4.68: Loss = 0.382355
Epoch 4.69: Loss = 0.48349
Epoch 4.70: Loss = 0.39267
Epoch 4.71: Loss = 0.453079
Epoch 4.72: Loss = 0.406891
Epoch 4.73: Loss = 0.447296
Epoch 4.74: Loss = 0.440155
Epoch 4.75: Loss = 0.416977
Epoch 4.76: Loss = 0.531998
Epoch 4.77: Loss = 0.446396
Epoch 4.78: Loss = 0.384933
Epoch 4.79: Loss = 0.484344
Epoch 4.80: Loss = 0.395905
Epoch 4.81: Loss = 0.344681
Epoch 4.82: Loss = 0.350449
Epoch 4.83: Loss = 0.483521
Epoch 4.84: Loss = 0.353928
Epoch 4.85: Loss = 0.363388
Epoch 4.86: Loss = 0.364899
Epoch 4.87: Loss = 0.408768
Epoch 4.88: Loss = 0.420258
Epoch 4.89: Loss = 0.356155
Epoch 4.90: Loss = 0.442429
Epoch 4.91: Loss = 0.499603
Epoch 4.92: Loss = 0.47818
Epoch 4.93: Loss = 0.41922
Epoch 4.94: Loss = 0.409607
Epoch 4.95: Loss = 0.463654
Epoch 4.96: Loss = 0.453995
Epoch 4.97: Loss = 0.360779
Epoch 4.98: Loss = 0.40506
Epoch 4.99: Loss = 0.389465
Epoch 4.100: Loss = 0.490189
Epoch 4.101: Loss = 0.438904
Epoch 4.102: Loss = 0.414642
Epoch 4.103: Loss = 0.475739
Epoch 4.104: Loss = 0.388489
Epoch 4.105: Loss = 0.397797
Epoch 4.106: Loss = 0.423187
Epoch 4.107: Loss = 0.449768
Epoch 4.108: Loss = 0.392426
Epoch 4.109: Loss = 0.375076
Epoch 4.110: Loss = 0.38031
Epoch 4.111: Loss = 0.480881
Epoch 4.112: Loss = 0.424088
Epoch 4.113: Loss = 0.393066
Epoch 4.114: Loss = 0.500137
Epoch 4.115: Loss = 0.424164
Epoch 4.116: Loss = 0.427429
Epoch 4.117: Loss = 0.452011
Epoch 4.118: Loss = 0.345764
Epoch 4.119: Loss = 0.414307
Epoch 4.120: Loss = 0.453186
TRAIN LOSS = 0.420319
TRAIN ACC = 87.6556 % (52596/60000)
Loss = 0.37352
Loss = 0.475357
Loss = 0.527267
Loss = 0.514801
Loss = 0.556717
Loss = 0.412827
Loss = 0.362549
Loss = 0.607422
Loss = 0.553024
Loss = 0.483612
Loss = 0.187531
Loss = 0.323212
Loss = 0.286545
Loss = 0.376541
Loss = 0.243042
Loss = 0.335495
Loss = 0.227753
Loss = 0.0677643
Loss = 0.218658
Loss = 0.501892
TEST LOSS = 0.381776
TEST ACC = 525.96 % (8890/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.43895
Epoch 5.2: Loss = 0.385574
Epoch 5.3: Loss = 0.427261
Epoch 5.4: Loss = 0.356049
Epoch 5.5: Loss = 0.381699
Epoch 5.6: Loss = 0.396393
Epoch 5.7: Loss = 0.488144
Epoch 5.8: Loss = 0.422348
Epoch 5.9: Loss = 0.333557
Epoch 5.10: Loss = 0.358582
Epoch 5.11: Loss = 0.425751
Epoch 5.12: Loss = 0.534485
Epoch 5.13: Loss = 0.469421
Epoch 5.14: Loss = 0.374207
Epoch 5.15: Loss = 0.368637
Epoch 5.16: Loss = 0.470001
Epoch 5.17: Loss = 0.44931
Epoch 5.18: Loss = 0.420883
Epoch 5.19: Loss = 0.388901
Epoch 5.20: Loss = 0.472046
Epoch 5.21: Loss = 0.418488
Epoch 5.22: Loss = 0.381042
Epoch 5.23: Loss = 0.325256
Epoch 5.24: Loss = 0.445557
Epoch 5.25: Loss = 0.428375
Epoch 5.26: Loss = 0.467514
Epoch 5.27: Loss = 0.485474
Epoch 5.28: Loss = 0.479675
Epoch 5.29: Loss = 0.362503
Epoch 5.30: Loss = 0.521835
Epoch 5.31: Loss = 0.468567
Epoch 5.32: Loss = 0.454163
Epoch 5.33: Loss = 0.433212
Epoch 5.34: Loss = 0.417648
Epoch 5.35: Loss = 0.413742
Epoch 5.36: Loss = 0.382416
Epoch 5.37: Loss = 0.408417
Epoch 5.38: Loss = 0.370285
Epoch 5.39: Loss = 0.361374
Epoch 5.40: Loss = 0.39859
Epoch 5.41: Loss = 0.369522
Epoch 5.42: Loss = 0.38797
Epoch 5.43: Loss = 0.350861
Epoch 5.44: Loss = 0.424667
Epoch 5.45: Loss = 0.330276
Epoch 5.46: Loss = 0.298553
Epoch 5.47: Loss = 0.441742
Epoch 5.48: Loss = 0.475128
Epoch 5.49: Loss = 0.447281
Epoch 5.50: Loss = 0.419556
Epoch 5.51: Loss = 0.399384
Epoch 5.52: Loss = 0.387238
Epoch 5.53: Loss = 0.495682
Epoch 5.54: Loss = 0.384888
Epoch 5.55: Loss = 0.367508
Epoch 5.56: Loss = 0.447144
Epoch 5.57: Loss = 0.444901
Epoch 5.58: Loss = 0.329666
Epoch 5.59: Loss = 0.336166
Epoch 5.60: Loss = 0.384506
Epoch 5.61: Loss = 0.419876
Epoch 5.62: Loss = 0.331909
Epoch 5.63: Loss = 0.517792
Epoch 5.64: Loss = 0.386276
Epoch 5.65: Loss = 0.480438
Epoch 5.66: Loss = 0.412933
Epoch 5.67: Loss = 0.501678
Epoch 5.68: Loss = 0.407257
Epoch 5.69: Loss = 0.366791
Epoch 5.70: Loss = 0.351318
Epoch 5.71: Loss = 0.45166
Epoch 5.72: Loss = 0.382217
Epoch 5.73: Loss = 0.355438
Epoch 5.74: Loss = 0.367569
Epoch 5.75: Loss = 0.357483
Epoch 5.76: Loss = 0.459824
Epoch 5.77: Loss = 0.327728
Epoch 5.78: Loss = 0.382523
Epoch 5.79: Loss = 0.430618
Epoch 5.80: Loss = 0.371933
Epoch 5.81: Loss = 0.438904
Epoch 5.82: Loss = 0.294449
Epoch 5.83: Loss = 0.379166
Epoch 5.84: Loss = 0.405884
Epoch 5.85: Loss = 0.423981
Epoch 5.86: Loss = 0.404785
Epoch 5.87: Loss = 0.401443
Epoch 5.88: Loss = 0.448883
Epoch 5.89: Loss = 0.340378
Epoch 5.90: Loss = 0.3564
Epoch 5.91: Loss = 0.338043
Epoch 5.92: Loss = 0.485641
Epoch 5.93: Loss = 0.505249
Epoch 5.94: Loss = 0.35614
Epoch 5.95: Loss = 0.442429
Epoch 5.96: Loss = 0.43811
Epoch 5.97: Loss = 0.362106
Epoch 5.98: Loss = 0.320541
Epoch 5.99: Loss = 0.431564
Epoch 5.100: Loss = 0.404633
Epoch 5.101: Loss = 0.446152
Epoch 5.102: Loss = 0.328094
Epoch 5.103: Loss = 0.41716
Epoch 5.104: Loss = 0.413544
Epoch 5.105: Loss = 0.474167
Epoch 5.106: Loss = 0.372345
Epoch 5.107: Loss = 0.385376
Epoch 5.108: Loss = 0.286789
Epoch 5.109: Loss = 0.37439
Epoch 5.110: Loss = 0.455521
Epoch 5.111: Loss = 0.366699
Epoch 5.112: Loss = 0.402252
Epoch 5.113: Loss = 0.35791
Epoch 5.114: Loss = 0.394196
Epoch 5.115: Loss = 0.469238
Epoch 5.116: Loss = 0.356201
Epoch 5.117: Loss = 0.487244
Epoch 5.118: Loss = 0.478958
Epoch 5.119: Loss = 0.445068
Epoch 5.120: Loss = 0.38475
TRAIN LOSS = 0.407059
TRAIN ACC = 88.1592 % (52898/60000)
Loss = 0.364014
Loss = 0.457779
Loss = 0.523193
Loss = 0.521194
Loss = 0.547318
Loss = 0.407913
Loss = 0.349655
Loss = 0.59938
Loss = 0.552231
Loss = 0.463684
Loss = 0.174362
Loss = 0.320435
Loss = 0.286423
Loss = 0.378082
Loss = 0.220901
Loss = 0.305313
Loss = 0.206467
Loss = 0.0666351
Loss = 0.22403
Loss = 0.482574
TEST LOSS = 0.372579
TEST ACC = 528.979 % (8932/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.387268
Epoch 6.2: Loss = 0.403717
Epoch 6.3: Loss = 0.479324
Epoch 6.4: Loss = 0.326935
Epoch 6.5: Loss = 0.457291
Epoch 6.6: Loss = 0.400726
Epoch 6.7: Loss = 0.466904
Epoch 6.8: Loss = 0.46489
Epoch 6.9: Loss = 0.373672
Epoch 6.10: Loss = 0.409714
Epoch 6.11: Loss = 0.431091
Epoch 6.12: Loss = 0.431717
Epoch 6.13: Loss = 0.378311
Epoch 6.14: Loss = 0.332672
Epoch 6.15: Loss = 0.376999
Epoch 6.16: Loss = 0.42067
Epoch 6.17: Loss = 0.477219
Epoch 6.18: Loss = 0.417679
Epoch 6.19: Loss = 0.423798
Epoch 6.20: Loss = 0.447006
Epoch 6.21: Loss = 0.367798
Epoch 6.22: Loss = 0.405823
Epoch 6.23: Loss = 0.391922
Epoch 6.24: Loss = 0.486282
Epoch 6.25: Loss = 0.360382
Epoch 6.26: Loss = 0.366272
Epoch 6.27: Loss = 0.344757
Epoch 6.28: Loss = 0.37648
Epoch 6.29: Loss = 0.360336
Epoch 6.30: Loss = 0.351898
Epoch 6.31: Loss = 0.355194
Epoch 6.32: Loss = 0.406067
Epoch 6.33: Loss = 0.427521
Epoch 6.34: Loss = 0.34816
Epoch 6.35: Loss = 0.424927
Epoch 6.36: Loss = 0.392624
Epoch 6.37: Loss = 0.343643
Epoch 6.38: Loss = 0.418747
Epoch 6.39: Loss = 0.439392
Epoch 6.40: Loss = 0.46933
Epoch 6.41: Loss = 0.419754
Epoch 6.42: Loss = 0.47171
Epoch 6.43: Loss = 0.302628
Epoch 6.44: Loss = 0.423706
Epoch 6.45: Loss = 0.394958
Epoch 6.46: Loss = 0.382416
Epoch 6.47: Loss = 0.414841
Epoch 6.48: Loss = 0.447342
Epoch 6.49: Loss = 0.469391
Epoch 6.50: Loss = 0.414215
Epoch 6.51: Loss = 0.43396
Epoch 6.52: Loss = 0.368835
Epoch 6.53: Loss = 0.362274
Epoch 6.54: Loss = 0.361282
Epoch 6.55: Loss = 0.403595
Epoch 6.56: Loss = 0.381287
Epoch 6.57: Loss = 0.409317
Epoch 6.58: Loss = 0.407272
Epoch 6.59: Loss = 0.378342
Epoch 6.60: Loss = 0.475143
Epoch 6.61: Loss = 0.483047
Epoch 6.62: Loss = 0.395584
Epoch 6.63: Loss = 0.401077
Epoch 6.64: Loss = 0.346252
Epoch 6.65: Loss = 0.445313
Epoch 6.66: Loss = 0.467499
Epoch 6.67: Loss = 0.477982
Epoch 6.68: Loss = 0.329163
Epoch 6.69: Loss = 0.444199
Epoch 6.70: Loss = 0.395279
Epoch 6.71: Loss = 0.405319
Epoch 6.72: Loss = 0.319016
Epoch 6.73: Loss = 0.526642
Epoch 6.74: Loss = 0.437622
Epoch 6.75: Loss = 0.469086
Epoch 6.76: Loss = 0.352722
Epoch 6.77: Loss = 0.386826
Epoch 6.78: Loss = 0.446457
Epoch 6.79: Loss = 0.42424
Epoch 6.80: Loss = 0.402832
Epoch 6.81: Loss = 0.407013
Epoch 6.82: Loss = 0.352448
Epoch 6.83: Loss = 0.345337
Epoch 6.84: Loss = 0.438339
Epoch 6.85: Loss = 0.347061
Epoch 6.86: Loss = 0.430496
Epoch 6.87: Loss = 0.408218
Epoch 6.88: Loss = 0.430481
Epoch 6.89: Loss = 0.412659
Epoch 6.90: Loss = 0.351822
Epoch 6.91: Loss = 0.484589
Epoch 6.92: Loss = 0.48761
Epoch 6.93: Loss = 0.340454
Epoch 6.94: Loss = 0.490936
Epoch 6.95: Loss = 0.40535
Epoch 6.96: Loss = 0.3358
Epoch 6.97: Loss = 0.377258
Epoch 6.98: Loss = 0.307495
Epoch 6.99: Loss = 0.38266
Epoch 6.100: Loss = 0.417252
Epoch 6.101: Loss = 0.432449
Epoch 6.102: Loss = 0.37677
Epoch 6.103: Loss = 0.377914
Epoch 6.104: Loss = 0.379684
Epoch 6.105: Loss = 0.275604
Epoch 6.106: Loss = 0.347321
Epoch 6.107: Loss = 0.43663
Epoch 6.108: Loss = 0.374924
Epoch 6.109: Loss = 0.371704
Epoch 6.110: Loss = 0.328766
Epoch 6.111: Loss = 0.44603
Epoch 6.112: Loss = 0.322815
Epoch 6.113: Loss = 0.408249
Epoch 6.114: Loss = 0.280334
Epoch 6.115: Loss = 0.439133
Epoch 6.116: Loss = 0.390518
Epoch 6.117: Loss = 0.432419
Epoch 6.118: Loss = 0.39592
Epoch 6.119: Loss = 0.335022
Epoch 6.120: Loss = 0.484116
TRAIN LOSS = 0.401764
TRAIN ACC = 88.6292 % (53180/60000)
Loss = 0.363983
Loss = 0.473999
Loss = 0.563065
Loss = 0.557022
Loss = 0.528366
Loss = 0.414597
Loss = 0.358719
Loss = 0.638412
Loss = 0.558243
Loss = 0.484695
Loss = 0.172684
Loss = 0.329529
Loss = 0.278366
Loss = 0.366592
Loss = 0.212219
Loss = 0.306824
Loss = 0.200058
Loss = 0.0615997
Loss = 0.23439
Loss = 0.467453
TEST LOSS = 0.378541
TEST ACC = 531.799 % (8947/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.377655
Epoch 7.2: Loss = 0.421951
Epoch 7.3: Loss = 0.398987
Epoch 7.4: Loss = 0.373871
Epoch 7.5: Loss = 0.350861
Epoch 7.6: Loss = 0.399643
Epoch 7.7: Loss = 0.441818
Epoch 7.8: Loss = 0.511642
Epoch 7.9: Loss = 0.38472
Epoch 7.10: Loss = 0.42305
Epoch 7.11: Loss = 0.487289
Epoch 7.12: Loss = 0.373993
Epoch 7.13: Loss = 0.437149
Epoch 7.14: Loss = 0.411789
Epoch 7.15: Loss = 0.45752
Epoch 7.16: Loss = 0.315613
Epoch 7.17: Loss = 0.302246
Epoch 7.18: Loss = 0.381393
Epoch 7.19: Loss = 0.349167
Epoch 7.20: Loss = 0.299637
Epoch 7.21: Loss = 0.340576
Epoch 7.22: Loss = 0.486053
Epoch 7.23: Loss = 0.357895
Epoch 7.24: Loss = 0.394608
Epoch 7.25: Loss = 0.443756
Epoch 7.26: Loss = 0.450256
Epoch 7.27: Loss = 0.439209
Epoch 7.28: Loss = 0.44455
Epoch 7.29: Loss = 0.332565
Epoch 7.30: Loss = 0.451813
Epoch 7.31: Loss = 0.530579
Epoch 7.32: Loss = 0.372543
Epoch 7.33: Loss = 0.398392
Epoch 7.34: Loss = 0.450684
Epoch 7.35: Loss = 0.409409
Epoch 7.36: Loss = 0.406799
Epoch 7.37: Loss = 0.362122
Epoch 7.38: Loss = 0.324905
Epoch 7.39: Loss = 0.388855
Epoch 7.40: Loss = 0.41394
Epoch 7.41: Loss = 0.354187
Epoch 7.42: Loss = 0.403854
Epoch 7.43: Loss = 0.373993
Epoch 7.44: Loss = 0.401001
Epoch 7.45: Loss = 0.297241
Epoch 7.46: Loss = 0.348907
Epoch 7.47: Loss = 0.430984
Epoch 7.48: Loss = 0.387939
Epoch 7.49: Loss = 0.331818
Epoch 7.50: Loss = 0.489883
Epoch 7.51: Loss = 0.4142
Epoch 7.52: Loss = 0.46756
Epoch 7.53: Loss = 0.427826
Epoch 7.54: Loss = 0.56778
Epoch 7.55: Loss = 0.394028
Epoch 7.56: Loss = 0.348267
Epoch 7.57: Loss = 0.446762
Epoch 7.58: Loss = 0.350037
Epoch 7.59: Loss = 0.392151
Epoch 7.60: Loss = 0.283401
Epoch 7.61: Loss = 0.497116
Epoch 7.62: Loss = 0.470505
Epoch 7.63: Loss = 0.355835
Epoch 7.64: Loss = 0.39183
Epoch 7.65: Loss = 0.37146
Epoch 7.66: Loss = 0.398224
Epoch 7.67: Loss = 0.293228
Epoch 7.68: Loss = 0.434799
Epoch 7.69: Loss = 0.393951
Epoch 7.70: Loss = 0.30513
Epoch 7.71: Loss = 0.41655
Epoch 7.72: Loss = 0.339691
Epoch 7.73: Loss = 0.41217
Epoch 7.74: Loss = 0.401947
Epoch 7.75: Loss = 0.378891
Epoch 7.76: Loss = 0.457184
Epoch 7.77: Loss = 0.306122
Epoch 7.78: Loss = 0.336975
Epoch 7.79: Loss = 0.550568
Epoch 7.80: Loss = 0.434174
Epoch 7.81: Loss = 0.366821
Epoch 7.82: Loss = 0.437836
Epoch 7.83: Loss = 0.401688
Epoch 7.84: Loss = 0.328964
Epoch 7.85: Loss = 0.398285
Epoch 7.86: Loss = 0.388092
Epoch 7.87: Loss = 0.437744
Epoch 7.88: Loss = 0.411133
Epoch 7.89: Loss = 0.419235
Epoch 7.90: Loss = 0.441116
Epoch 7.91: Loss = 0.405807
Epoch 7.92: Loss = 0.462906
Epoch 7.93: Loss = 0.305725
Epoch 7.94: Loss = 0.389862
Epoch 7.95: Loss = 0.389114
Epoch 7.96: Loss = 0.360184
Epoch 7.97: Loss = 0.471512
Epoch 7.98: Loss = 0.423706
Epoch 7.99: Loss = 0.392349
Epoch 7.100: Loss = 0.515152
Epoch 7.101: Loss = 0.401459
Epoch 7.102: Loss = 0.398056
Epoch 7.103: Loss = 0.494324
Epoch 7.104: Loss = 0.416718
Epoch 7.105: Loss = 0.41214
Epoch 7.106: Loss = 0.39592
Epoch 7.107: Loss = 0.470261
Epoch 7.108: Loss = 0.334381
Epoch 7.109: Loss = 0.40921
Epoch 7.110: Loss = 0.400116
Epoch 7.111: Loss = 0.431
Epoch 7.112: Loss = 0.445053
Epoch 7.113: Loss = 0.333618
Epoch 7.114: Loss = 0.422028
Epoch 7.115: Loss = 0.450638
Epoch 7.116: Loss = 0.410553
Epoch 7.117: Loss = 0.408325
Epoch 7.118: Loss = 0.331802
Epoch 7.119: Loss = 0.309723
Epoch 7.120: Loss = 0.393616
TRAIN LOSS = 0.401443
TRAIN ACC = 88.8626 % (53320/60000)
Loss = 0.367737
Loss = 0.474854
Loss = 0.537689
Loss = 0.573364
Loss = 0.533325
Loss = 0.380844
Loss = 0.343506
Loss = 0.635437
Loss = 0.551682
Loss = 0.477463
Loss = 0.171432
Loss = 0.300522
Loss = 0.316742
Loss = 0.354858
Loss = 0.206284
Loss = 0.300659
Loss = 0.227951
Loss = 0.0606384
Loss = 0.231079
Loss = 0.472687
TEST LOSS = 0.375938
TEST ACC = 533.199 % (8964/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.343201
Epoch 8.2: Loss = 0.489578
Epoch 8.3: Loss = 0.344391
Epoch 8.4: Loss = 0.383682
Epoch 8.5: Loss = 0.443207
Epoch 8.6: Loss = 0.288544
Epoch 8.7: Loss = 0.368912
Epoch 8.8: Loss = 0.353577
Epoch 8.9: Loss = 0.413742
Epoch 8.10: Loss = 0.415833
Epoch 8.11: Loss = 0.390076
Epoch 8.12: Loss = 0.4039
Epoch 8.13: Loss = 0.438599
Epoch 8.14: Loss = 0.350281
Epoch 8.15: Loss = 0.440109
Epoch 8.16: Loss = 0.372803
Epoch 8.17: Loss = 0.396652
Epoch 8.18: Loss = 0.400162
Epoch 8.19: Loss = 0.458817
Epoch 8.20: Loss = 0.37851
Epoch 8.21: Loss = 0.418808
Epoch 8.22: Loss = 0.341019
Epoch 8.23: Loss = 0.488464
Epoch 8.24: Loss = 0.39267
Epoch 8.25: Loss = 0.459976
Epoch 8.26: Loss = 0.301315
Epoch 8.27: Loss = 0.454361
Epoch 8.28: Loss = 0.348755
Epoch 8.29: Loss = 0.428421
Epoch 8.30: Loss = 0.380814
Epoch 8.31: Loss = 0.405121
Epoch 8.32: Loss = 0.427856
Epoch 8.33: Loss = 0.433929
Epoch 8.34: Loss = 0.32962
Epoch 8.35: Loss = 0.353043
Epoch 8.36: Loss = 0.448425
Epoch 8.37: Loss = 0.346436
Epoch 8.38: Loss = 0.412125
Epoch 8.39: Loss = 0.372742
Epoch 8.40: Loss = 0.416504
Epoch 8.41: Loss = 0.421219
Epoch 8.42: Loss = 0.475616
Epoch 8.43: Loss = 0.375473
Epoch 8.44: Loss = 0.379822
Epoch 8.45: Loss = 0.440796
Epoch 8.46: Loss = 0.308395
Epoch 8.47: Loss = 0.421341
Epoch 8.48: Loss = 0.415298
Epoch 8.49: Loss = 0.356613
Epoch 8.50: Loss = 0.361099
Epoch 8.51: Loss = 0.394943
Epoch 8.52: Loss = 0.481186
Epoch 8.53: Loss = 0.350906
Epoch 8.54: Loss = 0.399338
Epoch 8.55: Loss = 0.36441
Epoch 8.56: Loss = 0.387314
Epoch 8.57: Loss = 0.398117
Epoch 8.58: Loss = 0.343384
Epoch 8.59: Loss = 0.40535
Epoch 8.60: Loss = 0.365814
Epoch 8.61: Loss = 0.315125
Epoch 8.62: Loss = 0.377792
Epoch 8.63: Loss = 0.431915
Epoch 8.64: Loss = 0.421463
Epoch 8.65: Loss = 0.423004
Epoch 8.66: Loss = 0.436005
Epoch 8.67: Loss = 0.365845
Epoch 8.68: Loss = 0.438644
Epoch 8.69: Loss = 0.374573
Epoch 8.70: Loss = 0.352875
Epoch 8.71: Loss = 0.314636
Epoch 8.72: Loss = 0.403442
Epoch 8.73: Loss = 0.444443
Epoch 8.74: Loss = 0.394318
Epoch 8.75: Loss = 0.404404
Epoch 8.76: Loss = 0.251007
Epoch 8.77: Loss = 0.332535
Epoch 8.78: Loss = 0.452835
Epoch 8.79: Loss = 0.321579
Epoch 8.80: Loss = 0.378403
Epoch 8.81: Loss = 0.394653
Epoch 8.82: Loss = 0.405396
Epoch 8.83: Loss = 0.391846
Epoch 8.84: Loss = 0.335632
Epoch 8.85: Loss = 0.391479
Epoch 8.86: Loss = 0.316147
Epoch 8.87: Loss = 0.445862
Epoch 8.88: Loss = 0.388687
Epoch 8.89: Loss = 0.383148
Epoch 8.90: Loss = 0.442215
Epoch 8.91: Loss = 0.495178
Epoch 8.92: Loss = 0.430359
Epoch 8.93: Loss = 0.567062
Epoch 8.94: Loss = 0.416901
Epoch 8.95: Loss = 0.409363
Epoch 8.96: Loss = 0.447968
Epoch 8.97: Loss = 0.365814
Epoch 8.98: Loss = 0.375671
Epoch 8.99: Loss = 0.245209
Epoch 8.100: Loss = 0.51535
Epoch 8.101: Loss = 0.34877
Epoch 8.102: Loss = 0.371124
Epoch 8.103: Loss = 0.294144
Epoch 8.104: Loss = 0.38237
Epoch 8.105: Loss = 0.399124
Epoch 8.106: Loss = 0.412918
Epoch 8.107: Loss = 0.374817
Epoch 8.108: Loss = 0.430023
Epoch 8.109: Loss = 0.454651
Epoch 8.110: Loss = 0.39563
Epoch 8.111: Loss = 0.325592
Epoch 8.112: Loss = 0.341568
Epoch 8.113: Loss = 0.431366
Epoch 8.114: Loss = 0.471634
Epoch 8.115: Loss = 0.498169
Epoch 8.116: Loss = 0.360306
Epoch 8.117: Loss = 0.373169
Epoch 8.118: Loss = 0.423096
Epoch 8.119: Loss = 0.483337
Epoch 8.120: Loss = 0.357605
TRAIN LOSS = 0.395081
TRAIN ACC = 89.0961 % (53460/60000)
Loss = 0.347488
Loss = 0.471085
Loss = 0.507751
Loss = 0.54248
Loss = 0.500168
Loss = 0.381607
Loss = 0.319458
Loss = 0.612381
Loss = 0.536514
Loss = 0.465958
Loss = 0.152512
Loss = 0.298309
Loss = 0.284409
Loss = 0.328522
Loss = 0.205933
Loss = 0.303879
Loss = 0.222778
Loss = 0.0537109
Loss = 0.22435
Loss = 0.474182
TEST LOSS = 0.361674
TEST ACC = 534.599 % (9017/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.338333
Epoch 9.2: Loss = 0.333313
Epoch 9.3: Loss = 0.333176
Epoch 9.4: Loss = 0.471191
Epoch 9.5: Loss = 0.373123
Epoch 9.6: Loss = 0.434647
Epoch 9.7: Loss = 0.397614
Epoch 9.8: Loss = 0.356583
Epoch 9.9: Loss = 0.471466
Epoch 9.10: Loss = 0.41539
Epoch 9.11: Loss = 0.42189
Epoch 9.12: Loss = 0.355423
Epoch 9.13: Loss = 0.456451
Epoch 9.14: Loss = 0.305161
Epoch 9.15: Loss = 0.376984
Epoch 9.16: Loss = 0.356232
Epoch 9.17: Loss = 0.457626
Epoch 9.18: Loss = 0.464493
Epoch 9.19: Loss = 0.463699
Epoch 9.20: Loss = 0.501541
Epoch 9.21: Loss = 0.40715
Epoch 9.22: Loss = 0.370819
Epoch 9.23: Loss = 0.372375
Epoch 9.24: Loss = 0.455948
Epoch 9.25: Loss = 0.319672
Epoch 9.26: Loss = 0.405975
Epoch 9.27: Loss = 0.379547
Epoch 9.28: Loss = 0.440918
Epoch 9.29: Loss = 0.352829
Epoch 9.30: Loss = 0.345215
Epoch 9.31: Loss = 0.43367
Epoch 9.32: Loss = 0.344299
Epoch 9.33: Loss = 0.381378
Epoch 9.34: Loss = 0.403748
Epoch 9.35: Loss = 0.382996
Epoch 9.36: Loss = 0.355988
Epoch 9.37: Loss = 0.350342
Epoch 9.38: Loss = 0.386917
Epoch 9.39: Loss = 0.326859
Epoch 9.40: Loss = 0.34874
Epoch 9.41: Loss = 0.289093
Epoch 9.42: Loss = 0.356995
Epoch 9.43: Loss = 0.411636
Epoch 9.44: Loss = 0.380646
Epoch 9.45: Loss = 0.446793
Epoch 9.46: Loss = 0.304886
Epoch 9.47: Loss = 0.341049
Epoch 9.48: Loss = 0.362564
Epoch 9.49: Loss = 0.393341
Epoch 9.50: Loss = 0.43187
Epoch 9.51: Loss = 0.360947
Epoch 9.52: Loss = 0.367203
Epoch 9.53: Loss = 0.298965
Epoch 9.54: Loss = 0.412689
Epoch 9.55: Loss = 0.371613
Epoch 9.56: Loss = 0.414917
Epoch 9.57: Loss = 0.375107
Epoch 9.58: Loss = 0.419617
Epoch 9.59: Loss = 0.365448
Epoch 9.60: Loss = 0.381012
Epoch 9.61: Loss = 0.387543
Epoch 9.62: Loss = 0.318863
Epoch 9.63: Loss = 0.384445
Epoch 9.64: Loss = 0.422546
Epoch 9.65: Loss = 0.443848
Epoch 9.66: Loss = 0.372421
Epoch 9.67: Loss = 0.416443
Epoch 9.68: Loss = 0.346527
Epoch 9.69: Loss = 0.39061
Epoch 9.70: Loss = 0.430618
Epoch 9.71: Loss = 0.407211
Epoch 9.72: Loss = 0.29277
Epoch 9.73: Loss = 0.329819
Epoch 9.74: Loss = 0.386642
Epoch 9.75: Loss = 0.406769
Epoch 9.76: Loss = 0.35997
Epoch 9.77: Loss = 0.375671
Epoch 9.78: Loss = 0.47995
Epoch 9.79: Loss = 0.368591
Epoch 9.80: Loss = 0.450531
Epoch 9.81: Loss = 0.374161
Epoch 9.82: Loss = 0.386063
Epoch 9.83: Loss = 0.326401
Epoch 9.84: Loss = 0.384842
Epoch 9.85: Loss = 0.320496
Epoch 9.86: Loss = 0.40213
Epoch 9.87: Loss = 0.354324
Epoch 9.88: Loss = 0.444717
Epoch 9.89: Loss = 0.419083
Epoch 9.90: Loss = 0.378326
Epoch 9.91: Loss = 0.361023
Epoch 9.92: Loss = 0.366745
Epoch 9.93: Loss = 0.390701
Epoch 9.94: Loss = 0.431183
Epoch 9.95: Loss = 0.346878
Epoch 9.96: Loss = 0.36734
Epoch 9.97: Loss = 0.405106
Epoch 9.98: Loss = 0.417923
Epoch 9.99: Loss = 0.379517
Epoch 9.100: Loss = 0.336823
Epoch 9.101: Loss = 0.503647
Epoch 9.102: Loss = 0.41127
Epoch 9.103: Loss = 0.416489
Epoch 9.104: Loss = 0.443741
Epoch 9.105: Loss = 0.350952
Epoch 9.106: Loss = 0.447861
Epoch 9.107: Loss = 0.440353
Epoch 9.108: Loss = 0.393951
Epoch 9.109: Loss = 0.490646
Epoch 9.110: Loss = 0.42836
Epoch 9.111: Loss = 0.358185
Epoch 9.112: Loss = 0.344162
Epoch 9.113: Loss = 0.389984
Epoch 9.114: Loss = 0.424255
Epoch 9.115: Loss = 0.3909
Epoch 9.116: Loss = 0.289536
Epoch 9.117: Loss = 0.384308
Epoch 9.118: Loss = 0.392365
Epoch 9.119: Loss = 0.301682
Epoch 9.120: Loss = 0.35434
TRAIN LOSS = 0.387146
TRAIN ACC = 89.4745 % (53687/60000)
Loss = 0.332962
Loss = 0.436127
Loss = 0.477905
Loss = 0.551682
Loss = 0.484695
Loss = 0.376389
Loss = 0.334305
Loss = 0.598099
Loss = 0.508438
Loss = 0.462845
Loss = 0.170944
Loss = 0.291092
Loss = 0.305969
Loss = 0.350479
Loss = 0.19902
Loss = 0.283768
Loss = 0.219284
Loss = 0.0518494
Loss = 0.217545
Loss = 0.464706
TEST LOSS = 0.355905
TEST ACC = 536.87 % (9047/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.424744
Epoch 10.2: Loss = 0.354904
Epoch 10.3: Loss = 0.38562
Epoch 10.4: Loss = 0.335709
Epoch 10.5: Loss = 0.382065
Epoch 10.6: Loss = 0.349625
Epoch 10.7: Loss = 0.447662
Epoch 10.8: Loss = 0.345108
Epoch 10.9: Loss = 0.330963
Epoch 10.10: Loss = 0.439743
Epoch 10.11: Loss = 0.352829
Epoch 10.12: Loss = 0.367981
Epoch 10.13: Loss = 0.396271
Epoch 10.14: Loss = 0.32605
Epoch 10.15: Loss = 0.298035
Epoch 10.16: Loss = 0.391937
Epoch 10.17: Loss = 0.272919
Epoch 10.18: Loss = 0.365067
Epoch 10.19: Loss = 0.328568
Epoch 10.20: Loss = 0.3078
Epoch 10.21: Loss = 0.30278
Epoch 10.22: Loss = 0.357193
Epoch 10.23: Loss = 0.447601
Epoch 10.24: Loss = 0.368698
Epoch 10.25: Loss = 0.323746
Epoch 10.26: Loss = 0.429138
Epoch 10.27: Loss = 0.390686
Epoch 10.28: Loss = 0.354889
Epoch 10.29: Loss = 0.363708
Epoch 10.30: Loss = 0.46904
Epoch 10.31: Loss = 0.384216
Epoch 10.32: Loss = 0.49971
Epoch 10.33: Loss = 0.437088
Epoch 10.34: Loss = 0.374741
Epoch 10.35: Loss = 0.356339
Epoch 10.36: Loss = 0.405304
Epoch 10.37: Loss = 0.498093
Epoch 10.38: Loss = 0.453979
Epoch 10.39: Loss = 0.547531
Epoch 10.40: Loss = 0.43515
Epoch 10.41: Loss = 0.461365
Epoch 10.42: Loss = 0.334702
Epoch 10.43: Loss = 0.317825
Epoch 10.44: Loss = 0.384705
Epoch 10.45: Loss = 0.325485
Epoch 10.46: Loss = 0.296371
Epoch 10.47: Loss = 0.267944
Epoch 10.48: Loss = 0.461716
Epoch 10.49: Loss = 0.40744
Epoch 10.50: Loss = 0.359207
Epoch 10.51: Loss = 0.335205
Epoch 10.52: Loss = 0.343674
Epoch 10.53: Loss = 0.383987
Epoch 10.54: Loss = 0.374817
Epoch 10.55: Loss = 0.443726
Epoch 10.56: Loss = 0.347916
Epoch 10.57: Loss = 0.315796
Epoch 10.58: Loss = 0.38324
Epoch 10.59: Loss = 0.302841
Epoch 10.60: Loss = 0.40181
Epoch 10.61: Loss = 0.367432
Epoch 10.62: Loss = 0.38942
Epoch 10.63: Loss = 0.344421
Epoch 10.64: Loss = 0.280472
Epoch 10.65: Loss = 0.501389
Epoch 10.66: Loss = 0.450348
Epoch 10.67: Loss = 0.386917
Epoch 10.68: Loss = 0.356644
Epoch 10.69: Loss = 0.321518
Epoch 10.70: Loss = 0.373871
Epoch 10.71: Loss = 0.475845
Epoch 10.72: Loss = 0.304764
Epoch 10.73: Loss = 0.382324
Epoch 10.74: Loss = 0.310532
Epoch 10.75: Loss = 0.336746
Epoch 10.76: Loss = 0.440582
Epoch 10.77: Loss = 0.452393
Epoch 10.78: Loss = 0.345001
Epoch 10.79: Loss = 0.270554
Epoch 10.80: Loss = 0.461014
Epoch 10.81: Loss = 0.453262
Epoch 10.82: Loss = 0.43573
Epoch 10.83: Loss = 0.45575
Epoch 10.84: Loss = 0.344604
Epoch 10.85: Loss = 0.352631
Epoch 10.86: Loss = 0.313629
Epoch 10.87: Loss = 0.368881
Epoch 10.88: Loss = 0.484543
Epoch 10.89: Loss = 0.358765
Epoch 10.90: Loss = 0.297714
Epoch 10.91: Loss = 0.440903
Epoch 10.92: Loss = 0.418991
Epoch 10.93: Loss = 0.470337
Epoch 10.94: Loss = 0.354858
Epoch 10.95: Loss = 0.343491
Epoch 10.96: Loss = 0.398804
Epoch 10.97: Loss = 0.397354
Epoch 10.98: Loss = 0.531448
Epoch 10.99: Loss = 0.557251
Epoch 10.100: Loss = 0.396973
Epoch 10.101: Loss = 0.35376
Epoch 10.102: Loss = 0.374374
Epoch 10.103: Loss = 0.453934
Epoch 10.104: Loss = 0.356125
Epoch 10.105: Loss = 0.39563
Epoch 10.106: Loss = 0.519287
Epoch 10.107: Loss = 0.36499
Epoch 10.108: Loss = 0.395218
Epoch 10.109: Loss = 0.390884
Epoch 10.110: Loss = 0.267197
Epoch 10.111: Loss = 0.458389
Epoch 10.112: Loss = 0.383682
Epoch 10.113: Loss = 0.360153
Epoch 10.114: Loss = 0.38588
Epoch 10.115: Loss = 0.41156
Epoch 10.116: Loss = 0.331665
Epoch 10.117: Loss = 0.429581
Epoch 10.118: Loss = 0.425858
Epoch 10.119: Loss = 0.322571
Epoch 10.120: Loss = 0.373459
TRAIN LOSS = 0.38446
TRAIN ACC = 89.6042 % (53765/60000)
Loss = 0.325104
Loss = 0.442978
Loss = 0.489075
Loss = 0.546875
Loss = 0.515335
Loss = 0.35347
Loss = 0.330505
Loss = 0.606369
Loss = 0.509415
Loss = 0.461731
Loss = 0.15686
Loss = 0.297424
Loss = 0.328934
Loss = 0.359055
Loss = 0.187851
Loss = 0.268616
Loss = 0.22464
Loss = 0.0437469
Loss = 0.224899
Loss = 0.472366
TEST LOSS = 0.357262
TEST ACC = 537.65 % (9020/10000)

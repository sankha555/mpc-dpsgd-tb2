Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.44122
Epoch 1.2: Loss = 2.31734
Epoch 1.3: Loss = 2.1956
Epoch 1.4: Loss = 2.15048
Epoch 1.5: Loss = 2.05643
Epoch 1.6: Loss = 2.01184
Epoch 1.7: Loss = 1.94447
Epoch 1.8: Loss = 1.88086
Epoch 1.9: Loss = 1.85533
Epoch 1.10: Loss = 1.81705
Epoch 1.11: Loss = 1.78044
Epoch 1.12: Loss = 1.72728
Epoch 1.13: Loss = 1.67281
Epoch 1.14: Loss = 1.62294
Epoch 1.15: Loss = 1.61703
Epoch 1.16: Loss = 1.60373
Epoch 1.17: Loss = 1.5752
Epoch 1.18: Loss = 1.50853
Epoch 1.19: Loss = 1.45796
Epoch 1.20: Loss = 1.46458
Epoch 1.21: Loss = 1.40004
Epoch 1.22: Loss = 1.38812
Epoch 1.23: Loss = 1.41765
Epoch 1.24: Loss = 1.36334
Epoch 1.25: Loss = 1.34326
Epoch 1.26: Loss = 1.3156
Epoch 1.27: Loss = 1.30632
Epoch 1.28: Loss = 1.2458
Epoch 1.29: Loss = 1.23326
Epoch 1.30: Loss = 1.20702
Epoch 1.31: Loss = 1.20871
Epoch 1.32: Loss = 1.2075
Epoch 1.33: Loss = 1.23685
Epoch 1.34: Loss = 1.18723
Epoch 1.35: Loss = 1.16109
Epoch 1.36: Loss = 1.14407
Epoch 1.37: Loss = 1.14145
Epoch 1.38: Loss = 1.13411
Epoch 1.39: Loss = 1.17377
Epoch 1.40: Loss = 1.09369
Epoch 1.41: Loss = 1.16133
Epoch 1.42: Loss = 1.06224
Epoch 1.43: Loss = 1.06831
Epoch 1.44: Loss = 1.14928
Epoch 1.45: Loss = 1.03429
Epoch 1.46: Loss = 0.942566
Epoch 1.47: Loss = 1.09201
Epoch 1.48: Loss = 1.00334
Epoch 1.49: Loss = 1.03976
Epoch 1.50: Loss = 1.08722
Epoch 1.51: Loss = 1.03383
Epoch 1.52: Loss = 0.976929
Epoch 1.53: Loss = 0.932953
Epoch 1.54: Loss = 1.00851
Epoch 1.55: Loss = 0.992767
Epoch 1.56: Loss = 0.970703
Epoch 1.57: Loss = 0.98671
Epoch 1.58: Loss = 1.02068
Epoch 1.59: Loss = 0.899216
Epoch 1.60: Loss = 0.947708
Epoch 1.61: Loss = 0.955734
Epoch 1.62: Loss = 0.937164
Epoch 1.63: Loss = 0.90657
Epoch 1.64: Loss = 0.944473
Epoch 1.65: Loss = 0.976456
Epoch 1.66: Loss = 0.933701
Epoch 1.67: Loss = 0.910797
Epoch 1.68: Loss = 0.910721
Epoch 1.69: Loss = 0.87059
Epoch 1.70: Loss = 0.878555
Epoch 1.71: Loss = 0.89267
Epoch 1.72: Loss = 0.888245
Epoch 1.73: Loss = 0.81044
Epoch 1.74: Loss = 0.879044
Epoch 1.75: Loss = 0.860886
Epoch 1.76: Loss = 0.866135
Epoch 1.77: Loss = 0.863571
Epoch 1.78: Loss = 0.826996
Epoch 1.79: Loss = 0.873688
Epoch 1.80: Loss = 0.849808
Epoch 1.81: Loss = 0.827728
Epoch 1.82: Loss = 0.886047
Epoch 1.83: Loss = 0.814667
Epoch 1.84: Loss = 0.83136
Epoch 1.85: Loss = 0.923264
Epoch 1.86: Loss = 0.836823
Epoch 1.87: Loss = 0.752045
Epoch 1.88: Loss = 0.847733
Epoch 1.89: Loss = 0.890106
Epoch 1.90: Loss = 0.800858
Epoch 1.91: Loss = 0.877914
Epoch 1.92: Loss = 0.851486
Epoch 1.93: Loss = 0.824081
Epoch 1.94: Loss = 0.831314
Epoch 1.95: Loss = 0.776901
Epoch 1.96: Loss = 0.794525
Epoch 1.97: Loss = 0.737289
Epoch 1.98: Loss = 0.793594
Epoch 1.99: Loss = 0.862503
Epoch 1.100: Loss = 0.738693
Epoch 1.101: Loss = 0.745911
Epoch 1.102: Loss = 0.845596
Epoch 1.103: Loss = 0.735199
Epoch 1.104: Loss = 0.760208
Epoch 1.105: Loss = 0.738892
Epoch 1.106: Loss = 0.778564
Epoch 1.107: Loss = 0.797165
Epoch 1.108: Loss = 0.796265
Epoch 1.109: Loss = 0.765121
Epoch 1.110: Loss = 0.808624
Epoch 1.111: Loss = 0.770187
Epoch 1.112: Loss = 0.891342
Epoch 1.113: Loss = 0.771072
Epoch 1.114: Loss = 0.709854
Epoch 1.115: Loss = 0.818192
Epoch 1.116: Loss = 0.679932
Epoch 1.117: Loss = 0.857346
Epoch 1.118: Loss = 0.826019
Epoch 1.119: Loss = 0.745804
Epoch 1.120: Loss = 0.722534
TRAIN LOSS = 1.09935
TRAIN ACC = 62.8754 % (37727/60000)
Loss = 0.706207
Loss = 0.804138
Loss = 0.798264
Loss = 0.724609
Loss = 0.715408
Loss = 0.860397
Loss = 0.872864
Loss = 0.844177
Loss = 0.75499
Loss = 0.726257
Loss = 0.840591
Loss = 0.7789
Loss = 0.774948
Loss = 0.798187
Loss = 0.744568
Loss = 0.807861
Loss = 0.740524
Loss = 0.764999
Loss = 0.801193
Loss = 0.771469
TEST LOSS = 0.781527
TEST ACC = 377.269 % (7248/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.762573
Epoch 2.2: Loss = 0.736404
Epoch 2.3: Loss = 0.84407
Epoch 2.4: Loss = 0.791489
Epoch 2.5: Loss = 0.679626
Epoch 2.6: Loss = 0.753555
Epoch 2.7: Loss = 0.749161
Epoch 2.8: Loss = 0.751968
Epoch 2.9: Loss = 0.730957
Epoch 2.10: Loss = 0.746201
Epoch 2.11: Loss = 0.753891
Epoch 2.12: Loss = 0.7117
Epoch 2.13: Loss = 0.728577
Epoch 2.14: Loss = 0.778152
Epoch 2.15: Loss = 0.74585
Epoch 2.16: Loss = 0.767029
Epoch 2.17: Loss = 0.745514
Epoch 2.18: Loss = 0.755997
Epoch 2.19: Loss = 0.74379
Epoch 2.20: Loss = 0.715271
Epoch 2.21: Loss = 0.744751
Epoch 2.22: Loss = 0.734253
Epoch 2.23: Loss = 0.73317
Epoch 2.24: Loss = 0.623596
Epoch 2.25: Loss = 0.774231
Epoch 2.26: Loss = 0.749176
Epoch 2.27: Loss = 0.795074
Epoch 2.28: Loss = 0.789154
Epoch 2.29: Loss = 0.759933
Epoch 2.30: Loss = 0.67659
Epoch 2.31: Loss = 0.645096
Epoch 2.32: Loss = 0.735779
Epoch 2.33: Loss = 0.733948
Epoch 2.34: Loss = 0.784714
Epoch 2.35: Loss = 0.776947
Epoch 2.36: Loss = 0.653625
Epoch 2.37: Loss = 0.747284
Epoch 2.38: Loss = 0.737106
Epoch 2.39: Loss = 0.68129
Epoch 2.40: Loss = 0.712219
Epoch 2.41: Loss = 0.660233
Epoch 2.42: Loss = 0.755905
Epoch 2.43: Loss = 0.736725
Epoch 2.44: Loss = 0.681061
Epoch 2.45: Loss = 0.746887
Epoch 2.46: Loss = 0.708786
Epoch 2.47: Loss = 0.729065
Epoch 2.48: Loss = 0.762497
Epoch 2.49: Loss = 0.745895
Epoch 2.50: Loss = 0.688431
Epoch 2.51: Loss = 0.684418
Epoch 2.52: Loss = 0.686569
Epoch 2.53: Loss = 0.698578
Epoch 2.54: Loss = 0.620377
Epoch 2.55: Loss = 0.689804
Epoch 2.56: Loss = 0.630112
Epoch 2.57: Loss = 0.674896
Epoch 2.58: Loss = 0.721481
Epoch 2.59: Loss = 0.742935
Epoch 2.60: Loss = 0.730698
Epoch 2.61: Loss = 0.633362
Epoch 2.62: Loss = 0.702255
Epoch 2.63: Loss = 0.662933
Epoch 2.64: Loss = 0.75647
Epoch 2.65: Loss = 0.763855
Epoch 2.66: Loss = 0.697906
Epoch 2.67: Loss = 0.670486
Epoch 2.68: Loss = 0.705994
Epoch 2.69: Loss = 0.71666
Epoch 2.70: Loss = 0.654251
Epoch 2.71: Loss = 0.62529
Epoch 2.72: Loss = 0.702621
Epoch 2.73: Loss = 0.704086
Epoch 2.74: Loss = 0.714676
Epoch 2.75: Loss = 0.761627
Epoch 2.76: Loss = 0.789642
Epoch 2.77: Loss = 0.658981
Epoch 2.78: Loss = 0.618195
Epoch 2.79: Loss = 0.74939
Epoch 2.80: Loss = 0.595093
Epoch 2.81: Loss = 0.717499
Epoch 2.82: Loss = 0.757217
Epoch 2.83: Loss = 0.61499
Epoch 2.84: Loss = 0.770416
Epoch 2.85: Loss = 0.622971
Epoch 2.86: Loss = 0.748703
Epoch 2.87: Loss = 0.741516
Epoch 2.88: Loss = 0.717957
Epoch 2.89: Loss = 0.676743
Epoch 2.90: Loss = 0.701096
Epoch 2.91: Loss = 0.646225
Epoch 2.92: Loss = 0.715149
Epoch 2.93: Loss = 0.643066
Epoch 2.94: Loss = 0.599747
Epoch 2.95: Loss = 0.672882
Epoch 2.96: Loss = 0.646133
Epoch 2.97: Loss = 0.621933
Epoch 2.98: Loss = 0.762268
Epoch 2.99: Loss = 0.705338
Epoch 2.100: Loss = 0.620346
Epoch 2.101: Loss = 0.694046
Epoch 2.102: Loss = 0.628586
Epoch 2.103: Loss = 0.681076
Epoch 2.104: Loss = 0.711349
Epoch 2.105: Loss = 0.691299
Epoch 2.106: Loss = 0.651825
Epoch 2.107: Loss = 0.681091
Epoch 2.108: Loss = 0.657974
Epoch 2.109: Loss = 0.605423
Epoch 2.110: Loss = 0.662308
Epoch 2.111: Loss = 0.682724
Epoch 2.112: Loss = 0.731613
Epoch 2.113: Loss = 0.570816
Epoch 2.114: Loss = 0.571503
Epoch 2.115: Loss = 0.683212
Epoch 2.116: Loss = 0.674438
Epoch 2.117: Loss = 0.64241
Epoch 2.118: Loss = 0.665405
Epoch 2.119: Loss = 0.665176
Epoch 2.120: Loss = 0.691895
TRAIN LOSS = 0.704224
TRAIN ACC = 75.7065 % (45426/60000)
Loss = 0.610718
Loss = 0.705902
Loss = 0.674866
Loss = 0.617706
Loss = 0.620956
Loss = 0.780075
Loss = 0.7948
Loss = 0.770508
Loss = 0.667252
Loss = 0.622086
Loss = 0.754227
Loss = 0.720551
Loss = 0.678925
Loss = 0.690414
Loss = 0.666046
Loss = 0.710236
Loss = 0.651535
Loss = 0.690369
Loss = 0.730728
Loss = 0.677109
TEST LOSS = 0.69175
TEST ACC = 454.259 % (7580/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.685745
Epoch 3.2: Loss = 0.657883
Epoch 3.3: Loss = 0.600067
Epoch 3.4: Loss = 0.601944
Epoch 3.5: Loss = 0.66687
Epoch 3.6: Loss = 0.628464
Epoch 3.7: Loss = 0.686829
Epoch 3.8: Loss = 0.618927
Epoch 3.9: Loss = 0.618423
Epoch 3.10: Loss = 0.744446
Epoch 3.11: Loss = 0.622864
Epoch 3.12: Loss = 0.711823
Epoch 3.13: Loss = 0.64003
Epoch 3.14: Loss = 0.69429
Epoch 3.15: Loss = 0.633102
Epoch 3.16: Loss = 0.634277
Epoch 3.17: Loss = 0.69696
Epoch 3.18: Loss = 0.619263
Epoch 3.19: Loss = 0.62149
Epoch 3.20: Loss = 0.72644
Epoch 3.21: Loss = 0.695618
Epoch 3.22: Loss = 0.596924
Epoch 3.23: Loss = 0.626038
Epoch 3.24: Loss = 0.6427
Epoch 3.25: Loss = 0.699677
Epoch 3.26: Loss = 0.637299
Epoch 3.27: Loss = 0.590897
Epoch 3.28: Loss = 0.611633
Epoch 3.29: Loss = 0.721756
Epoch 3.30: Loss = 0.628357
Epoch 3.31: Loss = 0.752151
Epoch 3.32: Loss = 0.672272
Epoch 3.33: Loss = 0.701096
Epoch 3.34: Loss = 0.698929
Epoch 3.35: Loss = 0.632584
Epoch 3.36: Loss = 0.656235
Epoch 3.37: Loss = 0.684479
Epoch 3.38: Loss = 0.75798
Epoch 3.39: Loss = 0.63205
Epoch 3.40: Loss = 0.60112
Epoch 3.41: Loss = 0.638779
Epoch 3.42: Loss = 0.706116
Epoch 3.43: Loss = 0.664795
Epoch 3.44: Loss = 0.701508
Epoch 3.45: Loss = 0.565903
Epoch 3.46: Loss = 0.630142
Epoch 3.47: Loss = 0.635391
Epoch 3.48: Loss = 0.582428
Epoch 3.49: Loss = 0.72525
Epoch 3.50: Loss = 0.644806
Epoch 3.51: Loss = 0.633698
Epoch 3.52: Loss = 0.579041
Epoch 3.53: Loss = 0.619385
Epoch 3.54: Loss = 0.679428
Epoch 3.55: Loss = 0.581314
Epoch 3.56: Loss = 0.741592
Epoch 3.57: Loss = 0.720169
Epoch 3.58: Loss = 0.694595
Epoch 3.59: Loss = 0.58374
Epoch 3.60: Loss = 0.755661
Epoch 3.61: Loss = 0.605209
Epoch 3.62: Loss = 0.661072
Epoch 3.63: Loss = 0.560791
Epoch 3.64: Loss = 0.706573
Epoch 3.65: Loss = 0.666611
Epoch 3.66: Loss = 0.66394
Epoch 3.67: Loss = 0.624435
Epoch 3.68: Loss = 0.667358
Epoch 3.69: Loss = 0.749207
Epoch 3.70: Loss = 0.685226
Epoch 3.71: Loss = 0.617279
Epoch 3.72: Loss = 0.590408
Epoch 3.73: Loss = 0.661896
Epoch 3.74: Loss = 0.575012
Epoch 3.75: Loss = 0.674545
Epoch 3.76: Loss = 0.731018
Epoch 3.77: Loss = 0.584976
Epoch 3.78: Loss = 0.677841
Epoch 3.79: Loss = 0.692612
Epoch 3.80: Loss = 0.648407
Epoch 3.81: Loss = 0.602951
Epoch 3.82: Loss = 0.632797
Epoch 3.83: Loss = 0.64859
Epoch 3.84: Loss = 0.646805
Epoch 3.85: Loss = 0.536514
Epoch 3.86: Loss = 0.639877
Epoch 3.87: Loss = 0.583191
Epoch 3.88: Loss = 0.657593
Epoch 3.89: Loss = 0.610519
Epoch 3.90: Loss = 0.599838
Epoch 3.91: Loss = 0.64444
Epoch 3.92: Loss = 0.564224
Epoch 3.93: Loss = 0.67395
Epoch 3.94: Loss = 0.570679
Epoch 3.95: Loss = 0.715134
Epoch 3.96: Loss = 0.628769
Epoch 3.97: Loss = 0.605072
Epoch 3.98: Loss = 0.616837
Epoch 3.99: Loss = 0.606476
Epoch 3.100: Loss = 0.698578
Epoch 3.101: Loss = 0.597321
Epoch 3.102: Loss = 0.690521
Epoch 3.103: Loss = 0.658325
Epoch 3.104: Loss = 0.766891
Epoch 3.105: Loss = 0.598785
Epoch 3.106: Loss = 0.577087
Epoch 3.107: Loss = 0.634171
Epoch 3.108: Loss = 0.576767
Epoch 3.109: Loss = 0.636307
Epoch 3.110: Loss = 0.724319
Epoch 3.111: Loss = 0.632706
Epoch 3.112: Loss = 0.679276
Epoch 3.113: Loss = 0.691589
Epoch 3.114: Loss = 0.620132
Epoch 3.115: Loss = 0.600784
Epoch 3.116: Loss = 0.580841
Epoch 3.117: Loss = 0.68631
Epoch 3.118: Loss = 0.609955
Epoch 3.119: Loss = 0.594467
Epoch 3.120: Loss = 0.599594
TRAIN LOSS = 0.647659
TRAIN ACC = 78.1525 % (46894/60000)
Loss = 0.571548
Loss = 0.674179
Loss = 0.629929
Loss = 0.567215
Loss = 0.588089
Loss = 0.739746
Loss = 0.768219
Loss = 0.738342
Loss = 0.644943
Loss = 0.591965
Loss = 0.73912
Loss = 0.702728
Loss = 0.649643
Loss = 0.650421
Loss = 0.633926
Loss = 0.671677
Loss = 0.617584
Loss = 0.654633
Loss = 0.690613
Loss = 0.642273
TEST LOSS = 0.65834
TEST ACC = 468.939 % (7755/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.58577
Epoch 4.2: Loss = 0.697952
Epoch 4.3: Loss = 0.577179
Epoch 4.4: Loss = 0.614624
Epoch 4.5: Loss = 0.594498
Epoch 4.6: Loss = 0.625137
Epoch 4.7: Loss = 0.564621
Epoch 4.8: Loss = 0.640091
Epoch 4.9: Loss = 0.625381
Epoch 4.10: Loss = 0.606537
Epoch 4.11: Loss = 0.636703
Epoch 4.12: Loss = 0.696625
Epoch 4.13: Loss = 0.706833
Epoch 4.14: Loss = 0.629837
Epoch 4.15: Loss = 0.617279
Epoch 4.16: Loss = 0.61557
Epoch 4.17: Loss = 0.601608
Epoch 4.18: Loss = 0.648682
Epoch 4.19: Loss = 0.572708
Epoch 4.20: Loss = 0.608704
Epoch 4.21: Loss = 0.594467
Epoch 4.22: Loss = 0.612411
Epoch 4.23: Loss = 0.66832
Epoch 4.24: Loss = 0.670792
Epoch 4.25: Loss = 0.63797
Epoch 4.26: Loss = 0.595795
Epoch 4.27: Loss = 0.538803
Epoch 4.28: Loss = 0.558624
Epoch 4.29: Loss = 0.667618
Epoch 4.30: Loss = 0.616745
Epoch 4.31: Loss = 0.58754
Epoch 4.32: Loss = 0.5746
Epoch 4.33: Loss = 0.666641
Epoch 4.34: Loss = 0.605087
Epoch 4.35: Loss = 0.68808
Epoch 4.36: Loss = 0.72197
Epoch 4.37: Loss = 0.638428
Epoch 4.38: Loss = 0.682907
Epoch 4.39: Loss = 0.601135
Epoch 4.40: Loss = 0.678894
Epoch 4.41: Loss = 0.622223
Epoch 4.42: Loss = 0.687195
Epoch 4.43: Loss = 0.672852
Epoch 4.44: Loss = 0.616867
Epoch 4.45: Loss = 0.646027
Epoch 4.46: Loss = 0.639084
Epoch 4.47: Loss = 0.603928
Epoch 4.48: Loss = 0.593719
Epoch 4.49: Loss = 0.644592
Epoch 4.50: Loss = 0.67482
Epoch 4.51: Loss = 0.566757
Epoch 4.52: Loss = 0.657349
Epoch 4.53: Loss = 0.557358
Epoch 4.54: Loss = 0.631439
Epoch 4.55: Loss = 0.569458
Epoch 4.56: Loss = 0.677032
Epoch 4.57: Loss = 0.592468
Epoch 4.58: Loss = 0.617035
Epoch 4.59: Loss = 0.662491
Epoch 4.60: Loss = 0.567963
Epoch 4.61: Loss = 0.625473
Epoch 4.62: Loss = 0.585999
Epoch 4.63: Loss = 0.688019
Epoch 4.64: Loss = 0.648743
Epoch 4.65: Loss = 0.580124
Epoch 4.66: Loss = 0.5784
Epoch 4.67: Loss = 0.592575
Epoch 4.68: Loss = 0.630737
Epoch 4.69: Loss = 0.606369
Epoch 4.70: Loss = 0.615173
Epoch 4.71: Loss = 0.615723
Epoch 4.72: Loss = 0.707901
Epoch 4.73: Loss = 0.616028
Epoch 4.74: Loss = 0.648315
Epoch 4.75: Loss = 0.59906
Epoch 4.76: Loss = 0.591385
Epoch 4.77: Loss = 0.633743
Epoch 4.78: Loss = 0.730728
Epoch 4.79: Loss = 0.648392
Epoch 4.80: Loss = 0.699539
Epoch 4.81: Loss = 0.597351
Epoch 4.82: Loss = 0.578964
Epoch 4.83: Loss = 0.629395
Epoch 4.84: Loss = 0.545502
Epoch 4.85: Loss = 0.560104
Epoch 4.86: Loss = 0.588943
Epoch 4.87: Loss = 0.631744
Epoch 4.88: Loss = 0.639908
Epoch 4.89: Loss = 0.583954
Epoch 4.90: Loss = 0.593735
Epoch 4.91: Loss = 0.672226
Epoch 4.92: Loss = 0.536301
Epoch 4.93: Loss = 0.58342
Epoch 4.94: Loss = 0.665405
Epoch 4.95: Loss = 0.611206
Epoch 4.96: Loss = 0.684143
Epoch 4.97: Loss = 0.707336
Epoch 4.98: Loss = 0.602051
Epoch 4.99: Loss = 0.585205
Epoch 4.100: Loss = 0.584396
Epoch 4.101: Loss = 0.537231
Epoch 4.102: Loss = 0.617264
Epoch 4.103: Loss = 0.574585
Epoch 4.104: Loss = 0.568176
Epoch 4.105: Loss = 0.588516
Epoch 4.106: Loss = 0.620468
Epoch 4.107: Loss = 0.565506
Epoch 4.108: Loss = 0.502518
Epoch 4.109: Loss = 0.667206
Epoch 4.110: Loss = 0.604034
Epoch 4.111: Loss = 0.682724
Epoch 4.112: Loss = 0.644333
Epoch 4.113: Loss = 0.679474
Epoch 4.114: Loss = 0.523895
Epoch 4.115: Loss = 0.704147
Epoch 4.116: Loss = 0.608383
Epoch 4.117: Loss = 0.554565
Epoch 4.118: Loss = 0.556412
Epoch 4.119: Loss = 0.499252
Epoch 4.120: Loss = 0.623398
TRAIN LOSS = 0.619354
TRAIN ACC = 79.4495 % (47672/60000)
Loss = 0.550797
Loss = 0.646988
Loss = 0.594559
Loss = 0.546036
Loss = 0.571304
Loss = 0.712204
Loss = 0.747467
Loss = 0.702179
Loss = 0.620804
Loss = 0.574951
Loss = 0.717499
Loss = 0.692459
Loss = 0.628082
Loss = 0.623367
Loss = 0.609558
Loss = 0.651199
Loss = 0.600632
Loss = 0.636444
Loss = 0.662933
Loss = 0.624451
TEST LOSS = 0.635695
TEST ACC = 476.72 % (7898/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.735916
Epoch 5.2: Loss = 0.645966
Epoch 5.3: Loss = 0.624802
Epoch 5.4: Loss = 0.609894
Epoch 5.5: Loss = 0.694901
Epoch 5.6: Loss = 0.592117
Epoch 5.7: Loss = 0.63913
Epoch 5.8: Loss = 0.590103
Epoch 5.9: Loss = 0.579315
Epoch 5.10: Loss = 0.65094
Epoch 5.11: Loss = 0.607224
Epoch 5.12: Loss = 0.538834
Epoch 5.13: Loss = 0.690018
Epoch 5.14: Loss = 0.539948
Epoch 5.15: Loss = 0.569214
Epoch 5.16: Loss = 0.624969
Epoch 5.17: Loss = 0.593964
Epoch 5.18: Loss = 0.666733
Epoch 5.19: Loss = 0.615051
Epoch 5.20: Loss = 0.488617
Epoch 5.21: Loss = 0.606003
Epoch 5.22: Loss = 0.595093
Epoch 5.23: Loss = 0.500381
Epoch 5.24: Loss = 0.634003
Epoch 5.25: Loss = 0.648453
Epoch 5.26: Loss = 0.557312
Epoch 5.27: Loss = 0.609589
Epoch 5.28: Loss = 0.594345
Epoch 5.29: Loss = 0.630325
Epoch 5.30: Loss = 0.552719
Epoch 5.31: Loss = 0.682419
Epoch 5.32: Loss = 0.558197
Epoch 5.33: Loss = 0.661316
Epoch 5.34: Loss = 0.642181
Epoch 5.35: Loss = 0.600815
Epoch 5.36: Loss = 0.648041
Epoch 5.37: Loss = 0.583115
Epoch 5.38: Loss = 0.684555
Epoch 5.39: Loss = 0.564453
Epoch 5.40: Loss = 0.640671
Epoch 5.41: Loss = 0.597412
Epoch 5.42: Loss = 0.569473
Epoch 5.43: Loss = 0.543381
Epoch 5.44: Loss = 0.540588
Epoch 5.45: Loss = 0.639572
Epoch 5.46: Loss = 0.613083
Epoch 5.47: Loss = 0.68837
Epoch 5.48: Loss = 0.639847
Epoch 5.49: Loss = 0.540497
Epoch 5.50: Loss = 0.55658
Epoch 5.51: Loss = 0.68161
Epoch 5.52: Loss = 0.547348
Epoch 5.53: Loss = 0.694595
Epoch 5.54: Loss = 0.598053
Epoch 5.55: Loss = 0.617325
Epoch 5.56: Loss = 0.5979
Epoch 5.57: Loss = 0.556717
Epoch 5.58: Loss = 0.6073
Epoch 5.59: Loss = 0.637146
Epoch 5.60: Loss = 0.692596
Epoch 5.61: Loss = 0.52562
Epoch 5.62: Loss = 0.605881
Epoch 5.63: Loss = 0.618698
Epoch 5.64: Loss = 0.512558
Epoch 5.65: Loss = 0.653564
Epoch 5.66: Loss = 0.592773
Epoch 5.67: Loss = 0.550903
Epoch 5.68: Loss = 0.661697
Epoch 5.69: Loss = 0.538071
Epoch 5.70: Loss = 0.586243
Epoch 5.71: Loss = 0.565613
Epoch 5.72: Loss = 0.648483
Epoch 5.73: Loss = 0.657806
Epoch 5.74: Loss = 0.552261
Epoch 5.75: Loss = 0.556854
Epoch 5.76: Loss = 0.680344
Epoch 5.77: Loss = 0.549179
Epoch 5.78: Loss = 0.6427
Epoch 5.79: Loss = 0.551605
Epoch 5.80: Loss = 0.512054
Epoch 5.81: Loss = 0.610672
Epoch 5.82: Loss = 0.624008
Epoch 5.83: Loss = 0.680099
Epoch 5.84: Loss = 0.603302
Epoch 5.85: Loss = 0.534409
Epoch 5.86: Loss = 0.680008
Epoch 5.87: Loss = 0.711761
Epoch 5.88: Loss = 0.533829
Epoch 5.89: Loss = 0.628525
Epoch 5.90: Loss = 0.518539
Epoch 5.91: Loss = 0.647537
Epoch 5.92: Loss = 0.521179
Epoch 5.93: Loss = 0.622742
Epoch 5.94: Loss = 0.61821
Epoch 5.95: Loss = 0.634491
Epoch 5.96: Loss = 0.604782
Epoch 5.97: Loss = 0.620773
Epoch 5.98: Loss = 0.68602
Epoch 5.99: Loss = 0.620285
Epoch 5.100: Loss = 0.707092
Epoch 5.101: Loss = 0.564301
Epoch 5.102: Loss = 0.566422
Epoch 5.103: Loss = 0.509796
Epoch 5.104: Loss = 0.685501
Epoch 5.105: Loss = 0.63826
Epoch 5.106: Loss = 0.664139
Epoch 5.107: Loss = 0.612686
Epoch 5.108: Loss = 0.624329
Epoch 5.109: Loss = 0.53035
Epoch 5.110: Loss = 0.621857
Epoch 5.111: Loss = 0.534012
Epoch 5.112: Loss = 0.563995
Epoch 5.113: Loss = 0.521179
Epoch 5.114: Loss = 0.599686
Epoch 5.115: Loss = 0.767822
Epoch 5.116: Loss = 0.515488
Epoch 5.117: Loss = 0.674362
Epoch 5.118: Loss = 0.631912
Epoch 5.119: Loss = 0.542923
Epoch 5.120: Loss = 0.62027
TRAIN LOSS = 0.606781
TRAIN ACC = 80.1727 % (48105/60000)
Loss = 0.542923
Loss = 0.647675
Loss = 0.590759
Loss = 0.534271
Loss = 0.574326
Loss = 0.706146
Loss = 0.747223
Loss = 0.695465
Loss = 0.615738
Loss = 0.567337
Loss = 0.7314
Loss = 0.696518
Loss = 0.632462
Loss = 0.613617
Loss = 0.6091
Loss = 0.64978
Loss = 0.608505
Loss = 0.634384
Loss = 0.657623
Loss = 0.622986
TEST LOSS = 0.633912
TEST ACC = 481.049 % (7917/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.6828
Epoch 6.2: Loss = 0.50444
Epoch 6.3: Loss = 0.549942
Epoch 6.4: Loss = 0.669144
Epoch 6.5: Loss = 0.714478
Epoch 6.6: Loss = 0.503174
Epoch 6.7: Loss = 0.56192
Epoch 6.8: Loss = 0.652176
Epoch 6.9: Loss = 0.630966
Epoch 6.10: Loss = 0.677521
Epoch 6.11: Loss = 0.455612
Epoch 6.12: Loss = 0.554428
Epoch 6.13: Loss = 0.591507
Epoch 6.14: Loss = 0.686096
Epoch 6.15: Loss = 0.464752
Epoch 6.16: Loss = 0.565521
Epoch 6.17: Loss = 0.542175
Epoch 6.18: Loss = 0.602341
Epoch 6.19: Loss = 0.689392
Epoch 6.20: Loss = 0.514206
Epoch 6.21: Loss = 0.550735
Epoch 6.22: Loss = 0.590897
Epoch 6.23: Loss = 0.644073
Epoch 6.24: Loss = 0.594788
Epoch 6.25: Loss = 0.6371
Epoch 6.26: Loss = 0.532196
Epoch 6.27: Loss = 0.503448
Epoch 6.28: Loss = 0.637039
Epoch 6.29: Loss = 0.499283
Epoch 6.30: Loss = 0.658478
Epoch 6.31: Loss = 0.623932
Epoch 6.32: Loss = 0.587021
Epoch 6.33: Loss = 0.554062
Epoch 6.34: Loss = 0.750244
Epoch 6.35: Loss = 0.701263
Epoch 6.36: Loss = 0.552353
Epoch 6.37: Loss = 0.742676
Epoch 6.38: Loss = 0.611343
Epoch 6.39: Loss = 0.550537
Epoch 6.40: Loss = 0.599075
Epoch 6.41: Loss = 0.594437
Epoch 6.42: Loss = 0.520157
Epoch 6.43: Loss = 0.565643
Epoch 6.44: Loss = 0.636627
Epoch 6.45: Loss = 0.583755
Epoch 6.46: Loss = 0.635056
Epoch 6.47: Loss = 0.540283
Epoch 6.48: Loss = 0.535889
Epoch 6.49: Loss = 0.620972
Epoch 6.50: Loss = 0.669479
Epoch 6.51: Loss = 0.54718
Epoch 6.52: Loss = 0.620392
Epoch 6.53: Loss = 0.502838
Epoch 6.54: Loss = 0.560791
Epoch 6.55: Loss = 0.636841
Epoch 6.56: Loss = 0.628479
Epoch 6.57: Loss = 0.593674
Epoch 6.58: Loss = 0.639404
Epoch 6.59: Loss = 0.593903
Epoch 6.60: Loss = 0.615295
Epoch 6.61: Loss = 0.523804
Epoch 6.62: Loss = 0.673248
Epoch 6.63: Loss = 0.747238
Epoch 6.64: Loss = 0.694824
Epoch 6.65: Loss = 0.581345
Epoch 6.66: Loss = 0.629501
Epoch 6.67: Loss = 0.589722
Epoch 6.68: Loss = 0.558319
Epoch 6.69: Loss = 0.529404
Epoch 6.70: Loss = 0.634705
Epoch 6.71: Loss = 0.525406
Epoch 6.72: Loss = 0.641876
Epoch 6.73: Loss = 0.607498
Epoch 6.74: Loss = 0.589508
Epoch 6.75: Loss = 0.57872
Epoch 6.76: Loss = 0.585419
Epoch 6.77: Loss = 0.650192
Epoch 6.78: Loss = 0.569763
Epoch 6.79: Loss = 0.631973
Epoch 6.80: Loss = 0.562805
Epoch 6.81: Loss = 0.721817
Epoch 6.82: Loss = 0.592758
Epoch 6.83: Loss = 0.568039
Epoch 6.84: Loss = 0.56282
Epoch 6.85: Loss = 0.539474
Epoch 6.86: Loss = 0.530182
Epoch 6.87: Loss = 0.583466
Epoch 6.88: Loss = 0.556808
Epoch 6.89: Loss = 0.596954
Epoch 6.90: Loss = 0.618988
Epoch 6.91: Loss = 0.536255
Epoch 6.92: Loss = 0.636749
Epoch 6.93: Loss = 0.582001
Epoch 6.94: Loss = 0.667114
Epoch 6.95: Loss = 0.723358
Epoch 6.96: Loss = 0.548218
Epoch 6.97: Loss = 0.620636
Epoch 6.98: Loss = 0.509064
Epoch 6.99: Loss = 0.476334
Epoch 6.100: Loss = 0.633377
Epoch 6.101: Loss = 0.665894
Epoch 6.102: Loss = 0.60611
Epoch 6.103: Loss = 0.701187
Epoch 6.104: Loss = 0.66275
Epoch 6.105: Loss = 0.609909
Epoch 6.106: Loss = 0.553284
Epoch 6.107: Loss = 0.588608
Epoch 6.108: Loss = 0.525269
Epoch 6.109: Loss = 0.677261
Epoch 6.110: Loss = 0.458984
Epoch 6.111: Loss = 0.651031
Epoch 6.112: Loss = 0.654694
Epoch 6.113: Loss = 0.540482
Epoch 6.114: Loss = 0.581482
Epoch 6.115: Loss = 0.546021
Epoch 6.116: Loss = 0.646118
Epoch 6.117: Loss = 0.634781
Epoch 6.118: Loss = 0.611542
Epoch 6.119: Loss = 0.565292
Epoch 6.120: Loss = 0.621216
TRAIN LOSS = 0.598236
TRAIN ACC = 80.7693 % (48464/60000)
Loss = 0.538895
Loss = 0.642685
Loss = 0.585785
Loss = 0.529221
Loss = 0.577484
Loss = 0.704773
Loss = 0.744492
Loss = 0.690918
Loss = 0.62384
Loss = 0.571472
Loss = 0.742233
Loss = 0.69371
Loss = 0.631973
Loss = 0.607071
Loss = 0.608246
Loss = 0.641602
Loss = 0.601257
Loss = 0.638687
Loss = 0.654877
Loss = 0.612213
TEST LOSS = 0.632072
TEST ACC = 484.639 % (7983/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.564072
Epoch 7.2: Loss = 0.639008
Epoch 7.3: Loss = 0.598007
Epoch 7.4: Loss = 0.541168
Epoch 7.5: Loss = 0.556732
Epoch 7.6: Loss = 0.552811
Epoch 7.7: Loss = 0.542664
Epoch 7.8: Loss = 0.713196
Epoch 7.9: Loss = 0.591003
Epoch 7.10: Loss = 0.544785
Epoch 7.11: Loss = 0.542297
Epoch 7.12: Loss = 0.621796
Epoch 7.13: Loss = 0.553391
Epoch 7.14: Loss = 0.584839
Epoch 7.15: Loss = 0.551407
Epoch 7.16: Loss = 0.57637
Epoch 7.17: Loss = 0.648712
Epoch 7.18: Loss = 0.623169
Epoch 7.19: Loss = 0.707642
Epoch 7.20: Loss = 0.543518
Epoch 7.21: Loss = 0.635986
Epoch 7.22: Loss = 0.655441
Epoch 7.23: Loss = 0.54985
Epoch 7.24: Loss = 0.540298
Epoch 7.25: Loss = 0.558502
Epoch 7.26: Loss = 0.55748
Epoch 7.27: Loss = 0.621948
Epoch 7.28: Loss = 0.620285
Epoch 7.29: Loss = 0.584839
Epoch 7.30: Loss = 0.572144
Epoch 7.31: Loss = 0.526108
Epoch 7.32: Loss = 0.544373
Epoch 7.33: Loss = 0.682114
Epoch 7.34: Loss = 0.694427
Epoch 7.35: Loss = 0.643753
Epoch 7.36: Loss = 0.629318
Epoch 7.37: Loss = 0.639572
Epoch 7.38: Loss = 0.556595
Epoch 7.39: Loss = 0.641281
Epoch 7.40: Loss = 0.555832
Epoch 7.41: Loss = 0.578064
Epoch 7.42: Loss = 0.63327
Epoch 7.43: Loss = 0.633728
Epoch 7.44: Loss = 0.610077
Epoch 7.45: Loss = 0.648804
Epoch 7.46: Loss = 0.546082
Epoch 7.47: Loss = 0.662415
Epoch 7.48: Loss = 0.568207
Epoch 7.49: Loss = 0.680084
Epoch 7.50: Loss = 0.523117
Epoch 7.51: Loss = 0.659058
Epoch 7.52: Loss = 0.626709
Epoch 7.53: Loss = 0.555069
Epoch 7.54: Loss = 0.592743
Epoch 7.55: Loss = 0.555313
Epoch 7.56: Loss = 0.62262
Epoch 7.57: Loss = 0.630554
Epoch 7.58: Loss = 0.525284
Epoch 7.59: Loss = 0.578079
Epoch 7.60: Loss = 0.486969
Epoch 7.61: Loss = 0.555252
Epoch 7.62: Loss = 0.614151
Epoch 7.63: Loss = 0.720596
Epoch 7.64: Loss = 0.585815
Epoch 7.65: Loss = 0.530182
Epoch 7.66: Loss = 0.539719
Epoch 7.67: Loss = 0.706116
Epoch 7.68: Loss = 0.606033
Epoch 7.69: Loss = 0.57692
Epoch 7.70: Loss = 0.629211
Epoch 7.71: Loss = 0.594543
Epoch 7.72: Loss = 0.60321
Epoch 7.73: Loss = 0.698013
Epoch 7.74: Loss = 0.687164
Epoch 7.75: Loss = 0.694611
Epoch 7.76: Loss = 0.667877
Epoch 7.77: Loss = 0.545654
Epoch 7.78: Loss = 0.538132
Epoch 7.79: Loss = 0.485474
Epoch 7.80: Loss = 0.615891
Epoch 7.81: Loss = 0.56279
Epoch 7.82: Loss = 0.623291
Epoch 7.83: Loss = 0.612503
Epoch 7.84: Loss = 0.524765
Epoch 7.85: Loss = 0.62207
Epoch 7.86: Loss = 0.65741
Epoch 7.87: Loss = 0.456345
Epoch 7.88: Loss = 0.586349
Epoch 7.89: Loss = 0.552231
Epoch 7.90: Loss = 0.576248
Epoch 7.91: Loss = 0.468597
Epoch 7.92: Loss = 0.590118
Epoch 7.93: Loss = 0.502792
Epoch 7.94: Loss = 0.663284
Epoch 7.95: Loss = 0.538193
Epoch 7.96: Loss = 0.521347
Epoch 7.97: Loss = 0.62056
Epoch 7.98: Loss = 0.488892
Epoch 7.99: Loss = 0.638321
Epoch 7.100: Loss = 0.625778
Epoch 7.101: Loss = 0.525757
Epoch 7.102: Loss = 0.560699
Epoch 7.103: Loss = 0.561493
Epoch 7.104: Loss = 0.628113
Epoch 7.105: Loss = 0.682739
Epoch 7.106: Loss = 0.474136
Epoch 7.107: Loss = 0.559433
Epoch 7.108: Loss = 0.693771
Epoch 7.109: Loss = 0.527176
Epoch 7.110: Loss = 0.603973
Epoch 7.111: Loss = 0.594833
Epoch 7.112: Loss = 0.50087
Epoch 7.113: Loss = 0.54657
Epoch 7.114: Loss = 0.662064
Epoch 7.115: Loss = 0.670029
Epoch 7.116: Loss = 0.604599
Epoch 7.117: Loss = 0.586548
Epoch 7.118: Loss = 0.636475
Epoch 7.119: Loss = 0.532654
Epoch 7.120: Loss = 0.620728
TRAIN LOSS = 0.592697
TRAIN ACC = 81.0623 % (48639/60000)
Loss = 0.530228
Loss = 0.633347
Loss = 0.58046
Loss = 0.518448
Loss = 0.570953
Loss = 0.692566
Loss = 0.748764
Loss = 0.678101
Loss = 0.607925
Loss = 0.566391
Loss = 0.742676
Loss = 0.690567
Loss = 0.630417
Loss = 0.603607
Loss = 0.591888
Loss = 0.631912
Loss = 0.591187
Loss = 0.633087
Loss = 0.633072
Loss = 0.605087
TEST LOSS = 0.624034
TEST ACC = 486.389 % (8022/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.609177
Epoch 8.2: Loss = 0.527664
Epoch 8.3: Loss = 0.606155
Epoch 8.4: Loss = 0.637405
Epoch 8.5: Loss = 0.581421
Epoch 8.6: Loss = 0.576065
Epoch 8.7: Loss = 0.623932
Epoch 8.8: Loss = 0.539795
Epoch 8.9: Loss = 0.546066
Epoch 8.10: Loss = 0.564377
Epoch 8.11: Loss = 0.68956
Epoch 8.12: Loss = 0.631516
Epoch 8.13: Loss = 0.544739
Epoch 8.14: Loss = 0.576553
Epoch 8.15: Loss = 0.522308
Epoch 8.16: Loss = 0.597229
Epoch 8.17: Loss = 0.524307
Epoch 8.18: Loss = 0.560776
Epoch 8.19: Loss = 0.598282
Epoch 8.20: Loss = 0.629578
Epoch 8.21: Loss = 0.557785
Epoch 8.22: Loss = 0.56282
Epoch 8.23: Loss = 0.647964
Epoch 8.24: Loss = 0.676514
Epoch 8.25: Loss = 0.578278
Epoch 8.26: Loss = 0.563248
Epoch 8.27: Loss = 0.702148
Epoch 8.28: Loss = 0.550354
Epoch 8.29: Loss = 0.464478
Epoch 8.30: Loss = 0.642517
Epoch 8.31: Loss = 0.607346
Epoch 8.32: Loss = 0.533463
Epoch 8.33: Loss = 0.577774
Epoch 8.34: Loss = 0.58699
Epoch 8.35: Loss = 0.73674
Epoch 8.36: Loss = 0.636185
Epoch 8.37: Loss = 0.553589
Epoch 8.38: Loss = 0.504776
Epoch 8.39: Loss = 0.615479
Epoch 8.40: Loss = 0.636673
Epoch 8.41: Loss = 0.519119
Epoch 8.42: Loss = 0.650848
Epoch 8.43: Loss = 0.634354
Epoch 8.44: Loss = 0.691513
Epoch 8.45: Loss = 0.582199
Epoch 8.46: Loss = 0.503021
Epoch 8.47: Loss = 0.627487
Epoch 8.48: Loss = 0.597855
Epoch 8.49: Loss = 0.6297
Epoch 8.50: Loss = 0.647293
Epoch 8.51: Loss = 0.623215
Epoch 8.52: Loss = 0.655289
Epoch 8.53: Loss = 0.571213
Epoch 8.54: Loss = 0.55368
Epoch 8.55: Loss = 0.576584
Epoch 8.56: Loss = 0.600006
Epoch 8.57: Loss = 0.625763
Epoch 8.58: Loss = 0.52681
Epoch 8.59: Loss = 0.486481
Epoch 8.60: Loss = 0.51413
Epoch 8.61: Loss = 0.530228
Epoch 8.62: Loss = 0.611755
Epoch 8.63: Loss = 0.617416
Epoch 8.64: Loss = 0.69902
Epoch 8.65: Loss = 0.559494
Epoch 8.66: Loss = 0.495621
Epoch 8.67: Loss = 0.544373
Epoch 8.68: Loss = 0.573547
Epoch 8.69: Loss = 0.696854
Epoch 8.70: Loss = 0.605606
Epoch 8.71: Loss = 0.615433
Epoch 8.72: Loss = 0.68663
Epoch 8.73: Loss = 0.624176
Epoch 8.74: Loss = 0.549271
Epoch 8.75: Loss = 0.598328
Epoch 8.76: Loss = 0.48851
Epoch 8.77: Loss = 0.650635
Epoch 8.78: Loss = 0.494507
Epoch 8.79: Loss = 0.603943
Epoch 8.80: Loss = 0.712326
Epoch 8.81: Loss = 0.60173
Epoch 8.82: Loss = 0.62735
Epoch 8.83: Loss = 0.601624
Epoch 8.84: Loss = 0.507141
Epoch 8.85: Loss = 0.572006
Epoch 8.86: Loss = 0.566727
Epoch 8.87: Loss = 0.526932
Epoch 8.88: Loss = 0.470749
Epoch 8.89: Loss = 0.606888
Epoch 8.90: Loss = 0.564453
Epoch 8.91: Loss = 0.66626
Epoch 8.92: Loss = 0.567261
Epoch 8.93: Loss = 0.573822
Epoch 8.94: Loss = 0.549469
Epoch 8.95: Loss = 0.589218
Epoch 8.96: Loss = 0.566528
Epoch 8.97: Loss = 0.561356
Epoch 8.98: Loss = 0.632019
Epoch 8.99: Loss = 0.530045
Epoch 8.100: Loss = 0.510605
Epoch 8.101: Loss = 0.584915
Epoch 8.102: Loss = 0.548615
Epoch 8.103: Loss = 0.623093
Epoch 8.104: Loss = 0.574066
Epoch 8.105: Loss = 0.57605
Epoch 8.106: Loss = 0.636597
Epoch 8.107: Loss = 0.533066
Epoch 8.108: Loss = 0.568787
Epoch 8.109: Loss = 0.654694
Epoch 8.110: Loss = 0.584946
Epoch 8.111: Loss = 0.68457
Epoch 8.112: Loss = 0.558365
Epoch 8.113: Loss = 0.4944
Epoch 8.114: Loss = 0.565521
Epoch 8.115: Loss = 0.624115
Epoch 8.116: Loss = 0.568405
Epoch 8.117: Loss = 0.626465
Epoch 8.118: Loss = 0.538559
Epoch 8.119: Loss = 0.608978
Epoch 8.120: Loss = 0.566452
TRAIN LOSS = 0.587585
TRAIN ACC = 81.4087 % (48848/60000)
Loss = 0.523331
Loss = 0.619415
Loss = 0.5755
Loss = 0.510117
Loss = 0.560074
Loss = 0.685654
Loss = 0.747986
Loss = 0.674667
Loss = 0.601532
Loss = 0.560242
Loss = 0.74527
Loss = 0.691437
Loss = 0.619781
Loss = 0.588959
Loss = 0.585617
Loss = 0.622208
Loss = 0.594574
Loss = 0.634796
Loss = 0.631454
Loss = 0.605362
TEST LOSS = 0.618899
TEST ACC = 488.48 % (8068/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.576691
Epoch 9.2: Loss = 0.487396
Epoch 9.3: Loss = 0.582962
Epoch 9.4: Loss = 0.562943
Epoch 9.5: Loss = 0.617111
Epoch 9.6: Loss = 0.574463
Epoch 9.7: Loss = 0.567688
Epoch 9.8: Loss = 0.592072
Epoch 9.9: Loss = 0.556137
Epoch 9.10: Loss = 0.678467
Epoch 9.11: Loss = 0.577576
Epoch 9.12: Loss = 0.651062
Epoch 9.13: Loss = 0.547775
Epoch 9.14: Loss = 0.586105
Epoch 9.15: Loss = 0.555405
Epoch 9.16: Loss = 0.631653
Epoch 9.17: Loss = 0.645905
Epoch 9.18: Loss = 0.582825
Epoch 9.19: Loss = 0.571442
Epoch 9.20: Loss = 0.561386
Epoch 9.21: Loss = 0.545486
Epoch 9.22: Loss = 0.587112
Epoch 9.23: Loss = 0.479401
Epoch 9.24: Loss = 0.624283
Epoch 9.25: Loss = 0.580185
Epoch 9.26: Loss = 0.598328
Epoch 9.27: Loss = 0.544754
Epoch 9.28: Loss = 0.460495
Epoch 9.29: Loss = 0.55722
Epoch 9.30: Loss = 0.611557
Epoch 9.31: Loss = 0.5298
Epoch 9.32: Loss = 0.614105
Epoch 9.33: Loss = 0.635529
Epoch 9.34: Loss = 0.519211
Epoch 9.35: Loss = 0.564377
Epoch 9.36: Loss = 0.534256
Epoch 9.37: Loss = 0.6362
Epoch 9.38: Loss = 0.577988
Epoch 9.39: Loss = 0.641586
Epoch 9.40: Loss = 0.570129
Epoch 9.41: Loss = 0.56105
Epoch 9.42: Loss = 0.632568
Epoch 9.43: Loss = 0.641968
Epoch 9.44: Loss = 0.753433
Epoch 9.45: Loss = 0.585281
Epoch 9.46: Loss = 0.549408
Epoch 9.47: Loss = 0.594162
Epoch 9.48: Loss = 0.558121
Epoch 9.49: Loss = 0.631729
Epoch 9.50: Loss = 0.610046
Epoch 9.51: Loss = 0.595978
Epoch 9.52: Loss = 0.631149
Epoch 9.53: Loss = 0.613388
Epoch 9.54: Loss = 0.533936
Epoch 9.55: Loss = 0.624741
Epoch 9.56: Loss = 0.614685
Epoch 9.57: Loss = 0.586441
Epoch 9.58: Loss = 0.599106
Epoch 9.59: Loss = 0.559113
Epoch 9.60: Loss = 0.580414
Epoch 9.61: Loss = 0.605042
Epoch 9.62: Loss = 0.585037
Epoch 9.63: Loss = 0.570511
Epoch 9.64: Loss = 0.575134
Epoch 9.65: Loss = 0.6362
Epoch 9.66: Loss = 0.51561
Epoch 9.67: Loss = 0.52771
Epoch 9.68: Loss = 0.702148
Epoch 9.69: Loss = 0.598999
Epoch 9.70: Loss = 0.593292
Epoch 9.71: Loss = 0.511093
Epoch 9.72: Loss = 0.602997
Epoch 9.73: Loss = 0.517288
Epoch 9.74: Loss = 0.557846
Epoch 9.75: Loss = 0.644714
Epoch 9.76: Loss = 0.554993
Epoch 9.77: Loss = 0.476807
Epoch 9.78: Loss = 0.61142
Epoch 9.79: Loss = 0.644516
Epoch 9.80: Loss = 0.529892
Epoch 9.81: Loss = 0.626816
Epoch 9.82: Loss = 0.65033
Epoch 9.83: Loss = 0.492538
Epoch 9.84: Loss = 0.54306
Epoch 9.85: Loss = 0.57576
Epoch 9.86: Loss = 0.6241
Epoch 9.87: Loss = 0.552139
Epoch 9.88: Loss = 0.602066
Epoch 9.89: Loss = 0.523148
Epoch 9.90: Loss = 0.548798
Epoch 9.91: Loss = 0.560913
Epoch 9.92: Loss = 0.577271
Epoch 9.93: Loss = 0.62442
Epoch 9.94: Loss = 0.583008
Epoch 9.95: Loss = 0.638977
Epoch 9.96: Loss = 0.633591
Epoch 9.97: Loss = 0.524902
Epoch 9.98: Loss = 0.545837
Epoch 9.99: Loss = 0.597061
Epoch 9.100: Loss = 0.562881
Epoch 9.101: Loss = 0.50708
Epoch 9.102: Loss = 0.660873
Epoch 9.103: Loss = 0.511948
Epoch 9.104: Loss = 0.625702
Epoch 9.105: Loss = 0.591553
Epoch 9.106: Loss = 0.635452
Epoch 9.107: Loss = 0.506104
Epoch 9.108: Loss = 0.596085
Epoch 9.109: Loss = 0.686966
Epoch 9.110: Loss = 0.594376
Epoch 9.111: Loss = 0.613022
Epoch 9.112: Loss = 0.565521
Epoch 9.113: Loss = 0.509186
Epoch 9.114: Loss = 0.500397
Epoch 9.115: Loss = 0.613525
Epoch 9.116: Loss = 0.572113
Epoch 9.117: Loss = 0.609299
Epoch 9.118: Loss = 0.500793
Epoch 9.119: Loss = 0.686127
Epoch 9.120: Loss = 0.598755
TRAIN LOSS = 0.58342
TRAIN ACC = 81.6971 % (49020/60000)
Loss = 0.523529
Loss = 0.611404
Loss = 0.56636
Loss = 0.503952
Loss = 0.559952
Loss = 0.673386
Loss = 0.745148
Loss = 0.663132
Loss = 0.598114
Loss = 0.563324
Loss = 0.740356
Loss = 0.681717
Loss = 0.610413
Loss = 0.584442
Loss = 0.583191
Loss = 0.614059
Loss = 0.578766
Loss = 0.630066
Loss = 0.618256
Loss = 0.601364
TEST LOSS = 0.612546
TEST ACC = 490.199 % (8086/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.591797
Epoch 10.2: Loss = 0.517303
Epoch 10.3: Loss = 0.581131
Epoch 10.4: Loss = 0.614761
Epoch 10.5: Loss = 0.563339
Epoch 10.6: Loss = 0.561447
Epoch 10.7: Loss = 0.547165
Epoch 10.8: Loss = 0.556137
Epoch 10.9: Loss = 0.494186
Epoch 10.10: Loss = 0.566803
Epoch 10.11: Loss = 0.670013
Epoch 10.12: Loss = 0.544556
Epoch 10.13: Loss = 0.694855
Epoch 10.14: Loss = 0.622192
Epoch 10.15: Loss = 0.611969
Epoch 10.16: Loss = 0.674393
Epoch 10.17: Loss = 0.588577
Epoch 10.18: Loss = 0.613098
Epoch 10.19: Loss = 0.477585
Epoch 10.20: Loss = 0.562714
Epoch 10.21: Loss = 0.594467
Epoch 10.22: Loss = 0.558792
Epoch 10.23: Loss = 0.626526
Epoch 10.24: Loss = 0.59552
Epoch 10.25: Loss = 0.536652
Epoch 10.26: Loss = 0.520752
Epoch 10.27: Loss = 0.529007
Epoch 10.28: Loss = 0.53096
Epoch 10.29: Loss = 0.642838
Epoch 10.30: Loss = 0.595795
Epoch 10.31: Loss = 0.609695
Epoch 10.32: Loss = 0.614365
Epoch 10.33: Loss = 0.655334
Epoch 10.34: Loss = 0.638214
Epoch 10.35: Loss = 0.496933
Epoch 10.36: Loss = 0.571976
Epoch 10.37: Loss = 0.518723
Epoch 10.38: Loss = 0.543243
Epoch 10.39: Loss = 0.575272
Epoch 10.40: Loss = 0.536255
Epoch 10.41: Loss = 0.480225
Epoch 10.42: Loss = 0.630432
Epoch 10.43: Loss = 0.605621
Epoch 10.44: Loss = 0.441544
Epoch 10.45: Loss = 0.522552
Epoch 10.46: Loss = 0.630783
Epoch 10.47: Loss = 0.588776
Epoch 10.48: Loss = 0.513809
Epoch 10.49: Loss = 0.570511
Epoch 10.50: Loss = 0.661728
Epoch 10.51: Loss = 0.648865
Epoch 10.52: Loss = 0.490356
Epoch 10.53: Loss = 0.56604
Epoch 10.54: Loss = 0.436234
Epoch 10.55: Loss = 0.644379
Epoch 10.56: Loss = 0.648468
Epoch 10.57: Loss = 0.589355
Epoch 10.58: Loss = 0.56163
Epoch 10.59: Loss = 0.553619
Epoch 10.60: Loss = 0.51741
Epoch 10.61: Loss = 0.464706
Epoch 10.62: Loss = 0.575241
Epoch 10.63: Loss = 0.595398
Epoch 10.64: Loss = 0.618561
Epoch 10.65: Loss = 0.688278
Epoch 10.66: Loss = 0.727585
Epoch 10.67: Loss = 0.533752
Epoch 10.68: Loss = 0.481339
Epoch 10.69: Loss = 0.568542
Epoch 10.70: Loss = 0.470932
Epoch 10.71: Loss = 0.604965
Epoch 10.72: Loss = 0.589737
Epoch 10.73: Loss = 0.621124
Epoch 10.74: Loss = 0.630844
Epoch 10.75: Loss = 0.579788
Epoch 10.76: Loss = 0.558105
Epoch 10.77: Loss = 0.521988
Epoch 10.78: Loss = 0.618271
Epoch 10.79: Loss = 0.618408
Epoch 10.80: Loss = 0.605484
Epoch 10.81: Loss = 0.58429
Epoch 10.82: Loss = 0.530975
Epoch 10.83: Loss = 0.61554
Epoch 10.84: Loss = 0.596664
Epoch 10.85: Loss = 0.532333
Epoch 10.86: Loss = 0.558121
Epoch 10.87: Loss = 0.537125
Epoch 10.88: Loss = 0.6138
Epoch 10.89: Loss = 0.613235
Epoch 10.90: Loss = 0.64447
Epoch 10.91: Loss = 0.586395
Epoch 10.92: Loss = 0.589355
Epoch 10.93: Loss = 0.639221
Epoch 10.94: Loss = 0.570969
Epoch 10.95: Loss = 0.532883
Epoch 10.96: Loss = 0.65126
Epoch 10.97: Loss = 0.602875
Epoch 10.98: Loss = 0.480637
Epoch 10.99: Loss = 0.574661
Epoch 10.100: Loss = 0.611206
Epoch 10.101: Loss = 0.615952
Epoch 10.102: Loss = 0.65271
Epoch 10.103: Loss = 0.679886
Epoch 10.104: Loss = 0.681412
Epoch 10.105: Loss = 0.554276
Epoch 10.106: Loss = 0.662888
Epoch 10.107: Loss = 0.481049
Epoch 10.108: Loss = 0.519531
Epoch 10.109: Loss = 0.617096
Epoch 10.110: Loss = 0.503525
Epoch 10.111: Loss = 0.614319
Epoch 10.112: Loss = 0.566086
Epoch 10.113: Loss = 0.662857
Epoch 10.114: Loss = 0.576569
Epoch 10.115: Loss = 0.539413
Epoch 10.116: Loss = 0.598236
Epoch 10.117: Loss = 0.580902
Epoch 10.118: Loss = 0.577759
Epoch 10.119: Loss = 0.621384
Epoch 10.120: Loss = 0.567505
TRAIN LOSS = 0.580505
TRAIN ACC = 81.9244 % (49157/60000)
Loss = 0.522659
Loss = 0.626099
Loss = 0.56987
Loss = 0.513107
Loss = 0.571213
Loss = 0.677475
Loss = 0.748535
Loss = 0.657623
Loss = 0.608902
Loss = 0.565643
Loss = 0.745819
Loss = 0.702484
Loss = 0.619781
Loss = 0.603165
Loss = 0.605316
Loss = 0.621857
Loss = 0.585632
Loss = 0.647919
Loss = 0.624725
Loss = 0.600204
TEST LOSS = 0.620901
TEST ACC = 491.57 % (8134/10000)

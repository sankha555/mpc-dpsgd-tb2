Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.32352
Epoch 1.2: Loss = 2.31902
Epoch 1.3: Loss = 2.27953
Epoch 1.4: Loss = 2.26213
Epoch 1.5: Loss = 2.24408
Epoch 1.6: Loss = 2.22563
Epoch 1.7: Loss = 2.18579
Epoch 1.8: Loss = 2.14406
Epoch 1.9: Loss = 2.16705
Epoch 1.10: Loss = 2.13586
Epoch 1.11: Loss = 2.07457
Epoch 1.12: Loss = 2.03539
Epoch 1.13: Loss = 2.01271
Epoch 1.14: Loss = 2.00027
Epoch 1.15: Loss = 1.92601
Epoch 1.16: Loss = 1.94022
Epoch 1.17: Loss = 1.89105
Epoch 1.18: Loss = 1.86195
Epoch 1.19: Loss = 1.86047
Epoch 1.20: Loss = 1.82991
Epoch 1.21: Loss = 1.79749
Epoch 1.22: Loss = 1.73975
Epoch 1.23: Loss = 1.70827
Epoch 1.24: Loss = 1.66817
Epoch 1.25: Loss = 1.67728
Epoch 1.26: Loss = 1.65338
Epoch 1.27: Loss = 1.57164
Epoch 1.28: Loss = 1.5694
Epoch 1.29: Loss = 1.60461
Epoch 1.30: Loss = 1.51445
Epoch 1.31: Loss = 1.4438
Epoch 1.32: Loss = 1.43954
Epoch 1.33: Loss = 1.46266
Epoch 1.34: Loss = 1.44659
Epoch 1.35: Loss = 1.36896
Epoch 1.36: Loss = 1.3992
Epoch 1.37: Loss = 1.35924
Epoch 1.38: Loss = 1.28986
Epoch 1.39: Loss = 1.32155
Epoch 1.40: Loss = 1.30104
Epoch 1.41: Loss = 1.27144
Epoch 1.42: Loss = 1.17775
Epoch 1.43: Loss = 1.1916
Epoch 1.44: Loss = 1.16905
Epoch 1.45: Loss = 1.2471
Epoch 1.46: Loss = 1.14359
Epoch 1.47: Loss = 1.17821
Epoch 1.48: Loss = 1.10107
Epoch 1.49: Loss = 1.1133
Epoch 1.50: Loss = 1.07535
Epoch 1.51: Loss = 1.07553
Epoch 1.52: Loss = 1.03694
Epoch 1.53: Loss = 0.983154
Epoch 1.54: Loss = 0.95433
Epoch 1.55: Loss = 1.03349
Epoch 1.56: Loss = 0.98024
Epoch 1.57: Loss = 1.0067
Epoch 1.58: Loss = 0.916519
Epoch 1.59: Loss = 0.933441
Epoch 1.60: Loss = 0.920654
Epoch 1.61: Loss = 0.841644
Epoch 1.62: Loss = 0.900665
Epoch 1.63: Loss = 0.873077
Epoch 1.64: Loss = 0.912231
Epoch 1.65: Loss = 0.916794
Epoch 1.66: Loss = 0.866974
Epoch 1.67: Loss = 0.845505
Epoch 1.68: Loss = 0.871811
Epoch 1.69: Loss = 0.855652
Epoch 1.70: Loss = 0.779129
Epoch 1.71: Loss = 0.859222
Epoch 1.72: Loss = 0.860092
Epoch 1.73: Loss = 0.776367
Epoch 1.74: Loss = 0.807465
Epoch 1.75: Loss = 0.783142
Epoch 1.76: Loss = 0.811234
Epoch 1.77: Loss = 0.770203
Epoch 1.78: Loss = 0.79422
Epoch 1.79: Loss = 0.788086
Epoch 1.80: Loss = 0.836975
Epoch 1.81: Loss = 0.838654
Epoch 1.82: Loss = 0.76918
Epoch 1.83: Loss = 0.724503
Epoch 1.84: Loss = 0.729187
Epoch 1.85: Loss = 0.745071
Epoch 1.86: Loss = 0.736588
Epoch 1.87: Loss = 0.658447
Epoch 1.88: Loss = 0.686783
Epoch 1.89: Loss = 0.694809
Epoch 1.90: Loss = 0.632523
Epoch 1.91: Loss = 0.779007
Epoch 1.92: Loss = 0.757828
Epoch 1.93: Loss = 0.668808
Epoch 1.94: Loss = 0.630203
Epoch 1.95: Loss = 0.721252
Epoch 1.96: Loss = 0.700851
Epoch 1.97: Loss = 0.719833
Epoch 1.98: Loss = 0.667191
Epoch 1.99: Loss = 0.661163
Epoch 1.100: Loss = 0.611298
Epoch 1.101: Loss = 0.682144
Epoch 1.102: Loss = 0.709976
Epoch 1.103: Loss = 0.629395
Epoch 1.104: Loss = 0.659592
Epoch 1.105: Loss = 0.651077
Epoch 1.106: Loss = 0.68988
Epoch 1.107: Loss = 0.63649
Epoch 1.108: Loss = 0.616196
Epoch 1.109: Loss = 0.615372
Epoch 1.110: Loss = 0.588486
Epoch 1.111: Loss = 0.664597
Epoch 1.112: Loss = 0.578293
Epoch 1.113: Loss = 0.635574
Epoch 1.114: Loss = 0.630203
Epoch 1.115: Loss = 0.603638
Epoch 1.116: Loss = 0.51506
Epoch 1.117: Loss = 0.586563
Epoch 1.118: Loss = 0.600525
Epoch 1.119: Loss = 0.632187
Epoch 1.120: Loss = 0.597473
TRAIN LOSS = 1.14145
TRAIN ACC = 68.2327 % (40942/60000)
Loss = 0.646225
Loss = 0.695114
Loss = 0.791
Loss = 0.723801
Loss = 0.781357
Loss = 0.673172
Loss = 0.588791
Loss = 0.817322
Loss = 0.755966
Loss = 0.703796
Loss = 0.357635
Loss = 0.584656
Loss = 0.440323
Loss = 0.553024
Loss = 0.441956
Loss = 0.448883
Loss = 0.434341
Loss = 0.26474
Loss = 0.389252
Loss = 0.688034
TEST LOSS = 0.588969
TEST ACC = 409.419 % (8194/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.601624
Epoch 2.2: Loss = 0.532669
Epoch 2.3: Loss = 0.619034
Epoch 2.4: Loss = 0.605438
Epoch 2.5: Loss = 0.541702
Epoch 2.6: Loss = 0.636551
Epoch 2.7: Loss = 0.545929
Epoch 2.8: Loss = 0.570557
Epoch 2.9: Loss = 0.659912
Epoch 2.10: Loss = 0.618347
Epoch 2.11: Loss = 0.608795
Epoch 2.12: Loss = 0.574753
Epoch 2.13: Loss = 0.547791
Epoch 2.14: Loss = 0.56955
Epoch 2.15: Loss = 0.631546
Epoch 2.16: Loss = 0.610245
Epoch 2.17: Loss = 0.49794
Epoch 2.18: Loss = 0.576904
Epoch 2.19: Loss = 0.510452
Epoch 2.20: Loss = 0.500809
Epoch 2.21: Loss = 0.603989
Epoch 2.22: Loss = 0.55043
Epoch 2.23: Loss = 0.543564
Epoch 2.24: Loss = 0.533035
Epoch 2.25: Loss = 0.609879
Epoch 2.26: Loss = 0.592636
Epoch 2.27: Loss = 0.570465
Epoch 2.28: Loss = 0.503922
Epoch 2.29: Loss = 0.594513
Epoch 2.30: Loss = 0.479202
Epoch 2.31: Loss = 0.62851
Epoch 2.32: Loss = 0.528854
Epoch 2.33: Loss = 0.541656
Epoch 2.34: Loss = 0.512741
Epoch 2.35: Loss = 0.507584
Epoch 2.36: Loss = 0.533493
Epoch 2.37: Loss = 0.521225
Epoch 2.38: Loss = 0.563599
Epoch 2.39: Loss = 0.535156
Epoch 2.40: Loss = 0.552597
Epoch 2.41: Loss = 0.51506
Epoch 2.42: Loss = 0.425827
Epoch 2.43: Loss = 0.583969
Epoch 2.44: Loss = 0.505325
Epoch 2.45: Loss = 0.51001
Epoch 2.46: Loss = 0.54686
Epoch 2.47: Loss = 0.497757
Epoch 2.48: Loss = 0.463882
Epoch 2.49: Loss = 0.546158
Epoch 2.50: Loss = 0.509308
Epoch 2.51: Loss = 0.568161
Epoch 2.52: Loss = 0.579544
Epoch 2.53: Loss = 0.577728
Epoch 2.54: Loss = 0.527664
Epoch 2.55: Loss = 0.554321
Epoch 2.56: Loss = 0.579254
Epoch 2.57: Loss = 0.524582
Epoch 2.58: Loss = 0.509964
Epoch 2.59: Loss = 0.451935
Epoch 2.60: Loss = 0.472366
Epoch 2.61: Loss = 0.430801
Epoch 2.62: Loss = 0.51889
Epoch 2.63: Loss = 0.425201
Epoch 2.64: Loss = 0.488495
Epoch 2.65: Loss = 0.520035
Epoch 2.66: Loss = 0.585236
Epoch 2.67: Loss = 0.462708
Epoch 2.68: Loss = 0.590118
Epoch 2.69: Loss = 0.596542
Epoch 2.70: Loss = 0.49144
Epoch 2.71: Loss = 0.535294
Epoch 2.72: Loss = 0.562225
Epoch 2.73: Loss = 0.497604
Epoch 2.74: Loss = 0.52713
Epoch 2.75: Loss = 0.534943
Epoch 2.76: Loss = 0.483414
Epoch 2.77: Loss = 0.454208
Epoch 2.78: Loss = 0.531113
Epoch 2.79: Loss = 0.561066
Epoch 2.80: Loss = 0.487076
Epoch 2.81: Loss = 0.568039
Epoch 2.82: Loss = 0.534561
Epoch 2.83: Loss = 0.531754
Epoch 2.84: Loss = 0.491501
Epoch 2.85: Loss = 0.550613
Epoch 2.86: Loss = 0.487869
Epoch 2.87: Loss = 0.542099
Epoch 2.88: Loss = 0.511383
Epoch 2.89: Loss = 0.620834
Epoch 2.90: Loss = 0.574265
Epoch 2.91: Loss = 0.571274
Epoch 2.92: Loss = 0.517654
Epoch 2.93: Loss = 0.512634
Epoch 2.94: Loss = 0.420609
Epoch 2.95: Loss = 0.536682
Epoch 2.96: Loss = 0.451462
Epoch 2.97: Loss = 0.465027
Epoch 2.98: Loss = 0.559204
Epoch 2.99: Loss = 0.454025
Epoch 2.100: Loss = 0.42598
Epoch 2.101: Loss = 0.492966
Epoch 2.102: Loss = 0.543762
Epoch 2.103: Loss = 0.594772
Epoch 2.104: Loss = 0.53717
Epoch 2.105: Loss = 0.562485
Epoch 2.106: Loss = 0.446594
Epoch 2.107: Loss = 0.556641
Epoch 2.108: Loss = 0.495987
Epoch 2.109: Loss = 0.475021
Epoch 2.110: Loss = 0.509109
Epoch 2.111: Loss = 0.417999
Epoch 2.112: Loss = 0.600388
Epoch 2.113: Loss = 0.442963
Epoch 2.114: Loss = 0.474289
Epoch 2.115: Loss = 0.507874
Epoch 2.116: Loss = 0.48645
Epoch 2.117: Loss = 0.528595
Epoch 2.118: Loss = 0.558655
Epoch 2.119: Loss = 0.438065
Epoch 2.120: Loss = 0.462326
TRAIN LOSS = 0.531342
TRAIN ACC = 83.8013 % (50283/60000)
Loss = 0.469254
Loss = 0.587845
Loss = 0.663651
Loss = 0.663986
Loss = 0.705933
Loss = 0.511139
Loss = 0.452103
Loss = 0.720963
Loss = 0.623413
Loss = 0.585297
Loss = 0.221893
Loss = 0.410858
Loss = 0.351318
Loss = 0.413879
Loss = 0.281723
Loss = 0.313675
Loss = 0.298172
Loss = 0.12236
Loss = 0.262222
Loss = 0.594803
TEST LOSS = 0.462724
TEST ACC = 502.829 % (8629/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.390396
Epoch 3.2: Loss = 0.450546
Epoch 3.3: Loss = 0.482758
Epoch 3.4: Loss = 0.531693
Epoch 3.5: Loss = 0.469513
Epoch 3.6: Loss = 0.419617
Epoch 3.7: Loss = 0.444214
Epoch 3.8: Loss = 0.393478
Epoch 3.9: Loss = 0.549271
Epoch 3.10: Loss = 0.461243
Epoch 3.11: Loss = 0.499649
Epoch 3.12: Loss = 0.539795
Epoch 3.13: Loss = 0.467957
Epoch 3.14: Loss = 0.477737
Epoch 3.15: Loss = 0.43515
Epoch 3.16: Loss = 0.386703
Epoch 3.17: Loss = 0.436111
Epoch 3.18: Loss = 0.515671
Epoch 3.19: Loss = 0.519867
Epoch 3.20: Loss = 0.493362
Epoch 3.21: Loss = 0.435318
Epoch 3.22: Loss = 0.509094
Epoch 3.23: Loss = 0.566208
Epoch 3.24: Loss = 0.489883
Epoch 3.25: Loss = 0.440247
Epoch 3.26: Loss = 0.433197
Epoch 3.27: Loss = 0.452118
Epoch 3.28: Loss = 0.530136
Epoch 3.29: Loss = 0.459335
Epoch 3.30: Loss = 0.445709
Epoch 3.31: Loss = 0.356201
Epoch 3.32: Loss = 0.544662
Epoch 3.33: Loss = 0.431839
Epoch 3.34: Loss = 0.456146
Epoch 3.35: Loss = 0.511215
Epoch 3.36: Loss = 0.471802
Epoch 3.37: Loss = 0.521713
Epoch 3.38: Loss = 0.416611
Epoch 3.39: Loss = 0.530304
Epoch 3.40: Loss = 0.469284
Epoch 3.41: Loss = 0.532196
Epoch 3.42: Loss = 0.62468
Epoch 3.43: Loss = 0.562637
Epoch 3.44: Loss = 0.573578
Epoch 3.45: Loss = 0.437805
Epoch 3.46: Loss = 0.460571
Epoch 3.47: Loss = 0.459671
Epoch 3.48: Loss = 0.561523
Epoch 3.49: Loss = 0.504868
Epoch 3.50: Loss = 0.47731
Epoch 3.51: Loss = 0.525864
Epoch 3.52: Loss = 0.532852
Epoch 3.53: Loss = 0.550323
Epoch 3.54: Loss = 0.440704
Epoch 3.55: Loss = 0.583588
Epoch 3.56: Loss = 0.403198
Epoch 3.57: Loss = 0.550598
Epoch 3.58: Loss = 0.518524
Epoch 3.59: Loss = 0.53952
Epoch 3.60: Loss = 0.48613
Epoch 3.61: Loss = 0.458115
Epoch 3.62: Loss = 0.451141
Epoch 3.63: Loss = 0.468582
Epoch 3.64: Loss = 0.483551
Epoch 3.65: Loss = 0.601669
Epoch 3.66: Loss = 0.475479
Epoch 3.67: Loss = 0.524582
Epoch 3.68: Loss = 0.476135
Epoch 3.69: Loss = 0.529877
Epoch 3.70: Loss = 0.427338
Epoch 3.71: Loss = 0.483276
Epoch 3.72: Loss = 0.350891
Epoch 3.73: Loss = 0.521027
Epoch 3.74: Loss = 0.510147
Epoch 3.75: Loss = 0.436234
Epoch 3.76: Loss = 0.412415
Epoch 3.77: Loss = 0.503845
Epoch 3.78: Loss = 0.53212
Epoch 3.79: Loss = 0.526962
Epoch 3.80: Loss = 0.500809
Epoch 3.81: Loss = 0.440933
Epoch 3.82: Loss = 0.535019
Epoch 3.83: Loss = 0.433136
Epoch 3.84: Loss = 0.555115
Epoch 3.85: Loss = 0.499298
Epoch 3.86: Loss = 0.550095
Epoch 3.87: Loss = 0.57608
Epoch 3.88: Loss = 0.498535
Epoch 3.89: Loss = 0.445969
Epoch 3.90: Loss = 0.461243
Epoch 3.91: Loss = 0.545471
Epoch 3.92: Loss = 0.533478
Epoch 3.93: Loss = 0.525955
Epoch 3.94: Loss = 0.541595
Epoch 3.95: Loss = 0.347198
Epoch 3.96: Loss = 0.362717
Epoch 3.97: Loss = 0.553391
Epoch 3.98: Loss = 0.494537
Epoch 3.99: Loss = 0.409546
Epoch 3.100: Loss = 0.517197
Epoch 3.101: Loss = 0.368179
Epoch 3.102: Loss = 0.491882
Epoch 3.103: Loss = 0.469345
Epoch 3.104: Loss = 0.476852
Epoch 3.105: Loss = 0.486313
Epoch 3.106: Loss = 0.410278
Epoch 3.107: Loss = 0.420761
Epoch 3.108: Loss = 0.481918
Epoch 3.109: Loss = 0.471344
Epoch 3.110: Loss = 0.491714
Epoch 3.111: Loss = 0.51564
Epoch 3.112: Loss = 0.604675
Epoch 3.113: Loss = 0.489502
Epoch 3.114: Loss = 0.40538
Epoch 3.115: Loss = 0.520721
Epoch 3.116: Loss = 0.461121
Epoch 3.117: Loss = 0.565414
Epoch 3.118: Loss = 0.533096
Epoch 3.119: Loss = 0.55574
Epoch 3.120: Loss = 0.576538
TRAIN LOSS = 0.487167
TRAIN ACC = 85.8459 % (51510/60000)
Loss = 0.505112
Loss = 0.625214
Loss = 0.686295
Loss = 0.713791
Loss = 0.707657
Loss = 0.496719
Loss = 0.443954
Loss = 0.680267
Loss = 0.64032
Loss = 0.561371
Loss = 0.236389
Loss = 0.428452
Loss = 0.30162
Loss = 0.4272
Loss = 0.256454
Loss = 0.34343
Loss = 0.273224
Loss = 0.107605
Loss = 0.248978
Loss = 0.611099
TEST LOSS = 0.464757
TEST ACC = 515.099 % (8680/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.511932
Epoch 4.2: Loss = 0.510544
Epoch 4.3: Loss = 0.518036
Epoch 4.4: Loss = 0.528717
Epoch 4.5: Loss = 0.434891
Epoch 4.6: Loss = 0.42897
Epoch 4.7: Loss = 0.497559
Epoch 4.8: Loss = 0.537491
Epoch 4.9: Loss = 0.506424
Epoch 4.10: Loss = 0.481415
Epoch 4.11: Loss = 0.586685
Epoch 4.12: Loss = 0.408203
Epoch 4.13: Loss = 0.486954
Epoch 4.14: Loss = 0.381134
Epoch 4.15: Loss = 0.508835
Epoch 4.16: Loss = 0.595795
Epoch 4.17: Loss = 0.542358
Epoch 4.18: Loss = 0.482178
Epoch 4.19: Loss = 0.498367
Epoch 4.20: Loss = 0.671768
Epoch 4.21: Loss = 0.517639
Epoch 4.22: Loss = 0.587769
Epoch 4.23: Loss = 0.48172
Epoch 4.24: Loss = 0.551498
Epoch 4.25: Loss = 0.582733
Epoch 4.26: Loss = 0.425934
Epoch 4.27: Loss = 0.469833
Epoch 4.28: Loss = 0.5047
Epoch 4.29: Loss = 0.5168
Epoch 4.30: Loss = 0.534821
Epoch 4.31: Loss = 0.419601
Epoch 4.32: Loss = 0.524155
Epoch 4.33: Loss = 0.507248
Epoch 4.34: Loss = 0.441635
Epoch 4.35: Loss = 0.395355
Epoch 4.36: Loss = 0.492996
Epoch 4.37: Loss = 0.554306
Epoch 4.38: Loss = 0.493927
Epoch 4.39: Loss = 0.56604
Epoch 4.40: Loss = 0.545059
Epoch 4.41: Loss = 0.440872
Epoch 4.42: Loss = 0.52948
Epoch 4.43: Loss = 0.448517
Epoch 4.44: Loss = 0.622757
Epoch 4.45: Loss = 0.503693
Epoch 4.46: Loss = 0.549011
Epoch 4.47: Loss = 0.452805
Epoch 4.48: Loss = 0.495483
Epoch 4.49: Loss = 0.459824
Epoch 4.50: Loss = 0.507736
Epoch 4.51: Loss = 0.620941
Epoch 4.52: Loss = 0.464996
Epoch 4.53: Loss = 0.413773
Epoch 4.54: Loss = 0.518188
Epoch 4.55: Loss = 0.475327
Epoch 4.56: Loss = 0.411728
Epoch 4.57: Loss = 0.532196
Epoch 4.58: Loss = 0.570267
Epoch 4.59: Loss = 0.458435
Epoch 4.60: Loss = 0.523621
Epoch 4.61: Loss = 0.523087
Epoch 4.62: Loss = 0.630569
Epoch 4.63: Loss = 0.40654
Epoch 4.64: Loss = 0.531387
Epoch 4.65: Loss = 0.450592
Epoch 4.66: Loss = 0.48381
Epoch 4.67: Loss = 0.497086
Epoch 4.68: Loss = 0.554825
Epoch 4.69: Loss = 0.467087
Epoch 4.70: Loss = 0.558533
Epoch 4.71: Loss = 0.641296
Epoch 4.72: Loss = 0.521622
Epoch 4.73: Loss = 0.483383
Epoch 4.74: Loss = 0.539383
Epoch 4.75: Loss = 0.477249
Epoch 4.76: Loss = 0.690735
Epoch 4.77: Loss = 0.414001
Epoch 4.78: Loss = 0.463013
Epoch 4.79: Loss = 0.578644
Epoch 4.80: Loss = 0.473145
Epoch 4.81: Loss = 0.511627
Epoch 4.82: Loss = 0.516754
Epoch 4.83: Loss = 0.51236
Epoch 4.84: Loss = 0.584366
Epoch 4.85: Loss = 0.478836
Epoch 4.86: Loss = 0.480453
Epoch 4.87: Loss = 0.509506
Epoch 4.88: Loss = 0.469284
Epoch 4.89: Loss = 0.594788
Epoch 4.90: Loss = 0.533386
Epoch 4.91: Loss = 0.564072
Epoch 4.92: Loss = 0.538849
Epoch 4.93: Loss = 0.535706
Epoch 4.94: Loss = 0.498322
Epoch 4.95: Loss = 0.615005
Epoch 4.96: Loss = 0.437866
Epoch 4.97: Loss = 0.473114
Epoch 4.98: Loss = 0.429489
Epoch 4.99: Loss = 0.464523
Epoch 4.100: Loss = 0.535034
Epoch 4.101: Loss = 0.567825
Epoch 4.102: Loss = 0.538528
Epoch 4.103: Loss = 0.625961
Epoch 4.104: Loss = 0.522644
Epoch 4.105: Loss = 0.55751
Epoch 4.106: Loss = 0.433594
Epoch 4.107: Loss = 0.579376
Epoch 4.108: Loss = 0.505478
Epoch 4.109: Loss = 0.526047
Epoch 4.110: Loss = 0.460938
Epoch 4.111: Loss = 0.507126
Epoch 4.112: Loss = 0.530869
Epoch 4.113: Loss = 0.489014
Epoch 4.114: Loss = 0.505371
Epoch 4.115: Loss = 0.373856
Epoch 4.116: Loss = 0.546799
Epoch 4.117: Loss = 0.412247
Epoch 4.118: Loss = 0.55069
Epoch 4.119: Loss = 0.438675
Epoch 4.120: Loss = 0.622772
TRAIN LOSS = 0.509979
TRAIN ACC = 85.7758 % (51467/60000)
Loss = 0.587357
Loss = 0.67485
Loss = 0.785172
Loss = 0.772995
Loss = 0.773865
Loss = 0.55394
Loss = 0.477921
Loss = 0.785263
Loss = 0.666992
Loss = 0.647949
Loss = 0.237427
Loss = 0.400009
Loss = 0.327698
Loss = 0.390366
Loss = 0.232468
Loss = 0.368698
Loss = 0.270279
Loss = 0.120865
Loss = 0.268311
Loss = 0.694016
TEST LOSS = 0.501822
TEST ACC = 514.67 % (8653/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.526077
Epoch 5.2: Loss = 0.582367
Epoch 5.3: Loss = 0.566376
Epoch 5.4: Loss = 0.579803
Epoch 5.5: Loss = 0.61824
Epoch 5.6: Loss = 0.506744
Epoch 5.7: Loss = 0.559402
Epoch 5.8: Loss = 0.643005
Epoch 5.9: Loss = 0.697159
Epoch 5.10: Loss = 0.535538
Epoch 5.11: Loss = 0.517899
Epoch 5.12: Loss = 0.402786
Epoch 5.13: Loss = 0.496826
Epoch 5.14: Loss = 0.51651
Epoch 5.15: Loss = 0.582062
Epoch 5.16: Loss = 0.500412
Epoch 5.17: Loss = 0.612595
Epoch 5.18: Loss = 0.623566
Epoch 5.19: Loss = 0.49881
Epoch 5.20: Loss = 0.55072
Epoch 5.21: Loss = 0.507904
Epoch 5.22: Loss = 0.525711
Epoch 5.23: Loss = 0.426559
Epoch 5.24: Loss = 0.455643
Epoch 5.25: Loss = 0.627548
Epoch 5.26: Loss = 0.597198
Epoch 5.27: Loss = 0.501602
Epoch 5.28: Loss = 0.558777
Epoch 5.29: Loss = 0.391724
Epoch 5.30: Loss = 0.551086
Epoch 5.31: Loss = 0.486679
Epoch 5.32: Loss = 0.39061
Epoch 5.33: Loss = 0.488525
Epoch 5.34: Loss = 0.499237
Epoch 5.35: Loss = 0.484833
Epoch 5.36: Loss = 0.39888
Epoch 5.37: Loss = 0.538086
Epoch 5.38: Loss = 0.504272
Epoch 5.39: Loss = 0.564285
Epoch 5.40: Loss = 0.474442
Epoch 5.41: Loss = 0.487518
Epoch 5.42: Loss = 0.566589
Epoch 5.43: Loss = 0.452484
Epoch 5.44: Loss = 0.620316
Epoch 5.45: Loss = 0.637177
Epoch 5.46: Loss = 0.595322
Epoch 5.47: Loss = 0.514572
Epoch 5.48: Loss = 0.487274
Epoch 5.49: Loss = 0.615768
Epoch 5.50: Loss = 0.522095
Epoch 5.51: Loss = 0.532074
Epoch 5.52: Loss = 0.663803
Epoch 5.53: Loss = 0.67305
Epoch 5.54: Loss = 0.490662
Epoch 5.55: Loss = 0.484818
Epoch 5.56: Loss = 0.52301
Epoch 5.57: Loss = 0.487778
Epoch 5.58: Loss = 0.644608
Epoch 5.59: Loss = 0.539047
Epoch 5.60: Loss = 0.520432
Epoch 5.61: Loss = 0.712051
Epoch 5.62: Loss = 0.45784
Epoch 5.63: Loss = 0.536026
Epoch 5.64: Loss = 0.55098
Epoch 5.65: Loss = 0.545822
Epoch 5.66: Loss = 0.51651
Epoch 5.67: Loss = 0.497421
Epoch 5.68: Loss = 0.608505
Epoch 5.69: Loss = 0.510269
Epoch 5.70: Loss = 0.458984
Epoch 5.71: Loss = 0.587418
Epoch 5.72: Loss = 0.452911
Epoch 5.73: Loss = 0.569992
Epoch 5.74: Loss = 0.484741
Epoch 5.75: Loss = 0.632629
Epoch 5.76: Loss = 0.5681
Epoch 5.77: Loss = 0.449036
Epoch 5.78: Loss = 0.542175
Epoch 5.79: Loss = 0.462463
Epoch 5.80: Loss = 0.478699
Epoch 5.81: Loss = 0.652161
Epoch 5.82: Loss = 0.635223
Epoch 5.83: Loss = 0.574219
Epoch 5.84: Loss = 0.481522
Epoch 5.85: Loss = 0.390839
Epoch 5.86: Loss = 0.535492
Epoch 5.87: Loss = 0.605957
Epoch 5.88: Loss = 0.641953
Epoch 5.89: Loss = 0.470383
Epoch 5.90: Loss = 0.600616
Epoch 5.91: Loss = 0.604187
Epoch 5.92: Loss = 0.729752
Epoch 5.93: Loss = 0.592941
Epoch 5.94: Loss = 0.516418
Epoch 5.95: Loss = 0.479828
Epoch 5.96: Loss = 0.437424
Epoch 5.97: Loss = 0.545059
Epoch 5.98: Loss = 0.616776
Epoch 5.99: Loss = 0.537689
Epoch 5.100: Loss = 0.516464
Epoch 5.101: Loss = 0.495193
Epoch 5.102: Loss = 0.630447
Epoch 5.103: Loss = 0.42778
Epoch 5.104: Loss = 0.543411
Epoch 5.105: Loss = 0.538086
Epoch 5.106: Loss = 0.569
Epoch 5.107: Loss = 0.56871
Epoch 5.108: Loss = 0.497025
Epoch 5.109: Loss = 0.582626
Epoch 5.110: Loss = 0.500992
Epoch 5.111: Loss = 0.579483
Epoch 5.112: Loss = 0.652893
Epoch 5.113: Loss = 0.411041
Epoch 5.114: Loss = 0.506561
Epoch 5.115: Loss = 0.476089
Epoch 5.116: Loss = 0.605606
Epoch 5.117: Loss = 0.660538
Epoch 5.118: Loss = 0.628906
Epoch 5.119: Loss = 0.570404
Epoch 5.120: Loss = 0.61319
TRAIN LOSS = 0.541641
TRAIN ACC = 85.7697 % (51464/60000)
Loss = 0.564163
Loss = 0.778748
Loss = 0.765167
Loss = 0.859039
Loss = 0.820648
Loss = 0.556702
Loss = 0.583069
Loss = 0.827713
Loss = 0.746597
Loss = 0.653214
Loss = 0.229034
Loss = 0.480255
Loss = 0.369507
Loss = 0.399734
Loss = 0.248642
Loss = 0.334015
Loss = 0.272934
Loss = 0.136948
Loss = 0.253433
Loss = 0.65654
TEST LOSS = 0.526805
TEST ACC = 514.639 % (8659/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.611435
Epoch 6.2: Loss = 0.575577
Epoch 6.3: Loss = 0.632004
Epoch 6.4: Loss = 0.584427
Epoch 6.5: Loss = 0.510376
Epoch 6.6: Loss = 0.657043
Epoch 6.7: Loss = 0.504303
Epoch 6.8: Loss = 0.59082
Epoch 6.9: Loss = 0.565842
Epoch 6.10: Loss = 0.571564
Epoch 6.11: Loss = 0.593964
Epoch 6.12: Loss = 0.635086
Epoch 6.13: Loss = 0.680893
Epoch 6.14: Loss = 0.524979
Epoch 6.15: Loss = 0.580551
Epoch 6.16: Loss = 0.534515
Epoch 6.17: Loss = 0.398727
Epoch 6.18: Loss = 0.450439
Epoch 6.19: Loss = 0.460709
Epoch 6.20: Loss = 0.671082
Epoch 6.21: Loss = 0.587845
Epoch 6.22: Loss = 0.546951
Epoch 6.23: Loss = 0.490829
Epoch 6.24: Loss = 0.475983
Epoch 6.25: Loss = 0.580963
Epoch 6.26: Loss = 0.535263
Epoch 6.27: Loss = 0.58963
Epoch 6.28: Loss = 0.598602
Epoch 6.29: Loss = 0.537354
Epoch 6.30: Loss = 0.509995
Epoch 6.31: Loss = 0.495926
Epoch 6.32: Loss = 0.492447
Epoch 6.33: Loss = 0.668701
Epoch 6.34: Loss = 0.500214
Epoch 6.35: Loss = 0.536652
Epoch 6.36: Loss = 0.494812
Epoch 6.37: Loss = 0.53833
Epoch 6.38: Loss = 0.475327
Epoch 6.39: Loss = 0.671829
Epoch 6.40: Loss = 0.494278
Epoch 6.41: Loss = 0.516113
Epoch 6.42: Loss = 0.660416
Epoch 6.43: Loss = 0.621277
Epoch 6.44: Loss = 0.504547
Epoch 6.45: Loss = 0.555206
Epoch 6.46: Loss = 0.674454
Epoch 6.47: Loss = 0.408234
Epoch 6.48: Loss = 0.630142
Epoch 6.49: Loss = 0.585785
Epoch 6.50: Loss = 0.710403
Epoch 6.51: Loss = 0.540726
Epoch 6.52: Loss = 0.643448
Epoch 6.53: Loss = 0.54924
Epoch 6.54: Loss = 0.663147
Epoch 6.55: Loss = 0.694778
Epoch 6.56: Loss = 0.577988
Epoch 6.57: Loss = 0.516083
Epoch 6.58: Loss = 0.68605
Epoch 6.59: Loss = 0.532852
Epoch 6.60: Loss = 0.58255
Epoch 6.61: Loss = 0.578156
Epoch 6.62: Loss = 0.473099
Epoch 6.63: Loss = 0.498932
Epoch 6.64: Loss = 0.572052
Epoch 6.65: Loss = 0.608719
Epoch 6.66: Loss = 0.499985
Epoch 6.67: Loss = 0.655121
Epoch 6.68: Loss = 0.474854
Epoch 6.69: Loss = 0.593765
Epoch 6.70: Loss = 0.530319
Epoch 6.71: Loss = 0.48024
Epoch 6.72: Loss = 0.629074
Epoch 6.73: Loss = 0.412231
Epoch 6.74: Loss = 0.638672
Epoch 6.75: Loss = 0.550308
Epoch 6.76: Loss = 0.63707
Epoch 6.77: Loss = 0.582642
Epoch 6.78: Loss = 0.622406
Epoch 6.79: Loss = 0.567459
Epoch 6.80: Loss = 0.637146
Epoch 6.81: Loss = 0.665115
Epoch 6.82: Loss = 0.472748
Epoch 6.83: Loss = 0.62468
Epoch 6.84: Loss = 0.671829
Epoch 6.85: Loss = 0.595016
Epoch 6.86: Loss = 0.605927
Epoch 6.87: Loss = 0.480377
Epoch 6.88: Loss = 0.462479
Epoch 6.89: Loss = 0.552185
Epoch 6.90: Loss = 0.505402
Epoch 6.91: Loss = 0.567963
Epoch 6.92: Loss = 0.562973
Epoch 6.93: Loss = 0.563446
Epoch 6.94: Loss = 0.493729
Epoch 6.95: Loss = 0.548721
Epoch 6.96: Loss = 0.605331
Epoch 6.97: Loss = 0.600098
Epoch 6.98: Loss = 0.358612
Epoch 6.99: Loss = 0.429108
Epoch 6.100: Loss = 0.511871
Epoch 6.101: Loss = 0.634018
Epoch 6.102: Loss = 0.5802
Epoch 6.103: Loss = 0.589828
Epoch 6.104: Loss = 0.462311
Epoch 6.105: Loss = 0.757477
Epoch 6.106: Loss = 0.639145
Epoch 6.107: Loss = 0.777802
Epoch 6.108: Loss = 0.536209
Epoch 6.109: Loss = 0.519989
Epoch 6.110: Loss = 0.496323
Epoch 6.111: Loss = 0.602615
Epoch 6.112: Loss = 0.49382
Epoch 6.113: Loss = 0.43161
Epoch 6.114: Loss = 0.659714
Epoch 6.115: Loss = 0.521149
Epoch 6.116: Loss = 0.593567
Epoch 6.117: Loss = 0.613571
Epoch 6.118: Loss = 0.476929
Epoch 6.119: Loss = 0.568741
Epoch 6.120: Loss = 0.629272
TRAIN LOSS = 0.564301
TRAIN ACC = 86.1008 % (51663/60000)
Loss = 0.649231
Loss = 0.766602
Loss = 0.808701
Loss = 0.894562
Loss = 0.904694
Loss = 0.629395
Loss = 0.631165
Loss = 0.945862
Loss = 0.857346
Loss = 0.653641
Loss = 0.287216
Loss = 0.51593
Loss = 0.35582
Loss = 0.536804
Loss = 0.348038
Loss = 0.355606
Loss = 0.270508
Loss = 0.0955505
Loss = 0.345871
Loss = 0.735687
TEST LOSS = 0.579411
TEST ACC = 516.629 % (8631/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.63356
Epoch 7.2: Loss = 0.660126
Epoch 7.3: Loss = 0.657074
Epoch 7.4: Loss = 0.741653
Epoch 7.5: Loss = 0.649261
Epoch 7.6: Loss = 0.595886
Epoch 7.7: Loss = 0.527771
Epoch 7.8: Loss = 0.484818
Epoch 7.9: Loss = 0.550034
Epoch 7.10: Loss = 0.573776
Epoch 7.11: Loss = 0.66716
Epoch 7.12: Loss = 0.611862
Epoch 7.13: Loss = 0.713882
Epoch 7.14: Loss = 0.579071
Epoch 7.15: Loss = 0.640396
Epoch 7.16: Loss = 0.674835
Epoch 7.17: Loss = 0.544083
Epoch 7.18: Loss = 0.613739
Epoch 7.19: Loss = 0.435165
Epoch 7.20: Loss = 0.513016
Epoch 7.21: Loss = 0.535858
Epoch 7.22: Loss = 0.661652
Epoch 7.23: Loss = 0.569931
Epoch 7.24: Loss = 0.58342
Epoch 7.25: Loss = 0.527176
Epoch 7.26: Loss = 0.601089
Epoch 7.27: Loss = 0.641388
Epoch 7.28: Loss = 0.567734
Epoch 7.29: Loss = 0.637497
Epoch 7.30: Loss = 0.60495
Epoch 7.31: Loss = 0.726547
Epoch 7.32: Loss = 0.49118
Epoch 7.33: Loss = 0.650711
Epoch 7.34: Loss = 0.5298
Epoch 7.35: Loss = 0.611328
Epoch 7.36: Loss = 0.629593
Epoch 7.37: Loss = 0.660599
Epoch 7.38: Loss = 0.705017
Epoch 7.39: Loss = 0.632172
Epoch 7.40: Loss = 0.602371
Epoch 7.41: Loss = 0.686569
Epoch 7.42: Loss = 0.467194
Epoch 7.43: Loss = 0.57962
Epoch 7.44: Loss = 0.491501
Epoch 7.45: Loss = 0.68071
Epoch 7.46: Loss = 0.621811
Epoch 7.47: Loss = 0.748077
Epoch 7.48: Loss = 0.620682
Epoch 7.49: Loss = 0.499359
Epoch 7.50: Loss = 0.58815
Epoch 7.51: Loss = 0.6362
Epoch 7.52: Loss = 0.610245
Epoch 7.53: Loss = 0.539474
Epoch 7.54: Loss = 0.832367
Epoch 7.55: Loss = 0.575668
Epoch 7.56: Loss = 0.538055
Epoch 7.57: Loss = 0.570007
Epoch 7.58: Loss = 0.476822
Epoch 7.59: Loss = 0.505417
Epoch 7.60: Loss = 0.648544
Epoch 7.61: Loss = 0.581238
Epoch 7.62: Loss = 0.56575
Epoch 7.63: Loss = 0.747284
Epoch 7.64: Loss = 0.601013
Epoch 7.65: Loss = 0.586014
Epoch 7.66: Loss = 0.484116
Epoch 7.67: Loss = 0.622559
Epoch 7.68: Loss = 0.526764
Epoch 7.69: Loss = 0.625381
Epoch 7.70: Loss = 0.617004
Epoch 7.71: Loss = 0.558792
Epoch 7.72: Loss = 0.613113
Epoch 7.73: Loss = 0.570007
Epoch 7.74: Loss = 0.567307
Epoch 7.75: Loss = 0.617889
Epoch 7.76: Loss = 0.565521
Epoch 7.77: Loss = 0.591888
Epoch 7.78: Loss = 0.617477
Epoch 7.79: Loss = 0.660767
Epoch 7.80: Loss = 0.730103
Epoch 7.81: Loss = 0.608292
Epoch 7.82: Loss = 0.568375
Epoch 7.83: Loss = 0.709351
Epoch 7.84: Loss = 0.466217
Epoch 7.85: Loss = 0.705658
Epoch 7.86: Loss = 0.590149
Epoch 7.87: Loss = 0.651062
Epoch 7.88: Loss = 0.49736
Epoch 7.89: Loss = 0.55629
Epoch 7.90: Loss = 0.646622
Epoch 7.91: Loss = 0.527313
Epoch 7.92: Loss = 0.659424
Epoch 7.93: Loss = 0.693863
Epoch 7.94: Loss = 0.561478
Epoch 7.95: Loss = 0.745667
Epoch 7.96: Loss = 0.64801
Epoch 7.97: Loss = 0.562653
Epoch 7.98: Loss = 0.581085
Epoch 7.99: Loss = 0.695953
Epoch 7.100: Loss = 0.590836
Epoch 7.101: Loss = 0.653656
Epoch 7.102: Loss = 0.578125
Epoch 7.103: Loss = 0.582001
Epoch 7.104: Loss = 0.573547
Epoch 7.105: Loss = 0.574493
Epoch 7.106: Loss = 0.818558
Epoch 7.107: Loss = 0.578064
Epoch 7.108: Loss = 0.664337
Epoch 7.109: Loss = 0.692459
Epoch 7.110: Loss = 0.753036
Epoch 7.111: Loss = 0.56543
Epoch 7.112: Loss = 0.653549
Epoch 7.113: Loss = 0.709534
Epoch 7.114: Loss = 0.778702
Epoch 7.115: Loss = 0.657425
Epoch 7.116: Loss = 0.580063
Epoch 7.117: Loss = 0.616165
Epoch 7.118: Loss = 0.774933
Epoch 7.119: Loss = 0.519241
Epoch 7.120: Loss = 0.686279
TRAIN LOSS = 0.612335
TRAIN ACC = 85.5591 % (51337/60000)
Loss = 0.69136
Loss = 0.78952
Loss = 0.811081
Loss = 0.988052
Loss = 1.05632
Loss = 0.694717
Loss = 0.641785
Loss = 0.979172
Loss = 0.893173
Loss = 0.683609
Loss = 0.220657
Loss = 0.487717
Loss = 0.474319
Loss = 0.538177
Loss = 0.39447
Loss = 0.455307
Loss = 0.33606
Loss = 0.139618
Loss = 0.261627
Loss = 0.775406
TEST LOSS = 0.615607
TEST ACC = 513.37 % (8611/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.654129
Epoch 8.2: Loss = 0.677155
Epoch 8.3: Loss = 0.737305
Epoch 8.4: Loss = 0.720612
Epoch 8.5: Loss = 0.55658
Epoch 8.6: Loss = 0.634262
Epoch 8.7: Loss = 0.62059
Epoch 8.8: Loss = 0.674484
Epoch 8.9: Loss = 0.807892
Epoch 8.10: Loss = 0.588394
Epoch 8.11: Loss = 0.497696
Epoch 8.12: Loss = 0.673172
Epoch 8.13: Loss = 0.594177
Epoch 8.14: Loss = 0.578796
Epoch 8.15: Loss = 0.592331
Epoch 8.16: Loss = 0.615067
Epoch 8.17: Loss = 0.628189
Epoch 8.18: Loss = 0.720337
Epoch 8.19: Loss = 0.587494
Epoch 8.20: Loss = 0.553497
Epoch 8.21: Loss = 0.659439
Epoch 8.22: Loss = 0.492172
Epoch 8.23: Loss = 0.80246
Epoch 8.24: Loss = 0.593414
Epoch 8.25: Loss = 0.670731
Epoch 8.26: Loss = 0.582092
Epoch 8.27: Loss = 0.680511
Epoch 8.28: Loss = 0.683182
Epoch 8.29: Loss = 0.743896
Epoch 8.30: Loss = 0.549561
Epoch 8.31: Loss = 0.665283
Epoch 8.32: Loss = 0.628937
Epoch 8.33: Loss = 0.554169
Epoch 8.34: Loss = 0.498413
Epoch 8.35: Loss = 0.598419
Epoch 8.36: Loss = 0.701935
Epoch 8.37: Loss = 0.74057
Epoch 8.38: Loss = 0.627533
Epoch 8.39: Loss = 0.754013
Epoch 8.40: Loss = 0.715424
Epoch 8.41: Loss = 0.667877
Epoch 8.42: Loss = 0.858978
Epoch 8.43: Loss = 0.729324
Epoch 8.44: Loss = 0.616562
Epoch 8.45: Loss = 0.679611
Epoch 8.46: Loss = 0.82811
Epoch 8.47: Loss = 0.664948
Epoch 8.48: Loss = 0.655426
Epoch 8.49: Loss = 0.676773
Epoch 8.50: Loss = 0.852386
Epoch 8.51: Loss = 0.538757
Epoch 8.52: Loss = 0.714066
Epoch 8.53: Loss = 0.830017
Epoch 8.54: Loss = 0.474411
Epoch 8.55: Loss = 0.740631
Epoch 8.56: Loss = 0.697647
Epoch 8.57: Loss = 0.667618
Epoch 8.58: Loss = 0.619446
Epoch 8.59: Loss = 0.663589
Epoch 8.60: Loss = 0.677734
Epoch 8.61: Loss = 0.565781
Epoch 8.62: Loss = 0.705521
Epoch 8.63: Loss = 0.529831
Epoch 8.64: Loss = 0.638367
Epoch 8.65: Loss = 0.55751
Epoch 8.66: Loss = 0.640625
Epoch 8.67: Loss = 0.751236
Epoch 8.68: Loss = 0.556015
Epoch 8.69: Loss = 0.750412
Epoch 8.70: Loss = 0.757294
Epoch 8.71: Loss = 0.600708
Epoch 8.72: Loss = 0.724243
Epoch 8.73: Loss = 0.442261
Epoch 8.74: Loss = 0.619034
Epoch 8.75: Loss = 0.553024
Epoch 8.76: Loss = 0.504242
Epoch 8.77: Loss = 0.705704
Epoch 8.78: Loss = 0.537064
Epoch 8.79: Loss = 0.652908
Epoch 8.80: Loss = 0.652435
Epoch 8.81: Loss = 0.678146
Epoch 8.82: Loss = 0.559799
Epoch 8.83: Loss = 0.558655
Epoch 8.84: Loss = 0.569092
Epoch 8.85: Loss = 0.709534
Epoch 8.86: Loss = 0.637772
Epoch 8.87: Loss = 0.727936
Epoch 8.88: Loss = 0.510269
Epoch 8.89: Loss = 0.715927
Epoch 8.90: Loss = 0.745697
Epoch 8.91: Loss = 0.720352
Epoch 8.92: Loss = 0.674026
Epoch 8.93: Loss = 0.598145
Epoch 8.94: Loss = 0.735703
Epoch 8.95: Loss = 0.547668
Epoch 8.96: Loss = 0.723083
Epoch 8.97: Loss = 0.693039
Epoch 8.98: Loss = 0.550674
Epoch 8.99: Loss = 0.613037
Epoch 8.100: Loss = 0.742691
Epoch 8.101: Loss = 0.713898
Epoch 8.102: Loss = 0.698242
Epoch 8.103: Loss = 0.49025
Epoch 8.104: Loss = 0.580826
Epoch 8.105: Loss = 0.786285
Epoch 8.106: Loss = 0.704453
Epoch 8.107: Loss = 0.639648
Epoch 8.108: Loss = 0.6492
Epoch 8.109: Loss = 0.700577
Epoch 8.110: Loss = 0.812225
Epoch 8.111: Loss = 0.885864
Epoch 8.112: Loss = 0.56575
Epoch 8.113: Loss = 0.461914
Epoch 8.114: Loss = 0.611221
Epoch 8.115: Loss = 0.728165
Epoch 8.116: Loss = 0.804993
Epoch 8.117: Loss = 0.639847
Epoch 8.118: Loss = 0.670609
Epoch 8.119: Loss = 0.574417
Epoch 8.120: Loss = 0.522522
TRAIN LOSS = 0.651718
TRAIN ACC = 85.5179 % (51313/60000)
Loss = 0.687302
Loss = 0.816193
Loss = 0.921173
Loss = 0.928055
Loss = 1.09174
Loss = 0.712723
Loss = 0.569626
Loss = 1.01518
Loss = 0.884094
Loss = 0.75267
Loss = 0.223297
Loss = 0.507538
Loss = 0.563812
Loss = 0.561066
Loss = 0.402359
Loss = 0.435013
Loss = 0.342987
Loss = 0.122375
Loss = 0.278503
Loss = 0.879745
TEST LOSS = 0.634772
TEST ACC = 513.129 % (8614/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.7771
Epoch 9.2: Loss = 0.603531
Epoch 9.3: Loss = 0.750992
Epoch 9.4: Loss = 0.690018
Epoch 9.5: Loss = 0.555817
Epoch 9.6: Loss = 0.602478
Epoch 9.7: Loss = 0.611801
Epoch 9.8: Loss = 0.57666
Epoch 9.9: Loss = 0.679367
Epoch 9.10: Loss = 0.841995
Epoch 9.11: Loss = 0.666626
Epoch 9.12: Loss = 0.752548
Epoch 9.13: Loss = 0.671463
Epoch 9.14: Loss = 0.735382
Epoch 9.15: Loss = 0.518188
Epoch 9.16: Loss = 0.582565
Epoch 9.17: Loss = 0.631775
Epoch 9.18: Loss = 0.771454
Epoch 9.19: Loss = 0.691986
Epoch 9.20: Loss = 0.764679
Epoch 9.21: Loss = 0.665176
Epoch 9.22: Loss = 0.757141
Epoch 9.23: Loss = 0.572113
Epoch 9.24: Loss = 0.665573
Epoch 9.25: Loss = 0.597473
Epoch 9.26: Loss = 0.682602
Epoch 9.27: Loss = 0.905594
Epoch 9.28: Loss = 0.58165
Epoch 9.29: Loss = 0.682404
Epoch 9.30: Loss = 0.861786
Epoch 9.31: Loss = 0.589172
Epoch 9.32: Loss = 0.558044
Epoch 9.33: Loss = 0.746582
Epoch 9.34: Loss = 0.710236
Epoch 9.35: Loss = 0.553696
Epoch 9.36: Loss = 0.572388
Epoch 9.37: Loss = 0.741272
Epoch 9.38: Loss = 0.635254
Epoch 9.39: Loss = 0.72966
Epoch 9.40: Loss = 0.68074
Epoch 9.41: Loss = 0.72052
Epoch 9.42: Loss = 0.766357
Epoch 9.43: Loss = 0.685516
Epoch 9.44: Loss = 0.590103
Epoch 9.45: Loss = 0.607758
Epoch 9.46: Loss = 0.697006
Epoch 9.47: Loss = 0.706879
Epoch 9.48: Loss = 0.814621
Epoch 9.49: Loss = 0.655991
Epoch 9.50: Loss = 0.706116
Epoch 9.51: Loss = 0.607208
Epoch 9.52: Loss = 0.502701
Epoch 9.53: Loss = 0.560623
Epoch 9.54: Loss = 0.808029
Epoch 9.55: Loss = 0.602737
Epoch 9.56: Loss = 0.528671
Epoch 9.57: Loss = 0.711807
Epoch 9.58: Loss = 0.574493
Epoch 9.59: Loss = 0.694061
Epoch 9.60: Loss = 0.644867
Epoch 9.61: Loss = 0.688431
Epoch 9.62: Loss = 0.662827
Epoch 9.63: Loss = 0.938339
Epoch 9.64: Loss = 0.874237
Epoch 9.65: Loss = 0.855179
Epoch 9.66: Loss = 0.778564
Epoch 9.67: Loss = 0.676224
Epoch 9.68: Loss = 0.741455
Epoch 9.69: Loss = 0.707413
Epoch 9.70: Loss = 0.78891
Epoch 9.71: Loss = 0.772247
Epoch 9.72: Loss = 0.666229
Epoch 9.73: Loss = 0.846924
Epoch 9.74: Loss = 0.654709
Epoch 9.75: Loss = 0.741608
Epoch 9.76: Loss = 0.664337
Epoch 9.77: Loss = 0.647537
Epoch 9.78: Loss = 0.659958
Epoch 9.79: Loss = 0.659485
Epoch 9.80: Loss = 0.623505
Epoch 9.81: Loss = 0.861664
Epoch 9.82: Loss = 0.629105
Epoch 9.83: Loss = 0.695419
Epoch 9.84: Loss = 0.612259
Epoch 9.85: Loss = 0.788132
Epoch 9.86: Loss = 0.651535
Epoch 9.87: Loss = 0.762344
Epoch 9.88: Loss = 0.741486
Epoch 9.89: Loss = 0.693069
Epoch 9.90: Loss = 0.840561
Epoch 9.91: Loss = 0.547256
Epoch 9.92: Loss = 0.779892
Epoch 9.93: Loss = 0.726563
Epoch 9.94: Loss = 0.838348
Epoch 9.95: Loss = 0.730103
Epoch 9.96: Loss = 1.01096
Epoch 9.97: Loss = 0.698654
Epoch 9.98: Loss = 0.778122
Epoch 9.99: Loss = 0.790466
Epoch 9.100: Loss = 0.732834
Epoch 9.101: Loss = 0.906982
Epoch 9.102: Loss = 0.537506
Epoch 9.103: Loss = 0.576233
Epoch 9.104: Loss = 0.749207
Epoch 9.105: Loss = 0.566101
Epoch 9.106: Loss = 0.717621
Epoch 9.107: Loss = 0.886505
Epoch 9.108: Loss = 0.631882
Epoch 9.109: Loss = 0.668762
Epoch 9.110: Loss = 0.743408
Epoch 9.111: Loss = 0.732193
Epoch 9.112: Loss = 0.564087
Epoch 9.113: Loss = 0.813187
Epoch 9.114: Loss = 0.586639
Epoch 9.115: Loss = 0.637573
Epoch 9.116: Loss = 0.532455
Epoch 9.117: Loss = 0.734299
Epoch 9.118: Loss = 0.717911
Epoch 9.119: Loss = 0.832703
Epoch 9.120: Loss = 0.576691
TRAIN LOSS = 0.695175
TRAIN ACC = 85.524 % (51317/60000)
Loss = 0.72525
Loss = 0.925507
Loss = 1.01587
Loss = 1.03323
Loss = 1.14146
Loss = 0.777679
Loss = 0.617462
Loss = 1.10402
Loss = 0.963699
Loss = 0.796829
Loss = 0.183777
Loss = 0.586823
Loss = 0.51564
Loss = 0.668396
Loss = 0.332169
Loss = 0.372421
Loss = 0.32576
Loss = 0.18161
Loss = 0.304504
Loss = 0.988449
TEST LOSS = 0.678028
TEST ACC = 513.17 % (8588/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.81897
Epoch 10.2: Loss = 0.715683
Epoch 10.3: Loss = 0.695221
Epoch 10.4: Loss = 0.711914
Epoch 10.5: Loss = 0.835083
Epoch 10.6: Loss = 0.711075
Epoch 10.7: Loss = 0.805054
Epoch 10.8: Loss = 0.959061
Epoch 10.9: Loss = 0.68129
Epoch 10.10: Loss = 0.659424
Epoch 10.11: Loss = 0.703339
Epoch 10.12: Loss = 0.849167
Epoch 10.13: Loss = 0.623093
Epoch 10.14: Loss = 0.601974
Epoch 10.15: Loss = 0.816772
Epoch 10.16: Loss = 0.893921
Epoch 10.17: Loss = 0.819534
Epoch 10.18: Loss = 0.809662
Epoch 10.19: Loss = 0.730881
Epoch 10.20: Loss = 0.753067
Epoch 10.21: Loss = 0.861938
Epoch 10.22: Loss = 0.56601
Epoch 10.23: Loss = 0.777206
Epoch 10.24: Loss = 0.694122
Epoch 10.25: Loss = 0.891098
Epoch 10.26: Loss = 0.845734
Epoch 10.27: Loss = 0.838425
Epoch 10.28: Loss = 0.667191
Epoch 10.29: Loss = 0.569794
Epoch 10.30: Loss = 0.796555
Epoch 10.31: Loss = 0.69693
Epoch 10.32: Loss = 0.620117
Epoch 10.33: Loss = 0.577362
Epoch 10.34: Loss = 0.81752
Epoch 10.35: Loss = 0.503616
Epoch 10.36: Loss = 0.770966
Epoch 10.37: Loss = 0.773956
Epoch 10.38: Loss = 0.711029
Epoch 10.39: Loss = 0.743088
Epoch 10.40: Loss = 0.772614
Epoch 10.41: Loss = 0.656982
Epoch 10.42: Loss = 0.72995
Epoch 10.43: Loss = 0.871567
Epoch 10.44: Loss = 0.753983
Epoch 10.45: Loss = 0.785172
Epoch 10.46: Loss = 0.597946
Epoch 10.47: Loss = 1.02614
Epoch 10.48: Loss = 31.7916
Epoch 10.49: Loss = 12.3761
Epoch 10.50: Loss = 6.32695
Epoch 10.51: Loss = 4.13931
Epoch 10.52: Loss = 4.77863
Epoch 10.53: Loss = 3.60426
Epoch 10.54: Loss = 2.45067
Epoch 10.55: Loss = 26.2011
Epoch 10.56: Loss = 13.2575
Epoch 10.57: Loss = 7.77393
Epoch 10.58: Loss = 5.28102
Epoch 10.59: Loss = 5.49947
Epoch 10.60: Loss = 3.83156
Epoch 10.61: Loss = 3.61885
Epoch 10.62: Loss = 3.31834
Epoch 10.63: Loss = 3.37195
Epoch 10.64: Loss = 2.95811
Epoch 10.65: Loss = 3.04036
Epoch 10.66: Loss = 2.10025
Epoch 10.67: Loss = 2.62589
Epoch 10.68: Loss = 1.62962
Epoch 10.69: Loss = 1.63879
Epoch 10.70: Loss = 1.63089
Epoch 10.71: Loss = 1.63225
Epoch 10.72: Loss = 1.44237
Epoch 10.73: Loss = 1.46689
Epoch 10.74: Loss = 1.39444
Epoch 10.75: Loss = 1.5237
Epoch 10.76: Loss = 1.27748
Epoch 10.77: Loss = 1.27029
Epoch 10.78: Loss = 1.48761
Epoch 10.79: Loss = 1.22012
Epoch 10.80: Loss = 1.32426
Epoch 10.81: Loss = 0.806793
Epoch 10.82: Loss = 0.968964
Epoch 10.83: Loss = 0.904694
Epoch 10.84: Loss = 0.897018
Epoch 10.85: Loss = 1.0477
Epoch 10.86: Loss = 1.01096
Epoch 10.87: Loss = 1.30682
Epoch 10.88: Loss = 0.950867
Epoch 10.89: Loss = 1.10626
Epoch 10.90: Loss = 0.889587
Epoch 10.91: Loss = 0.924713
Epoch 10.92: Loss = 0.982727
Epoch 10.93: Loss = 1.08153
Epoch 10.94: Loss = 0.903549
Epoch 10.95: Loss = 0.815552
Epoch 10.96: Loss = 1.08563
Epoch 10.97: Loss = 1.15678
Epoch 10.98: Loss = 0.883041
Epoch 10.99: Loss = 1.05339
Epoch 10.100: Loss = 1.02962
Epoch 10.101: Loss = 1.12973
Epoch 10.102: Loss = 1.04272
Epoch 10.103: Loss = 1.08087
Epoch 10.104: Loss = 0.781677
Epoch 10.105: Loss = 0.850555
Epoch 10.106: Loss = 0.737808
Epoch 10.107: Loss = 1.24005
Epoch 10.108: Loss = 1.04456
Epoch 10.109: Loss = 1.09607
Epoch 10.110: Loss = 0.916733
Epoch 10.111: Loss = 0.749008
Epoch 10.112: Loss = 0.817856
Epoch 10.113: Loss = 0.859238
Epoch 10.114: Loss = 0.91243
Epoch 10.115: Loss = 1.3244
Epoch 10.116: Loss = 0.809616
Epoch 10.117: Loss = 0.82254
Epoch 10.118: Loss = 0.917358
Epoch 10.119: Loss = 0.762833
Epoch 10.120: Loss = 0.939575
TRAIN LOSS = 2.00868
TRAIN ACC = 80.3726 % (48226/60000)
Loss = 0.85199
Loss = 1.04747
Loss = 1.11922
Loss = 1.24982
Loss = 1.48106
Loss = 0.985474
Loss = 0.848846
Loss = 1.37102
Loss = 1.28596
Loss = 0.975845
Loss = 0.282394
Loss = 0.759415
Loss = 0.929398
Loss = 1.1747
Loss = 0.505142
Loss = 0.717117
Loss = 0.836624
Loss = 0.202148
Loss = 0.383591
Loss = 1.28033
TEST LOSS = 0.914378
TEST ACC = 482.259 % (8439/10000)

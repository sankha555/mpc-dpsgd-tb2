Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.32587
Epoch 1.2: Loss = 2.31757
Epoch 1.3: Loss = 2.2476
Epoch 1.4: Loss = 2.23792
Epoch 1.5: Loss = 2.17847
Epoch 1.6: Loss = 2.14783
Epoch 1.7: Loss = 2.11066
Epoch 1.8: Loss = 2.04982
Epoch 1.9: Loss = 2.03749
Epoch 1.10: Loss = 1.9912
Epoch 1.11: Loss = 1.94809
Epoch 1.12: Loss = 1.92148
Epoch 1.13: Loss = 1.86349
Epoch 1.14: Loss = 1.81886
Epoch 1.15: Loss = 1.80362
Epoch 1.16: Loss = 1.74738
Epoch 1.17: Loss = 1.74467
Epoch 1.18: Loss = 1.67888
Epoch 1.19: Loss = 1.69638
Epoch 1.20: Loss = 1.63885
Epoch 1.21: Loss = 1.62552
Epoch 1.22: Loss = 1.54607
Epoch 1.23: Loss = 1.53947
Epoch 1.24: Loss = 1.51773
Epoch 1.25: Loss = 1.43823
Epoch 1.26: Loss = 1.46704
Epoch 1.27: Loss = 1.41707
Epoch 1.28: Loss = 1.4012
Epoch 1.29: Loss = 1.34282
Epoch 1.30: Loss = 1.39616
Epoch 1.31: Loss = 1.3503
Epoch 1.32: Loss = 1.32196
Epoch 1.33: Loss = 1.31
Epoch 1.34: Loss = 1.29436
Epoch 1.35: Loss = 1.26369
Epoch 1.36: Loss = 1.21143
Epoch 1.37: Loss = 1.20937
Epoch 1.38: Loss = 1.26425
Epoch 1.39: Loss = 1.20393
Epoch 1.40: Loss = 1.16852
Epoch 1.41: Loss = 1.1815
Epoch 1.42: Loss = 1.1319
Epoch 1.43: Loss = 1.12587
Epoch 1.44: Loss = 1.11394
Epoch 1.45: Loss = 1.06389
Epoch 1.46: Loss = 1.04379
Epoch 1.47: Loss = 1.04364
Epoch 1.48: Loss = 1.0603
Epoch 1.49: Loss = 1.0054
Epoch 1.50: Loss = 1.02968
Epoch 1.51: Loss = 0.983093
Epoch 1.52: Loss = 0.951675
Epoch 1.53: Loss = 0.977341
Epoch 1.54: Loss = 0.944916
Epoch 1.55: Loss = 0.941681
Epoch 1.56: Loss = 0.917358
Epoch 1.57: Loss = 0.912979
Epoch 1.58: Loss = 0.920334
Epoch 1.59: Loss = 0.846146
Epoch 1.60: Loss = 0.88533
Epoch 1.61: Loss = 0.904968
Epoch 1.62: Loss = 0.881348
Epoch 1.63: Loss = 0.892502
Epoch 1.64: Loss = 0.810532
Epoch 1.65: Loss = 0.819031
Epoch 1.66: Loss = 0.869049
Epoch 1.67: Loss = 0.829712
Epoch 1.68: Loss = 0.894073
Epoch 1.69: Loss = 0.829483
Epoch 1.70: Loss = 0.757294
Epoch 1.71: Loss = 0.791611
Epoch 1.72: Loss = 0.711182
Epoch 1.73: Loss = 0.728012
Epoch 1.74: Loss = 0.780121
Epoch 1.75: Loss = 0.769806
Epoch 1.76: Loss = 0.766083
Epoch 1.77: Loss = 0.743347
Epoch 1.78: Loss = 0.713745
Epoch 1.79: Loss = 0.780807
Epoch 1.80: Loss = 0.772934
Epoch 1.81: Loss = 0.752563
Epoch 1.82: Loss = 0.731705
Epoch 1.83: Loss = 0.703934
Epoch 1.84: Loss = 0.709702
Epoch 1.85: Loss = 0.650024
Epoch 1.86: Loss = 0.728439
Epoch 1.87: Loss = 0.743881
Epoch 1.88: Loss = 0.710098
Epoch 1.89: Loss = 0.7043
Epoch 1.90: Loss = 0.652206
Epoch 1.91: Loss = 0.652802
Epoch 1.92: Loss = 0.704895
Epoch 1.93: Loss = 0.665726
Epoch 1.94: Loss = 0.754852
Epoch 1.95: Loss = 0.733429
Epoch 1.96: Loss = 0.654922
Epoch 1.97: Loss = 0.657166
Epoch 1.98: Loss = 0.653183
Epoch 1.99: Loss = 0.659271
Epoch 1.100: Loss = 0.643738
Epoch 1.101: Loss = 0.719467
Epoch 1.102: Loss = 0.678665
Epoch 1.103: Loss = 0.672348
Epoch 1.104: Loss = 0.638626
Epoch 1.105: Loss = 0.586105
Epoch 1.106: Loss = 0.634094
Epoch 1.107: Loss = 0.629364
Epoch 1.108: Loss = 0.693802
Epoch 1.109: Loss = 0.573715
Epoch 1.110: Loss = 0.666473
Epoch 1.111: Loss = 0.666718
Epoch 1.112: Loss = 0.635544
Epoch 1.113: Loss = 0.6082
Epoch 1.114: Loss = 0.644913
Epoch 1.115: Loss = 0.618607
Epoch 1.116: Loss = 0.647797
Epoch 1.117: Loss = 0.581955
Epoch 1.118: Loss = 0.586975
Epoch 1.119: Loss = 0.587341
Epoch 1.120: Loss = 0.578812
TRAIN LOSS = 1.07866
TRAIN ACC = 69.3512 % (41613/60000)
Loss = 0.586868
Loss = 0.61554
Loss = 0.716019
Loss = 0.688705
Loss = 0.723343
Loss = 0.618027
Loss = 0.59787
Loss = 0.738083
Loss = 0.691254
Loss = 0.642212
Loss = 0.355789
Loss = 0.477997
Loss = 0.373016
Loss = 0.558243
Loss = 0.472
Loss = 0.493683
Loss = 0.427689
Loss = 0.221558
Loss = 0.418213
Loss = 0.683868
TEST LOSS = 0.554999
TEST ACC = 416.129 % (8405/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.565796
Epoch 2.2: Loss = 0.563507
Epoch 2.3: Loss = 0.612
Epoch 2.4: Loss = 0.551773
Epoch 2.5: Loss = 0.619797
Epoch 2.6: Loss = 0.59436
Epoch 2.7: Loss = 0.622467
Epoch 2.8: Loss = 0.564255
Epoch 2.9: Loss = 0.543839
Epoch 2.10: Loss = 0.596634
Epoch 2.11: Loss = 0.593735
Epoch 2.12: Loss = 0.60675
Epoch 2.13: Loss = 0.61322
Epoch 2.14: Loss = 0.535614
Epoch 2.15: Loss = 0.577255
Epoch 2.16: Loss = 0.623337
Epoch 2.17: Loss = 0.601837
Epoch 2.18: Loss = 0.630615
Epoch 2.19: Loss = 0.623764
Epoch 2.20: Loss = 0.532715
Epoch 2.21: Loss = 0.55304
Epoch 2.22: Loss = 0.557083
Epoch 2.23: Loss = 0.542847
Epoch 2.24: Loss = 0.534454
Epoch 2.25: Loss = 0.611115
Epoch 2.26: Loss = 0.605637
Epoch 2.27: Loss = 0.570618
Epoch 2.28: Loss = 0.484207
Epoch 2.29: Loss = 0.532425
Epoch 2.30: Loss = 0.532425
Epoch 2.31: Loss = 0.582733
Epoch 2.32: Loss = 0.559586
Epoch 2.33: Loss = 0.511734
Epoch 2.34: Loss = 0.627731
Epoch 2.35: Loss = 0.564468
Epoch 2.36: Loss = 0.576187
Epoch 2.37: Loss = 0.519135
Epoch 2.38: Loss = 0.56926
Epoch 2.39: Loss = 0.540665
Epoch 2.40: Loss = 0.544357
Epoch 2.41: Loss = 0.527161
Epoch 2.42: Loss = 0.52565
Epoch 2.43: Loss = 0.533768
Epoch 2.44: Loss = 0.515259
Epoch 2.45: Loss = 0.599396
Epoch 2.46: Loss = 0.463043
Epoch 2.47: Loss = 0.504623
Epoch 2.48: Loss = 0.58139
Epoch 2.49: Loss = 0.536087
Epoch 2.50: Loss = 0.564606
Epoch 2.51: Loss = 0.566132
Epoch 2.52: Loss = 0.549103
Epoch 2.53: Loss = 0.513794
Epoch 2.54: Loss = 0.498199
Epoch 2.55: Loss = 0.515305
Epoch 2.56: Loss = 0.521484
Epoch 2.57: Loss = 0.491592
Epoch 2.58: Loss = 0.500259
Epoch 2.59: Loss = 0.463638
Epoch 2.60: Loss = 0.533401
Epoch 2.61: Loss = 0.539886
Epoch 2.62: Loss = 0.597397
Epoch 2.63: Loss = 0.595047
Epoch 2.64: Loss = 0.540115
Epoch 2.65: Loss = 0.475876
Epoch 2.66: Loss = 0.560242
Epoch 2.67: Loss = 0.473709
Epoch 2.68: Loss = 0.481476
Epoch 2.69: Loss = 0.509277
Epoch 2.70: Loss = 0.506348
Epoch 2.71: Loss = 0.594269
Epoch 2.72: Loss = 0.480133
Epoch 2.73: Loss = 0.604523
Epoch 2.74: Loss = 0.505081
Epoch 2.75: Loss = 0.496506
Epoch 2.76: Loss = 0.50148
Epoch 2.77: Loss = 0.493301
Epoch 2.78: Loss = 0.5439
Epoch 2.79: Loss = 0.535492
Epoch 2.80: Loss = 0.526474
Epoch 2.81: Loss = 0.549026
Epoch 2.82: Loss = 0.436234
Epoch 2.83: Loss = 0.628799
Epoch 2.84: Loss = 0.554245
Epoch 2.85: Loss = 0.539917
Epoch 2.86: Loss = 0.444794
Epoch 2.87: Loss = 0.408478
Epoch 2.88: Loss = 0.542496
Epoch 2.89: Loss = 0.494614
Epoch 2.90: Loss = 0.612442
Epoch 2.91: Loss = 0.529755
Epoch 2.92: Loss = 0.483444
Epoch 2.93: Loss = 0.441803
Epoch 2.94: Loss = 0.500671
Epoch 2.95: Loss = 0.465485
Epoch 2.96: Loss = 0.528503
Epoch 2.97: Loss = 0.567123
Epoch 2.98: Loss = 0.476746
Epoch 2.99: Loss = 0.488861
Epoch 2.100: Loss = 0.437698
Epoch 2.101: Loss = 0.477722
Epoch 2.102: Loss = 0.520889
Epoch 2.103: Loss = 0.468781
Epoch 2.104: Loss = 0.555374
Epoch 2.105: Loss = 0.489746
Epoch 2.106: Loss = 0.502747
Epoch 2.107: Loss = 0.536346
Epoch 2.108: Loss = 0.551559
Epoch 2.109: Loss = 0.471909
Epoch 2.110: Loss = 0.476578
Epoch 2.111: Loss = 0.578232
Epoch 2.112: Loss = 0.439575
Epoch 2.113: Loss = 0.471039
Epoch 2.114: Loss = 0.465103
Epoch 2.115: Loss = 0.515869
Epoch 2.116: Loss = 0.463043
Epoch 2.117: Loss = 0.523499
Epoch 2.118: Loss = 0.481628
Epoch 2.119: Loss = 0.431473
Epoch 2.120: Loss = 0.434052
TRAIN LOSS = 0.532883
TRAIN ACC = 83.6884 % (50215/60000)
Loss = 0.456039
Loss = 0.527145
Loss = 0.625687
Loss = 0.602585
Loss = 0.643021
Loss = 0.492874
Loss = 0.472015
Loss = 0.68071
Loss = 0.60376
Loss = 0.570633
Loss = 0.268265
Loss = 0.363571
Loss = 0.334198
Loss = 0.468857
Loss = 0.308075
Loss = 0.412125
Loss = 0.329163
Loss = 0.115173
Loss = 0.334457
Loss = 0.596268
TEST LOSS = 0.460231
TEST ACC = 502.148 % (8619/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.445709
Epoch 3.2: Loss = 0.460007
Epoch 3.3: Loss = 0.560989
Epoch 3.4: Loss = 0.5186
Epoch 3.5: Loss = 0.446793
Epoch 3.6: Loss = 0.475784
Epoch 3.7: Loss = 0.486176
Epoch 3.8: Loss = 0.518295
Epoch 3.9: Loss = 0.439896
Epoch 3.10: Loss = 0.469543
Epoch 3.11: Loss = 0.496078
Epoch 3.12: Loss = 0.437881
Epoch 3.13: Loss = 0.519608
Epoch 3.14: Loss = 0.525101
Epoch 3.15: Loss = 0.493286
Epoch 3.16: Loss = 0.549988
Epoch 3.17: Loss = 0.47345
Epoch 3.18: Loss = 0.537811
Epoch 3.19: Loss = 0.496857
Epoch 3.20: Loss = 0.424759
Epoch 3.21: Loss = 0.475571
Epoch 3.22: Loss = 0.560287
Epoch 3.23: Loss = 0.460632
Epoch 3.24: Loss = 0.554199
Epoch 3.25: Loss = 0.515015
Epoch 3.26: Loss = 0.562851
Epoch 3.27: Loss = 0.508789
Epoch 3.28: Loss = 0.499939
Epoch 3.29: Loss = 0.454651
Epoch 3.30: Loss = 0.498764
Epoch 3.31: Loss = 0.486816
Epoch 3.32: Loss = 0.463043
Epoch 3.33: Loss = 0.43895
Epoch 3.34: Loss = 0.596939
Epoch 3.35: Loss = 0.477356
Epoch 3.36: Loss = 0.464615
Epoch 3.37: Loss = 0.48819
Epoch 3.38: Loss = 0.498138
Epoch 3.39: Loss = 0.505692
Epoch 3.40: Loss = 0.433487
Epoch 3.41: Loss = 0.421494
Epoch 3.42: Loss = 0.568115
Epoch 3.43: Loss = 0.464127
Epoch 3.44: Loss = 0.49939
Epoch 3.45: Loss = 0.412033
Epoch 3.46: Loss = 0.458237
Epoch 3.47: Loss = 0.57048
Epoch 3.48: Loss = 0.556473
Epoch 3.49: Loss = 0.404663
Epoch 3.50: Loss = 0.546524
Epoch 3.51: Loss = 0.438217
Epoch 3.52: Loss = 0.488647
Epoch 3.53: Loss = 0.550201
Epoch 3.54: Loss = 0.462372
Epoch 3.55: Loss = 0.3974
Epoch 3.56: Loss = 0.489334
Epoch 3.57: Loss = 0.494003
Epoch 3.58: Loss = 0.454987
Epoch 3.59: Loss = 0.441727
Epoch 3.60: Loss = 0.462799
Epoch 3.61: Loss = 0.424652
Epoch 3.62: Loss = 0.450821
Epoch 3.63: Loss = 0.412415
Epoch 3.64: Loss = 0.50235
Epoch 3.65: Loss = 0.502975
Epoch 3.66: Loss = 0.517792
Epoch 3.67: Loss = 0.364777
Epoch 3.68: Loss = 0.490997
Epoch 3.69: Loss = 0.504639
Epoch 3.70: Loss = 0.475845
Epoch 3.71: Loss = 0.507477
Epoch 3.72: Loss = 0.510849
Epoch 3.73: Loss = 0.444229
Epoch 3.74: Loss = 0.547272
Epoch 3.75: Loss = 0.51767
Epoch 3.76: Loss = 0.439896
Epoch 3.77: Loss = 0.500671
Epoch 3.78: Loss = 0.490311
Epoch 3.79: Loss = 0.450699
Epoch 3.80: Loss = 0.461227
Epoch 3.81: Loss = 0.543991
Epoch 3.82: Loss = 0.494965
Epoch 3.83: Loss = 0.412689
Epoch 3.84: Loss = 0.456406
Epoch 3.85: Loss = 0.439346
Epoch 3.86: Loss = 0.364105
Epoch 3.87: Loss = 0.488281
Epoch 3.88: Loss = 0.419495
Epoch 3.89: Loss = 0.47908
Epoch 3.90: Loss = 0.513107
Epoch 3.91: Loss = 0.427505
Epoch 3.92: Loss = 0.471085
Epoch 3.93: Loss = 0.45787
Epoch 3.94: Loss = 0.459015
Epoch 3.95: Loss = 0.374893
Epoch 3.96: Loss = 0.556351
Epoch 3.97: Loss = 0.484283
Epoch 3.98: Loss = 0.580078
Epoch 3.99: Loss = 0.636673
Epoch 3.100: Loss = 0.460114
Epoch 3.101: Loss = 0.462692
Epoch 3.102: Loss = 0.440506
Epoch 3.103: Loss = 0.441589
Epoch 3.104: Loss = 0.461304
Epoch 3.105: Loss = 0.479874
Epoch 3.106: Loss = 0.493011
Epoch 3.107: Loss = 0.377121
Epoch 3.108: Loss = 0.542053
Epoch 3.109: Loss = 0.403824
Epoch 3.110: Loss = 0.484344
Epoch 3.111: Loss = 0.483505
Epoch 3.112: Loss = 0.405273
Epoch 3.113: Loss = 0.486725
Epoch 3.114: Loss = 0.455902
Epoch 3.115: Loss = 0.537247
Epoch 3.116: Loss = 0.413467
Epoch 3.117: Loss = 0.454422
Epoch 3.118: Loss = 0.480042
Epoch 3.119: Loss = 0.453018
Epoch 3.120: Loss = 0.469528
TRAIN LOSS = 0.479706
TRAIN ACC = 85.5255 % (51318/60000)
Loss = 0.426178
Loss = 0.508514
Loss = 0.627441
Loss = 0.602081
Loss = 0.646378
Loss = 0.459869
Loss = 0.433029
Loss = 0.693756
Loss = 0.614426
Loss = 0.559998
Loss = 0.211914
Loss = 0.335556
Loss = 0.320511
Loss = 0.438324
Loss = 0.279892
Loss = 0.374969
Loss = 0.282654
Loss = 0.0756683
Loss = 0.262283
Loss = 0.558975
TEST LOSS = 0.435621
TEST ACC = 513.179 % (8714/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.423599
Epoch 4.2: Loss = 0.407272
Epoch 4.3: Loss = 0.470169
Epoch 4.4: Loss = 0.455917
Epoch 4.5: Loss = 0.518555
Epoch 4.6: Loss = 0.425552
Epoch 4.7: Loss = 0.421265
Epoch 4.8: Loss = 0.514038
Epoch 4.9: Loss = 0.520096
Epoch 4.10: Loss = 0.389786
Epoch 4.11: Loss = 0.453537
Epoch 4.12: Loss = 0.42804
Epoch 4.13: Loss = 0.419952
Epoch 4.14: Loss = 0.486221
Epoch 4.15: Loss = 0.400253
Epoch 4.16: Loss = 0.526566
Epoch 4.17: Loss = 0.384476
Epoch 4.18: Loss = 0.472458
Epoch 4.19: Loss = 0.5103
Epoch 4.20: Loss = 0.415054
Epoch 4.21: Loss = 0.520416
Epoch 4.22: Loss = 0.427856
Epoch 4.23: Loss = 0.422318
Epoch 4.24: Loss = 0.514404
Epoch 4.25: Loss = 0.461029
Epoch 4.26: Loss = 0.440262
Epoch 4.27: Loss = 0.436172
Epoch 4.28: Loss = 0.41835
Epoch 4.29: Loss = 0.438385
Epoch 4.30: Loss = 0.459274
Epoch 4.31: Loss = 0.427078
Epoch 4.32: Loss = 0.517288
Epoch 4.33: Loss = 0.46666
Epoch 4.34: Loss = 0.512695
Epoch 4.35: Loss = 0.49292
Epoch 4.36: Loss = 0.474243
Epoch 4.37: Loss = 0.412033
Epoch 4.38: Loss = 0.476425
Epoch 4.39: Loss = 0.539825
Epoch 4.40: Loss = 0.514236
Epoch 4.41: Loss = 0.512665
Epoch 4.42: Loss = 0.42688
Epoch 4.43: Loss = 0.4048
Epoch 4.44: Loss = 0.4254
Epoch 4.45: Loss = 0.476166
Epoch 4.46: Loss = 0.540909
Epoch 4.47: Loss = 0.396194
Epoch 4.48: Loss = 0.44809
Epoch 4.49: Loss = 0.537704
Epoch 4.50: Loss = 0.513351
Epoch 4.51: Loss = 0.554321
Epoch 4.52: Loss = 0.440094
Epoch 4.53: Loss = 0.528137
Epoch 4.54: Loss = 0.356918
Epoch 4.55: Loss = 0.429871
Epoch 4.56: Loss = 0.461578
Epoch 4.57: Loss = 0.491196
Epoch 4.58: Loss = 0.408691
Epoch 4.59: Loss = 0.411072
Epoch 4.60: Loss = 0.51355
Epoch 4.61: Loss = 0.512146
Epoch 4.62: Loss = 0.449677
Epoch 4.63: Loss = 0.459763
Epoch 4.64: Loss = 0.486786
Epoch 4.65: Loss = 0.499084
Epoch 4.66: Loss = 0.425919
Epoch 4.67: Loss = 0.433884
Epoch 4.68: Loss = 0.363892
Epoch 4.69: Loss = 0.50769
Epoch 4.70: Loss = 0.504349
Epoch 4.71: Loss = 0.479874
Epoch 4.72: Loss = 0.518906
Epoch 4.73: Loss = 0.425171
Epoch 4.74: Loss = 0.440186
Epoch 4.75: Loss = 0.634613
Epoch 4.76: Loss = 0.462616
Epoch 4.77: Loss = 0.506729
Epoch 4.78: Loss = 0.46788
Epoch 4.79: Loss = 0.403488
Epoch 4.80: Loss = 0.427185
Epoch 4.81: Loss = 0.563599
Epoch 4.82: Loss = 0.457458
Epoch 4.83: Loss = 0.468887
Epoch 4.84: Loss = 0.480438
Epoch 4.85: Loss = 0.448868
Epoch 4.86: Loss = 0.438873
Epoch 4.87: Loss = 0.494507
Epoch 4.88: Loss = 0.47525
Epoch 4.89: Loss = 0.486816
Epoch 4.90: Loss = 0.452072
Epoch 4.91: Loss = 0.530228
Epoch 4.92: Loss = 0.549011
Epoch 4.93: Loss = 0.416107
Epoch 4.94: Loss = 0.486267
Epoch 4.95: Loss = 0.397018
Epoch 4.96: Loss = 0.473175
Epoch 4.97: Loss = 0.552933
Epoch 4.98: Loss = 0.404358
Epoch 4.99: Loss = 0.466675
Epoch 4.100: Loss = 0.41394
Epoch 4.101: Loss = 0.473297
Epoch 4.102: Loss = 0.485596
Epoch 4.103: Loss = 0.460648
Epoch 4.104: Loss = 0.505905
Epoch 4.105: Loss = 0.471008
Epoch 4.106: Loss = 0.477142
Epoch 4.107: Loss = 0.427368
Epoch 4.108: Loss = 0.420883
Epoch 4.109: Loss = 0.560852
Epoch 4.110: Loss = 0.478241
Epoch 4.111: Loss = 0.427689
Epoch 4.112: Loss = 0.413528
Epoch 4.113: Loss = 0.475296
Epoch 4.114: Loss = 0.394867
Epoch 4.115: Loss = 0.523254
Epoch 4.116: Loss = 0.483292
Epoch 4.117: Loss = 0.495941
Epoch 4.118: Loss = 0.490631
Epoch 4.119: Loss = 0.488739
Epoch 4.120: Loss = 0.495911
TRAIN LOSS = 0.466736
TRAIN ACC = 86.3785 % (51830/60000)
Loss = 0.375122
Loss = 0.495728
Loss = 0.648987
Loss = 0.610184
Loss = 0.66832
Loss = 0.441071
Loss = 0.432831
Loss = 0.749863
Loss = 0.63945
Loss = 0.595169
Loss = 0.213776
Loss = 0.337662
Loss = 0.373474
Loss = 0.460861
Loss = 0.241959
Loss = 0.351517
Loss = 0.276871
Loss = 0.0725555
Loss = 0.269394
Loss = 0.56868
TEST LOSS = 0.441173
TEST ACC = 518.3 % (8721/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.531982
Epoch 5.2: Loss = 0.44664
Epoch 5.3: Loss = 0.416779
Epoch 5.4: Loss = 0.499298
Epoch 5.5: Loss = 0.484711
Epoch 5.6: Loss = 0.462357
Epoch 5.7: Loss = 0.409805
Epoch 5.8: Loss = 0.483139
Epoch 5.9: Loss = 0.409164
Epoch 5.10: Loss = 0.456406
Epoch 5.11: Loss = 0.492584
Epoch 5.12: Loss = 0.409119
Epoch 5.13: Loss = 0.480286
Epoch 5.14: Loss = 0.433487
Epoch 5.15: Loss = 0.450195
Epoch 5.16: Loss = 0.450836
Epoch 5.17: Loss = 0.48497
Epoch 5.18: Loss = 0.496719
Epoch 5.19: Loss = 0.441391
Epoch 5.20: Loss = 0.460403
Epoch 5.21: Loss = 0.433762
Epoch 5.22: Loss = 0.536728
Epoch 5.23: Loss = 0.41861
Epoch 5.24: Loss = 0.478073
Epoch 5.25: Loss = 0.39624
Epoch 5.26: Loss = 0.454681
Epoch 5.27: Loss = 0.457474
Epoch 5.28: Loss = 0.60643
Epoch 5.29: Loss = 0.401352
Epoch 5.30: Loss = 0.440308
Epoch 5.31: Loss = 0.40918
Epoch 5.32: Loss = 0.4617
Epoch 5.33: Loss = 0.434326
Epoch 5.34: Loss = 0.468872
Epoch 5.35: Loss = 0.528885
Epoch 5.36: Loss = 0.453964
Epoch 5.37: Loss = 0.455246
Epoch 5.38: Loss = 0.532227
Epoch 5.39: Loss = 0.563629
Epoch 5.40: Loss = 0.436279
Epoch 5.41: Loss = 0.523499
Epoch 5.42: Loss = 0.512146
Epoch 5.43: Loss = 0.452179
Epoch 5.44: Loss = 0.449585
Epoch 5.45: Loss = 0.513626
Epoch 5.46: Loss = 0.377838
Epoch 5.47: Loss = 0.472015
Epoch 5.48: Loss = 0.459
Epoch 5.49: Loss = 0.547455
Epoch 5.50: Loss = 0.412888
Epoch 5.51: Loss = 0.507187
Epoch 5.52: Loss = 0.373398
Epoch 5.53: Loss = 0.452744
Epoch 5.54: Loss = 0.388138
Epoch 5.55: Loss = 0.428223
Epoch 5.56: Loss = 0.497223
Epoch 5.57: Loss = 0.385544
Epoch 5.58: Loss = 0.580002
Epoch 5.59: Loss = 0.447159
Epoch 5.60: Loss = 0.517853
Epoch 5.61: Loss = 0.477219
Epoch 5.62: Loss = 0.452026
Epoch 5.63: Loss = 0.404373
Epoch 5.64: Loss = 0.431946
Epoch 5.65: Loss = 0.504929
Epoch 5.66: Loss = 0.501862
Epoch 5.67: Loss = 0.421555
Epoch 5.68: Loss = 0.448471
Epoch 5.69: Loss = 0.563385
Epoch 5.70: Loss = 0.582199
Epoch 5.71: Loss = 0.446365
Epoch 5.72: Loss = 0.539001
Epoch 5.73: Loss = 0.555115
Epoch 5.74: Loss = 0.379044
Epoch 5.75: Loss = 0.591034
Epoch 5.76: Loss = 0.508957
Epoch 5.77: Loss = 0.445511
Epoch 5.78: Loss = 0.489319
Epoch 5.79: Loss = 0.48587
Epoch 5.80: Loss = 0.48793
Epoch 5.81: Loss = 0.425461
Epoch 5.82: Loss = 0.375473
Epoch 5.83: Loss = 0.386658
Epoch 5.84: Loss = 0.466476
Epoch 5.85: Loss = 0.51767
Epoch 5.86: Loss = 0.448898
Epoch 5.87: Loss = 0.548065
Epoch 5.88: Loss = 0.45961
Epoch 5.89: Loss = 0.428406
Epoch 5.90: Loss = 0.465897
Epoch 5.91: Loss = 0.460129
Epoch 5.92: Loss = 0.423355
Epoch 5.93: Loss = 0.536285
Epoch 5.94: Loss = 0.526886
Epoch 5.95: Loss = 0.450394
Epoch 5.96: Loss = 0.509186
Epoch 5.97: Loss = 0.432663
Epoch 5.98: Loss = 0.495499
Epoch 5.99: Loss = 0.457901
Epoch 5.100: Loss = 0.500183
Epoch 5.101: Loss = 0.396576
Epoch 5.102: Loss = 0.507294
Epoch 5.103: Loss = 0.440857
Epoch 5.104: Loss = 0.486725
Epoch 5.105: Loss = 0.488129
Epoch 5.106: Loss = 0.471085
Epoch 5.107: Loss = 0.452148
Epoch 5.108: Loss = 0.356186
Epoch 5.109: Loss = 0.460907
Epoch 5.110: Loss = 0.526321
Epoch 5.111: Loss = 0.382095
Epoch 5.112: Loss = 0.405746
Epoch 5.113: Loss = 0.537186
Epoch 5.114: Loss = 0.539032
Epoch 5.115: Loss = 0.551178
Epoch 5.116: Loss = 0.461868
Epoch 5.117: Loss = 0.425415
Epoch 5.118: Loss = 0.555527
Epoch 5.119: Loss = 0.434219
Epoch 5.120: Loss = 0.508743
TRAIN LOSS = 0.46904
TRAIN ACC = 86.615 % (51972/60000)
Loss = 0.418884
Loss = 0.531631
Loss = 0.642151
Loss = 0.609756
Loss = 0.675583
Loss = 0.43634
Loss = 0.407715
Loss = 0.746567
Loss = 0.653793
Loss = 0.586609
Loss = 0.225357
Loss = 0.289398
Loss = 0.397629
Loss = 0.473343
Loss = 0.230606
Loss = 0.323517
Loss = 0.262115
Loss = 0.0605011
Loss = 0.2789
Loss = 0.574448
TEST LOSS = 0.441242
TEST ACC = 519.719 % (8742/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.485229
Epoch 6.2: Loss = 0.553207
Epoch 6.3: Loss = 0.557602
Epoch 6.4: Loss = 0.418549
Epoch 6.5: Loss = 0.477692
Epoch 6.6: Loss = 0.466812
Epoch 6.7: Loss = 0.500122
Epoch 6.8: Loss = 0.626343
Epoch 6.9: Loss = 0.546997
Epoch 6.10: Loss = 0.451218
Epoch 6.11: Loss = 0.4953
Epoch 6.12: Loss = 0.441849
Epoch 6.13: Loss = 0.49675
Epoch 6.14: Loss = 0.501541
Epoch 6.15: Loss = 0.552643
Epoch 6.16: Loss = 0.463272
Epoch 6.17: Loss = 0.507797
Epoch 6.18: Loss = 0.497574
Epoch 6.19: Loss = 0.413712
Epoch 6.20: Loss = 0.41658
Epoch 6.21: Loss = 0.446396
Epoch 6.22: Loss = 0.474487
Epoch 6.23: Loss = 0.441513
Epoch 6.24: Loss = 0.516876
Epoch 6.25: Loss = 0.425278
Epoch 6.26: Loss = 0.472458
Epoch 6.27: Loss = 0.50528
Epoch 6.28: Loss = 0.362732
Epoch 6.29: Loss = 0.407562
Epoch 6.30: Loss = 0.400192
Epoch 6.31: Loss = 0.380447
Epoch 6.32: Loss = 0.498749
Epoch 6.33: Loss = 0.421387
Epoch 6.34: Loss = 0.478226
Epoch 6.35: Loss = 0.464951
Epoch 6.36: Loss = 0.421295
Epoch 6.37: Loss = 0.494675
Epoch 6.38: Loss = 0.26416
Epoch 6.39: Loss = 0.49437
Epoch 6.40: Loss = 0.371826
Epoch 6.41: Loss = 0.401184
Epoch 6.42: Loss = 0.459641
Epoch 6.43: Loss = 0.439301
Epoch 6.44: Loss = 0.446457
Epoch 6.45: Loss = 0.598083
Epoch 6.46: Loss = 0.399765
Epoch 6.47: Loss = 0.410568
Epoch 6.48: Loss = 0.538498
Epoch 6.49: Loss = 0.43457
Epoch 6.50: Loss = 0.480133
Epoch 6.51: Loss = 0.603256
Epoch 6.52: Loss = 0.701218
Epoch 6.53: Loss = 0.404724
Epoch 6.54: Loss = 0.49942
Epoch 6.55: Loss = 0.391815
Epoch 6.56: Loss = 0.474777
Epoch 6.57: Loss = 0.440445
Epoch 6.58: Loss = 0.541794
Epoch 6.59: Loss = 0.513153
Epoch 6.60: Loss = 0.446304
Epoch 6.61: Loss = 0.518219
Epoch 6.62: Loss = 0.424362
Epoch 6.63: Loss = 0.479218
Epoch 6.64: Loss = 0.455521
Epoch 6.65: Loss = 0.466797
Epoch 6.66: Loss = 0.417542
Epoch 6.67: Loss = 0.373703
Epoch 6.68: Loss = 0.441849
Epoch 6.69: Loss = 0.414398
Epoch 6.70: Loss = 0.586334
Epoch 6.71: Loss = 0.529251
Epoch 6.72: Loss = 0.460022
Epoch 6.73: Loss = 0.41452
Epoch 6.74: Loss = 0.496567
Epoch 6.75: Loss = 0.467697
Epoch 6.76: Loss = 0.526123
Epoch 6.77: Loss = 0.375824
Epoch 6.78: Loss = 0.462021
Epoch 6.79: Loss = 0.479202
Epoch 6.80: Loss = 0.471344
Epoch 6.81: Loss = 0.459
Epoch 6.82: Loss = 0.482391
Epoch 6.83: Loss = 0.520508
Epoch 6.84: Loss = 0.488876
Epoch 6.85: Loss = 0.416031
Epoch 6.86: Loss = 0.464569
Epoch 6.87: Loss = 0.512833
Epoch 6.88: Loss = 0.620361
Epoch 6.89: Loss = 0.56218
Epoch 6.90: Loss = 0.407974
Epoch 6.91: Loss = 0.503586
Epoch 6.92: Loss = 0.522324
Epoch 6.93: Loss = 0.414246
Epoch 6.94: Loss = 0.390305
Epoch 6.95: Loss = 0.466766
Epoch 6.96: Loss = 0.509872
Epoch 6.97: Loss = 0.554993
Epoch 6.98: Loss = 0.509277
Epoch 6.99: Loss = 0.524765
Epoch 6.100: Loss = 0.583054
Epoch 6.101: Loss = 0.417969
Epoch 6.102: Loss = 0.603592
Epoch 6.103: Loss = 0.618073
Epoch 6.104: Loss = 0.639923
Epoch 6.105: Loss = 0.460297
Epoch 6.106: Loss = 0.425201
Epoch 6.107: Loss = 0.453552
Epoch 6.108: Loss = 0.345215
Epoch 6.109: Loss = 0.48259
Epoch 6.110: Loss = 0.525986
Epoch 6.111: Loss = 0.530899
Epoch 6.112: Loss = 0.406662
Epoch 6.113: Loss = 0.387009
Epoch 6.114: Loss = 0.418259
Epoch 6.115: Loss = 0.396667
Epoch 6.116: Loss = 0.429825
Epoch 6.117: Loss = 0.444061
Epoch 6.118: Loss = 0.454025
Epoch 6.119: Loss = 0.414047
Epoch 6.120: Loss = 0.46521
TRAIN LOSS = 0.472778
TRAIN ACC = 86.6348 % (51983/60000)
Loss = 0.430405
Loss = 0.543228
Loss = 0.668335
Loss = 0.624329
Loss = 0.662292
Loss = 0.470596
Loss = 0.395691
Loss = 0.710373
Loss = 0.663956
Loss = 0.57222
Loss = 0.268692
Loss = 0.333038
Loss = 0.397568
Loss = 0.450211
Loss = 0.255524
Loss = 0.39563
Loss = 0.278534
Loss = 0.0724487
Loss = 0.264328
Loss = 0.618637
TEST LOSS = 0.453802
TEST ACC = 519.829 % (8747/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.32402
Epoch 7.2: Loss = 0.43013
Epoch 7.3: Loss = 0.533691
Epoch 7.4: Loss = 0.432343
Epoch 7.5: Loss = 0.537262
Epoch 7.6: Loss = 0.47493
Epoch 7.7: Loss = 0.512421
Epoch 7.8: Loss = 0.575226
Epoch 7.9: Loss = 0.344208
Epoch 7.10: Loss = 0.471466
Epoch 7.11: Loss = 0.553406
Epoch 7.12: Loss = 0.459686
Epoch 7.13: Loss = 0.438873
Epoch 7.14: Loss = 0.490112
Epoch 7.15: Loss = 0.463058
Epoch 7.16: Loss = 0.463135
Epoch 7.17: Loss = 0.446014
Epoch 7.18: Loss = 0.521042
Epoch 7.19: Loss = 0.434479
Epoch 7.20: Loss = 0.447052
Epoch 7.21: Loss = 0.401337
Epoch 7.22: Loss = 0.42009
Epoch 7.23: Loss = 0.462952
Epoch 7.24: Loss = 0.497208
Epoch 7.25: Loss = 0.396744
Epoch 7.26: Loss = 0.528198
Epoch 7.27: Loss = 0.496735
Epoch 7.28: Loss = 0.443649
Epoch 7.29: Loss = 0.392151
Epoch 7.30: Loss = 0.390106
Epoch 7.31: Loss = 0.400101
Epoch 7.32: Loss = 0.387039
Epoch 7.33: Loss = 0.430176
Epoch 7.34: Loss = 0.488098
Epoch 7.35: Loss = 0.564377
Epoch 7.36: Loss = 0.449692
Epoch 7.37: Loss = 0.42717
Epoch 7.38: Loss = 0.4487
Epoch 7.39: Loss = 0.578323
Epoch 7.40: Loss = 0.530457
Epoch 7.41: Loss = 0.559448
Epoch 7.42: Loss = 0.545242
Epoch 7.43: Loss = 0.547913
Epoch 7.44: Loss = 0.564178
Epoch 7.45: Loss = 0.432251
Epoch 7.46: Loss = 0.438736
Epoch 7.47: Loss = 0.458359
Epoch 7.48: Loss = 0.544495
Epoch 7.49: Loss = 0.482574
Epoch 7.50: Loss = 0.513702
Epoch 7.51: Loss = 0.516434
Epoch 7.52: Loss = 0.481415
Epoch 7.53: Loss = 0.449707
Epoch 7.54: Loss = 0.54361
Epoch 7.55: Loss = 0.456085
Epoch 7.56: Loss = 0.365845
Epoch 7.57: Loss = 0.359268
Epoch 7.58: Loss = 0.468277
Epoch 7.59: Loss = 0.465607
Epoch 7.60: Loss = 0.398468
Epoch 7.61: Loss = 0.567352
Epoch 7.62: Loss = 0.452469
Epoch 7.63: Loss = 0.402481
Epoch 7.64: Loss = 0.517517
Epoch 7.65: Loss = 0.475998
Epoch 7.66: Loss = 0.519424
Epoch 7.67: Loss = 0.514038
Epoch 7.68: Loss = 0.442627
Epoch 7.69: Loss = 0.471558
Epoch 7.70: Loss = 0.561676
Epoch 7.71: Loss = 0.530579
Epoch 7.72: Loss = 0.457092
Epoch 7.73: Loss = 0.333954
Epoch 7.74: Loss = 0.552353
Epoch 7.75: Loss = 0.367966
Epoch 7.76: Loss = 0.460007
Epoch 7.77: Loss = 0.456589
Epoch 7.78: Loss = 0.381531
Epoch 7.79: Loss = 0.475433
Epoch 7.80: Loss = 0.521606
Epoch 7.81: Loss = 0.408524
Epoch 7.82: Loss = 0.508163
Epoch 7.83: Loss = 0.551483
Epoch 7.84: Loss = 0.47345
Epoch 7.85: Loss = 0.363617
Epoch 7.86: Loss = 0.504013
Epoch 7.87: Loss = 0.395828
Epoch 7.88: Loss = 0.530533
Epoch 7.89: Loss = 0.423065
Epoch 7.90: Loss = 0.438675
Epoch 7.91: Loss = 0.541031
Epoch 7.92: Loss = 0.434845
Epoch 7.93: Loss = 0.56926
Epoch 7.94: Loss = 0.406845
Epoch 7.95: Loss = 0.515503
Epoch 7.96: Loss = 0.412491
Epoch 7.97: Loss = 0.517197
Epoch 7.98: Loss = 0.549225
Epoch 7.99: Loss = 0.449051
Epoch 7.100: Loss = 0.484055
Epoch 7.101: Loss = 0.475739
Epoch 7.102: Loss = 0.535446
Epoch 7.103: Loss = 0.538681
Epoch 7.104: Loss = 0.535995
Epoch 7.105: Loss = 0.471466
Epoch 7.106: Loss = 0.477264
Epoch 7.107: Loss = 0.486099
Epoch 7.108: Loss = 0.507126
Epoch 7.109: Loss = 0.525986
Epoch 7.110: Loss = 0.484863
Epoch 7.111: Loss = 0.598053
Epoch 7.112: Loss = 0.478439
Epoch 7.113: Loss = 0.42778
Epoch 7.114: Loss = 0.463989
Epoch 7.115: Loss = 0.472183
Epoch 7.116: Loss = 0.462845
Epoch 7.117: Loss = 0.479218
Epoch 7.118: Loss = 0.470215
Epoch 7.119: Loss = 0.436203
Epoch 7.120: Loss = 0.447937
TRAIN LOSS = 0.473267
TRAIN ACC = 86.8271 % (52099/60000)
Loss = 0.413925
Loss = 0.536942
Loss = 0.730103
Loss = 0.606796
Loss = 0.675613
Loss = 0.466461
Loss = 0.38208
Loss = 0.718979
Loss = 0.629715
Loss = 0.581177
Loss = 0.238388
Loss = 0.342484
Loss = 0.338669
Loss = 0.425186
Loss = 0.274292
Loss = 0.387695
Loss = 0.254745
Loss = 0.0572815
Loss = 0.266647
Loss = 0.65831
TEST LOSS = 0.449274
TEST ACC = 520.988 % (8760/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.432617
Epoch 8.2: Loss = 0.466934
Epoch 8.3: Loss = 0.477966
Epoch 8.4: Loss = 0.44339
Epoch 8.5: Loss = 0.377655
Epoch 8.6: Loss = 0.394821
Epoch 8.7: Loss = 0.509537
Epoch 8.8: Loss = 0.523331
Epoch 8.9: Loss = 0.513
Epoch 8.10: Loss = 0.522171
Epoch 8.11: Loss = 0.526993
Epoch 8.12: Loss = 0.424545
Epoch 8.13: Loss = 0.425873
Epoch 8.14: Loss = 0.4599
Epoch 8.15: Loss = 0.512558
Epoch 8.16: Loss = 0.56102
Epoch 8.17: Loss = 0.519852
Epoch 8.18: Loss = 0.429077
Epoch 8.19: Loss = 0.520798
Epoch 8.20: Loss = 0.461288
Epoch 8.21: Loss = 0.502625
Epoch 8.22: Loss = 0.382385
Epoch 8.23: Loss = 0.436066
Epoch 8.24: Loss = 0.543747
Epoch 8.25: Loss = 0.401077
Epoch 8.26: Loss = 0.494171
Epoch 8.27: Loss = 0.59404
Epoch 8.28: Loss = 0.508072
Epoch 8.29: Loss = 0.476791
Epoch 8.30: Loss = 0.568497
Epoch 8.31: Loss = 0.42926
Epoch 8.32: Loss = 0.49472
Epoch 8.33: Loss = 0.490646
Epoch 8.34: Loss = 0.389862
Epoch 8.35: Loss = 0.298431
Epoch 8.36: Loss = 0.434311
Epoch 8.37: Loss = 0.4617
Epoch 8.38: Loss = 0.47673
Epoch 8.39: Loss = 0.40358
Epoch 8.40: Loss = 0.568253
Epoch 8.41: Loss = 0.615295
Epoch 8.42: Loss = 0.492889
Epoch 8.43: Loss = 0.500732
Epoch 8.44: Loss = 0.515686
Epoch 8.45: Loss = 0.459473
Epoch 8.46: Loss = 0.424454
Epoch 8.47: Loss = 0.404007
Epoch 8.48: Loss = 0.496918
Epoch 8.49: Loss = 0.378036
Epoch 8.50: Loss = 0.48262
Epoch 8.51: Loss = 0.523376
Epoch 8.52: Loss = 0.457291
Epoch 8.53: Loss = 0.487503
Epoch 8.54: Loss = 0.503845
Epoch 8.55: Loss = 0.395111
Epoch 8.56: Loss = 0.539917
Epoch 8.57: Loss = 0.382477
Epoch 8.58: Loss = 0.537659
Epoch 8.59: Loss = 0.348694
Epoch 8.60: Loss = 0.564896
Epoch 8.61: Loss = 0.406433
Epoch 8.62: Loss = 0.442459
Epoch 8.63: Loss = 0.537201
Epoch 8.64: Loss = 0.438187
Epoch 8.65: Loss = 0.469223
Epoch 8.66: Loss = 0.574448
Epoch 8.67: Loss = 0.565094
Epoch 8.68: Loss = 0.516602
Epoch 8.69: Loss = 0.444748
Epoch 8.70: Loss = 0.474976
Epoch 8.71: Loss = 0.563614
Epoch 8.72: Loss = 0.447205
Epoch 8.73: Loss = 0.554245
Epoch 8.74: Loss = 0.454391
Epoch 8.75: Loss = 0.585464
Epoch 8.76: Loss = 0.453888
Epoch 8.77: Loss = 0.582489
Epoch 8.78: Loss = 0.460068
Epoch 8.79: Loss = 0.355148
Epoch 8.80: Loss = 0.437851
Epoch 8.81: Loss = 0.430191
Epoch 8.82: Loss = 0.513733
Epoch 8.83: Loss = 0.553864
Epoch 8.84: Loss = 0.411743
Epoch 8.85: Loss = 0.431244
Epoch 8.86: Loss = 0.535889
Epoch 8.87: Loss = 0.496414
Epoch 8.88: Loss = 0.40303
Epoch 8.89: Loss = 0.398911
Epoch 8.90: Loss = 0.51265
Epoch 8.91: Loss = 0.428024
Epoch 8.92: Loss = 0.523361
Epoch 8.93: Loss = 0.568008
Epoch 8.94: Loss = 0.489792
Epoch 8.95: Loss = 0.52037
Epoch 8.96: Loss = 0.530777
Epoch 8.97: Loss = 0.465149
Epoch 8.98: Loss = 0.473083
Epoch 8.99: Loss = 0.502075
Epoch 8.100: Loss = 0.392487
Epoch 8.101: Loss = 0.419952
Epoch 8.102: Loss = 0.420273
Epoch 8.103: Loss = 0.439346
Epoch 8.104: Loss = 0.655151
Epoch 8.105: Loss = 0.294495
Epoch 8.106: Loss = 0.403671
Epoch 8.107: Loss = 0.56839
Epoch 8.108: Loss = 0.47757
Epoch 8.109: Loss = 0.399612
Epoch 8.110: Loss = 0.504959
Epoch 8.111: Loss = 0.378723
Epoch 8.112: Loss = 0.466904
Epoch 8.113: Loss = 0.46199
Epoch 8.114: Loss = 0.488983
Epoch 8.115: Loss = 0.522629
Epoch 8.116: Loss = 0.528198
Epoch 8.117: Loss = 0.399353
Epoch 8.118: Loss = 0.496567
Epoch 8.119: Loss = 0.469589
Epoch 8.120: Loss = 0.459198
TRAIN LOSS = 0.474747
TRAIN ACC = 87.1704 % (52305/60000)
Loss = 0.408051
Loss = 0.5578
Loss = 0.715591
Loss = 0.620438
Loss = 0.666031
Loss = 0.472153
Loss = 0.387421
Loss = 0.739868
Loss = 0.644348
Loss = 0.609207
Loss = 0.210434
Loss = 0.336929
Loss = 0.34285
Loss = 0.412659
Loss = 0.248596
Loss = 0.298508
Loss = 0.220566
Loss = 0.0474854
Loss = 0.28775
Loss = 0.593338
TEST LOSS = 0.441001
TEST ACC = 523.048 % (8813/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.47438
Epoch 9.2: Loss = 0.498428
Epoch 9.3: Loss = 0.481705
Epoch 9.4: Loss = 0.424957
Epoch 9.5: Loss = 0.456116
Epoch 9.6: Loss = 0.452911
Epoch 9.7: Loss = 0.539215
Epoch 9.8: Loss = 0.511642
Epoch 9.9: Loss = 0.483429
Epoch 9.10: Loss = 0.563889
Epoch 9.11: Loss = 0.517487
Epoch 9.12: Loss = 0.521042
Epoch 9.13: Loss = 0.487305
Epoch 9.14: Loss = 0.526154
Epoch 9.15: Loss = 0.386002
Epoch 9.16: Loss = 0.537018
Epoch 9.17: Loss = 0.385651
Epoch 9.18: Loss = 0.482071
Epoch 9.19: Loss = 0.44693
Epoch 9.20: Loss = 0.496552
Epoch 9.21: Loss = 0.432724
Epoch 9.22: Loss = 0.560562
Epoch 9.23: Loss = 0.458191
Epoch 9.24: Loss = 0.540115
Epoch 9.25: Loss = 0.608124
Epoch 9.26: Loss = 0.555893
Epoch 9.27: Loss = 0.503906
Epoch 9.28: Loss = 0.416901
Epoch 9.29: Loss = 0.516678
Epoch 9.30: Loss = 0.485947
Epoch 9.31: Loss = 0.483826
Epoch 9.32: Loss = 0.44574
Epoch 9.33: Loss = 0.43306
Epoch 9.34: Loss = 0.620087
Epoch 9.35: Loss = 0.443542
Epoch 9.36: Loss = 0.532867
Epoch 9.37: Loss = 0.43718
Epoch 9.38: Loss = 0.395615
Epoch 9.39: Loss = 0.427612
Epoch 9.40: Loss = 0.535767
Epoch 9.41: Loss = 0.464157
Epoch 9.42: Loss = 0.431793
Epoch 9.43: Loss = 0.524063
Epoch 9.44: Loss = 0.513641
Epoch 9.45: Loss = 0.391037
Epoch 9.46: Loss = 0.37854
Epoch 9.47: Loss = 0.565567
Epoch 9.48: Loss = 0.401047
Epoch 9.49: Loss = 0.398605
Epoch 9.50: Loss = 0.537094
Epoch 9.51: Loss = 0.508438
Epoch 9.52: Loss = 0.385696
Epoch 9.53: Loss = 0.372711
Epoch 9.54: Loss = 0.50972
Epoch 9.55: Loss = 0.459381
Epoch 9.56: Loss = 0.453644
Epoch 9.57: Loss = 0.486893
Epoch 9.58: Loss = 0.567413
Epoch 9.59: Loss = 0.578278
Epoch 9.60: Loss = 0.420883
Epoch 9.61: Loss = 0.439606
Epoch 9.62: Loss = 0.472794
Epoch 9.63: Loss = 0.540787
Epoch 9.64: Loss = 0.494568
Epoch 9.65: Loss = 0.461472
Epoch 9.66: Loss = 0.44841
Epoch 9.67: Loss = 0.511292
Epoch 9.68: Loss = 0.43808
Epoch 9.69: Loss = 0.482239
Epoch 9.70: Loss = 0.486847
Epoch 9.71: Loss = 0.507721
Epoch 9.72: Loss = 0.417557
Epoch 9.73: Loss = 0.506821
Epoch 9.74: Loss = 0.490128
Epoch 9.75: Loss = 0.45929
Epoch 9.76: Loss = 0.5737
Epoch 9.77: Loss = 0.461716
Epoch 9.78: Loss = 0.543549
Epoch 9.79: Loss = 0.365189
Epoch 9.80: Loss = 0.518173
Epoch 9.81: Loss = 0.46904
Epoch 9.82: Loss = 0.503128
Epoch 9.83: Loss = 0.577789
Epoch 9.84: Loss = 0.525162
Epoch 9.85: Loss = 0.505112
Epoch 9.86: Loss = 0.487152
Epoch 9.87: Loss = 0.52771
Epoch 9.88: Loss = 0.432037
Epoch 9.89: Loss = 0.535553
Epoch 9.90: Loss = 0.515411
Epoch 9.91: Loss = 0.431473
Epoch 9.92: Loss = 0.414017
Epoch 9.93: Loss = 0.498779
Epoch 9.94: Loss = 0.522476
Epoch 9.95: Loss = 0.49205
Epoch 9.96: Loss = 0.466431
Epoch 9.97: Loss = 0.48822
Epoch 9.98: Loss = 0.52037
Epoch 9.99: Loss = 0.490768
Epoch 9.100: Loss = 0.590668
Epoch 9.101: Loss = 0.511871
Epoch 9.102: Loss = 0.524994
Epoch 9.103: Loss = 0.566864
Epoch 9.104: Loss = 0.518936
Epoch 9.105: Loss = 0.366745
Epoch 9.106: Loss = 0.545212
Epoch 9.107: Loss = 0.514542
Epoch 9.108: Loss = 0.473633
Epoch 9.109: Loss = 0.415573
Epoch 9.110: Loss = 0.506027
Epoch 9.111: Loss = 0.480667
Epoch 9.112: Loss = 0.482819
Epoch 9.113: Loss = 0.506714
Epoch 9.114: Loss = 0.569382
Epoch 9.115: Loss = 0.488647
Epoch 9.116: Loss = 0.481934
Epoch 9.117: Loss = 0.434662
Epoch 9.118: Loss = 0.550827
Epoch 9.119: Loss = 0.494095
Epoch 9.120: Loss = 0.55574
TRAIN LOSS = 0.487167
TRAIN ACC = 87.1277 % (52279/60000)
Loss = 0.426636
Loss = 0.56929
Loss = 0.71785
Loss = 0.636246
Loss = 0.704193
Loss = 0.505875
Loss = 0.417496
Loss = 0.758789
Loss = 0.607437
Loss = 0.611313
Loss = 0.235153
Loss = 0.358795
Loss = 0.374725
Loss = 0.433197
Loss = 0.26532
Loss = 0.294952
Loss = 0.243103
Loss = 0.0503082
Loss = 0.293671
Loss = 0.669998
TEST LOSS = 0.458717
TEST ACC = 522.789 % (8792/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.425018
Epoch 10.2: Loss = 0.519775
Epoch 10.3: Loss = 0.442581
Epoch 10.4: Loss = 0.439926
Epoch 10.5: Loss = 0.504669
Epoch 10.6: Loss = 0.381012
Epoch 10.7: Loss = 0.633224
Epoch 10.8: Loss = 0.354218
Epoch 10.9: Loss = 0.537201
Epoch 10.10: Loss = 0.48233
Epoch 10.11: Loss = 0.44989
Epoch 10.12: Loss = 0.505692
Epoch 10.13: Loss = 0.586639
Epoch 10.14: Loss = 0.511093
Epoch 10.15: Loss = 0.488617
Epoch 10.16: Loss = 0.542679
Epoch 10.17: Loss = 0.543228
Epoch 10.18: Loss = 0.512466
Epoch 10.19: Loss = 0.656891
Epoch 10.20: Loss = 0.41803
Epoch 10.21: Loss = 0.490265
Epoch 10.22: Loss = 0.663437
Epoch 10.23: Loss = 0.461395
Epoch 10.24: Loss = 0.558105
Epoch 10.25: Loss = 0.465668
Epoch 10.26: Loss = 0.444153
Epoch 10.27: Loss = 0.479141
Epoch 10.28: Loss = 0.475266
Epoch 10.29: Loss = 0.479767
Epoch 10.30: Loss = 0.576599
Epoch 10.31: Loss = 0.409729
Epoch 10.32: Loss = 0.445862
Epoch 10.33: Loss = 0.527756
Epoch 10.34: Loss = 0.5896
Epoch 10.35: Loss = 0.482758
Epoch 10.36: Loss = 0.470383
Epoch 10.37: Loss = 0.434891
Epoch 10.38: Loss = 0.50206
Epoch 10.39: Loss = 0.442657
Epoch 10.40: Loss = 0.619919
Epoch 10.41: Loss = 0.324921
Epoch 10.42: Loss = 0.459396
Epoch 10.43: Loss = 0.441788
Epoch 10.44: Loss = 0.462372
Epoch 10.45: Loss = 0.497513
Epoch 10.46: Loss = 0.493301
Epoch 10.47: Loss = 0.495239
Epoch 10.48: Loss = 0.50322
Epoch 10.49: Loss = 0.404297
Epoch 10.50: Loss = 0.401688
Epoch 10.51: Loss = 0.598175
Epoch 10.52: Loss = 0.51059
Epoch 10.53: Loss = 0.449844
Epoch 10.54: Loss = 0.542572
Epoch 10.55: Loss = 0.467117
Epoch 10.56: Loss = 0.380356
Epoch 10.57: Loss = 0.489014
Epoch 10.58: Loss = 0.47374
Epoch 10.59: Loss = 0.384995
Epoch 10.60: Loss = 0.515381
Epoch 10.61: Loss = 0.391037
Epoch 10.62: Loss = 0.535721
Epoch 10.63: Loss = 0.422623
Epoch 10.64: Loss = 0.510956
Epoch 10.65: Loss = 0.527588
Epoch 10.66: Loss = 0.500656
Epoch 10.67: Loss = 0.555344
Epoch 10.68: Loss = 0.480865
Epoch 10.69: Loss = 0.553528
Epoch 10.70: Loss = 0.48793
Epoch 10.71: Loss = 0.462662
Epoch 10.72: Loss = 0.562561
Epoch 10.73: Loss = 0.535263
Epoch 10.74: Loss = 0.499893
Epoch 10.75: Loss = 0.625092
Epoch 10.76: Loss = 0.487625
Epoch 10.77: Loss = 0.512436
Epoch 10.78: Loss = 0.511169
Epoch 10.79: Loss = 0.434128
Epoch 10.80: Loss = 0.558945
Epoch 10.81: Loss = 0.420532
Epoch 10.82: Loss = 0.525055
Epoch 10.83: Loss = 0.52034
Epoch 10.84: Loss = 0.498459
Epoch 10.85: Loss = 0.424301
Epoch 10.86: Loss = 0.469025
Epoch 10.87: Loss = 0.550797
Epoch 10.88: Loss = 0.392166
Epoch 10.89: Loss = 0.579254
Epoch 10.90: Loss = 0.680298
Epoch 10.91: Loss = 0.453079
Epoch 10.92: Loss = 0.481293
Epoch 10.93: Loss = 0.590607
Epoch 10.94: Loss = 0.461182
Epoch 10.95: Loss = 0.533768
Epoch 10.96: Loss = 0.417053
Epoch 10.97: Loss = 0.478745
Epoch 10.98: Loss = 0.442413
Epoch 10.99: Loss = 0.508484
Epoch 10.100: Loss = 0.550659
Epoch 10.101: Loss = 0.468002
Epoch 10.102: Loss = 0.388031
Epoch 10.103: Loss = 0.522064
Epoch 10.104: Loss = 0.555832
Epoch 10.105: Loss = 0.42511
Epoch 10.106: Loss = 0.684036
Epoch 10.107: Loss = 0.561066
Epoch 10.108: Loss = 0.559189
Epoch 10.109: Loss = 0.544815
Epoch 10.110: Loss = 0.444504
Epoch 10.111: Loss = 0.488785
Epoch 10.112: Loss = 0.425629
Epoch 10.113: Loss = 0.583771
Epoch 10.114: Loss = 0.426468
Epoch 10.115: Loss = 0.543655
Epoch 10.116: Loss = 0.508926
Epoch 10.117: Loss = 0.447479
Epoch 10.118: Loss = 0.499039
Epoch 10.119: Loss = 0.38269
Epoch 10.120: Loss = 0.51767
TRAIN LOSS = 0.494675
TRAIN ACC = 87.2559 % (52356/60000)
Loss = 0.419586
Loss = 0.547165
Loss = 0.732208
Loss = 0.634781
Loss = 0.718369
Loss = 0.506973
Loss = 0.434097
Loss = 0.826172
Loss = 0.610886
Loss = 0.54808
Loss = 0.251892
Loss = 0.374252
Loss = 0.421051
Loss = 0.428635
Loss = 0.23024
Loss = 0.31218
Loss = 0.237137
Loss = 0.0498352
Loss = 0.296906
Loss = 0.637772
TEST LOSS = 0.460911
TEST ACC = 523.56 % (8797/10000)

Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.37308
Epoch 1.2: Loss = 2.29037
Epoch 1.3: Loss = 2.23703
Epoch 1.4: Loss = 2.15709
Epoch 1.5: Loss = 2.11592
Epoch 1.6: Loss = 2.06898
Epoch 1.7: Loss = 2.02203
Epoch 1.8: Loss = 1.97705
Epoch 1.9: Loss = 1.92203
Epoch 1.10: Loss = 1.84383
Epoch 1.11: Loss = 1.85066
Epoch 1.12: Loss = 1.82187
Epoch 1.13: Loss = 1.75838
Epoch 1.14: Loss = 1.71002
Epoch 1.15: Loss = 1.6889
Epoch 1.16: Loss = 1.67134
Epoch 1.17: Loss = 1.62877
Epoch 1.18: Loss = 1.58369
Epoch 1.19: Loss = 1.52745
Epoch 1.20: Loss = 1.53555
Epoch 1.21: Loss = 1.48158
Epoch 1.22: Loss = 1.44833
Epoch 1.23: Loss = 1.43317
Epoch 1.24: Loss = 1.47678
Epoch 1.25: Loss = 1.41849
Epoch 1.26: Loss = 1.33989
Epoch 1.27: Loss = 1.32652
Epoch 1.28: Loss = 1.31006
Epoch 1.29: Loss = 1.31158
Epoch 1.30: Loss = 1.26022
Epoch 1.31: Loss = 1.27554
Epoch 1.32: Loss = 1.26126
Epoch 1.33: Loss = 1.17563
Epoch 1.34: Loss = 1.2131
Epoch 1.35: Loss = 1.24709
Epoch 1.36: Loss = 1.23251
Epoch 1.37: Loss = 1.2114
Epoch 1.38: Loss = 1.17851
Epoch 1.39: Loss = 1.12672
Epoch 1.40: Loss = 1.13379
Epoch 1.41: Loss = 1.1757
Epoch 1.42: Loss = 1.09525
Epoch 1.43: Loss = 1.05838
Epoch 1.44: Loss = 1.04857
Epoch 1.45: Loss = 1.08092
Epoch 1.46: Loss = 1.06642
Epoch 1.47: Loss = 1.05173
Epoch 1.48: Loss = 1.02934
Epoch 1.49: Loss = 1.05617
Epoch 1.50: Loss = 1.00716
Epoch 1.51: Loss = 0.990555
Epoch 1.52: Loss = 1.04317
Epoch 1.53: Loss = 1.03697
Epoch 1.54: Loss = 0.93689
Epoch 1.55: Loss = 0.993576
Epoch 1.56: Loss = 1.00925
Epoch 1.57: Loss = 1.03081
Epoch 1.58: Loss = 0.980164
Epoch 1.59: Loss = 0.962753
Epoch 1.60: Loss = 1.00974
Epoch 1.61: Loss = 0.898666
Epoch 1.62: Loss = 0.997498
Epoch 1.63: Loss = 0.875717
Epoch 1.64: Loss = 0.922104
Epoch 1.65: Loss = 0.92012
Epoch 1.66: Loss = 0.930328
Epoch 1.67: Loss = 0.855988
Epoch 1.68: Loss = 0.978043
Epoch 1.69: Loss = 0.91423
Epoch 1.70: Loss = 0.889679
Epoch 1.71: Loss = 0.842285
Epoch 1.72: Loss = 0.858704
Epoch 1.73: Loss = 0.917725
Epoch 1.74: Loss = 0.951202
Epoch 1.75: Loss = 0.8741
Epoch 1.76: Loss = 0.876038
Epoch 1.77: Loss = 0.847534
Epoch 1.78: Loss = 0.858871
Epoch 1.79: Loss = 0.782288
Epoch 1.80: Loss = 0.859634
Epoch 1.81: Loss = 0.817841
Epoch 1.82: Loss = 0.836044
Epoch 1.83: Loss = 0.856064
Epoch 1.84: Loss = 0.853378
Epoch 1.85: Loss = 0.823349
Epoch 1.86: Loss = 0.885361
Epoch 1.87: Loss = 0.877258
Epoch 1.88: Loss = 0.752472
Epoch 1.89: Loss = 0.88179
Epoch 1.90: Loss = 0.821487
Epoch 1.91: Loss = 0.887436
Epoch 1.92: Loss = 0.826889
Epoch 1.93: Loss = 0.884033
Epoch 1.94: Loss = 0.833725
Epoch 1.95: Loss = 0.874725
Epoch 1.96: Loss = 0.798447
Epoch 1.97: Loss = 0.727081
Epoch 1.98: Loss = 0.823853
Epoch 1.99: Loss = 0.792282
Epoch 1.100: Loss = 0.765167
Epoch 1.101: Loss = 0.844772
Epoch 1.102: Loss = 0.841766
Epoch 1.103: Loss = 0.825317
Epoch 1.104: Loss = 0.790573
Epoch 1.105: Loss = 0.750443
Epoch 1.106: Loss = 0.880554
Epoch 1.107: Loss = 0.812164
Epoch 1.108: Loss = 0.795547
Epoch 1.109: Loss = 0.781387
Epoch 1.110: Loss = 0.794952
Epoch 1.111: Loss = 0.718643
Epoch 1.112: Loss = 0.717667
Epoch 1.113: Loss = 0.788208
Epoch 1.114: Loss = 0.757874
Epoch 1.115: Loss = 0.799149
Epoch 1.116: Loss = 0.707428
Epoch 1.117: Loss = 0.848007
Epoch 1.118: Loss = 0.699661
Epoch 1.119: Loss = 0.745087
Epoch 1.120: Loss = 0.727081
TRAIN LOSS = 1.12006
TRAIN ACC = 61.1496 % (36692/60000)
Loss = 0.693802
Loss = 0.815399
Loss = 0.807953
Loss = 0.716995
Loss = 0.738342
Loss = 0.859344
Loss = 0.885956
Loss = 0.818832
Loss = 0.761993
Loss = 0.709244
Loss = 0.812607
Loss = 0.776794
Loss = 0.805466
Loss = 0.801056
Loss = 0.748032
Loss = 0.816086
Loss = 0.746155
Loss = 0.799484
Loss = 0.824005
Loss = 0.757507
TEST LOSS = 0.784752
TEST ACC = 366.919 % (7235/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.784821
Epoch 2.2: Loss = 0.757645
Epoch 2.3: Loss = 0.830551
Epoch 2.4: Loss = 0.729019
Epoch 2.5: Loss = 0.761734
Epoch 2.6: Loss = 0.839523
Epoch 2.7: Loss = 0.720947
Epoch 2.8: Loss = 0.801285
Epoch 2.9: Loss = 0.686554
Epoch 2.10: Loss = 0.613647
Epoch 2.11: Loss = 0.796921
Epoch 2.12: Loss = 0.775681
Epoch 2.13: Loss = 0.780273
Epoch 2.14: Loss = 0.790161
Epoch 2.15: Loss = 0.744049
Epoch 2.16: Loss = 0.830109
Epoch 2.17: Loss = 0.699585
Epoch 2.18: Loss = 0.748611
Epoch 2.19: Loss = 0.719818
Epoch 2.20: Loss = 0.846268
Epoch 2.21: Loss = 0.720627
Epoch 2.22: Loss = 0.686356
Epoch 2.23: Loss = 0.779633
Epoch 2.24: Loss = 0.815964
Epoch 2.25: Loss = 0.760696
Epoch 2.26: Loss = 0.668472
Epoch 2.27: Loss = 0.727386
Epoch 2.28: Loss = 0.7379
Epoch 2.29: Loss = 0.789047
Epoch 2.30: Loss = 0.703705
Epoch 2.31: Loss = 0.782104
Epoch 2.32: Loss = 0.724823
Epoch 2.33: Loss = 0.671982
Epoch 2.34: Loss = 0.786652
Epoch 2.35: Loss = 0.765152
Epoch 2.36: Loss = 0.802216
Epoch 2.37: Loss = 0.776611
Epoch 2.38: Loss = 0.730072
Epoch 2.39: Loss = 0.776794
Epoch 2.40: Loss = 0.725281
Epoch 2.41: Loss = 0.79422
Epoch 2.42: Loss = 0.756454
Epoch 2.43: Loss = 0.724945
Epoch 2.44: Loss = 0.65065
Epoch 2.45: Loss = 0.783325
Epoch 2.46: Loss = 0.821915
Epoch 2.47: Loss = 0.691101
Epoch 2.48: Loss = 0.670578
Epoch 2.49: Loss = 0.762009
Epoch 2.50: Loss = 0.735703
Epoch 2.51: Loss = 0.624268
Epoch 2.52: Loss = 0.778656
Epoch 2.53: Loss = 0.779114
Epoch 2.54: Loss = 0.638992
Epoch 2.55: Loss = 0.765259
Epoch 2.56: Loss = 0.788422
Epoch 2.57: Loss = 0.798401
Epoch 2.58: Loss = 0.750122
Epoch 2.59: Loss = 0.735092
Epoch 2.60: Loss = 0.804413
Epoch 2.61: Loss = 0.654419
Epoch 2.62: Loss = 0.753342
Epoch 2.63: Loss = 0.641769
Epoch 2.64: Loss = 0.668198
Epoch 2.65: Loss = 0.728348
Epoch 2.66: Loss = 0.694077
Epoch 2.67: Loss = 0.6521
Epoch 2.68: Loss = 0.842697
Epoch 2.69: Loss = 0.709549
Epoch 2.70: Loss = 0.730072
Epoch 2.71: Loss = 0.618195
Epoch 2.72: Loss = 0.748718
Epoch 2.73: Loss = 0.832932
Epoch 2.74: Loss = 0.804245
Epoch 2.75: Loss = 0.675537
Epoch 2.76: Loss = 0.715958
Epoch 2.77: Loss = 0.679199
Epoch 2.78: Loss = 0.725937
Epoch 2.79: Loss = 0.707489
Epoch 2.80: Loss = 0.714142
Epoch 2.81: Loss = 0.69342
Epoch 2.82: Loss = 0.6604
Epoch 2.83: Loss = 0.749863
Epoch 2.84: Loss = 0.663956
Epoch 2.85: Loss = 0.707458
Epoch 2.86: Loss = 0.755264
Epoch 2.87: Loss = 0.745377
Epoch 2.88: Loss = 0.640503
Epoch 2.89: Loss = 0.832153
Epoch 2.90: Loss = 0.733917
Epoch 2.91: Loss = 0.833649
Epoch 2.92: Loss = 0.748047
Epoch 2.93: Loss = 0.820724
Epoch 2.94: Loss = 0.68515
Epoch 2.95: Loss = 0.750153
Epoch 2.96: Loss = 0.67218
Epoch 2.97: Loss = 0.623657
Epoch 2.98: Loss = 0.702728
Epoch 2.99: Loss = 0.71727
Epoch 2.100: Loss = 0.690506
Epoch 2.101: Loss = 0.769409
Epoch 2.102: Loss = 0.775162
Epoch 2.103: Loss = 0.724121
Epoch 2.104: Loss = 0.67598
Epoch 2.105: Loss = 0.67186
Epoch 2.106: Loss = 0.850616
Epoch 2.107: Loss = 0.781662
Epoch 2.108: Loss = 0.773911
Epoch 2.109: Loss = 0.756104
Epoch 2.110: Loss = 0.751724
Epoch 2.111: Loss = 0.649078
Epoch 2.112: Loss = 0.66481
Epoch 2.113: Loss = 0.724243
Epoch 2.114: Loss = 0.704132
Epoch 2.115: Loss = 0.715652
Epoch 2.116: Loss = 0.663773
Epoch 2.117: Loss = 0.797165
Epoch 2.118: Loss = 0.631805
Epoch 2.119: Loss = 0.729187
Epoch 2.120: Loss = 0.667603
TRAIN LOSS = 0.735382
TRAIN ACC = 74.6552 % (44795/60000)
Loss = 0.660233
Loss = 0.836777
Loss = 0.75647
Loss = 0.683289
Loss = 0.720673
Loss = 0.87085
Loss = 0.865982
Loss = 0.827393
Loss = 0.768814
Loss = 0.682602
Loss = 0.813797
Loss = 0.812485
Loss = 0.798965
Loss = 0.769913
Loss = 0.731049
Loss = 0.78862
Loss = 0.741852
Loss = 0.822083
Loss = 0.848389
Loss = 0.751221
TEST LOSS = 0.777572
TEST ACC = 447.949 % (7539/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.753815
Epoch 3.2: Loss = 0.70723
Epoch 3.3: Loss = 0.791153
Epoch 3.4: Loss = 0.697266
Epoch 3.5: Loss = 0.718353
Epoch 3.6: Loss = 0.781982
Epoch 3.7: Loss = 0.672897
Epoch 3.8: Loss = 0.824005
Epoch 3.9: Loss = 0.6595
Epoch 3.10: Loss = 0.516937
Epoch 3.11: Loss = 0.809875
Epoch 3.12: Loss = 0.7453
Epoch 3.13: Loss = 0.767471
Epoch 3.14: Loss = 0.723053
Epoch 3.15: Loss = 0.681488
Epoch 3.16: Loss = 0.807587
Epoch 3.17: Loss = 0.655655
Epoch 3.18: Loss = 0.736206
Epoch 3.19: Loss = 0.680695
Epoch 3.20: Loss = 0.848343
Epoch 3.21: Loss = 0.690842
Epoch 3.22: Loss = 0.619766
Epoch 3.23: Loss = 0.7966
Epoch 3.24: Loss = 0.817154
Epoch 3.25: Loss = 0.713837
Epoch 3.26: Loss = 0.643478
Epoch 3.27: Loss = 0.696152
Epoch 3.28: Loss = 0.725388
Epoch 3.29: Loss = 0.738953
Epoch 3.30: Loss = 0.679199
Epoch 3.31: Loss = 0.76088
Epoch 3.32: Loss = 0.692413
Epoch 3.33: Loss = 0.627914
Epoch 3.34: Loss = 0.81842
Epoch 3.35: Loss = 0.705399
Epoch 3.36: Loss = 0.794113
Epoch 3.37: Loss = 0.760803
Epoch 3.38: Loss = 0.740128
Epoch 3.39: Loss = 0.79538
Epoch 3.40: Loss = 0.709229
Epoch 3.41: Loss = 0.781891
Epoch 3.42: Loss = 0.704422
Epoch 3.43: Loss = 0.70517
Epoch 3.44: Loss = 0.595047
Epoch 3.45: Loss = 0.778366
Epoch 3.46: Loss = 0.785233
Epoch 3.47: Loss = 0.698761
Epoch 3.48: Loss = 0.660767
Epoch 3.49: Loss = 0.770142
Epoch 3.50: Loss = 0.684235
Epoch 3.51: Loss = 0.597672
Epoch 3.52: Loss = 0.795334
Epoch 3.53: Loss = 0.780289
Epoch 3.54: Loss = 0.621689
Epoch 3.55: Loss = 0.73114
Epoch 3.56: Loss = 0.750092
Epoch 3.57: Loss = 0.787048
Epoch 3.58: Loss = 0.703552
Epoch 3.59: Loss = 0.751709
Epoch 3.60: Loss = 0.786896
Epoch 3.61: Loss = 0.68396
Epoch 3.62: Loss = 0.777756
Epoch 3.63: Loss = 0.642258
Epoch 3.64: Loss = 0.636826
Epoch 3.65: Loss = 0.751846
Epoch 3.66: Loss = 0.667755
Epoch 3.67: Loss = 0.686325
Epoch 3.68: Loss = 0.83075
Epoch 3.69: Loss = 0.742035
Epoch 3.70: Loss = 0.753448
Epoch 3.71: Loss = 0.616531
Epoch 3.72: Loss = 0.744431
Epoch 3.73: Loss = 0.812637
Epoch 3.74: Loss = 0.807281
Epoch 3.75: Loss = 0.681412
Epoch 3.76: Loss = 0.674652
Epoch 3.77: Loss = 0.670792
Epoch 3.78: Loss = 0.714905
Epoch 3.79: Loss = 0.71669
Epoch 3.80: Loss = 0.73381
Epoch 3.81: Loss = 0.68956
Epoch 3.82: Loss = 0.6539
Epoch 3.83: Loss = 0.720459
Epoch 3.84: Loss = 0.655411
Epoch 3.85: Loss = 0.691772
Epoch 3.86: Loss = 0.755264
Epoch 3.87: Loss = 0.717636
Epoch 3.88: Loss = 0.63829
Epoch 3.89: Loss = 0.789841
Epoch 3.90: Loss = 0.747086
Epoch 3.91: Loss = 0.825165
Epoch 3.92: Loss = 0.762207
Epoch 3.93: Loss = 0.80394
Epoch 3.94: Loss = 0.681107
Epoch 3.95: Loss = 0.737061
Epoch 3.96: Loss = 0.701263
Epoch 3.97: Loss = 0.635391
Epoch 3.98: Loss = 0.691528
Epoch 3.99: Loss = 0.689804
Epoch 3.100: Loss = 0.684921
Epoch 3.101: Loss = 0.779465
Epoch 3.102: Loss = 0.764969
Epoch 3.103: Loss = 0.684769
Epoch 3.104: Loss = 0.617844
Epoch 3.105: Loss = 0.671402
Epoch 3.106: Loss = 0.827164
Epoch 3.107: Loss = 0.783783
Epoch 3.108: Loss = 0.813568
Epoch 3.109: Loss = 0.75972
Epoch 3.110: Loss = 0.728729
Epoch 3.111: Loss = 0.638962
Epoch 3.112: Loss = 0.702728
Epoch 3.113: Loss = 0.739212
Epoch 3.114: Loss = 0.734741
Epoch 3.115: Loss = 0.735275
Epoch 3.116: Loss = 0.653427
Epoch 3.117: Loss = 0.777161
Epoch 3.118: Loss = 0.671677
Epoch 3.119: Loss = 0.718552
Epoch 3.120: Loss = 0.712158
TRAIN LOSS = 0.72258
TRAIN ACC = 77.0309 % (46221/60000)
Loss = 0.621185
Loss = 0.845612
Loss = 0.733994
Loss = 0.647598
Loss = 0.662857
Loss = 0.86116
Loss = 0.880402
Loss = 0.835724
Loss = 0.728622
Loss = 0.671982
Loss = 0.864075
Loss = 0.794449
Loss = 0.757675
Loss = 0.775009
Loss = 0.701294
Loss = 0.778915
Loss = 0.720718
Loss = 0.806946
Loss = 0.813461
Loss = 0.74881
TEST LOSS = 0.762524
TEST ACC = 462.209 % (7658/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.702805
Epoch 4.2: Loss = 0.679489
Epoch 4.3: Loss = 0.809433
Epoch 4.4: Loss = 0.665787
Epoch 4.5: Loss = 0.739441
Epoch 4.6: Loss = 0.797592
Epoch 4.7: Loss = 0.659988
Epoch 4.8: Loss = 0.882629
Epoch 4.9: Loss = 0.6082
Epoch 4.10: Loss = 0.531326
Epoch 4.11: Loss = 0.792725
Epoch 4.12: Loss = 0.696274
Epoch 4.13: Loss = 0.756287
Epoch 4.14: Loss = 0.74556
Epoch 4.15: Loss = 0.691742
Epoch 4.16: Loss = 0.780685
Epoch 4.17: Loss = 0.642975
Epoch 4.18: Loss = 0.736465
Epoch 4.19: Loss = 0.689163
Epoch 4.20: Loss = 0.7948
Epoch 4.21: Loss = 0.660873
Epoch 4.22: Loss = 0.614258
Epoch 4.23: Loss = 0.793869
Epoch 4.24: Loss = 0.808151
Epoch 4.25: Loss = 0.755341
Epoch 4.26: Loss = 0.607422
Epoch 4.27: Loss = 0.680847
Epoch 4.28: Loss = 0.795502
Epoch 4.29: Loss = 0.74791
Epoch 4.30: Loss = 0.688446
Epoch 4.31: Loss = 0.817123
Epoch 4.32: Loss = 0.695862
Epoch 4.33: Loss = 0.667374
Epoch 4.34: Loss = 0.822998
Epoch 4.35: Loss = 0.71759
Epoch 4.36: Loss = 0.825104
Epoch 4.37: Loss = 0.791504
Epoch 4.38: Loss = 0.767258
Epoch 4.39: Loss = 0.805389
Epoch 4.40: Loss = 0.703552
Epoch 4.41: Loss = 0.795822
Epoch 4.42: Loss = 0.749359
Epoch 4.43: Loss = 0.73291
Epoch 4.44: Loss = 0.592422
Epoch 4.45: Loss = 0.754059
Epoch 4.46: Loss = 0.792252
Epoch 4.47: Loss = 0.71344
Epoch 4.48: Loss = 0.693176
Epoch 4.49: Loss = 0.782654
Epoch 4.50: Loss = 0.756668
Epoch 4.51: Loss = 0.643967
Epoch 4.52: Loss = 0.808533
Epoch 4.53: Loss = 0.811371
Epoch 4.54: Loss = 0.622589
Epoch 4.55: Loss = 0.77327
Epoch 4.56: Loss = 0.832718
Epoch 4.57: Loss = 0.826523
Epoch 4.58: Loss = 0.718521
Epoch 4.59: Loss = 0.776871
Epoch 4.60: Loss = 0.814377
Epoch 4.61: Loss = 0.710114
Epoch 4.62: Loss = 0.78244
Epoch 4.63: Loss = 0.617401
Epoch 4.64: Loss = 0.651505
Epoch 4.65: Loss = 0.732773
Epoch 4.66: Loss = 0.681854
Epoch 4.67: Loss = 0.730377
Epoch 4.68: Loss = 0.886673
Epoch 4.69: Loss = 0.772552
Epoch 4.70: Loss = 0.730896
Epoch 4.71: Loss = 0.585571
Epoch 4.72: Loss = 0.755814
Epoch 4.73: Loss = 0.830414
Epoch 4.74: Loss = 0.771103
Epoch 4.75: Loss = 0.698883
Epoch 4.76: Loss = 0.72142
Epoch 4.77: Loss = 0.707779
Epoch 4.78: Loss = 0.717926
Epoch 4.79: Loss = 0.719147
Epoch 4.80: Loss = 0.713806
Epoch 4.81: Loss = 0.676834
Epoch 4.82: Loss = 0.672958
Epoch 4.83: Loss = 0.789017
Epoch 4.84: Loss = 0.706329
Epoch 4.85: Loss = 0.766052
Epoch 4.86: Loss = 0.752258
Epoch 4.87: Loss = 0.733902
Epoch 4.88: Loss = 0.66832
Epoch 4.89: Loss = 0.801315
Epoch 4.90: Loss = 0.785873
Epoch 4.91: Loss = 0.805893
Epoch 4.92: Loss = 0.75795
Epoch 4.93: Loss = 0.809631
Epoch 4.94: Loss = 0.72525
Epoch 4.95: Loss = 0.773605
Epoch 4.96: Loss = 0.701996
Epoch 4.97: Loss = 0.619568
Epoch 4.98: Loss = 0.739517
Epoch 4.99: Loss = 0.741119
Epoch 4.100: Loss = 0.752716
Epoch 4.101: Loss = 0.772751
Epoch 4.102: Loss = 0.815445
Epoch 4.103: Loss = 0.70343
Epoch 4.104: Loss = 0.659927
Epoch 4.105: Loss = 0.69487
Epoch 4.106: Loss = 0.859329
Epoch 4.107: Loss = 0.756256
Epoch 4.108: Loss = 0.832138
Epoch 4.109: Loss = 0.781662
Epoch 4.110: Loss = 0.728027
Epoch 4.111: Loss = 0.657043
Epoch 4.112: Loss = 0.715012
Epoch 4.113: Loss = 0.728012
Epoch 4.114: Loss = 0.784973
Epoch 4.115: Loss = 0.743958
Epoch 4.116: Loss = 0.674591
Epoch 4.117: Loss = 0.785995
Epoch 4.118: Loss = 0.748001
Epoch 4.119: Loss = 0.7155
Epoch 4.120: Loss = 0.70694
TRAIN LOSS = 0.736053
TRAIN ACC = 77.8397 % (46706/60000)
Loss = 0.695923
Loss = 0.836517
Loss = 0.742996
Loss = 0.701324
Loss = 0.743515
Loss = 0.834091
Loss = 0.929642
Loss = 0.832245
Loss = 0.768372
Loss = 0.701981
Loss = 0.893951
Loss = 0.860001
Loss = 0.798172
Loss = 0.808945
Loss = 0.756393
Loss = 0.800766
Loss = 0.747192
Loss = 0.844498
Loss = 0.86824
Loss = 0.776688
TEST LOSS = 0.797072
TEST ACC = 467.059 % (7722/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.766327
Epoch 5.2: Loss = 0.747681
Epoch 5.3: Loss = 0.750351
Epoch 5.4: Loss = 0.694183
Epoch 5.5: Loss = 0.738617
Epoch 5.6: Loss = 0.824982
Epoch 5.7: Loss = 0.738602
Epoch 5.8: Loss = 0.877182
Epoch 5.9: Loss = 0.615143
Epoch 5.10: Loss = 0.533463
Epoch 5.11: Loss = 0.812408
Epoch 5.12: Loss = 0.723999
Epoch 5.13: Loss = 0.7677
Epoch 5.14: Loss = 0.761368
Epoch 5.15: Loss = 0.745941
Epoch 5.16: Loss = 0.797729
Epoch 5.17: Loss = 0.681534
Epoch 5.18: Loss = 0.743881
Epoch 5.19: Loss = 0.715912
Epoch 5.20: Loss = 0.831284
Epoch 5.21: Loss = 0.68457
Epoch 5.22: Loss = 0.608582
Epoch 5.23: Loss = 0.780609
Epoch 5.24: Loss = 0.866898
Epoch 5.25: Loss = 0.733963
Epoch 5.26: Loss = 0.595764
Epoch 5.27: Loss = 0.66713
Epoch 5.28: Loss = 0.806168
Epoch 5.29: Loss = 0.760895
Epoch 5.30: Loss = 0.68013
Epoch 5.31: Loss = 0.814911
Epoch 5.32: Loss = 0.692261
Epoch 5.33: Loss = 0.657272
Epoch 5.34: Loss = 0.811707
Epoch 5.35: Loss = 0.777679
Epoch 5.36: Loss = 0.880066
Epoch 5.37: Loss = 0.819855
Epoch 5.38: Loss = 0.742249
Epoch 5.39: Loss = 0.827744
Epoch 5.40: Loss = 0.713577
Epoch 5.41: Loss = 0.797516
Epoch 5.42: Loss = 0.745499
Epoch 5.43: Loss = 0.745346
Epoch 5.44: Loss = 0.570847
Epoch 5.45: Loss = 0.829132
Epoch 5.46: Loss = 0.806152
Epoch 5.47: Loss = 0.741287
Epoch 5.48: Loss = 0.732986
Epoch 5.49: Loss = 0.799393
Epoch 5.50: Loss = 0.780563
Epoch 5.51: Loss = 0.635803
Epoch 5.52: Loss = 0.81279
Epoch 5.53: Loss = 0.87001
Epoch 5.54: Loss = 0.65593
Epoch 5.55: Loss = 0.799683
Epoch 5.56: Loss = 0.828323
Epoch 5.57: Loss = 0.835007
Epoch 5.58: Loss = 0.727203
Epoch 5.59: Loss = 0.811646
Epoch 5.60: Loss = 0.886673
Epoch 5.61: Loss = 0.716965
Epoch 5.62: Loss = 0.837891
Epoch 5.63: Loss = 0.659531
Epoch 5.64: Loss = 0.662796
Epoch 5.65: Loss = 0.800064
Epoch 5.66: Loss = 0.660141
Epoch 5.67: Loss = 0.769562
Epoch 5.68: Loss = 0.948715
Epoch 5.69: Loss = 0.748184
Epoch 5.70: Loss = 0.75351
Epoch 5.71: Loss = 0.583023
Epoch 5.72: Loss = 0.8069
Epoch 5.73: Loss = 0.880936
Epoch 5.74: Loss = 0.827103
Epoch 5.75: Loss = 0.751266
Epoch 5.76: Loss = 0.733505
Epoch 5.77: Loss = 0.698578
Epoch 5.78: Loss = 0.700638
Epoch 5.79: Loss = 0.715652
Epoch 5.80: Loss = 0.786331
Epoch 5.81: Loss = 0.719452
Epoch 5.82: Loss = 0.709839
Epoch 5.83: Loss = 0.790787
Epoch 5.84: Loss = 0.661469
Epoch 5.85: Loss = 0.77916
Epoch 5.86: Loss = 0.766083
Epoch 5.87: Loss = 0.697662
Epoch 5.88: Loss = 0.645523
Epoch 5.89: Loss = 0.820374
Epoch 5.90: Loss = 0.774597
Epoch 5.91: Loss = 0.866013
Epoch 5.92: Loss = 0.805054
Epoch 5.93: Loss = 0.84935
Epoch 5.94: Loss = 0.772995
Epoch 5.95: Loss = 0.822311
Epoch 5.96: Loss = 0.774704
Epoch 5.97: Loss = 0.598907
Epoch 5.98: Loss = 0.757767
Epoch 5.99: Loss = 0.770294
Epoch 5.100: Loss = 0.791809
Epoch 5.101: Loss = 0.82402
Epoch 5.102: Loss = 0.845093
Epoch 5.103: Loss = 0.759628
Epoch 5.104: Loss = 0.682693
Epoch 5.105: Loss = 0.664719
Epoch 5.106: Loss = 0.925293
Epoch 5.107: Loss = 0.834518
Epoch 5.108: Loss = 0.889145
Epoch 5.109: Loss = 0.82132
Epoch 5.110: Loss = 0.790161
Epoch 5.111: Loss = 0.77446
Epoch 5.112: Loss = 0.777252
Epoch 5.113: Loss = 0.747208
Epoch 5.114: Loss = 0.862152
Epoch 5.115: Loss = 0.812241
Epoch 5.116: Loss = 0.718597
Epoch 5.117: Loss = 0.88118
Epoch 5.118: Loss = 0.813477
Epoch 5.119: Loss = 0.747177
Epoch 5.120: Loss = 0.773178
TRAIN LOSS = 0.761353
TRAIN ACC = 78.1891 % (46916/60000)
Loss = 0.707382
Loss = 0.905777
Loss = 0.775711
Loss = 0.774399
Loss = 0.820282
Loss = 0.91185
Loss = 0.964172
Loss = 0.907761
Loss = 0.857117
Loss = 0.693954
Loss = 0.987656
Loss = 0.859161
Loss = 0.853302
Loss = 0.844696
Loss = 0.778351
Loss = 0.865982
Loss = 0.806915
Loss = 0.894913
Loss = 0.911026
Loss = 0.879654
TEST LOSS = 0.850003
TEST ACC = 469.159 % (7741/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.778732
Epoch 6.2: Loss = 0.800522
Epoch 6.3: Loss = 0.834305
Epoch 6.4: Loss = 0.756622
Epoch 6.5: Loss = 0.770554
Epoch 6.6: Loss = 0.952606
Epoch 6.7: Loss = 0.777344
Epoch 6.8: Loss = 0.997604
Epoch 6.9: Loss = 0.641693
Epoch 6.10: Loss = 0.590286
Epoch 6.11: Loss = 0.923035
Epoch 6.12: Loss = 0.808884
Epoch 6.13: Loss = 0.827759
Epoch 6.14: Loss = 0.836411
Epoch 6.15: Loss = 0.786789
Epoch 6.16: Loss = 0.8703
Epoch 6.17: Loss = 0.695313
Epoch 6.18: Loss = 0.828552
Epoch 6.19: Loss = 0.76947
Epoch 6.20: Loss = 0.860367
Epoch 6.21: Loss = 0.708496
Epoch 6.22: Loss = 0.693436
Epoch 6.23: Loss = 0.804321
Epoch 6.24: Loss = 0.933151
Epoch 6.25: Loss = 0.765915
Epoch 6.26: Loss = 0.624405
Epoch 6.27: Loss = 0.738953
Epoch 6.28: Loss = 0.859253
Epoch 6.29: Loss = 0.804214
Epoch 6.30: Loss = 0.765091
Epoch 6.31: Loss = 0.872437
Epoch 6.32: Loss = 0.709641
Epoch 6.33: Loss = 0.684845
Epoch 6.34: Loss = 0.909973
Epoch 6.35: Loss = 0.795532
Epoch 6.36: Loss = 0.949814
Epoch 6.37: Loss = 0.862015
Epoch 6.38: Loss = 0.838974
Epoch 6.39: Loss = 0.856079
Epoch 6.40: Loss = 0.78125
Epoch 6.41: Loss = 0.841766
Epoch 6.42: Loss = 0.793518
Epoch 6.43: Loss = 0.783188
Epoch 6.44: Loss = 0.60817
Epoch 6.45: Loss = 0.876831
Epoch 6.46: Loss = 0.818604
Epoch 6.47: Loss = 0.769974
Epoch 6.48: Loss = 0.718155
Epoch 6.49: Loss = 0.756683
Epoch 6.50: Loss = 0.847031
Epoch 6.51: Loss = 0.67041
Epoch 6.52: Loss = 0.858795
Epoch 6.53: Loss = 0.941727
Epoch 6.54: Loss = 0.604919
Epoch 6.55: Loss = 0.849808
Epoch 6.56: Loss = 0.894119
Epoch 6.57: Loss = 0.897415
Epoch 6.58: Loss = 0.768539
Epoch 6.59: Loss = 0.85141
Epoch 6.60: Loss = 0.876556
Epoch 6.61: Loss = 0.773819
Epoch 6.62: Loss = 0.919724
Epoch 6.63: Loss = 0.68869
Epoch 6.64: Loss = 0.672348
Epoch 6.65: Loss = 0.85379
Epoch 6.66: Loss = 0.693573
Epoch 6.67: Loss = 0.808075
Epoch 6.68: Loss = 1.00699
Epoch 6.69: Loss = 0.826523
Epoch 6.70: Loss = 0.849854
Epoch 6.71: Loss = 0.601654
Epoch 6.72: Loss = 0.872269
Epoch 6.73: Loss = 0.937668
Epoch 6.74: Loss = 0.887848
Epoch 6.75: Loss = 0.784805
Epoch 6.76: Loss = 0.805298
Epoch 6.77: Loss = 0.726288
Epoch 6.78: Loss = 0.769653
Epoch 6.79: Loss = 0.739594
Epoch 6.80: Loss = 0.839584
Epoch 6.81: Loss = 0.785263
Epoch 6.82: Loss = 0.747894
Epoch 6.83: Loss = 0.884415
Epoch 6.84: Loss = 0.735794
Epoch 6.85: Loss = 0.773254
Epoch 6.86: Loss = 0.834167
Epoch 6.87: Loss = 0.734756
Epoch 6.88: Loss = 0.656845
Epoch 6.89: Loss = 0.849274
Epoch 6.90: Loss = 0.791595
Epoch 6.91: Loss = 0.903214
Epoch 6.92: Loss = 0.833328
Epoch 6.93: Loss = 0.832947
Epoch 6.94: Loss = 0.770935
Epoch 6.95: Loss = 0.801956
Epoch 6.96: Loss = 0.785645
Epoch 6.97: Loss = 0.627228
Epoch 6.98: Loss = 0.705627
Epoch 6.99: Loss = 0.785278
Epoch 6.100: Loss = 0.843384
Epoch 6.101: Loss = 0.838837
Epoch 6.102: Loss = 0.854767
Epoch 6.103: Loss = 0.785416
Epoch 6.104: Loss = 0.70813
Epoch 6.105: Loss = 0.684509
Epoch 6.106: Loss = 0.874069
Epoch 6.107: Loss = 0.867096
Epoch 6.108: Loss = 0.912842
Epoch 6.109: Loss = 0.835037
Epoch 6.110: Loss = 0.79509
Epoch 6.111: Loss = 0.787537
Epoch 6.112: Loss = 0.776382
Epoch 6.113: Loss = 0.851151
Epoch 6.114: Loss = 0.828094
Epoch 6.115: Loss = 0.808441
Epoch 6.116: Loss = 0.726028
Epoch 6.117: Loss = 0.79126
Epoch 6.118: Loss = 0.789139
Epoch 6.119: Loss = 0.733994
Epoch 6.120: Loss = 0.765518
TRAIN LOSS = 0.798737
TRAIN ACC = 78.5034 % (47104/60000)
Loss = 0.659897
Loss = 0.849747
Loss = 0.737289
Loss = 0.715851
Loss = 0.785355
Loss = 0.941177
Loss = 0.957199
Loss = 0.892975
Loss = 0.822342
Loss = 0.638123
Loss = 0.990433
Loss = 0.904327
Loss = 0.818192
Loss = 0.802994
Loss = 0.783783
Loss = 0.79686
Loss = 0.802032
Loss = 0.893631
Loss = 0.915298
Loss = 0.81366
TEST LOSS = 0.826058
TEST ACC = 471.039 % (7826/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.79718
Epoch 7.2: Loss = 0.742477
Epoch 7.3: Loss = 0.880447
Epoch 7.4: Loss = 0.726028
Epoch 7.5: Loss = 0.773727
Epoch 7.6: Loss = 0.948456
Epoch 7.7: Loss = 0.748123
Epoch 7.8: Loss = 0.927505
Epoch 7.9: Loss = 0.640961
Epoch 7.10: Loss = 0.587128
Epoch 7.11: Loss = 0.873932
Epoch 7.12: Loss = 0.801697
Epoch 7.13: Loss = 0.856842
Epoch 7.14: Loss = 0.832123
Epoch 7.15: Loss = 0.796997
Epoch 7.16: Loss = 0.901077
Epoch 7.17: Loss = 0.688599
Epoch 7.18: Loss = 0.845184
Epoch 7.19: Loss = 0.757935
Epoch 7.20: Loss = 0.879166
Epoch 7.21: Loss = 0.654587
Epoch 7.22: Loss = 0.712646
Epoch 7.23: Loss = 0.836533
Epoch 7.24: Loss = 0.979233
Epoch 7.25: Loss = 0.778275
Epoch 7.26: Loss = 0.638535
Epoch 7.27: Loss = 0.737991
Epoch 7.28: Loss = 0.854691
Epoch 7.29: Loss = 0.784668
Epoch 7.30: Loss = 0.726059
Epoch 7.31: Loss = 0.864655
Epoch 7.32: Loss = 0.742767
Epoch 7.33: Loss = 0.667587
Epoch 7.34: Loss = 0.872711
Epoch 7.35: Loss = 0.846313
Epoch 7.36: Loss = 0.90509
Epoch 7.37: Loss = 0.848785
Epoch 7.38: Loss = 0.772827
Epoch 7.39: Loss = 0.894608
Epoch 7.40: Loss = 0.76474
Epoch 7.41: Loss = 0.836014
Epoch 7.42: Loss = 0.835434
Epoch 7.43: Loss = 0.810242
Epoch 7.44: Loss = 0.656113
Epoch 7.45: Loss = 0.947372
Epoch 7.46: Loss = 0.889969
Epoch 7.47: Loss = 0.77562
Epoch 7.48: Loss = 0.765549
Epoch 7.49: Loss = 0.756638
Epoch 7.50: Loss = 0.836731
Epoch 7.51: Loss = 0.687103
Epoch 7.52: Loss = 0.821976
Epoch 7.53: Loss = 0.923721
Epoch 7.54: Loss = 0.666321
Epoch 7.55: Loss = 0.860352
Epoch 7.56: Loss = 0.885422
Epoch 7.57: Loss = 0.893433
Epoch 7.58: Loss = 0.767975
Epoch 7.59: Loss = 0.857895
Epoch 7.60: Loss = 0.943726
Epoch 7.61: Loss = 0.757935
Epoch 7.62: Loss = 0.834061
Epoch 7.63: Loss = 0.73436
Epoch 7.64: Loss = 0.677155
Epoch 7.65: Loss = 0.844208
Epoch 7.66: Loss = 0.695526
Epoch 7.67: Loss = 0.815536
Epoch 7.68: Loss = 1.00113
Epoch 7.69: Loss = 0.756485
Epoch 7.70: Loss = 0.852509
Epoch 7.71: Loss = 0.60257
Epoch 7.72: Loss = 0.89946
Epoch 7.73: Loss = 0.946991
Epoch 7.74: Loss = 0.838913
Epoch 7.75: Loss = 0.791977
Epoch 7.76: Loss = 0.763809
Epoch 7.77: Loss = 0.754044
Epoch 7.78: Loss = 0.747742
Epoch 7.79: Loss = 0.823288
Epoch 7.80: Loss = 0.865891
Epoch 7.81: Loss = 0.811005
Epoch 7.82: Loss = 0.731842
Epoch 7.83: Loss = 0.861389
Epoch 7.84: Loss = 0.723526
Epoch 7.85: Loss = 0.783691
Epoch 7.86: Loss = 0.874802
Epoch 7.87: Loss = 0.773544
Epoch 7.88: Loss = 0.65303
Epoch 7.89: Loss = 0.931534
Epoch 7.90: Loss = 0.775787
Epoch 7.91: Loss = 0.893433
Epoch 7.92: Loss = 0.859726
Epoch 7.93: Loss = 0.838394
Epoch 7.94: Loss = 0.784775
Epoch 7.95: Loss = 0.873566
Epoch 7.96: Loss = 0.8358
Epoch 7.97: Loss = 0.640411
Epoch 7.98: Loss = 0.773041
Epoch 7.99: Loss = 0.868668
Epoch 7.100: Loss = 0.847656
Epoch 7.101: Loss = 0.912079
Epoch 7.102: Loss = 0.895508
Epoch 7.103: Loss = 0.861557
Epoch 7.104: Loss = 0.761993
Epoch 7.105: Loss = 0.67247
Epoch 7.106: Loss = 0.837875
Epoch 7.107: Loss = 0.853668
Epoch 7.108: Loss = 0.94014
Epoch 7.109: Loss = 0.821838
Epoch 7.110: Loss = 0.835327
Epoch 7.111: Loss = 0.838257
Epoch 7.112: Loss = 0.799728
Epoch 7.113: Loss = 0.930511
Epoch 7.114: Loss = 0.895248
Epoch 7.115: Loss = 0.883316
Epoch 7.116: Loss = 0.745071
Epoch 7.117: Loss = 0.931305
Epoch 7.118: Loss = 0.828613
Epoch 7.119: Loss = 0.784653
Epoch 7.120: Loss = 0.787628
TRAIN LOSS = 0.810532
TRAIN ACC = 78.2196 % (46934/60000)
Loss = 0.686188
Loss = 0.972061
Loss = 0.792969
Loss = 0.725998
Loss = 0.801529
Loss = 1.00987
Loss = 0.967804
Loss = 0.955765
Loss = 0.862579
Loss = 0.748322
Loss = 1.0013
Loss = 0.962204
Loss = 0.848022
Loss = 0.83252
Loss = 0.855026
Loss = 0.868149
Loss = 0.839645
Loss = 0.93866
Loss = 0.905701
Loss = 0.912262
TEST LOSS = 0.874328
TEST ACC = 469.339 % (7794/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.825546
Epoch 8.2: Loss = 0.759995
Epoch 8.3: Loss = 0.898361
Epoch 8.4: Loss = 0.82428
Epoch 8.5: Loss = 0.905106
Epoch 8.6: Loss = 0.985413
Epoch 8.7: Loss = 0.774429
Epoch 8.8: Loss = 0.976593
Epoch 8.9: Loss = 0.710587
Epoch 8.10: Loss = 0.609436
Epoch 8.11: Loss = 0.979523
Epoch 8.12: Loss = 0.877365
Epoch 8.13: Loss = 0.8927
Epoch 8.14: Loss = 0.851929
Epoch 8.15: Loss = 0.877014
Epoch 8.16: Loss = 0.921127
Epoch 8.17: Loss = 0.708664
Epoch 8.18: Loss = 0.881882
Epoch 8.19: Loss = 0.811401
Epoch 8.20: Loss = 0.903473
Epoch 8.21: Loss = 0.71524
Epoch 8.22: Loss = 0.777069
Epoch 8.23: Loss = 0.855011
Epoch 8.24: Loss = 0.941559
Epoch 8.25: Loss = 0.814682
Epoch 8.26: Loss = 0.697144
Epoch 8.27: Loss = 0.7323
Epoch 8.28: Loss = 0.857437
Epoch 8.29: Loss = 0.853043
Epoch 8.30: Loss = 0.773285
Epoch 8.31: Loss = 0.902069
Epoch 8.32: Loss = 0.786255
Epoch 8.33: Loss = 0.773193
Epoch 8.34: Loss = 0.934235
Epoch 8.35: Loss = 0.899979
Epoch 8.36: Loss = 0.988358
Epoch 8.37: Loss = 0.85466
Epoch 8.38: Loss = 0.85051
Epoch 8.39: Loss = 0.989044
Epoch 8.40: Loss = 0.858078
Epoch 8.41: Loss = 0.922745
Epoch 8.42: Loss = 0.883209
Epoch 8.43: Loss = 0.858917
Epoch 8.44: Loss = 0.656555
Epoch 8.45: Loss = 0.96785
Epoch 8.46: Loss = 0.890549
Epoch 8.47: Loss = 0.818222
Epoch 8.48: Loss = 0.731903
Epoch 8.49: Loss = 0.909256
Epoch 8.50: Loss = 0.895111
Epoch 8.51: Loss = 0.721298
Epoch 8.52: Loss = 0.904327
Epoch 8.53: Loss = 0.927719
Epoch 8.54: Loss = 0.76062
Epoch 8.55: Loss = 1.03174
Epoch 8.56: Loss = 0.942688
Epoch 8.57: Loss = 0.970123
Epoch 8.58: Loss = 0.880447
Epoch 8.59: Loss = 0.908798
Epoch 8.60: Loss = 0.950104
Epoch 8.61: Loss = 0.780609
Epoch 8.62: Loss = 0.920898
Epoch 8.63: Loss = 0.750015
Epoch 8.64: Loss = 0.76886
Epoch 8.65: Loss = 0.930252
Epoch 8.66: Loss = 0.779282
Epoch 8.67: Loss = 0.839096
Epoch 8.68: Loss = 1.0713
Epoch 8.69: Loss = 0.815765
Epoch 8.70: Loss = 0.87883
Epoch 8.71: Loss = 0.647308
Epoch 8.72: Loss = 0.914368
Epoch 8.73: Loss = 1.04204
Epoch 8.74: Loss = 0.934143
Epoch 8.75: Loss = 0.845627
Epoch 8.76: Loss = 0.813141
Epoch 8.77: Loss = 0.802002
Epoch 8.78: Loss = 0.742432
Epoch 8.79: Loss = 0.883942
Epoch 8.80: Loss = 0.868759
Epoch 8.81: Loss = 0.785202
Epoch 8.82: Loss = 0.737595
Epoch 8.83: Loss = 0.844406
Epoch 8.84: Loss = 0.731842
Epoch 8.85: Loss = 0.800278
Epoch 8.86: Loss = 0.88031
Epoch 8.87: Loss = 0.774857
Epoch 8.88: Loss = 0.696655
Epoch 8.89: Loss = 0.853531
Epoch 8.90: Loss = 0.811569
Epoch 8.91: Loss = 0.918076
Epoch 8.92: Loss = 0.891479
Epoch 8.93: Loss = 0.844376
Epoch 8.94: Loss = 0.79306
Epoch 8.95: Loss = 0.934906
Epoch 8.96: Loss = 0.886063
Epoch 8.97: Loss = 0.65361
Epoch 8.98: Loss = 0.74588
Epoch 8.99: Loss = 0.843002
Epoch 8.100: Loss = 0.87706
Epoch 8.101: Loss = 0.922714
Epoch 8.102: Loss = 0.928818
Epoch 8.103: Loss = 0.883545
Epoch 8.104: Loss = 0.762344
Epoch 8.105: Loss = 0.696198
Epoch 8.106: Loss = 0.885452
Epoch 8.107: Loss = 0.857391
Epoch 8.108: Loss = 0.974503
Epoch 8.109: Loss = 0.976028
Epoch 8.110: Loss = 0.8535
Epoch 8.111: Loss = 0.82515
Epoch 8.112: Loss = 0.878357
Epoch 8.113: Loss = 0.947205
Epoch 8.114: Loss = 0.879028
Epoch 8.115: Loss = 0.893921
Epoch 8.116: Loss = 0.787842
Epoch 8.117: Loss = 0.974731
Epoch 8.118: Loss = 0.862961
Epoch 8.119: Loss = 0.764481
Epoch 8.120: Loss = 0.795456
TRAIN LOSS = 0.850372
TRAIN ACC = 78.3142 % (46991/60000)
Loss = 0.714066
Loss = 0.964111
Loss = 0.793274
Loss = 0.780426
Loss = 0.837189
Loss = 0.937302
Loss = 1.01176
Loss = 0.929916
Loss = 0.919418
Loss = 0.779465
Loss = 1.01132
Loss = 0.98085
Loss = 0.953369
Loss = 0.905502
Loss = 0.849426
Loss = 0.867157
Loss = 0.870956
Loss = 0.930466
Loss = 0.911407
Loss = 0.915268
TEST LOSS = 0.893133
TEST ACC = 469.91 % (7849/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.781189
Epoch 9.2: Loss = 0.798828
Epoch 9.3: Loss = 0.965652
Epoch 9.4: Loss = 0.742386
Epoch 9.5: Loss = 0.898315
Epoch 9.6: Loss = 0.992615
Epoch 9.7: Loss = 0.794434
Epoch 9.8: Loss = 0.88652
Epoch 9.9: Loss = 0.676086
Epoch 9.10: Loss = 0.591293
Epoch 9.11: Loss = 0.988907
Epoch 9.12: Loss = 0.861801
Epoch 9.13: Loss = 0.838913
Epoch 9.14: Loss = 0.879898
Epoch 9.15: Loss = 0.885101
Epoch 9.16: Loss = 0.994431
Epoch 9.17: Loss = 0.724823
Epoch 9.18: Loss = 0.856827
Epoch 9.19: Loss = 0.741135
Epoch 9.20: Loss = 0.884689
Epoch 9.21: Loss = 0.743927
Epoch 9.22: Loss = 0.772385
Epoch 9.23: Loss = 0.948196
Epoch 9.24: Loss = 1.07236
Epoch 9.25: Loss = 0.813263
Epoch 9.26: Loss = 0.719925
Epoch 9.27: Loss = 0.771896
Epoch 9.28: Loss = 0.9039
Epoch 9.29: Loss = 0.7854
Epoch 9.30: Loss = 0.775803
Epoch 9.31: Loss = 0.911331
Epoch 9.32: Loss = 0.806503
Epoch 9.33: Loss = 0.742493
Epoch 9.34: Loss = 0.857376
Epoch 9.35: Loss = 0.856537
Epoch 9.36: Loss = 1.03917
Epoch 9.37: Loss = 0.852646
Epoch 9.38: Loss = 0.822479
Epoch 9.39: Loss = 0.928207
Epoch 9.40: Loss = 0.856186
Epoch 9.41: Loss = 0.895844
Epoch 9.42: Loss = 0.900162
Epoch 9.43: Loss = 0.903885
Epoch 9.44: Loss = 0.630249
Epoch 9.45: Loss = 0.941406
Epoch 9.46: Loss = 0.862289
Epoch 9.47: Loss = 0.884613
Epoch 9.48: Loss = 0.806351
Epoch 9.49: Loss = 0.87735
Epoch 9.50: Loss = 0.882935
Epoch 9.51: Loss = 0.739395
Epoch 9.52: Loss = 0.897202
Epoch 9.53: Loss = 0.915848
Epoch 9.54: Loss = 0.769592
Epoch 9.55: Loss = 0.898148
Epoch 9.56: Loss = 0.998474
Epoch 9.57: Loss = 1.01259
Epoch 9.58: Loss = 0.813202
Epoch 9.59: Loss = 0.924911
Epoch 9.60: Loss = 0.939896
Epoch 9.61: Loss = 0.79155
Epoch 9.62: Loss = 0.94426
Epoch 9.63: Loss = 0.832703
Epoch 9.64: Loss = 0.734726
Epoch 9.65: Loss = 0.916168
Epoch 9.66: Loss = 0.735184
Epoch 9.67: Loss = 29.8515
Epoch 9.68: Loss = 13.0003
Epoch 9.69: Loss = 8.01227
Epoch 9.70: Loss = 4.43617
Epoch 9.71: Loss = 3.95535
Epoch 9.72: Loss = 4.51253
Epoch 9.73: Loss = 4.70831
Epoch 9.74: Loss = 4.94395
Epoch 9.75: Loss = 3.81651
Epoch 9.76: Loss = 3.53937
Epoch 9.77: Loss = 3.07726
Epoch 9.78: Loss = 3.38753
Epoch 9.79: Loss = 2.68382
Epoch 9.80: Loss = 3.10629
Epoch 9.81: Loss = 2.46791
Epoch 9.82: Loss = 2.44537
Epoch 9.83: Loss = 2.92599
Epoch 9.84: Loss = 2.25269
Epoch 9.85: Loss = 2.15126
Epoch 9.86: Loss = 2.33261
Epoch 9.87: Loss = 2.25255
Epoch 9.88: Loss = 1.65077
Epoch 9.89: Loss = 1.88959
Epoch 9.90: Loss = 1.5782
Epoch 9.91: Loss = 1.62129
Epoch 9.92: Loss = 1.26118
Epoch 9.93: Loss = 1.13898
Epoch 9.94: Loss = 0.931671
Epoch 9.95: Loss = 1.01381
Epoch 9.96: Loss = 0.91008
Epoch 9.97: Loss = 0.681656
Epoch 9.98: Loss = 0.810471
Epoch 9.99: Loss = 0.94458
Epoch 9.100: Loss = 0.857803
Epoch 9.101: Loss = 0.936127
Epoch 9.102: Loss = 0.968674
Epoch 9.103: Loss = 0.963181
Epoch 9.104: Loss = 0.82959
Epoch 9.105: Loss = 0.772552
Epoch 9.106: Loss = 0.957092
Epoch 9.107: Loss = 0.949707
Epoch 9.108: Loss = 0.96785
Epoch 9.109: Loss = 0.996094
Epoch 9.110: Loss = 0.878448
Epoch 9.111: Loss = 0.797165
Epoch 9.112: Loss = 0.879623
Epoch 9.113: Loss = 0.961166
Epoch 9.114: Loss = 0.848495
Epoch 9.115: Loss = 0.939758
Epoch 9.116: Loss = 0.853714
Epoch 9.117: Loss = 1.01125
Epoch 9.118: Loss = 0.848099
Epoch 9.119: Loss = 0.772522
Epoch 9.120: Loss = 0.843063
TRAIN LOSS = 1.66118
TRAIN ACC = 76.8387 % (46105/60000)
Loss = 0.77977
Loss = 1.08328
Loss = 0.861145
Loss = 0.889587
Loss = 0.876129
Loss = 1.10275
Loss = 1.0154
Loss = 1.07198
Loss = 0.921951
Loss = 0.817963
Loss = 1.13602
Loss = 0.984589
Loss = 0.958084
Loss = 0.972565
Loss = 0.982361
Loss = 0.986038
Loss = 0.922836
Loss = 1.04425
Loss = 1.01511
Loss = 0.898026
TEST LOSS = 0.965991
TEST ACC = 461.049 % (7764/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.838654
Epoch 10.2: Loss = 0.807648
Epoch 10.3: Loss = 1.00513
Epoch 10.4: Loss = 0.857758
Epoch 10.5: Loss = 0.926788
Epoch 10.6: Loss = 0.997269
Epoch 10.7: Loss = 0.88765
Epoch 10.8: Loss = 0.948334
Epoch 10.9: Loss = 0.727173
Epoch 10.10: Loss = 0.652832
Epoch 10.11: Loss = 0.990128
Epoch 10.12: Loss = 0.938217
Epoch 10.13: Loss = 0.96582
Epoch 10.14: Loss = 0.91275
Epoch 10.15: Loss = 0.947449
Epoch 10.16: Loss = 1.06792
Epoch 10.17: Loss = 0.783569
Epoch 10.18: Loss = 0.968903
Epoch 10.19: Loss = 0.830215
Epoch 10.20: Loss = 0.990295
Epoch 10.21: Loss = 0.827072
Epoch 10.22: Loss = 0.726379
Epoch 10.23: Loss = 0.885025
Epoch 10.24: Loss = 0.977524
Epoch 10.25: Loss = 0.856094
Epoch 10.26: Loss = 0.726959
Epoch 10.27: Loss = 0.849655
Epoch 10.28: Loss = 1.00366
Epoch 10.29: Loss = 0.866135
Epoch 10.30: Loss = 0.901047
Epoch 10.31: Loss = 0.991623
Epoch 10.32: Loss = 0.817413
Epoch 10.33: Loss = 0.77301
Epoch 10.34: Loss = 0.932938
Epoch 10.35: Loss = 0.866791
Epoch 10.36: Loss = 0.961349
Epoch 10.37: Loss = 0.95134
Epoch 10.38: Loss = 0.871155
Epoch 10.39: Loss = 0.928467
Epoch 10.40: Loss = 0.881302
Epoch 10.41: Loss = 0.923462
Epoch 10.42: Loss = 0.904846
Epoch 10.43: Loss = 0.985291
Epoch 10.44: Loss = 0.766876
Epoch 10.45: Loss = 1.02396
Epoch 10.46: Loss = 0.938187
Epoch 10.47: Loss = 0.911682
Epoch 10.48: Loss = 0.929672
Epoch 10.49: Loss = 0.908707
Epoch 10.50: Loss = 0.943542
Epoch 10.51: Loss = 0.732468
Epoch 10.52: Loss = 0.93956
Epoch 10.53: Loss = 0.970657
Epoch 10.54: Loss = 0.726685
Epoch 10.55: Loss = 0.942291
Epoch 10.56: Loss = 0.96785
Epoch 10.57: Loss = 0.979172
Epoch 10.58: Loss = 0.794846
Epoch 10.59: Loss = 0.932571
Epoch 10.60: Loss = 0.955368
Epoch 10.61: Loss = 0.801346
Epoch 10.62: Loss = 0.96228
Epoch 10.63: Loss = 0.79332
Epoch 10.64: Loss = 0.740097
Epoch 10.65: Loss = 0.90773
Epoch 10.66: Loss = 0.792603
Epoch 10.67: Loss = 0.875565
Epoch 10.68: Loss = 1.12671
Epoch 10.69: Loss = 0.848587
Epoch 10.70: Loss = 0.876907
Epoch 10.71: Loss = 0.621796
Epoch 10.72: Loss = 0.931168
Epoch 10.73: Loss = 1.14594
Epoch 10.74: Loss = 0.912445
Epoch 10.75: Loss = 0.822266
Epoch 10.76: Loss = 0.82869
Epoch 10.77: Loss = 0.782578
Epoch 10.78: Loss = 0.826935
Epoch 10.79: Loss = 0.92926
Epoch 10.80: Loss = 0.856934
Epoch 10.81: Loss = 0.842148
Epoch 10.82: Loss = 0.833221
Epoch 10.83: Loss = 0.878357
Epoch 10.84: Loss = 0.80513
Epoch 10.85: Loss = 0.888092
Epoch 10.86: Loss = 0.897995
Epoch 10.87: Loss = 0.782394
Epoch 10.88: Loss = 0.745575
Epoch 10.89: Loss = 0.979507
Epoch 10.90: Loss = 0.872528
Epoch 10.91: Loss = 0.999405
Epoch 10.92: Loss = 0.95517
Epoch 10.93: Loss = 0.844467
Epoch 10.94: Loss = 0.829468
Epoch 10.95: Loss = 0.921204
Epoch 10.96: Loss = 0.854904
Epoch 10.97: Loss = 0.643951
Epoch 10.98: Loss = 0.800827
Epoch 10.99: Loss = 0.837845
Epoch 10.100: Loss = 0.856003
Epoch 10.101: Loss = 0.918442
Epoch 10.102: Loss = 0.957245
Epoch 10.103: Loss = 0.947021
Epoch 10.104: Loss = 0.785065
Epoch 10.105: Loss = 0.723236
Epoch 10.106: Loss = 0.928757
Epoch 10.107: Loss = 0.887833
Epoch 10.108: Loss = 0.906723
Epoch 10.109: Loss = 0.911331
Epoch 10.110: Loss = 0.897217
Epoch 10.111: Loss = 0.782806
Epoch 10.112: Loss = 0.894028
Epoch 10.113: Loss = 0.927109
Epoch 10.114: Loss = 0.826614
Epoch 10.115: Loss = 0.914215
Epoch 10.116: Loss = 0.759354
Epoch 10.117: Loss = 0.995117
Epoch 10.118: Loss = 0.867325
Epoch 10.119: Loss = 0.799271
Epoch 10.120: Loss = 0.830994
TRAIN LOSS = 0.881088
TRAIN ACC = 78.3066 % (46986/60000)
Loss = 0.79538
Loss = 1.04706
Loss = 0.846085
Loss = 0.858353
Loss = 0.88443
Loss = 1.024
Loss = 0.999298
Loss = 1.04008
Loss = 0.891693
Loss = 0.745041
Loss = 1.08806
Loss = 0.960541
Loss = 0.894394
Loss = 0.935974
Loss = 0.924454
Loss = 0.887863
Loss = 0.884201
Loss = 1.00507
Loss = 0.979858
Loss = 0.849762
TEST LOSS = 0.92708
TEST ACC = 469.859 % (7786/10000)

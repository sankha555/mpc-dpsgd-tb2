Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.3826
Epoch 1.2: Loss = 2.34438
Epoch 1.3: Loss = 2.26329
Epoch 1.4: Loss = 2.21167
Epoch 1.5: Loss = 2.1516
Epoch 1.6: Loss = 2.12016
Epoch 1.7: Loss = 2.08281
Epoch 1.8: Loss = 2.04079
Epoch 1.9: Loss = 1.99884
Epoch 1.10: Loss = 2.00304
Epoch 1.11: Loss = 1.9288
Epoch 1.12: Loss = 1.91574
Epoch 1.13: Loss = 1.84442
Epoch 1.14: Loss = 1.83655
Epoch 1.15: Loss = 1.80061
Epoch 1.16: Loss = 1.74072
Epoch 1.17: Loss = 1.74998
Epoch 1.18: Loss = 1.70612
Epoch 1.19: Loss = 1.61665
Epoch 1.20: Loss = 1.57356
Epoch 1.21: Loss = 1.5788
Epoch 1.22: Loss = 1.57896
Epoch 1.23: Loss = 1.56514
Epoch 1.24: Loss = 1.48616
Epoch 1.25: Loss = 1.45331
Epoch 1.26: Loss = 1.50552
Epoch 1.27: Loss = 1.4258
Epoch 1.28: Loss = 1.39648
Epoch 1.29: Loss = 1.34134
Epoch 1.30: Loss = 1.33969
Epoch 1.31: Loss = 1.33121
Epoch 1.32: Loss = 1.36275
Epoch 1.33: Loss = 1.23853
Epoch 1.34: Loss = 1.21503
Epoch 1.35: Loss = 1.21185
Epoch 1.36: Loss = 1.21387
Epoch 1.37: Loss = 1.19469
Epoch 1.38: Loss = 1.21371
Epoch 1.39: Loss = 1.11
Epoch 1.40: Loss = 1.09593
Epoch 1.41: Loss = 1.142
Epoch 1.42: Loss = 1.03539
Epoch 1.43: Loss = 1.07874
Epoch 1.44: Loss = 1.09492
Epoch 1.45: Loss = 1.02812
Epoch 1.46: Loss = 1.00134
Epoch 1.47: Loss = 1.0365
Epoch 1.48: Loss = 0.968872
Epoch 1.49: Loss = 0.963715
Epoch 1.50: Loss = 0.921387
Epoch 1.51: Loss = 0.948624
Epoch 1.52: Loss = 0.9104
Epoch 1.53: Loss = 0.988907
Epoch 1.54: Loss = 0.931396
Epoch 1.55: Loss = 0.908188
Epoch 1.56: Loss = 0.93071
Epoch 1.57: Loss = 0.945969
Epoch 1.58: Loss = 0.855179
Epoch 1.59: Loss = 0.875839
Epoch 1.60: Loss = 0.927658
Epoch 1.61: Loss = 0.822571
Epoch 1.62: Loss = 0.791061
Epoch 1.63: Loss = 0.840179
Epoch 1.64: Loss = 0.785477
Epoch 1.65: Loss = 0.827866
Epoch 1.66: Loss = 0.766052
Epoch 1.67: Loss = 0.850555
Epoch 1.68: Loss = 0.803192
Epoch 1.69: Loss = 0.81485
Epoch 1.70: Loss = 0.772446
Epoch 1.71: Loss = 0.749313
Epoch 1.72: Loss = 0.772766
Epoch 1.73: Loss = 0.765289
Epoch 1.74: Loss = 0.795578
Epoch 1.75: Loss = 0.742279
Epoch 1.76: Loss = 0.788269
Epoch 1.77: Loss = 0.713074
Epoch 1.78: Loss = 0.704147
Epoch 1.79: Loss = 0.68927
Epoch 1.80: Loss = 0.709457
Epoch 1.81: Loss = 0.733093
Epoch 1.82: Loss = 0.743378
Epoch 1.83: Loss = 0.608154
Epoch 1.84: Loss = 0.675446
Epoch 1.85: Loss = 0.711929
Epoch 1.86: Loss = 0.708221
Epoch 1.87: Loss = 0.66095
Epoch 1.88: Loss = 0.612503
Epoch 1.89: Loss = 0.686356
Epoch 1.90: Loss = 0.64679
Epoch 1.91: Loss = 0.674713
Epoch 1.92: Loss = 0.67482
Epoch 1.93: Loss = 0.699097
Epoch 1.94: Loss = 0.6418
Epoch 1.95: Loss = 0.61026
Epoch 1.96: Loss = 0.568695
Epoch 1.97: Loss = 0.668671
Epoch 1.98: Loss = 0.604141
Epoch 1.99: Loss = 0.645798
Epoch 1.100: Loss = 0.593613
Epoch 1.101: Loss = 0.586319
Epoch 1.102: Loss = 0.596741
Epoch 1.103: Loss = 0.64296
Epoch 1.104: Loss = 0.562454
Epoch 1.105: Loss = 0.600647
Epoch 1.106: Loss = 0.600204
Epoch 1.107: Loss = 0.604126
Epoch 1.108: Loss = 0.615601
Epoch 1.109: Loss = 0.592575
Epoch 1.110: Loss = 0.643204
Epoch 1.111: Loss = 0.610336
Epoch 1.112: Loss = 0.585541
Epoch 1.113: Loss = 0.56337
Epoch 1.114: Loss = 0.592346
Epoch 1.115: Loss = 0.58667
Epoch 1.116: Loss = 0.522751
Epoch 1.117: Loss = 0.584885
Epoch 1.118: Loss = 0.599091
Epoch 1.119: Loss = 0.521957
Epoch 1.120: Loss = 0.556137
TRAIN LOSS = 1.05089
TRAIN ACC = 71.4401 % (42866/60000)
Loss = 0.602478
Loss = 0.62738
Loss = 0.726364
Loss = 0.676468
Loss = 0.701416
Loss = 0.611984
Loss = 0.572418
Loss = 0.708267
Loss = 0.708054
Loss = 0.642166
Loss = 0.317917
Loss = 0.513885
Loss = 0.326691
Loss = 0.538834
Loss = 0.417969
Loss = 0.418777
Loss = 0.393158
Loss = 0.212189
Loss = 0.390823
Loss = 0.637573
TEST LOSS = 0.53724
TEST ACC = 428.659 % (8442/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.591904
Epoch 2.2: Loss = 0.573914
Epoch 2.3: Loss = 0.546402
Epoch 2.4: Loss = 0.521683
Epoch 2.5: Loss = 0.514236
Epoch 2.6: Loss = 0.566071
Epoch 2.7: Loss = 0.519348
Epoch 2.8: Loss = 0.52887
Epoch 2.9: Loss = 0.572662
Epoch 2.10: Loss = 0.590408
Epoch 2.11: Loss = 0.507111
Epoch 2.12: Loss = 0.580414
Epoch 2.13: Loss = 0.53392
Epoch 2.14: Loss = 0.55217
Epoch 2.15: Loss = 0.539078
Epoch 2.16: Loss = 0.578049
Epoch 2.17: Loss = 0.52449
Epoch 2.18: Loss = 0.533249
Epoch 2.19: Loss = 0.568253
Epoch 2.20: Loss = 0.510559
Epoch 2.21: Loss = 0.485214
Epoch 2.22: Loss = 0.557785
Epoch 2.23: Loss = 0.523254
Epoch 2.24: Loss = 0.540359
Epoch 2.25: Loss = 0.561981
Epoch 2.26: Loss = 0.469299
Epoch 2.27: Loss = 0.555008
Epoch 2.28: Loss = 0.447586
Epoch 2.29: Loss = 0.524918
Epoch 2.30: Loss = 0.505798
Epoch 2.31: Loss = 0.48497
Epoch 2.32: Loss = 0.493576
Epoch 2.33: Loss = 0.526215
Epoch 2.34: Loss = 0.443069
Epoch 2.35: Loss = 0.47728
Epoch 2.36: Loss = 0.616028
Epoch 2.37: Loss = 0.499863
Epoch 2.38: Loss = 0.491135
Epoch 2.39: Loss = 0.543442
Epoch 2.40: Loss = 0.495773
Epoch 2.41: Loss = 0.510895
Epoch 2.42: Loss = 0.502792
Epoch 2.43: Loss = 0.442078
Epoch 2.44: Loss = 0.451859
Epoch 2.45: Loss = 0.524841
Epoch 2.46: Loss = 0.519806
Epoch 2.47: Loss = 0.490387
Epoch 2.48: Loss = 0.527618
Epoch 2.49: Loss = 0.539978
Epoch 2.50: Loss = 0.514236
Epoch 2.51: Loss = 0.470505
Epoch 2.52: Loss = 0.504257
Epoch 2.53: Loss = 0.45488
Epoch 2.54: Loss = 0.431595
Epoch 2.55: Loss = 0.540894
Epoch 2.56: Loss = 0.417572
Epoch 2.57: Loss = 0.53772
Epoch 2.58: Loss = 0.477737
Epoch 2.59: Loss = 0.460114
Epoch 2.60: Loss = 0.505417
Epoch 2.61: Loss = 0.515442
Epoch 2.62: Loss = 0.511093
Epoch 2.63: Loss = 0.513062
Epoch 2.64: Loss = 0.442307
Epoch 2.65: Loss = 0.431458
Epoch 2.66: Loss = 0.455124
Epoch 2.67: Loss = 0.493225
Epoch 2.68: Loss = 0.54274
Epoch 2.69: Loss = 0.54686
Epoch 2.70: Loss = 0.435867
Epoch 2.71: Loss = 0.502945
Epoch 2.72: Loss = 0.537613
Epoch 2.73: Loss = 0.47142
Epoch 2.74: Loss = 0.500931
Epoch 2.75: Loss = 0.462616
Epoch 2.76: Loss = 0.533463
Epoch 2.77: Loss = 0.499542
Epoch 2.78: Loss = 0.48761
Epoch 2.79: Loss = 0.473038
Epoch 2.80: Loss = 0.495834
Epoch 2.81: Loss = 0.476532
Epoch 2.82: Loss = 0.464218
Epoch 2.83: Loss = 0.487701
Epoch 2.84: Loss = 0.504959
Epoch 2.85: Loss = 0.528091
Epoch 2.86: Loss = 0.44101
Epoch 2.87: Loss = 0.488403
Epoch 2.88: Loss = 0.406891
Epoch 2.89: Loss = 0.453339
Epoch 2.90: Loss = 0.468277
Epoch 2.91: Loss = 0.478561
Epoch 2.92: Loss = 0.48938
Epoch 2.93: Loss = 0.435776
Epoch 2.94: Loss = 0.421722
Epoch 2.95: Loss = 0.440201
Epoch 2.96: Loss = 0.460861
Epoch 2.97: Loss = 0.530579
Epoch 2.98: Loss = 0.471771
Epoch 2.99: Loss = 0.4189
Epoch 2.100: Loss = 0.470413
Epoch 2.101: Loss = 0.41153
Epoch 2.102: Loss = 0.441986
Epoch 2.103: Loss = 0.455765
Epoch 2.104: Loss = 0.405563
Epoch 2.105: Loss = 0.447327
Epoch 2.106: Loss = 0.461945
Epoch 2.107: Loss = 0.504669
Epoch 2.108: Loss = 0.514557
Epoch 2.109: Loss = 0.457901
Epoch 2.110: Loss = 0.431122
Epoch 2.111: Loss = 0.460831
Epoch 2.112: Loss = 0.510986
Epoch 2.113: Loss = 0.482651
Epoch 2.114: Loss = 0.437912
Epoch 2.115: Loss = 0.481461
Epoch 2.116: Loss = 0.415176
Epoch 2.117: Loss = 0.491974
Epoch 2.118: Loss = 0.434174
Epoch 2.119: Loss = 0.387085
Epoch 2.120: Loss = 0.4039
TRAIN LOSS = 0.493744
TRAIN ACC = 85.2905 % (51177/60000)
Loss = 0.444199
Loss = 0.52037
Loss = 0.596954
Loss = 0.562607
Loss = 0.587006
Loss = 0.483429
Loss = 0.428848
Loss = 0.596527
Loss = 0.569763
Loss = 0.515305
Loss = 0.215134
Loss = 0.404099
Loss = 0.272461
Loss = 0.415634
Loss = 0.2901
Loss = 0.327057
Loss = 0.278885
Loss = 0.1129
Loss = 0.283981
Loss = 0.560974
TEST LOSS = 0.423311
TEST ACC = 511.769 % (8736/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.432419
Epoch 3.2: Loss = 0.443771
Epoch 3.3: Loss = 0.379898
Epoch 3.4: Loss = 0.497177
Epoch 3.5: Loss = 0.429932
Epoch 3.6: Loss = 0.402588
Epoch 3.7: Loss = 0.439835
Epoch 3.8: Loss = 0.445557
Epoch 3.9: Loss = 0.5112
Epoch 3.10: Loss = 0.450607
Epoch 3.11: Loss = 0.446655
Epoch 3.12: Loss = 0.475632
Epoch 3.13: Loss = 0.478012
Epoch 3.14: Loss = 0.443237
Epoch 3.15: Loss = 0.430191
Epoch 3.16: Loss = 0.487488
Epoch 3.17: Loss = 0.468872
Epoch 3.18: Loss = 0.430939
Epoch 3.19: Loss = 0.540436
Epoch 3.20: Loss = 0.385437
Epoch 3.21: Loss = 0.455353
Epoch 3.22: Loss = 0.438339
Epoch 3.23: Loss = 0.504639
Epoch 3.24: Loss = 0.37767
Epoch 3.25: Loss = 0.391907
Epoch 3.26: Loss = 0.44545
Epoch 3.27: Loss = 0.431412
Epoch 3.28: Loss = 0.396149
Epoch 3.29: Loss = 0.414108
Epoch 3.30: Loss = 0.382919
Epoch 3.31: Loss = 0.527832
Epoch 3.32: Loss = 0.396133
Epoch 3.33: Loss = 0.459274
Epoch 3.34: Loss = 0.384842
Epoch 3.35: Loss = 0.482086
Epoch 3.36: Loss = 0.397705
Epoch 3.37: Loss = 0.484039
Epoch 3.38: Loss = 0.448578
Epoch 3.39: Loss = 0.404099
Epoch 3.40: Loss = 0.483551
Epoch 3.41: Loss = 0.365433
Epoch 3.42: Loss = 0.431427
Epoch 3.43: Loss = 0.482513
Epoch 3.44: Loss = 0.397507
Epoch 3.45: Loss = 0.51889
Epoch 3.46: Loss = 0.436981
Epoch 3.47: Loss = 0.431793
Epoch 3.48: Loss = 0.474564
Epoch 3.49: Loss = 0.352753
Epoch 3.50: Loss = 0.389923
Epoch 3.51: Loss = 0.432922
Epoch 3.52: Loss = 0.371368
Epoch 3.53: Loss = 0.468079
Epoch 3.54: Loss = 0.441849
Epoch 3.55: Loss = 0.480576
Epoch 3.56: Loss = 0.442566
Epoch 3.57: Loss = 0.457733
Epoch 3.58: Loss = 0.383575
Epoch 3.59: Loss = 0.434937
Epoch 3.60: Loss = 0.436691
Epoch 3.61: Loss = 0.407547
Epoch 3.62: Loss = 0.411407
Epoch 3.63: Loss = 0.416336
Epoch 3.64: Loss = 0.451294
Epoch 3.65: Loss = 0.442413
Epoch 3.66: Loss = 0.409149
Epoch 3.67: Loss = 0.491867
Epoch 3.68: Loss = 0.508911
Epoch 3.69: Loss = 0.413147
Epoch 3.70: Loss = 0.451477
Epoch 3.71: Loss = 0.431274
Epoch 3.72: Loss = 0.420486
Epoch 3.73: Loss = 0.396347
Epoch 3.74: Loss = 0.399872
Epoch 3.75: Loss = 0.460587
Epoch 3.76: Loss = 0.371552
Epoch 3.77: Loss = 0.411987
Epoch 3.78: Loss = 0.463608
Epoch 3.79: Loss = 0.501785
Epoch 3.80: Loss = 0.409866
Epoch 3.81: Loss = 0.452164
Epoch 3.82: Loss = 0.394272
Epoch 3.83: Loss = 0.470703
Epoch 3.84: Loss = 0.356277
Epoch 3.85: Loss = 0.4505
Epoch 3.86: Loss = 0.392807
Epoch 3.87: Loss = 0.479904
Epoch 3.88: Loss = 0.479706
Epoch 3.89: Loss = 0.378448
Epoch 3.90: Loss = 0.38063
Epoch 3.91: Loss = 0.491455
Epoch 3.92: Loss = 0.38176
Epoch 3.93: Loss = 0.464371
Epoch 3.94: Loss = 0.419449
Epoch 3.95: Loss = 0.513535
Epoch 3.96: Loss = 0.375717
Epoch 3.97: Loss = 0.479889
Epoch 3.98: Loss = 0.433975
Epoch 3.99: Loss = 0.353485
Epoch 3.100: Loss = 0.451218
Epoch 3.101: Loss = 0.41745
Epoch 3.102: Loss = 0.371933
Epoch 3.103: Loss = 0.418152
Epoch 3.104: Loss = 0.421112
Epoch 3.105: Loss = 0.364822
Epoch 3.106: Loss = 0.445114
Epoch 3.107: Loss = 0.342468
Epoch 3.108: Loss = 0.416092
Epoch 3.109: Loss = 0.396927
Epoch 3.110: Loss = 0.405426
Epoch 3.111: Loss = 0.48584
Epoch 3.112: Loss = 0.43689
Epoch 3.113: Loss = 0.392441
Epoch 3.114: Loss = 0.485519
Epoch 3.115: Loss = 0.451721
Epoch 3.116: Loss = 0.465302
Epoch 3.117: Loss = 0.443558
Epoch 3.118: Loss = 0.514801
Epoch 3.119: Loss = 0.333084
Epoch 3.120: Loss = 0.420456
TRAIN LOSS = 0.433823
TRAIN ACC = 87.0728 % (52246/60000)
Loss = 0.389969
Loss = 0.489487
Loss = 0.556503
Loss = 0.556671
Loss = 0.556976
Loss = 0.440643
Loss = 0.375397
Loss = 0.588272
Loss = 0.537781
Loss = 0.486435
Loss = 0.18576
Loss = 0.385849
Loss = 0.287628
Loss = 0.37825
Loss = 0.244324
Loss = 0.308441
Loss = 0.241043
Loss = 0.0874176
Loss = 0.247437
Loss = 0.536667
TEST LOSS = 0.394047
TEST ACC = 522.459 % (8859/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.432953
Epoch 4.2: Loss = 0.407791
Epoch 4.3: Loss = 0.414459
Epoch 4.4: Loss = 0.355011
Epoch 4.5: Loss = 0.353928
Epoch 4.6: Loss = 0.361923
Epoch 4.7: Loss = 0.450516
Epoch 4.8: Loss = 0.517334
Epoch 4.9: Loss = 0.416565
Epoch 4.10: Loss = 0.42926
Epoch 4.11: Loss = 0.456696
Epoch 4.12: Loss = 0.456345
Epoch 4.13: Loss = 0.327286
Epoch 4.14: Loss = 0.408737
Epoch 4.15: Loss = 0.359131
Epoch 4.16: Loss = 0.415909
Epoch 4.17: Loss = 0.504562
Epoch 4.18: Loss = 0.448792
Epoch 4.19: Loss = 0.375412
Epoch 4.20: Loss = 0.400452
Epoch 4.21: Loss = 0.437714
Epoch 4.22: Loss = 0.418793
Epoch 4.23: Loss = 0.330307
Epoch 4.24: Loss = 0.42543
Epoch 4.25: Loss = 0.413284
Epoch 4.26: Loss = 0.44902
Epoch 4.27: Loss = 0.418106
Epoch 4.28: Loss = 0.426636
Epoch 4.29: Loss = 0.412827
Epoch 4.30: Loss = 0.415054
Epoch 4.31: Loss = 0.429688
Epoch 4.32: Loss = 0.445145
Epoch 4.33: Loss = 0.450363
Epoch 4.34: Loss = 0.360367
Epoch 4.35: Loss = 0.34021
Epoch 4.36: Loss = 0.426361
Epoch 4.37: Loss = 0.421082
Epoch 4.38: Loss = 0.399704
Epoch 4.39: Loss = 0.401093
Epoch 4.40: Loss = 0.476822
Epoch 4.41: Loss = 0.43219
Epoch 4.42: Loss = 0.337189
Epoch 4.43: Loss = 0.374741
Epoch 4.44: Loss = 0.433548
Epoch 4.45: Loss = 0.38324
Epoch 4.46: Loss = 0.465805
Epoch 4.47: Loss = 0.480423
Epoch 4.48: Loss = 0.401581
Epoch 4.49: Loss = 0.380478
Epoch 4.50: Loss = 0.464752
Epoch 4.51: Loss = 0.409103
Epoch 4.52: Loss = 0.429001
Epoch 4.53: Loss = 0.40683
Epoch 4.54: Loss = 0.503922
Epoch 4.55: Loss = 0.41983
Epoch 4.56: Loss = 0.44194
Epoch 4.57: Loss = 0.331802
Epoch 4.58: Loss = 0.406876
Epoch 4.59: Loss = 0.427322
Epoch 4.60: Loss = 0.305222
Epoch 4.61: Loss = 0.372147
Epoch 4.62: Loss = 0.521164
Epoch 4.63: Loss = 0.411591
Epoch 4.64: Loss = 0.44075
Epoch 4.65: Loss = 0.447678
Epoch 4.66: Loss = 0.455917
Epoch 4.67: Loss = 0.413467
Epoch 4.68: Loss = 0.417435
Epoch 4.69: Loss = 0.414963
Epoch 4.70: Loss = 0.546173
Epoch 4.71: Loss = 0.436874
Epoch 4.72: Loss = 0.412903
Epoch 4.73: Loss = 0.466812
Epoch 4.74: Loss = 0.41243
Epoch 4.75: Loss = 0.356049
Epoch 4.76: Loss = 0.395279
Epoch 4.77: Loss = 0.403168
Epoch 4.78: Loss = 0.390778
Epoch 4.79: Loss = 0.342712
Epoch 4.80: Loss = 0.392242
Epoch 4.81: Loss = 0.379395
Epoch 4.82: Loss = 0.330887
Epoch 4.83: Loss = 0.380447
Epoch 4.84: Loss = 0.49762
Epoch 4.85: Loss = 0.375137
Epoch 4.86: Loss = 0.417679
Epoch 4.87: Loss = 0.487701
Epoch 4.88: Loss = 0.402603
Epoch 4.89: Loss = 0.490097
Epoch 4.90: Loss = 0.411301
Epoch 4.91: Loss = 0.397644
Epoch 4.92: Loss = 0.366974
Epoch 4.93: Loss = 0.388702
Epoch 4.94: Loss = 0.3983
Epoch 4.95: Loss = 0.446533
Epoch 4.96: Loss = 0.487167
Epoch 4.97: Loss = 0.386642
Epoch 4.98: Loss = 0.38829
Epoch 4.99: Loss = 0.440048
Epoch 4.100: Loss = 0.363861
Epoch 4.101: Loss = 0.363815
Epoch 4.102: Loss = 0.424896
Epoch 4.103: Loss = 0.431702
Epoch 4.104: Loss = 0.497925
Epoch 4.105: Loss = 0.430847
Epoch 4.106: Loss = 0.507828
Epoch 4.107: Loss = 0.417572
Epoch 4.108: Loss = 0.411911
Epoch 4.109: Loss = 0.413315
Epoch 4.110: Loss = 0.471558
Epoch 4.111: Loss = 0.360687
Epoch 4.112: Loss = 0.423019
Epoch 4.113: Loss = 0.38295
Epoch 4.114: Loss = 0.404831
Epoch 4.115: Loss = 0.350479
Epoch 4.116: Loss = 0.494232
Epoch 4.117: Loss = 0.517334
Epoch 4.118: Loss = 0.527542
Epoch 4.119: Loss = 0.377136
Epoch 4.120: Loss = 0.404495
TRAIN LOSS = 0.41774
TRAIN ACC = 87.7945 % (52679/60000)
Loss = 0.377365
Loss = 0.464783
Loss = 0.559067
Loss = 0.566711
Loss = 0.558258
Loss = 0.42804
Loss = 0.356094
Loss = 0.601974
Loss = 0.533615
Loss = 0.50444
Loss = 0.167419
Loss = 0.334152
Loss = 0.321182
Loss = 0.379044
Loss = 0.232086
Loss = 0.324753
Loss = 0.227951
Loss = 0.074173
Loss = 0.24263
Loss = 0.577515
TEST LOSS = 0.391563
TEST ACC = 526.79 % (8877/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.437698
Epoch 5.2: Loss = 0.422943
Epoch 5.3: Loss = 0.421341
Epoch 5.4: Loss = 0.44397
Epoch 5.5: Loss = 0.413223
Epoch 5.6: Loss = 0.392303
Epoch 5.7: Loss = 0.473404
Epoch 5.8: Loss = 0.368591
Epoch 5.9: Loss = 0.486771
Epoch 5.10: Loss = 0.389557
Epoch 5.11: Loss = 0.403152
Epoch 5.12: Loss = 0.349014
Epoch 5.13: Loss = 0.468002
Epoch 5.14: Loss = 0.387802
Epoch 5.15: Loss = 0.457657
Epoch 5.16: Loss = 0.405487
Epoch 5.17: Loss = 0.412521
Epoch 5.18: Loss = 0.378586
Epoch 5.19: Loss = 0.525253
Epoch 5.20: Loss = 0.460983
Epoch 5.21: Loss = 0.3992
Epoch 5.22: Loss = 0.384872
Epoch 5.23: Loss = 0.434677
Epoch 5.24: Loss = 0.426346
Epoch 5.25: Loss = 0.386459
Epoch 5.26: Loss = 0.401413
Epoch 5.27: Loss = 0.365631
Epoch 5.28: Loss = 0.420395
Epoch 5.29: Loss = 0.459534
Epoch 5.30: Loss = 0.45665
Epoch 5.31: Loss = 0.423981
Epoch 5.32: Loss = 0.313187
Epoch 5.33: Loss = 0.338486
Epoch 5.34: Loss = 0.353394
Epoch 5.35: Loss = 0.427322
Epoch 5.36: Loss = 0.439331
Epoch 5.37: Loss = 0.428238
Epoch 5.38: Loss = 0.381592
Epoch 5.39: Loss = 0.367142
Epoch 5.40: Loss = 0.39682
Epoch 5.41: Loss = 0.419815
Epoch 5.42: Loss = 0.371109
Epoch 5.43: Loss = 0.419312
Epoch 5.44: Loss = 0.430344
Epoch 5.45: Loss = 0.426437
Epoch 5.46: Loss = 0.421417
Epoch 5.47: Loss = 0.495651
Epoch 5.48: Loss = 0.377289
Epoch 5.49: Loss = 0.348907
Epoch 5.50: Loss = 0.398224
Epoch 5.51: Loss = 0.377792
Epoch 5.52: Loss = 0.448944
Epoch 5.53: Loss = 0.414291
Epoch 5.54: Loss = 0.398941
Epoch 5.55: Loss = 0.386719
Epoch 5.56: Loss = 0.351395
Epoch 5.57: Loss = 0.439926
Epoch 5.58: Loss = 0.400467
Epoch 5.59: Loss = 0.386124
Epoch 5.60: Loss = 0.389694
Epoch 5.61: Loss = 0.432251
Epoch 5.62: Loss = 0.449829
Epoch 5.63: Loss = 0.432938
Epoch 5.64: Loss = 0.415512
Epoch 5.65: Loss = 0.347977
Epoch 5.66: Loss = 0.387451
Epoch 5.67: Loss = 0.445969
Epoch 5.68: Loss = 0.453568
Epoch 5.69: Loss = 0.373901
Epoch 5.70: Loss = 0.480682
Epoch 5.71: Loss = 0.363266
Epoch 5.72: Loss = 0.412338
Epoch 5.73: Loss = 0.440369
Epoch 5.74: Loss = 0.474304
Epoch 5.75: Loss = 0.374023
Epoch 5.76: Loss = 0.345673
Epoch 5.77: Loss = 0.443481
Epoch 5.78: Loss = 0.474731
Epoch 5.79: Loss = 0.509705
Epoch 5.80: Loss = 0.411575
Epoch 5.81: Loss = 0.450424
Epoch 5.82: Loss = 0.391876
Epoch 5.83: Loss = 0.46286
Epoch 5.84: Loss = 0.514496
Epoch 5.85: Loss = 0.394196
Epoch 5.86: Loss = 0.37323
Epoch 5.87: Loss = 0.36087
Epoch 5.88: Loss = 0.449249
Epoch 5.89: Loss = 0.365005
Epoch 5.90: Loss = 0.428162
Epoch 5.91: Loss = 0.374191
Epoch 5.92: Loss = 0.428452
Epoch 5.93: Loss = 0.346176
Epoch 5.94: Loss = 0.391586
Epoch 5.95: Loss = 0.454865
Epoch 5.96: Loss = 0.406586
Epoch 5.97: Loss = 0.278366
Epoch 5.98: Loss = 0.496521
Epoch 5.99: Loss = 0.403458
Epoch 5.100: Loss = 0.38475
Epoch 5.101: Loss = 0.448563
Epoch 5.102: Loss = 0.375244
Epoch 5.103: Loss = 0.490829
Epoch 5.104: Loss = 0.442505
Epoch 5.105: Loss = 0.384933
Epoch 5.106: Loss = 0.426285
Epoch 5.107: Loss = 0.395355
Epoch 5.108: Loss = 0.478897
Epoch 5.109: Loss = 0.492462
Epoch 5.110: Loss = 0.382553
Epoch 5.111: Loss = 0.417923
Epoch 5.112: Loss = 0.383698
Epoch 5.113: Loss = 0.449539
Epoch 5.114: Loss = 0.429031
Epoch 5.115: Loss = 0.418304
Epoch 5.116: Loss = 0.419739
Epoch 5.117: Loss = 0.445618
Epoch 5.118: Loss = 0.377625
Epoch 5.119: Loss = 0.366409
Epoch 5.120: Loss = 0.409363
TRAIN LOSS = 0.413879
TRAIN ACC = 88.179 % (52910/60000)
Loss = 0.375412
Loss = 0.490295
Loss = 0.568283
Loss = 0.577347
Loss = 0.544785
Loss = 0.406815
Loss = 0.368027
Loss = 0.603546
Loss = 0.550415
Loss = 0.494949
Loss = 0.139557
Loss = 0.342178
Loss = 0.284241
Loss = 0.353012
Loss = 0.225159
Loss = 0.28389
Loss = 0.21698
Loss = 0.0588531
Loss = 0.225739
Loss = 0.578735
TEST LOSS = 0.384411
TEST ACC = 529.099 % (8915/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.461151
Epoch 6.2: Loss = 0.39119
Epoch 6.3: Loss = 0.504807
Epoch 6.4: Loss = 0.361908
Epoch 6.5: Loss = 0.416168
Epoch 6.6: Loss = 0.391235
Epoch 6.7: Loss = 0.370361
Epoch 6.8: Loss = 0.430634
Epoch 6.9: Loss = 0.292679
Epoch 6.10: Loss = 0.397644
Epoch 6.11: Loss = 0.345535
Epoch 6.12: Loss = 0.417297
Epoch 6.13: Loss = 0.395462
Epoch 6.14: Loss = 0.475388
Epoch 6.15: Loss = 0.43782
Epoch 6.16: Loss = 0.405304
Epoch 6.17: Loss = 0.433792
Epoch 6.18: Loss = 0.419556
Epoch 6.19: Loss = 0.368317
Epoch 6.20: Loss = 0.452438
Epoch 6.21: Loss = 0.422058
Epoch 6.22: Loss = 0.377792
Epoch 6.23: Loss = 0.377441
Epoch 6.24: Loss = 0.449829
Epoch 6.25: Loss = 0.474442
Epoch 6.26: Loss = 0.415009
Epoch 6.27: Loss = 0.405579
Epoch 6.28: Loss = 0.340149
Epoch 6.29: Loss = 0.402939
Epoch 6.30: Loss = 0.387772
Epoch 6.31: Loss = 0.461319
Epoch 6.32: Loss = 0.461456
Epoch 6.33: Loss = 0.323837
Epoch 6.34: Loss = 0.404953
Epoch 6.35: Loss = 0.409042
Epoch 6.36: Loss = 0.386444
Epoch 6.37: Loss = 0.4478
Epoch 6.38: Loss = 0.50589
Epoch 6.39: Loss = 0.480606
Epoch 6.40: Loss = 0.458038
Epoch 6.41: Loss = 0.331253
Epoch 6.42: Loss = 0.343292
Epoch 6.43: Loss = 0.502899
Epoch 6.44: Loss = 0.457199
Epoch 6.45: Loss = 0.397995
Epoch 6.46: Loss = 0.379761
Epoch 6.47: Loss = 0.358917
Epoch 6.48: Loss = 0.363403
Epoch 6.49: Loss = 0.429947
Epoch 6.50: Loss = 0.346115
Epoch 6.51: Loss = 0.427658
Epoch 6.52: Loss = 0.415848
Epoch 6.53: Loss = 0.364594
Epoch 6.54: Loss = 0.50531
Epoch 6.55: Loss = 0.441498
Epoch 6.56: Loss = 0.392502
Epoch 6.57: Loss = 0.417358
Epoch 6.58: Loss = 0.374451
Epoch 6.59: Loss = 0.416336
Epoch 6.60: Loss = 0.361191
Epoch 6.61: Loss = 0.420578
Epoch 6.62: Loss = 0.307266
Epoch 6.63: Loss = 0.474915
Epoch 6.64: Loss = 0.420776
Epoch 6.65: Loss = 0.365311
Epoch 6.66: Loss = 0.397018
Epoch 6.67: Loss = 0.49057
Epoch 6.68: Loss = 0.39035
Epoch 6.69: Loss = 0.44014
Epoch 6.70: Loss = 0.50087
Epoch 6.71: Loss = 0.445648
Epoch 6.72: Loss = 0.355164
Epoch 6.73: Loss = 0.418915
Epoch 6.74: Loss = 0.351791
Epoch 6.75: Loss = 0.427383
Epoch 6.76: Loss = 0.417358
Epoch 6.77: Loss = 0.363663
Epoch 6.78: Loss = 0.31839
Epoch 6.79: Loss = 0.386475
Epoch 6.80: Loss = 0.40744
Epoch 6.81: Loss = 0.345261
Epoch 6.82: Loss = 0.465668
Epoch 6.83: Loss = 0.40271
Epoch 6.84: Loss = 0.38736
Epoch 6.85: Loss = 0.415161
Epoch 6.86: Loss = 0.447998
Epoch 6.87: Loss = 0.370575
Epoch 6.88: Loss = 0.416809
Epoch 6.89: Loss = 0.40831
Epoch 6.90: Loss = 0.407822
Epoch 6.91: Loss = 0.387421
Epoch 6.92: Loss = 0.477219
Epoch 6.93: Loss = 0.438263
Epoch 6.94: Loss = 0.365158
Epoch 6.95: Loss = 0.365448
Epoch 6.96: Loss = 0.373154
Epoch 6.97: Loss = 0.444412
Epoch 6.98: Loss = 0.526764
Epoch 6.99: Loss = 0.382111
Epoch 6.100: Loss = 0.33815
Epoch 6.101: Loss = 0.436813
Epoch 6.102: Loss = 0.448486
Epoch 6.103: Loss = 0.274429
Epoch 6.104: Loss = 0.467041
Epoch 6.105: Loss = 0.393372
Epoch 6.106: Loss = 0.458923
Epoch 6.107: Loss = 0.363831
Epoch 6.108: Loss = 0.440842
Epoch 6.109: Loss = 0.383652
Epoch 6.110: Loss = 0.385864
Epoch 6.111: Loss = 0.442947
Epoch 6.112: Loss = 0.385895
Epoch 6.113: Loss = 0.337036
Epoch 6.114: Loss = 0.439041
Epoch 6.115: Loss = 0.428162
Epoch 6.116: Loss = 0.411026
Epoch 6.117: Loss = 0.435974
Epoch 6.118: Loss = 0.450653
Epoch 6.119: Loss = 0.465363
Epoch 6.120: Loss = 0.370361
TRAIN LOSS = 0.408951
TRAIN ACC = 88.5437 % (53129/60000)
Loss = 0.368927
Loss = 0.468048
Loss = 0.563034
Loss = 0.58287
Loss = 0.533646
Loss = 0.416061
Loss = 0.353653
Loss = 0.587097
Loss = 0.539688
Loss = 0.472107
Loss = 0.133667
Loss = 0.374985
Loss = 0.287094
Loss = 0.355316
Loss = 0.215454
Loss = 0.311844
Loss = 0.226196
Loss = 0.0516357
Loss = 0.18631
Loss = 0.59993
TEST LOSS = 0.381378
TEST ACC = 531.29 % (8941/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.47493
Epoch 7.2: Loss = 0.405594
Epoch 7.3: Loss = 0.429672
Epoch 7.4: Loss = 0.448166
Epoch 7.5: Loss = 0.359909
Epoch 7.6: Loss = 0.357513
Epoch 7.7: Loss = 0.393143
Epoch 7.8: Loss = 0.415314
Epoch 7.9: Loss = 0.386459
Epoch 7.10: Loss = 0.36972
Epoch 7.11: Loss = 0.498672
Epoch 7.12: Loss = 0.40712
Epoch 7.13: Loss = 0.373413
Epoch 7.14: Loss = 0.326126
Epoch 7.15: Loss = 0.378891
Epoch 7.16: Loss = 0.351608
Epoch 7.17: Loss = 0.368134
Epoch 7.18: Loss = 0.481567
Epoch 7.19: Loss = 0.412231
Epoch 7.20: Loss = 0.452011
Epoch 7.21: Loss = 0.455338
Epoch 7.22: Loss = 0.373596
Epoch 7.23: Loss = 0.452194
Epoch 7.24: Loss = 0.41156
Epoch 7.25: Loss = 0.36792
Epoch 7.26: Loss = 0.375504
Epoch 7.27: Loss = 0.388
Epoch 7.28: Loss = 0.360092
Epoch 7.29: Loss = 0.461533
Epoch 7.30: Loss = 0.33783
Epoch 7.31: Loss = 0.452988
Epoch 7.32: Loss = 0.39534
Epoch 7.33: Loss = 0.465912
Epoch 7.34: Loss = 0.473648
Epoch 7.35: Loss = 0.370941
Epoch 7.36: Loss = 0.411758
Epoch 7.37: Loss = 0.365646
Epoch 7.38: Loss = 0.40625
Epoch 7.39: Loss = 0.371094
Epoch 7.40: Loss = 0.392288
Epoch 7.41: Loss = 0.444427
Epoch 7.42: Loss = 0.318451
Epoch 7.43: Loss = 0.412338
Epoch 7.44: Loss = 0.497421
Epoch 7.45: Loss = 0.354401
Epoch 7.46: Loss = 0.398041
Epoch 7.47: Loss = 0.397964
Epoch 7.48: Loss = 0.314285
Epoch 7.49: Loss = 0.325958
Epoch 7.50: Loss = 0.401947
Epoch 7.51: Loss = 0.373871
Epoch 7.52: Loss = 0.451523
Epoch 7.53: Loss = 0.434891
Epoch 7.54: Loss = 0.429764
Epoch 7.55: Loss = 0.462646
Epoch 7.56: Loss = 0.408051
Epoch 7.57: Loss = 0.360764
Epoch 7.58: Loss = 0.421051
Epoch 7.59: Loss = 0.494141
Epoch 7.60: Loss = 0.312592
Epoch 7.61: Loss = 0.372223
Epoch 7.62: Loss = 0.437027
Epoch 7.63: Loss = 0.338013
Epoch 7.64: Loss = 0.373077
Epoch 7.65: Loss = 0.400162
Epoch 7.66: Loss = 0.486298
Epoch 7.67: Loss = 0.460953
Epoch 7.68: Loss = 0.421173
Epoch 7.69: Loss = 0.492538
Epoch 7.70: Loss = 0.359985
Epoch 7.71: Loss = 0.351059
Epoch 7.72: Loss = 0.443985
Epoch 7.73: Loss = 0.35228
Epoch 7.74: Loss = 0.449341
Epoch 7.75: Loss = 0.342728
Epoch 7.76: Loss = 0.515167
Epoch 7.77: Loss = 0.538544
Epoch 7.78: Loss = 0.385483
Epoch 7.79: Loss = 0.354721
Epoch 7.80: Loss = 0.316635
Epoch 7.81: Loss = 0.402588
Epoch 7.82: Loss = 0.335693
Epoch 7.83: Loss = 0.427063
Epoch 7.84: Loss = 0.329697
Epoch 7.85: Loss = 0.387268
Epoch 7.86: Loss = 0.413086
Epoch 7.87: Loss = 0.4077
Epoch 7.88: Loss = 0.434891
Epoch 7.89: Loss = 0.409851
Epoch 7.90: Loss = 0.357849
Epoch 7.91: Loss = 0.41185
Epoch 7.92: Loss = 0.439377
Epoch 7.93: Loss = 0.441757
Epoch 7.94: Loss = 0.406113
Epoch 7.95: Loss = 0.386642
Epoch 7.96: Loss = 0.373718
Epoch 7.97: Loss = 0.379196
Epoch 7.98: Loss = 0.44928
Epoch 7.99: Loss = 0.466324
Epoch 7.100: Loss = 0.406128
Epoch 7.101: Loss = 0.310959
Epoch 7.102: Loss = 0.480331
Epoch 7.103: Loss = 0.439896
Epoch 7.104: Loss = 0.336609
Epoch 7.105: Loss = 0.470688
Epoch 7.106: Loss = 0.423019
Epoch 7.107: Loss = 0.36261
Epoch 7.108: Loss = 0.461136
Epoch 7.109: Loss = 0.413834
Epoch 7.110: Loss = 0.408218
Epoch 7.111: Loss = 0.367752
Epoch 7.112: Loss = 0.390717
Epoch 7.113: Loss = 0.419022
Epoch 7.114: Loss = 0.387619
Epoch 7.115: Loss = 0.466766
Epoch 7.116: Loss = 0.372421
Epoch 7.117: Loss = 0.345505
Epoch 7.118: Loss = 0.440353
Epoch 7.119: Loss = 0.330811
Epoch 7.120: Loss = 0.34581
TRAIN LOSS = 0.403534
TRAIN ACC = 88.8397 % (53306/60000)
Loss = 0.361526
Loss = 0.453278
Loss = 0.544998
Loss = 0.581589
Loss = 0.533981
Loss = 0.394196
Loss = 0.332306
Loss = 0.580383
Loss = 0.516739
Loss = 0.455383
Loss = 0.132904
Loss = 0.331619
Loss = 0.299286
Loss = 0.339188
Loss = 0.21225
Loss = 0.343781
Loss = 0.239319
Loss = 0.0505981
Loss = 0.185791
Loss = 0.57048
TEST LOSS = 0.37298
TEST ACC = 533.06 % (8952/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.43306
Epoch 8.2: Loss = 0.36972
Epoch 8.3: Loss = 0.312378
Epoch 8.4: Loss = 0.447159
Epoch 8.5: Loss = 0.439713
Epoch 8.6: Loss = 0.424301
Epoch 8.7: Loss = 0.464172
Epoch 8.8: Loss = 0.366791
Epoch 8.9: Loss = 0.399048
Epoch 8.10: Loss = 0.405548
Epoch 8.11: Loss = 0.416992
Epoch 8.12: Loss = 0.416183
Epoch 8.13: Loss = 0.378571
Epoch 8.14: Loss = 0.422729
Epoch 8.15: Loss = 0.356995
Epoch 8.16: Loss = 0.423752
Epoch 8.17: Loss = 0.473495
Epoch 8.18: Loss = 0.422562
Epoch 8.19: Loss = 0.470245
Epoch 8.20: Loss = 0.511932
Epoch 8.21: Loss = 0.405106
Epoch 8.22: Loss = 0.447998
Epoch 8.23: Loss = 0.371292
Epoch 8.24: Loss = 0.402145
Epoch 8.25: Loss = 0.369064
Epoch 8.26: Loss = 0.41983
Epoch 8.27: Loss = 0.395706
Epoch 8.28: Loss = 0.306
Epoch 8.29: Loss = 0.472122
Epoch 8.30: Loss = 0.382507
Epoch 8.31: Loss = 0.371292
Epoch 8.32: Loss = 0.44873
Epoch 8.33: Loss = 0.367249
Epoch 8.34: Loss = 0.489136
Epoch 8.35: Loss = 0.403076
Epoch 8.36: Loss = 0.49617
Epoch 8.37: Loss = 0.368958
Epoch 8.38: Loss = 0.432892
Epoch 8.39: Loss = 0.572754
Epoch 8.40: Loss = 0.482147
Epoch 8.41: Loss = 0.479477
Epoch 8.42: Loss = 0.381638
Epoch 8.43: Loss = 0.359787
Epoch 8.44: Loss = 0.434067
Epoch 8.45: Loss = 0.412781
Epoch 8.46: Loss = 0.26442
Epoch 8.47: Loss = 0.451218
Epoch 8.48: Loss = 0.354813
Epoch 8.49: Loss = 0.496796
Epoch 8.50: Loss = 0.392563
Epoch 8.51: Loss = 0.414185
Epoch 8.52: Loss = 0.296936
Epoch 8.53: Loss = 0.473221
Epoch 8.54: Loss = 0.271561
Epoch 8.55: Loss = 0.388885
Epoch 8.56: Loss = 0.420761
Epoch 8.57: Loss = 0.382675
Epoch 8.58: Loss = 0.496475
Epoch 8.59: Loss = 0.40918
Epoch 8.60: Loss = 0.425827
Epoch 8.61: Loss = 0.500397
Epoch 8.62: Loss = 0.335739
Epoch 8.63: Loss = 0.460251
Epoch 8.64: Loss = 0.354553
Epoch 8.65: Loss = 0.469559
Epoch 8.66: Loss = 0.408218
Epoch 8.67: Loss = 0.305283
Epoch 8.68: Loss = 0.356812
Epoch 8.69: Loss = 0.451126
Epoch 8.70: Loss = 0.304993
Epoch 8.71: Loss = 0.322403
Epoch 8.72: Loss = 0.461823
Epoch 8.73: Loss = 0.371964
Epoch 8.74: Loss = 0.465714
Epoch 8.75: Loss = 0.397949
Epoch 8.76: Loss = 0.369675
Epoch 8.77: Loss = 0.400253
Epoch 8.78: Loss = 0.458908
Epoch 8.79: Loss = 0.480789
Epoch 8.80: Loss = 0.416473
Epoch 8.81: Loss = 0.470413
Epoch 8.82: Loss = 0.447006
Epoch 8.83: Loss = 0.45694
Epoch 8.84: Loss = 0.377716
Epoch 8.85: Loss = 0.261353
Epoch 8.86: Loss = 0.334717
Epoch 8.87: Loss = 0.465118
Epoch 8.88: Loss = 0.427246
Epoch 8.89: Loss = 0.290909
Epoch 8.90: Loss = 0.377228
Epoch 8.91: Loss = 0.298401
Epoch 8.92: Loss = 0.470825
Epoch 8.93: Loss = 0.369659
Epoch 8.94: Loss = 0.393967
Epoch 8.95: Loss = 0.410965
Epoch 8.96: Loss = 0.377808
Epoch 8.97: Loss = 0.319534
Epoch 8.98: Loss = 0.436966
Epoch 8.99: Loss = 0.298309
Epoch 8.100: Loss = 0.441345
Epoch 8.101: Loss = 0.428757
Epoch 8.102: Loss = 0.31842
Epoch 8.103: Loss = 0.439133
Epoch 8.104: Loss = 0.424118
Epoch 8.105: Loss = 0.41391
Epoch 8.106: Loss = 0.359924
Epoch 8.107: Loss = 0.520645
Epoch 8.108: Loss = 0.337799
Epoch 8.109: Loss = 0.499329
Epoch 8.110: Loss = 0.437759
Epoch 8.111: Loss = 0.318604
Epoch 8.112: Loss = 0.414688
Epoch 8.113: Loss = 0.417709
Epoch 8.114: Loss = 0.466339
Epoch 8.115: Loss = 0.446106
Epoch 8.116: Loss = 0.428406
Epoch 8.117: Loss = 0.43071
Epoch 8.118: Loss = 0.344086
Epoch 8.119: Loss = 0.325226
Epoch 8.120: Loss = 0.327026
TRAIN LOSS = 0.405701
TRAIN ACC = 88.8809 % (53331/60000)
Loss = 0.368866
Loss = 0.455276
Loss = 0.56517
Loss = 0.581467
Loss = 0.526031
Loss = 0.382248
Loss = 0.328583
Loss = 0.577164
Loss = 0.52832
Loss = 0.450607
Loss = 0.157761
Loss = 0.361755
Loss = 0.284256
Loss = 0.333801
Loss = 0.199295
Loss = 0.315674
Loss = 0.225128
Loss = 0.045929
Loss = 0.208435
Loss = 0.56485
TEST LOSS = 0.373031
TEST ACC = 533.308 % (8957/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.433456
Epoch 9.2: Loss = 0.357529
Epoch 9.3: Loss = 0.405609
Epoch 9.4: Loss = 0.338028
Epoch 9.5: Loss = 0.3517
Epoch 9.6: Loss = 0.39386
Epoch 9.7: Loss = 0.426743
Epoch 9.8: Loss = 0.375671
Epoch 9.9: Loss = 0.370667
Epoch 9.10: Loss = 0.437164
Epoch 9.11: Loss = 0.411926
Epoch 9.12: Loss = 0.324997
Epoch 9.13: Loss = 0.347244
Epoch 9.14: Loss = 0.46759
Epoch 9.15: Loss = 0.389404
Epoch 9.16: Loss = 0.443497
Epoch 9.17: Loss = 0.397842
Epoch 9.18: Loss = 0.385208
Epoch 9.19: Loss = 0.336609
Epoch 9.20: Loss = 0.443008
Epoch 9.21: Loss = 0.455322
Epoch 9.22: Loss = 0.407349
Epoch 9.23: Loss = 0.428253
Epoch 9.24: Loss = 0.39711
Epoch 9.25: Loss = 0.465363
Epoch 9.26: Loss = 0.393692
Epoch 9.27: Loss = 0.442596
Epoch 9.28: Loss = 0.350433
Epoch 9.29: Loss = 0.520172
Epoch 9.30: Loss = 0.538147
Epoch 9.31: Loss = 0.4216
Epoch 9.32: Loss = 0.421555
Epoch 9.33: Loss = 0.403412
Epoch 9.34: Loss = 0.481888
Epoch 9.35: Loss = 0.317459
Epoch 9.36: Loss = 0.336868
Epoch 9.37: Loss = 0.331955
Epoch 9.38: Loss = 0.412155
Epoch 9.39: Loss = 0.412689
Epoch 9.40: Loss = 0.352905
Epoch 9.41: Loss = 0.373276
Epoch 9.42: Loss = 0.389771
Epoch 9.43: Loss = 0.296783
Epoch 9.44: Loss = 0.398621
Epoch 9.45: Loss = 0.399673
Epoch 9.46: Loss = 0.378067
Epoch 9.47: Loss = 0.379471
Epoch 9.48: Loss = 0.425156
Epoch 9.49: Loss = 0.344864
Epoch 9.50: Loss = 0.441269
Epoch 9.51: Loss = 0.369904
Epoch 9.52: Loss = 0.391785
Epoch 9.53: Loss = 0.463821
Epoch 9.54: Loss = 0.451218
Epoch 9.55: Loss = 0.511826
Epoch 9.56: Loss = 0.31868
Epoch 9.57: Loss = 0.424484
Epoch 9.58: Loss = 0.394592
Epoch 9.59: Loss = 0.387604
Epoch 9.60: Loss = 0.390671
Epoch 9.61: Loss = 0.322739
Epoch 9.62: Loss = 0.462082
Epoch 9.63: Loss = 0.378143
Epoch 9.64: Loss = 0.432861
Epoch 9.65: Loss = 0.324615
Epoch 9.66: Loss = 0.482117
Epoch 9.67: Loss = 0.376373
Epoch 9.68: Loss = 0.459198
Epoch 9.69: Loss = 0.489685
Epoch 9.70: Loss = 0.420334
Epoch 9.71: Loss = 0.288818
Epoch 9.72: Loss = 0.392456
Epoch 9.73: Loss = 0.431992
Epoch 9.74: Loss = 0.568115
Epoch 9.75: Loss = 0.388382
Epoch 9.76: Loss = 0.413788
Epoch 9.77: Loss = 0.391876
Epoch 9.78: Loss = 0.459381
Epoch 9.79: Loss = 0.409546
Epoch 9.80: Loss = 0.530029
Epoch 9.81: Loss = 0.37291
Epoch 9.82: Loss = 0.466782
Epoch 9.83: Loss = 0.432755
Epoch 9.84: Loss = 0.496872
Epoch 9.85: Loss = 0.419693
Epoch 9.86: Loss = 0.376511
Epoch 9.87: Loss = 0.372574
Epoch 9.88: Loss = 0.415085
Epoch 9.89: Loss = 0.362061
Epoch 9.90: Loss = 0.333038
Epoch 9.91: Loss = 0.359131
Epoch 9.92: Loss = 0.480286
Epoch 9.93: Loss = 0.309174
Epoch 9.94: Loss = 0.398422
Epoch 9.95: Loss = 0.364471
Epoch 9.96: Loss = 0.354645
Epoch 9.97: Loss = 0.398422
Epoch 9.98: Loss = 0.378662
Epoch 9.99: Loss = 0.444946
Epoch 9.100: Loss = 0.454895
Epoch 9.101: Loss = 0.439865
Epoch 9.102: Loss = 0.38504
Epoch 9.103: Loss = 0.396225
Epoch 9.104: Loss = 0.443527
Epoch 9.105: Loss = 0.372391
Epoch 9.106: Loss = 0.366104
Epoch 9.107: Loss = 0.384308
Epoch 9.108: Loss = 0.454865
Epoch 9.109: Loss = 0.356583
Epoch 9.110: Loss = 0.458557
Epoch 9.111: Loss = 0.420685
Epoch 9.112: Loss = 0.384552
Epoch 9.113: Loss = 0.384125
Epoch 9.114: Loss = 0.395813
Epoch 9.115: Loss = 0.376068
Epoch 9.116: Loss = 0.370621
Epoch 9.117: Loss = 0.465775
Epoch 9.118: Loss = 0.400986
Epoch 9.119: Loss = 0.351776
Epoch 9.120: Loss = 0.350403
TRAIN LOSS = 0.403595
TRAIN ACC = 89.1693 % (53504/60000)
Loss = 0.378143
Loss = 0.471146
Loss = 0.525375
Loss = 0.58345
Loss = 0.542236
Loss = 0.392792
Loss = 0.316055
Loss = 0.588196
Loss = 0.560226
Loss = 0.456451
Loss = 0.169357
Loss = 0.338303
Loss = 0.309647
Loss = 0.342422
Loss = 0.196457
Loss = 0.318054
Loss = 0.224213
Loss = 0.0455322
Loss = 0.233887
Loss = 0.565002
TEST LOSS = 0.377847
TEST ACC = 535.039 % (8987/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.393234
Epoch 10.2: Loss = 0.451324
Epoch 10.3: Loss = 0.302368
Epoch 10.4: Loss = 0.387985
Epoch 10.5: Loss = 0.351898
Epoch 10.6: Loss = 0.31337
Epoch 10.7: Loss = 0.375992
Epoch 10.8: Loss = 0.546478
Epoch 10.9: Loss = 0.554214
Epoch 10.10: Loss = 0.474884
Epoch 10.11: Loss = 0.368729
Epoch 10.12: Loss = 0.333084
Epoch 10.13: Loss = 0.285294
Epoch 10.14: Loss = 0.412949
Epoch 10.15: Loss = 0.332596
Epoch 10.16: Loss = 0.467545
Epoch 10.17: Loss = 0.587738
Epoch 10.18: Loss = 0.473724
Epoch 10.19: Loss = 0.415268
Epoch 10.20: Loss = 0.365463
Epoch 10.21: Loss = 0.397095
Epoch 10.22: Loss = 0.446854
Epoch 10.23: Loss = 0.430328
Epoch 10.24: Loss = 0.332092
Epoch 10.25: Loss = 0.49118
Epoch 10.26: Loss = 0.397614
Epoch 10.27: Loss = 0.363983
Epoch 10.28: Loss = 0.353363
Epoch 10.29: Loss = 0.420334
Epoch 10.30: Loss = 0.469131
Epoch 10.31: Loss = 0.403198
Epoch 10.32: Loss = 0.403336
Epoch 10.33: Loss = 0.31665
Epoch 10.34: Loss = 0.411591
Epoch 10.35: Loss = 0.369629
Epoch 10.36: Loss = 0.356918
Epoch 10.37: Loss = 0.361176
Epoch 10.38: Loss = 0.471542
Epoch 10.39: Loss = 0.432343
Epoch 10.40: Loss = 0.491531
Epoch 10.41: Loss = 0.343185
Epoch 10.42: Loss = 0.475677
Epoch 10.43: Loss = 0.409302
Epoch 10.44: Loss = 0.399887
Epoch 10.45: Loss = 0.481262
Epoch 10.46: Loss = 0.427979
Epoch 10.47: Loss = 0.413086
Epoch 10.48: Loss = 0.468246
Epoch 10.49: Loss = 0.34462
Epoch 10.50: Loss = 0.487961
Epoch 10.51: Loss = 0.39003
Epoch 10.52: Loss = 0.52066
Epoch 10.53: Loss = 0.380325
Epoch 10.54: Loss = 0.434799
Epoch 10.55: Loss = 0.428589
Epoch 10.56: Loss = 0.408157
Epoch 10.57: Loss = 0.430771
Epoch 10.58: Loss = 0.47171
Epoch 10.59: Loss = 0.354538
Epoch 10.60: Loss = 0.384293
Epoch 10.61: Loss = 0.409927
Epoch 10.62: Loss = 0.318512
Epoch 10.63: Loss = 0.262573
Epoch 10.64: Loss = 0.370209
Epoch 10.65: Loss = 0.408997
Epoch 10.66: Loss = 0.379486
Epoch 10.67: Loss = 0.33992
Epoch 10.68: Loss = 0.407486
Epoch 10.69: Loss = 0.341354
Epoch 10.70: Loss = 0.425308
Epoch 10.71: Loss = 0.393494
Epoch 10.72: Loss = 0.474945
Epoch 10.73: Loss = 0.445099
Epoch 10.74: Loss = 0.449661
Epoch 10.75: Loss = 0.322952
Epoch 10.76: Loss = 0.422867
Epoch 10.77: Loss = 0.457825
Epoch 10.78: Loss = 0.31897
Epoch 10.79: Loss = 0.408859
Epoch 10.80: Loss = 0.329239
Epoch 10.81: Loss = 0.367599
Epoch 10.82: Loss = 0.402069
Epoch 10.83: Loss = 0.484634
Epoch 10.84: Loss = 0.382843
Epoch 10.85: Loss = 0.492111
Epoch 10.86: Loss = 0.335281
Epoch 10.87: Loss = 0.440033
Epoch 10.88: Loss = 0.42511
Epoch 10.89: Loss = 0.485138
Epoch 10.90: Loss = 0.264618
Epoch 10.91: Loss = 0.382645
Epoch 10.92: Loss = 0.466156
Epoch 10.93: Loss = 0.354065
Epoch 10.94: Loss = 0.541809
Epoch 10.95: Loss = 0.445877
Epoch 10.96: Loss = 0.373093
Epoch 10.97: Loss = 0.410324
Epoch 10.98: Loss = 0.396057
Epoch 10.99: Loss = 0.418777
Epoch 10.100: Loss = 0.427475
Epoch 10.101: Loss = 0.332642
Epoch 10.102: Loss = 0.423676
Epoch 10.103: Loss = 0.437836
Epoch 10.104: Loss = 0.393936
Epoch 10.105: Loss = 0.444077
Epoch 10.106: Loss = 0.392517
Epoch 10.107: Loss = 0.484879
Epoch 10.108: Loss = 0.439926
Epoch 10.109: Loss = 0.475555
Epoch 10.110: Loss = 0.341934
Epoch 10.111: Loss = 0.437027
Epoch 10.112: Loss = 0.366745
Epoch 10.113: Loss = 0.427826
Epoch 10.114: Loss = 0.341675
Epoch 10.115: Loss = 0.376419
Epoch 10.116: Loss = 0.305435
Epoch 10.117: Loss = 0.282104
Epoch 10.118: Loss = 0.599655
Epoch 10.119: Loss = 0.377487
Epoch 10.120: Loss = 0.330902
TRAIN LOSS = 0.406387
TRAIN ACC = 89.3143 % (53591/60000)
Loss = 0.393799
Loss = 0.463516
Loss = 0.534393
Loss = 0.608612
Loss = 0.562729
Loss = 0.40242
Loss = 0.348633
Loss = 0.611908
Loss = 0.563797
Loss = 0.455002
Loss = 0.153
Loss = 0.330002
Loss = 0.324677
Loss = 0.334534
Loss = 0.177933
Loss = 0.292572
Loss = 0.222733
Loss = 0.0460358
Loss = 0.197861
Loss = 0.56134
TEST LOSS = 0.379275
TEST ACC = 535.909 % (9003/10000)

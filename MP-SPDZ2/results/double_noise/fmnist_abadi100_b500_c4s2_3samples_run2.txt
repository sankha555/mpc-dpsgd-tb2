Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.56294
Epoch 1.2: Loss = 2.3334
Epoch 1.3: Loss = 2.29739
Epoch 1.4: Loss = 2.21472
Epoch 1.5: Loss = 2.1254
Epoch 1.6: Loss = 2.04796
Epoch 1.7: Loss = 1.98701
Epoch 1.8: Loss = 1.89558
Epoch 1.9: Loss = 1.86412
Epoch 1.10: Loss = 1.79623
Epoch 1.11: Loss = 1.82172
Epoch 1.12: Loss = 1.73022
Epoch 1.13: Loss = 1.71317
Epoch 1.14: Loss = 1.72705
Epoch 1.15: Loss = 1.60509
Epoch 1.16: Loss = 1.58714
Epoch 1.17: Loss = 1.55479
Epoch 1.18: Loss = 1.57028
Epoch 1.19: Loss = 1.5155
Epoch 1.20: Loss = 1.49754
Epoch 1.21: Loss = 1.42433
Epoch 1.22: Loss = 1.41966
Epoch 1.23: Loss = 1.43979
Epoch 1.24: Loss = 1.35518
Epoch 1.25: Loss = 1.41129
Epoch 1.26: Loss = 1.36584
Epoch 1.27: Loss = 1.33131
Epoch 1.28: Loss = 1.26091
Epoch 1.29: Loss = 1.30254
Epoch 1.30: Loss = 1.34245
Epoch 1.31: Loss = 1.28864
Epoch 1.32: Loss = 1.2636
Epoch 1.33: Loss = 1.25177
Epoch 1.34: Loss = 1.24565
Epoch 1.35: Loss = 1.19374
Epoch 1.36: Loss = 1.18672
Epoch 1.37: Loss = 1.1265
Epoch 1.38: Loss = 1.16682
Epoch 1.39: Loss = 1.16385
Epoch 1.40: Loss = 1.15999
Epoch 1.41: Loss = 1.15297
Epoch 1.42: Loss = 1.17601
Epoch 1.43: Loss = 1.06047
Epoch 1.44: Loss = 1.06792
Epoch 1.45: Loss = 1.14259
Epoch 1.46: Loss = 1.04672
Epoch 1.47: Loss = 1.06393
Epoch 1.48: Loss = 1.07718
Epoch 1.49: Loss = 1.10526
Epoch 1.50: Loss = 1.00201
Epoch 1.51: Loss = 0.98497
Epoch 1.52: Loss = 1.04118
Epoch 1.53: Loss = 1.04716
Epoch 1.54: Loss = 1.00713
Epoch 1.55: Loss = 0.966415
Epoch 1.56: Loss = 1.02463
Epoch 1.57: Loss = 1.04121
Epoch 1.58: Loss = 1.03815
Epoch 1.59: Loss = 1.04216
Epoch 1.60: Loss = 1.03212
Epoch 1.61: Loss = 0.929138
Epoch 1.62: Loss = 0.993103
Epoch 1.63: Loss = 0.975006
Epoch 1.64: Loss = 0.970947
Epoch 1.65: Loss = 0.904343
Epoch 1.66: Loss = 0.922668
Epoch 1.67: Loss = 0.894775
Epoch 1.68: Loss = 0.948288
Epoch 1.69: Loss = 0.924911
Epoch 1.70: Loss = 0.925262
Epoch 1.71: Loss = 0.910278
Epoch 1.72: Loss = 0.878983
Epoch 1.73: Loss = 0.88063
Epoch 1.74: Loss = 0.887436
Epoch 1.75: Loss = 0.905579
Epoch 1.76: Loss = 0.922684
Epoch 1.77: Loss = 0.93013
Epoch 1.78: Loss = 0.888519
Epoch 1.79: Loss = 0.885803
Epoch 1.80: Loss = 0.928192
Epoch 1.81: Loss = 0.930099
Epoch 1.82: Loss = 0.82872
Epoch 1.83: Loss = 0.877121
Epoch 1.84: Loss = 0.86618
Epoch 1.85: Loss = 0.823563
Epoch 1.86: Loss = 0.884308
Epoch 1.87: Loss = 0.774094
Epoch 1.88: Loss = 0.878448
Epoch 1.89: Loss = 0.9207
Epoch 1.90: Loss = 0.838394
Epoch 1.91: Loss = 0.915665
Epoch 1.92: Loss = 0.863617
Epoch 1.93: Loss = 0.850067
Epoch 1.94: Loss = 0.774353
Epoch 1.95: Loss = 0.765686
Epoch 1.96: Loss = 0.851944
Epoch 1.97: Loss = 0.84317
Epoch 1.98: Loss = 0.859329
Epoch 1.99: Loss = 0.916275
Epoch 1.100: Loss = 0.835312
Epoch 1.101: Loss = 0.842392
Epoch 1.102: Loss = 0.855667
Epoch 1.103: Loss = 0.865463
Epoch 1.104: Loss = 0.793808
Epoch 1.105: Loss = 0.813446
Epoch 1.106: Loss = 0.836914
Epoch 1.107: Loss = 0.83107
Epoch 1.108: Loss = 0.858093
Epoch 1.109: Loss = 0.73851
Epoch 1.110: Loss = 0.813126
Epoch 1.111: Loss = 0.747971
Epoch 1.112: Loss = 0.791077
Epoch 1.113: Loss = 0.836365
Epoch 1.114: Loss = 0.794739
Epoch 1.115: Loss = 0.806473
Epoch 1.116: Loss = 0.790131
Epoch 1.117: Loss = 0.839417
Epoch 1.118: Loss = 0.755341
Epoch 1.119: Loss = 0.796005
Epoch 1.120: Loss = 0.740906
TRAIN LOSS = 1.13268
TRAIN ACC = 60.9268 % (36558/60000)
Loss = 0.735428
Loss = 0.835709
Loss = 0.834015
Loss = 0.750229
Loss = 0.727509
Loss = 0.867569
Loss = 0.884369
Loss = 0.811859
Loss = 0.772812
Loss = 0.743958
Loss = 0.842422
Loss = 0.80307
Loss = 0.795425
Loss = 0.804016
Loss = 0.770737
Loss = 0.834152
Loss = 0.758865
Loss = 0.781128
Loss = 0.821503
Loss = 0.793472
TEST LOSS = 0.798412
TEST ACC = 365.579 % (7184/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.845367
Epoch 2.2: Loss = 0.683945
Epoch 2.3: Loss = 0.801895
Epoch 2.4: Loss = 0.735825
Epoch 2.5: Loss = 0.725479
Epoch 2.6: Loss = 0.776413
Epoch 2.7: Loss = 0.787476
Epoch 2.8: Loss = 0.78949
Epoch 2.9: Loss = 0.72731
Epoch 2.10: Loss = 0.770782
Epoch 2.11: Loss = 0.758484
Epoch 2.12: Loss = 0.799927
Epoch 2.13: Loss = 0.852646
Epoch 2.14: Loss = 0.741394
Epoch 2.15: Loss = 0.847778
Epoch 2.16: Loss = 0.761612
Epoch 2.17: Loss = 0.830032
Epoch 2.18: Loss = 0.821838
Epoch 2.19: Loss = 0.761719
Epoch 2.20: Loss = 0.793869
Epoch 2.21: Loss = 0.840363
Epoch 2.22: Loss = 0.699768
Epoch 2.23: Loss = 0.727203
Epoch 2.24: Loss = 0.813782
Epoch 2.25: Loss = 0.804977
Epoch 2.26: Loss = 0.746811
Epoch 2.27: Loss = 0.727478
Epoch 2.28: Loss = 0.737961
Epoch 2.29: Loss = 0.770691
Epoch 2.30: Loss = 0.776825
Epoch 2.31: Loss = 0.752563
Epoch 2.32: Loss = 0.780914
Epoch 2.33: Loss = 0.745499
Epoch 2.34: Loss = 0.731125
Epoch 2.35: Loss = 0.734589
Epoch 2.36: Loss = 0.718048
Epoch 2.37: Loss = 0.716568
Epoch 2.38: Loss = 0.799988
Epoch 2.39: Loss = 0.685974
Epoch 2.40: Loss = 0.707047
Epoch 2.41: Loss = 0.751343
Epoch 2.42: Loss = 0.719925
Epoch 2.43: Loss = 0.767227
Epoch 2.44: Loss = 0.746124
Epoch 2.45: Loss = 0.733597
Epoch 2.46: Loss = 0.683151
Epoch 2.47: Loss = 0.732117
Epoch 2.48: Loss = 0.716156
Epoch 2.49: Loss = 0.76651
Epoch 2.50: Loss = 0.693176
Epoch 2.51: Loss = 0.725967
Epoch 2.52: Loss = 0.716003
Epoch 2.53: Loss = 0.678391
Epoch 2.54: Loss = 0.792648
Epoch 2.55: Loss = 0.682358
Epoch 2.56: Loss = 0.753159
Epoch 2.57: Loss = 0.712646
Epoch 2.58: Loss = 0.683609
Epoch 2.59: Loss = 0.758698
Epoch 2.60: Loss = 0.735504
Epoch 2.61: Loss = 0.71817
Epoch 2.62: Loss = 0.695709
Epoch 2.63: Loss = 0.721573
Epoch 2.64: Loss = 0.723099
Epoch 2.65: Loss = 0.693268
Epoch 2.66: Loss = 0.791351
Epoch 2.67: Loss = 0.663803
Epoch 2.68: Loss = 0.723526
Epoch 2.69: Loss = 0.729065
Epoch 2.70: Loss = 0.64151
Epoch 2.71: Loss = 0.723572
Epoch 2.72: Loss = 0.67363
Epoch 2.73: Loss = 0.728348
Epoch 2.74: Loss = 0.656097
Epoch 2.75: Loss = 0.669647
Epoch 2.76: Loss = 0.648712
Epoch 2.77: Loss = 0.741638
Epoch 2.78: Loss = 0.710648
Epoch 2.79: Loss = 0.789352
Epoch 2.80: Loss = 0.75351
Epoch 2.81: Loss = 0.704575
Epoch 2.82: Loss = 0.719376
Epoch 2.83: Loss = 0.700241
Epoch 2.84: Loss = 0.774231
Epoch 2.85: Loss = 0.685669
Epoch 2.86: Loss = 0.721069
Epoch 2.87: Loss = 0.689453
Epoch 2.88: Loss = 0.702789
Epoch 2.89: Loss = 0.666504
Epoch 2.90: Loss = 0.693192
Epoch 2.91: Loss = 0.697556
Epoch 2.92: Loss = 0.770813
Epoch 2.93: Loss = 0.719482
Epoch 2.94: Loss = 0.715607
Epoch 2.95: Loss = 0.593643
Epoch 2.96: Loss = 0.624252
Epoch 2.97: Loss = 0.67627
Epoch 2.98: Loss = 0.669556
Epoch 2.99: Loss = 0.724258
Epoch 2.100: Loss = 0.685623
Epoch 2.101: Loss = 0.728714
Epoch 2.102: Loss = 0.614578
Epoch 2.103: Loss = 0.703064
Epoch 2.104: Loss = 0.666046
Epoch 2.105: Loss = 0.72937
Epoch 2.106: Loss = 0.705826
Epoch 2.107: Loss = 0.832565
Epoch 2.108: Loss = 0.77948
Epoch 2.109: Loss = 0.788956
Epoch 2.110: Loss = 0.764755
Epoch 2.111: Loss = 0.689545
Epoch 2.112: Loss = 0.753967
Epoch 2.113: Loss = 0.727951
Epoch 2.114: Loss = 0.746185
Epoch 2.115: Loss = 0.779984
Epoch 2.116: Loss = 0.711182
Epoch 2.117: Loss = 0.660172
Epoch 2.118: Loss = 0.723083
Epoch 2.119: Loss = 0.715683
Epoch 2.120: Loss = 0.644699
TRAIN LOSS = 0.731491
TRAIN ACC = 74.8734 % (44926/60000)
Loss = 0.655121
Loss = 0.764069
Loss = 0.736313
Loss = 0.633118
Loss = 0.636932
Loss = 0.818512
Loss = 0.840042
Loss = 0.774048
Loss = 0.708298
Loss = 0.657608
Loss = 0.804932
Loss = 0.760849
Loss = 0.741501
Loss = 0.73497
Loss = 0.701828
Loss = 0.761871
Loss = 0.692551
Loss = 0.727325
Loss = 0.769653
Loss = 0.713623
TEST LOSS = 0.731658
TEST ACC = 449.26 % (7514/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.658676
Epoch 3.2: Loss = 0.676285
Epoch 3.3: Loss = 0.69455
Epoch 3.4: Loss = 0.735275
Epoch 3.5: Loss = 0.637146
Epoch 3.6: Loss = 0.652786
Epoch 3.7: Loss = 0.657318
Epoch 3.8: Loss = 0.73114
Epoch 3.9: Loss = 0.777618
Epoch 3.10: Loss = 0.714111
Epoch 3.11: Loss = 0.644455
Epoch 3.12: Loss = 0.687469
Epoch 3.13: Loss = 0.776672
Epoch 3.14: Loss = 0.696259
Epoch 3.15: Loss = 0.660919
Epoch 3.16: Loss = 0.761444
Epoch 3.17: Loss = 0.791
Epoch 3.18: Loss = 0.697266
Epoch 3.19: Loss = 0.698837
Epoch 3.20: Loss = 0.743362
Epoch 3.21: Loss = 0.680344
Epoch 3.22: Loss = 0.755829
Epoch 3.23: Loss = 0.668396
Epoch 3.24: Loss = 0.60939
Epoch 3.25: Loss = 0.770798
Epoch 3.26: Loss = 0.676224
Epoch 3.27: Loss = 0.65155
Epoch 3.28: Loss = 0.655853
Epoch 3.29: Loss = 0.649033
Epoch 3.30: Loss = 0.662766
Epoch 3.31: Loss = 0.76619
Epoch 3.32: Loss = 0.787689
Epoch 3.33: Loss = 0.706604
Epoch 3.34: Loss = 0.671875
Epoch 3.35: Loss = 0.639954
Epoch 3.36: Loss = 0.64064
Epoch 3.37: Loss = 0.653412
Epoch 3.38: Loss = 0.606476
Epoch 3.39: Loss = 0.742859
Epoch 3.40: Loss = 0.777832
Epoch 3.41: Loss = 0.772827
Epoch 3.42: Loss = 0.661133
Epoch 3.43: Loss = 0.657684
Epoch 3.44: Loss = 0.701172
Epoch 3.45: Loss = 0.60495
Epoch 3.46: Loss = 0.678009
Epoch 3.47: Loss = 0.659378
Epoch 3.48: Loss = 0.664978
Epoch 3.49: Loss = 0.630768
Epoch 3.50: Loss = 0.646225
Epoch 3.51: Loss = 0.755249
Epoch 3.52: Loss = 0.642593
Epoch 3.53: Loss = 0.661255
Epoch 3.54: Loss = 0.787216
Epoch 3.55: Loss = 0.759506
Epoch 3.56: Loss = 0.795181
Epoch 3.57: Loss = 0.728149
Epoch 3.58: Loss = 0.70726
Epoch 3.59: Loss = 0.695724
Epoch 3.60: Loss = 0.621246
Epoch 3.61: Loss = 0.712219
Epoch 3.62: Loss = 0.580826
Epoch 3.63: Loss = 0.63089
Epoch 3.64: Loss = 0.696381
Epoch 3.65: Loss = 0.670349
Epoch 3.66: Loss = 0.723389
Epoch 3.67: Loss = 0.693893
Epoch 3.68: Loss = 0.687103
Epoch 3.69: Loss = 0.766113
Epoch 3.70: Loss = 0.826935
Epoch 3.71: Loss = 0.595673
Epoch 3.72: Loss = 0.639862
Epoch 3.73: Loss = 0.687286
Epoch 3.74: Loss = 0.724548
Epoch 3.75: Loss = 0.77359
Epoch 3.76: Loss = 0.743332
Epoch 3.77: Loss = 0.577728
Epoch 3.78: Loss = 0.729752
Epoch 3.79: Loss = 0.632996
Epoch 3.80: Loss = 0.722946
Epoch 3.81: Loss = 0.703842
Epoch 3.82: Loss = 0.690125
Epoch 3.83: Loss = 0.670578
Epoch 3.84: Loss = 0.718658
Epoch 3.85: Loss = 0.624847
Epoch 3.86: Loss = 0.656677
Epoch 3.87: Loss = 0.689362
Epoch 3.88: Loss = 0.635468
Epoch 3.89: Loss = 0.71434
Epoch 3.90: Loss = 0.649124
Epoch 3.91: Loss = 0.684799
Epoch 3.92: Loss = 0.698044
Epoch 3.93: Loss = 0.717834
Epoch 3.94: Loss = 0.59201
Epoch 3.95: Loss = 0.679276
Epoch 3.96: Loss = 0.609177
Epoch 3.97: Loss = 0.742874
Epoch 3.98: Loss = 0.671112
Epoch 3.99: Loss = 0.609451
Epoch 3.100: Loss = 0.702301
Epoch 3.101: Loss = 0.663239
Epoch 3.102: Loss = 0.667786
Epoch 3.103: Loss = 0.710556
Epoch 3.104: Loss = 0.737137
Epoch 3.105: Loss = 0.632645
Epoch 3.106: Loss = 0.621658
Epoch 3.107: Loss = 0.691727
Epoch 3.108: Loss = 0.622971
Epoch 3.109: Loss = 0.621445
Epoch 3.110: Loss = 0.60968
Epoch 3.111: Loss = 0.806549
Epoch 3.112: Loss = 0.724075
Epoch 3.113: Loss = 0.652802
Epoch 3.114: Loss = 0.622055
Epoch 3.115: Loss = 0.600189
Epoch 3.116: Loss = 0.68602
Epoch 3.117: Loss = 0.585297
Epoch 3.118: Loss = 0.733475
Epoch 3.119: Loss = 0.705505
Epoch 3.120: Loss = 0.59346
TRAIN LOSS = 0.685516
TRAIN ACC = 77.3926 % (46437/60000)
Loss = 0.617172
Loss = 0.725937
Loss = 0.672363
Loss = 0.591553
Loss = 0.611893
Loss = 0.771103
Loss = 0.821472
Loss = 0.72142
Loss = 0.677414
Loss = 0.623138
Loss = 0.773224
Loss = 0.738358
Loss = 0.712051
Loss = 0.689453
Loss = 0.673828
Loss = 0.729614
Loss = 0.649261
Loss = 0.706879
Loss = 0.728973
Loss = 0.667801
TEST LOSS = 0.695145
TEST ACC = 464.369 % (7698/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.620331
Epoch 4.2: Loss = 0.728043
Epoch 4.3: Loss = 0.676834
Epoch 4.4: Loss = 0.669952
Epoch 4.5: Loss = 0.661926
Epoch 4.6: Loss = 0.660797
Epoch 4.7: Loss = 0.668472
Epoch 4.8: Loss = 0.656662
Epoch 4.9: Loss = 0.69632
Epoch 4.10: Loss = 0.661804
Epoch 4.11: Loss = 0.778748
Epoch 4.12: Loss = 0.636002
Epoch 4.13: Loss = 0.729401
Epoch 4.14: Loss = 0.741104
Epoch 4.15: Loss = 0.665894
Epoch 4.16: Loss = 0.688293
Epoch 4.17: Loss = 0.613235
Epoch 4.18: Loss = 0.797058
Epoch 4.19: Loss = 0.677322
Epoch 4.20: Loss = 0.700928
Epoch 4.21: Loss = 0.747894
Epoch 4.22: Loss = 0.732208
Epoch 4.23: Loss = 0.656387
Epoch 4.24: Loss = 0.58313
Epoch 4.25: Loss = 0.655594
Epoch 4.26: Loss = 0.687759
Epoch 4.27: Loss = 0.657913
Epoch 4.28: Loss = 0.695816
Epoch 4.29: Loss = 0.647049
Epoch 4.30: Loss = 0.764816
Epoch 4.31: Loss = 0.762558
Epoch 4.32: Loss = 0.603256
Epoch 4.33: Loss = 0.747849
Epoch 4.34: Loss = 0.636902
Epoch 4.35: Loss = 0.685379
Epoch 4.36: Loss = 0.637344
Epoch 4.37: Loss = 0.54689
Epoch 4.38: Loss = 0.753113
Epoch 4.39: Loss = 0.692307
Epoch 4.40: Loss = 0.644287
Epoch 4.41: Loss = 0.588684
Epoch 4.42: Loss = 0.740799
Epoch 4.43: Loss = 0.591599
Epoch 4.44: Loss = 0.664154
Epoch 4.45: Loss = 0.599991
Epoch 4.46: Loss = 0.651901
Epoch 4.47: Loss = 0.787552
Epoch 4.48: Loss = 0.642532
Epoch 4.49: Loss = 0.698532
Epoch 4.50: Loss = 0.67688
Epoch 4.51: Loss = 0.59967
Epoch 4.52: Loss = 0.634964
Epoch 4.53: Loss = 0.612305
Epoch 4.54: Loss = 0.616791
Epoch 4.55: Loss = 0.597733
Epoch 4.56: Loss = 0.602341
Epoch 4.57: Loss = 0.672836
Epoch 4.58: Loss = 0.550644
Epoch 4.59: Loss = 0.643494
Epoch 4.60: Loss = 0.575195
Epoch 4.61: Loss = 0.730759
Epoch 4.62: Loss = 0.57576
Epoch 4.63: Loss = 0.652283
Epoch 4.64: Loss = 0.616974
Epoch 4.65: Loss = 0.701141
Epoch 4.66: Loss = 0.6521
Epoch 4.67: Loss = 0.633499
Epoch 4.68: Loss = 0.63501
Epoch 4.69: Loss = 0.678223
Epoch 4.70: Loss = 0.641418
Epoch 4.71: Loss = 0.762817
Epoch 4.72: Loss = 0.564819
Epoch 4.73: Loss = 0.682007
Epoch 4.74: Loss = 0.663361
Epoch 4.75: Loss = 0.636444
Epoch 4.76: Loss = 0.647736
Epoch 4.77: Loss = 0.662476
Epoch 4.78: Loss = 0.618759
Epoch 4.79: Loss = 0.732315
Epoch 4.80: Loss = 0.658417
Epoch 4.81: Loss = 0.685379
Epoch 4.82: Loss = 0.611389
Epoch 4.83: Loss = 0.610168
Epoch 4.84: Loss = 0.668732
Epoch 4.85: Loss = 0.51329
Epoch 4.86: Loss = 0.611343
Epoch 4.87: Loss = 0.597839
Epoch 4.88: Loss = 0.616959
Epoch 4.89: Loss = 0.669052
Epoch 4.90: Loss = 0.62088
Epoch 4.91: Loss = 0.682343
Epoch 4.92: Loss = 0.677505
Epoch 4.93: Loss = 0.740936
Epoch 4.94: Loss = 0.485184
Epoch 4.95: Loss = 0.656906
Epoch 4.96: Loss = 0.560471
Epoch 4.97: Loss = 0.743515
Epoch 4.98: Loss = 0.657745
Epoch 4.99: Loss = 0.573105
Epoch 4.100: Loss = 0.704987
Epoch 4.101: Loss = 0.690262
Epoch 4.102: Loss = 0.566055
Epoch 4.103: Loss = 0.644821
Epoch 4.104: Loss = 0.682022
Epoch 4.105: Loss = 0.811356
Epoch 4.106: Loss = 0.720642
Epoch 4.107: Loss = 0.71904
Epoch 4.108: Loss = 0.575729
Epoch 4.109: Loss = 0.579041
Epoch 4.110: Loss = 0.620224
Epoch 4.111: Loss = 0.6651
Epoch 4.112: Loss = 0.63501
Epoch 4.113: Loss = 0.709366
Epoch 4.114: Loss = 0.716461
Epoch 4.115: Loss = 0.593582
Epoch 4.116: Loss = 0.658859
Epoch 4.117: Loss = 0.625839
Epoch 4.118: Loss = 0.817245
Epoch 4.119: Loss = 0.665909
Epoch 4.120: Loss = 0.603058
TRAIN LOSS = 0.659607
TRAIN ACC = 78.6163 % (47172/60000)
Loss = 0.597183
Loss = 0.68869
Loss = 0.660202
Loss = 0.565918
Loss = 0.57576
Loss = 0.757553
Loss = 0.817841
Loss = 0.731964
Loss = 0.641998
Loss = 0.605255
Loss = 0.776566
Loss = 0.720886
Loss = 0.686356
Loss = 0.668045
Loss = 0.645889
Loss = 0.702072
Loss = 0.643555
Loss = 0.692719
Loss = 0.701645
Loss = 0.652283
TEST LOSS = 0.676619
TEST ACC = 471.719 % (7810/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.505722
Epoch 5.2: Loss = 0.490189
Epoch 5.3: Loss = 0.64978
Epoch 5.4: Loss = 0.625839
Epoch 5.5: Loss = 0.677017
Epoch 5.6: Loss = 0.608276
Epoch 5.7: Loss = 0.590881
Epoch 5.8: Loss = 0.675751
Epoch 5.9: Loss = 0.614273
Epoch 5.10: Loss = 0.608215
Epoch 5.11: Loss = 0.730759
Epoch 5.12: Loss = 0.700851
Epoch 5.13: Loss = 0.699036
Epoch 5.14: Loss = 0.682388
Epoch 5.15: Loss = 0.635605
Epoch 5.16: Loss = 0.716568
Epoch 5.17: Loss = 0.701233
Epoch 5.18: Loss = 0.64151
Epoch 5.19: Loss = 0.637222
Epoch 5.20: Loss = 0.620392
Epoch 5.21: Loss = 0.662399
Epoch 5.22: Loss = 0.632965
Epoch 5.23: Loss = 0.678513
Epoch 5.24: Loss = 0.721817
Epoch 5.25: Loss = 0.584427
Epoch 5.26: Loss = 0.672516
Epoch 5.27: Loss = 0.674484
Epoch 5.28: Loss = 0.682434
Epoch 5.29: Loss = 0.599258
Epoch 5.30: Loss = 0.61322
Epoch 5.31: Loss = 0.650909
Epoch 5.32: Loss = 0.615677
Epoch 5.33: Loss = 0.578506
Epoch 5.34: Loss = 0.573837
Epoch 5.35: Loss = 0.565567
Epoch 5.36: Loss = 0.689972
Epoch 5.37: Loss = 0.668137
Epoch 5.38: Loss = 0.718811
Epoch 5.39: Loss = 0.635483
Epoch 5.40: Loss = 0.639053
Epoch 5.41: Loss = 0.709076
Epoch 5.42: Loss = 0.6297
Epoch 5.43: Loss = 0.667801
Epoch 5.44: Loss = 0.628616
Epoch 5.45: Loss = 0.654846
Epoch 5.46: Loss = 0.686752
Epoch 5.47: Loss = 0.74588
Epoch 5.48: Loss = 0.668045
Epoch 5.49: Loss = 0.647079
Epoch 5.50: Loss = 0.70787
Epoch 5.51: Loss = 0.546738
Epoch 5.52: Loss = 0.617203
Epoch 5.53: Loss = 0.690231
Epoch 5.54: Loss = 0.67395
Epoch 5.55: Loss = 0.667679
Epoch 5.56: Loss = 0.632034
Epoch 5.57: Loss = 0.622879
Epoch 5.58: Loss = 0.760895
Epoch 5.59: Loss = 0.586075
Epoch 5.60: Loss = 0.664581
Epoch 5.61: Loss = 0.691681
Epoch 5.62: Loss = 0.639191
Epoch 5.63: Loss = 0.679703
Epoch 5.64: Loss = 0.625076
Epoch 5.65: Loss = 0.632904
Epoch 5.66: Loss = 0.725601
Epoch 5.67: Loss = 0.622726
Epoch 5.68: Loss = 0.60675
Epoch 5.69: Loss = 0.591385
Epoch 5.70: Loss = 0.640015
Epoch 5.71: Loss = 0.591293
Epoch 5.72: Loss = 0.586533
Epoch 5.73: Loss = 0.68602
Epoch 5.74: Loss = 0.612595
Epoch 5.75: Loss = 0.703751
Epoch 5.76: Loss = 0.725616
Epoch 5.77: Loss = 0.674194
Epoch 5.78: Loss = 0.717667
Epoch 5.79: Loss = 0.727707
Epoch 5.80: Loss = 0.584641
Epoch 5.81: Loss = 0.600174
Epoch 5.82: Loss = 0.595688
Epoch 5.83: Loss = 0.578186
Epoch 5.84: Loss = 0.706985
Epoch 5.85: Loss = 0.675674
Epoch 5.86: Loss = 0.674866
Epoch 5.87: Loss = 0.756302
Epoch 5.88: Loss = 0.695786
Epoch 5.89: Loss = 0.622803
Epoch 5.90: Loss = 0.567368
Epoch 5.91: Loss = 0.673676
Epoch 5.92: Loss = 0.575989
Epoch 5.93: Loss = 0.72142
Epoch 5.94: Loss = 0.703094
Epoch 5.95: Loss = 0.590973
Epoch 5.96: Loss = 0.654938
Epoch 5.97: Loss = 0.599411
Epoch 5.98: Loss = 0.694595
Epoch 5.99: Loss = 0.730392
Epoch 5.100: Loss = 0.719452
Epoch 5.101: Loss = 0.760422
Epoch 5.102: Loss = 0.673141
Epoch 5.103: Loss = 0.648453
Epoch 5.104: Loss = 0.665283
Epoch 5.105: Loss = 0.698318
Epoch 5.106: Loss = 0.603775
Epoch 5.107: Loss = 0.663132
Epoch 5.108: Loss = 0.657227
Epoch 5.109: Loss = 0.535751
Epoch 5.110: Loss = 0.603317
Epoch 5.111: Loss = 0.685715
Epoch 5.112: Loss = 0.64006
Epoch 5.113: Loss = 0.682449
Epoch 5.114: Loss = 0.626434
Epoch 5.115: Loss = 0.574249
Epoch 5.116: Loss = 0.757889
Epoch 5.117: Loss = 0.597351
Epoch 5.118: Loss = 0.66835
Epoch 5.119: Loss = 0.559708
Epoch 5.120: Loss = 0.622147
TRAIN LOSS = 0.65004
TRAIN ACC = 79.4556 % (47676/60000)
Loss = 0.568298
Loss = 0.666031
Loss = 0.644943
Loss = 0.564163
Loss = 0.578156
Loss = 0.743988
Loss = 0.813522
Loss = 0.695999
Loss = 0.638931
Loss = 0.596436
Loss = 0.753113
Loss = 0.723358
Loss = 0.665894
Loss = 0.664749
Loss = 0.637299
Loss = 0.684479
Loss = 0.620651
Loss = 0.694992
Loss = 0.6922
Loss = 0.636856
TEST LOSS = 0.664203
TEST ACC = 476.759 % (7862/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.674515
Epoch 6.2: Loss = 0.512787
Epoch 6.3: Loss = 0.702774
Epoch 6.4: Loss = 0.680069
Epoch 6.5: Loss = 0.684464
Epoch 6.6: Loss = 0.723572
Epoch 6.7: Loss = 0.757599
Epoch 6.8: Loss = 0.512573
Epoch 6.9: Loss = 0.593704
Epoch 6.10: Loss = 0.663834
Epoch 6.11: Loss = 0.637344
Epoch 6.12: Loss = 0.589813
Epoch 6.13: Loss = 0.64093
Epoch 6.14: Loss = 0.611038
Epoch 6.15: Loss = 0.625519
Epoch 6.16: Loss = 0.676483
Epoch 6.17: Loss = 0.695541
Epoch 6.18: Loss = 0.620346
Epoch 6.19: Loss = 0.689178
Epoch 6.20: Loss = 0.589355
Epoch 6.21: Loss = 0.5495
Epoch 6.22: Loss = 0.646133
Epoch 6.23: Loss = 0.711563
Epoch 6.24: Loss = 0.740662
Epoch 6.25: Loss = 0.74852
Epoch 6.26: Loss = 0.594986
Epoch 6.27: Loss = 0.563599
Epoch 6.28: Loss = 0.701752
Epoch 6.29: Loss = 0.599884
Epoch 6.30: Loss = 0.578949
Epoch 6.31: Loss = 0.762207
Epoch 6.32: Loss = 0.640289
Epoch 6.33: Loss = 0.718002
Epoch 6.34: Loss = 0.686859
Epoch 6.35: Loss = 0.616562
Epoch 6.36: Loss = 0.706192
Epoch 6.37: Loss = 0.568787
Epoch 6.38: Loss = 0.674072
Epoch 6.39: Loss = 0.549484
Epoch 6.40: Loss = 0.594437
Epoch 6.41: Loss = 0.650833
Epoch 6.42: Loss = 0.720642
Epoch 6.43: Loss = 0.603119
Epoch 6.44: Loss = 0.612411
Epoch 6.45: Loss = 0.587112
Epoch 6.46: Loss = 0.60199
Epoch 6.47: Loss = 0.657532
Epoch 6.48: Loss = 0.552383
Epoch 6.49: Loss = 0.65004
Epoch 6.50: Loss = 0.635086
Epoch 6.51: Loss = 0.654999
Epoch 6.52: Loss = 0.613449
Epoch 6.53: Loss = 0.704987
Epoch 6.54: Loss = 0.637726
Epoch 6.55: Loss = 0.693619
Epoch 6.56: Loss = 0.572479
Epoch 6.57: Loss = 0.67952
Epoch 6.58: Loss = 0.672089
Epoch 6.59: Loss = 0.643433
Epoch 6.60: Loss = 0.701523
Epoch 6.61: Loss = 0.665451
Epoch 6.62: Loss = 0.572357
Epoch 6.63: Loss = 0.598572
Epoch 6.64: Loss = 0.65332
Epoch 6.65: Loss = 0.720596
Epoch 6.66: Loss = 0.641937
Epoch 6.67: Loss = 0.703079
Epoch 6.68: Loss = 0.733215
Epoch 6.69: Loss = 0.640305
Epoch 6.70: Loss = 0.593994
Epoch 6.71: Loss = 0.588623
Epoch 6.72: Loss = 0.588654
Epoch 6.73: Loss = 0.641693
Epoch 6.74: Loss = 0.670578
Epoch 6.75: Loss = 0.566513
Epoch 6.76: Loss = 0.696274
Epoch 6.77: Loss = 0.61322
Epoch 6.78: Loss = 0.741486
Epoch 6.79: Loss = 0.594147
Epoch 6.80: Loss = 0.613235
Epoch 6.81: Loss = 0.664642
Epoch 6.82: Loss = 0.525391
Epoch 6.83: Loss = 0.617462
Epoch 6.84: Loss = 0.576553
Epoch 6.85: Loss = 0.648529
Epoch 6.86: Loss = 0.62822
Epoch 6.87: Loss = 0.665466
Epoch 6.88: Loss = 0.674698
Epoch 6.89: Loss = 0.637161
Epoch 6.90: Loss = 0.6017
Epoch 6.91: Loss = 0.678268
Epoch 6.92: Loss = 0.63533
Epoch 6.93: Loss = 0.757904
Epoch 6.94: Loss = 0.641205
Epoch 6.95: Loss = 0.491577
Epoch 6.96: Loss = 0.606308
Epoch 6.97: Loss = 0.750122
Epoch 6.98: Loss = 0.574844
Epoch 6.99: Loss = 0.590256
Epoch 6.100: Loss = 0.674408
Epoch 6.101: Loss = 0.69606
Epoch 6.102: Loss = 0.480225
Epoch 6.103: Loss = 0.55632
Epoch 6.104: Loss = 0.65506
Epoch 6.105: Loss = 0.576065
Epoch 6.106: Loss = 0.757996
Epoch 6.107: Loss = 0.592514
Epoch 6.108: Loss = 0.691544
Epoch 6.109: Loss = 0.612671
Epoch 6.110: Loss = 0.584351
Epoch 6.111: Loss = 0.544876
Epoch 6.112: Loss = 0.565384
Epoch 6.113: Loss = 0.657516
Epoch 6.114: Loss = 0.630035
Epoch 6.115: Loss = 0.582108
Epoch 6.116: Loss = 0.634186
Epoch 6.117: Loss = 0.717194
Epoch 6.118: Loss = 0.643631
Epoch 6.119: Loss = 0.630005
Epoch 6.120: Loss = 0.599976
TRAIN LOSS = 0.638046
TRAIN ACC = 79.9805 % (47990/60000)
Loss = 0.579803
Loss = 0.658691
Loss = 0.637924
Loss = 0.555771
Loss = 0.58989
Loss = 0.731857
Loss = 0.806702
Loss = 0.699265
Loss = 0.646774
Loss = 0.601624
Loss = 0.776093
Loss = 0.746048
Loss = 0.683441
Loss = 0.656967
Loss = 0.652145
Loss = 0.672852
Loss = 0.624222
Loss = 0.686584
Loss = 0.692505
Loss = 0.624008
TEST LOSS = 0.666158
TEST ACC = 479.9 % (7926/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.716888
Epoch 7.2: Loss = 0.691498
Epoch 7.3: Loss = 0.663025
Epoch 7.4: Loss = 0.545593
Epoch 7.5: Loss = 0.596619
Epoch 7.6: Loss = 0.67009
Epoch 7.7: Loss = 0.618484
Epoch 7.8: Loss = 0.593063
Epoch 7.9: Loss = 0.619354
Epoch 7.10: Loss = 0.580154
Epoch 7.11: Loss = 0.67363
Epoch 7.12: Loss = 0.656311
Epoch 7.13: Loss = 0.587921
Epoch 7.14: Loss = 0.549988
Epoch 7.15: Loss = 0.669846
Epoch 7.16: Loss = 0.681671
Epoch 7.17: Loss = 0.648911
Epoch 7.18: Loss = 0.519516
Epoch 7.19: Loss = 0.557281
Epoch 7.20: Loss = 0.664108
Epoch 7.21: Loss = 0.699753
Epoch 7.22: Loss = 0.498062
Epoch 7.23: Loss = 0.787292
Epoch 7.24: Loss = 0.578186
Epoch 7.25: Loss = 0.626801
Epoch 7.26: Loss = 0.64772
Epoch 7.27: Loss = 0.62059
Epoch 7.28: Loss = 0.675598
Epoch 7.29: Loss = 0.570343
Epoch 7.30: Loss = 0.604462
Epoch 7.31: Loss = 0.625916
Epoch 7.32: Loss = 0.662003
Epoch 7.33: Loss = 0.60733
Epoch 7.34: Loss = 0.568237
Epoch 7.35: Loss = 0.589523
Epoch 7.36: Loss = 0.58786
Epoch 7.37: Loss = 0.618423
Epoch 7.38: Loss = 0.645615
Epoch 7.39: Loss = 0.745712
Epoch 7.40: Loss = 0.629181
Epoch 7.41: Loss = 0.738007
Epoch 7.42: Loss = 0.551056
Epoch 7.43: Loss = 0.762436
Epoch 7.44: Loss = 0.631332
Epoch 7.45: Loss = 0.539413
Epoch 7.46: Loss = 0.639526
Epoch 7.47: Loss = 0.725357
Epoch 7.48: Loss = 0.502533
Epoch 7.49: Loss = 0.700729
Epoch 7.50: Loss = 0.624741
Epoch 7.51: Loss = 0.620193
Epoch 7.52: Loss = 0.636246
Epoch 7.53: Loss = 0.63623
Epoch 7.54: Loss = 0.627014
Epoch 7.55: Loss = 0.67189
Epoch 7.56: Loss = 0.631638
Epoch 7.57: Loss = 0.706818
Epoch 7.58: Loss = 0.593506
Epoch 7.59: Loss = 0.6082
Epoch 7.60: Loss = 0.74678
Epoch 7.61: Loss = 0.573166
Epoch 7.62: Loss = 0.546265
Epoch 7.63: Loss = 0.723709
Epoch 7.64: Loss = 0.650101
Epoch 7.65: Loss = 0.625931
Epoch 7.66: Loss = 0.705399
Epoch 7.67: Loss = 0.635147
Epoch 7.68: Loss = 0.623734
Epoch 7.69: Loss = 0.552277
Epoch 7.70: Loss = 0.72435
Epoch 7.71: Loss = 0.733444
Epoch 7.72: Loss = 0.698044
Epoch 7.73: Loss = 0.597626
Epoch 7.74: Loss = 0.611847
Epoch 7.75: Loss = 0.65657
Epoch 7.76: Loss = 0.663315
Epoch 7.77: Loss = 0.602509
Epoch 7.78: Loss = 0.657974
Epoch 7.79: Loss = 0.595749
Epoch 7.80: Loss = 0.726913
Epoch 7.81: Loss = 0.58519
Epoch 7.82: Loss = 0.499939
Epoch 7.83: Loss = 0.776001
Epoch 7.84: Loss = 0.568283
Epoch 7.85: Loss = 0.657425
Epoch 7.86: Loss = 0.621201
Epoch 7.87: Loss = 0.641968
Epoch 7.88: Loss = 0.617767
Epoch 7.89: Loss = 0.645981
Epoch 7.90: Loss = 0.790436
Epoch 7.91: Loss = 0.658203
Epoch 7.92: Loss = 0.601639
Epoch 7.93: Loss = 0.666412
Epoch 7.94: Loss = 0.569366
Epoch 7.95: Loss = 0.493958
Epoch 7.96: Loss = 0.671036
Epoch 7.97: Loss = 0.661606
Epoch 7.98: Loss = 0.608643
Epoch 7.99: Loss = 0.547333
Epoch 7.100: Loss = 0.59523
Epoch 7.101: Loss = 0.683228
Epoch 7.102: Loss = 0.57225
Epoch 7.103: Loss = 0.654572
Epoch 7.104: Loss = 0.593613
Epoch 7.105: Loss = 0.63916
Epoch 7.106: Loss = 0.597794
Epoch 7.107: Loss = 0.622864
Epoch 7.108: Loss = 0.640854
Epoch 7.109: Loss = 0.800369
Epoch 7.110: Loss = 0.762589
Epoch 7.111: Loss = 0.695129
Epoch 7.112: Loss = 0.555023
Epoch 7.113: Loss = 0.713974
Epoch 7.114: Loss = 0.578415
Epoch 7.115: Loss = 0.693832
Epoch 7.116: Loss = 0.722778
Epoch 7.117: Loss = 0.642349
Epoch 7.118: Loss = 0.601166
Epoch 7.119: Loss = 0.608322
Epoch 7.120: Loss = 0.706299
TRAIN LOSS = 0.637115
TRAIN ACC = 80.4443 % (48269/60000)
Loss = 0.574722
Loss = 0.665192
Loss = 0.657532
Loss = 0.547394
Loss = 0.597656
Loss = 0.730667
Loss = 0.815613
Loss = 0.701431
Loss = 0.649658
Loss = 0.602386
Loss = 0.813278
Loss = 0.757263
Loss = 0.704132
Loss = 0.676788
Loss = 0.639343
Loss = 0.666214
Loss = 0.643356
Loss = 0.702209
Loss = 0.684082
Loss = 0.632294
TEST LOSS = 0.67306
TEST ACC = 482.689 % (7961/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.594238
Epoch 8.2: Loss = 0.611816
Epoch 8.3: Loss = 0.661987
Epoch 8.4: Loss = 0.602036
Epoch 8.5: Loss = 0.67865
Epoch 8.6: Loss = 0.710617
Epoch 8.7: Loss = 0.625809
Epoch 8.8: Loss = 0.699097
Epoch 8.9: Loss = 0.562271
Epoch 8.10: Loss = 0.721344
Epoch 8.11: Loss = 0.641388
Epoch 8.12: Loss = 0.679047
Epoch 8.13: Loss = 0.740509
Epoch 8.14: Loss = 0.57637
Epoch 8.15: Loss = 0.580597
Epoch 8.16: Loss = 0.620743
Epoch 8.17: Loss = 0.669479
Epoch 8.18: Loss = 0.573395
Epoch 8.19: Loss = 0.557983
Epoch 8.20: Loss = 0.616547
Epoch 8.21: Loss = 0.665421
Epoch 8.22: Loss = 0.695847
Epoch 8.23: Loss = 0.519989
Epoch 8.24: Loss = 0.641235
Epoch 8.25: Loss = 0.57074
Epoch 8.26: Loss = 0.583511
Epoch 8.27: Loss = 0.646484
Epoch 8.28: Loss = 0.726166
Epoch 8.29: Loss = 0.513519
Epoch 8.30: Loss = 0.640625
Epoch 8.31: Loss = 0.665695
Epoch 8.32: Loss = 0.596527
Epoch 8.33: Loss = 0.617157
Epoch 8.34: Loss = 0.569809
Epoch 8.35: Loss = 0.61499
Epoch 8.36: Loss = 0.65271
Epoch 8.37: Loss = 0.62027
Epoch 8.38: Loss = 0.574478
Epoch 8.39: Loss = 0.560104
Epoch 8.40: Loss = 0.576965
Epoch 8.41: Loss = 0.656342
Epoch 8.42: Loss = 0.591949
Epoch 8.43: Loss = 0.542648
Epoch 8.44: Loss = 0.708862
Epoch 8.45: Loss = 0.68869
Epoch 8.46: Loss = 0.73671
Epoch 8.47: Loss = 0.63118
Epoch 8.48: Loss = 0.683304
Epoch 8.49: Loss = 0.666168
Epoch 8.50: Loss = 0.600403
Epoch 8.51: Loss = 0.600998
Epoch 8.52: Loss = 0.6539
Epoch 8.53: Loss = 0.659836
Epoch 8.54: Loss = 0.562057
Epoch 8.55: Loss = 0.776276
Epoch 8.56: Loss = 0.590607
Epoch 8.57: Loss = 0.770218
Epoch 8.58: Loss = 0.676895
Epoch 8.59: Loss = 0.58609
Epoch 8.60: Loss = 0.646652
Epoch 8.61: Loss = 0.585587
Epoch 8.62: Loss = 0.741058
Epoch 8.63: Loss = 0.649597
Epoch 8.64: Loss = 0.651306
Epoch 8.65: Loss = 0.710968
Epoch 8.66: Loss = 0.571823
Epoch 8.67: Loss = 0.595261
Epoch 8.68: Loss = 0.67395
Epoch 8.69: Loss = 0.696243
Epoch 8.70: Loss = 0.539948
Epoch 8.71: Loss = 0.598587
Epoch 8.72: Loss = 0.62735
Epoch 8.73: Loss = 0.545975
Epoch 8.74: Loss = 0.526672
Epoch 8.75: Loss = 0.598495
Epoch 8.76: Loss = 0.611298
Epoch 8.77: Loss = 0.725922
Epoch 8.78: Loss = 0.617889
Epoch 8.79: Loss = 0.577301
Epoch 8.80: Loss = 0.706207
Epoch 8.81: Loss = 0.603882
Epoch 8.82: Loss = 0.717804
Epoch 8.83: Loss = 0.714218
Epoch 8.84: Loss = 0.70314
Epoch 8.85: Loss = 0.703522
Epoch 8.86: Loss = 0.664398
Epoch 8.87: Loss = 0.661789
Epoch 8.88: Loss = 0.532959
Epoch 8.89: Loss = 0.660019
Epoch 8.90: Loss = 0.670456
Epoch 8.91: Loss = 0.559235
Epoch 8.92: Loss = 0.773651
Epoch 8.93: Loss = 0.71048
Epoch 8.94: Loss = 0.781799
Epoch 8.95: Loss = 0.631989
Epoch 8.96: Loss = 0.697739
Epoch 8.97: Loss = 0.556473
Epoch 8.98: Loss = 0.579941
Epoch 8.99: Loss = 0.644562
Epoch 8.100: Loss = 0.684158
Epoch 8.101: Loss = 0.788834
Epoch 8.102: Loss = 0.63298
Epoch 8.103: Loss = 0.545563
Epoch 8.104: Loss = 0.562668
Epoch 8.105: Loss = 0.674423
Epoch 8.106: Loss = 0.721848
Epoch 8.107: Loss = 0.525055
Epoch 8.108: Loss = 0.540787
Epoch 8.109: Loss = 0.743103
Epoch 8.110: Loss = 0.669189
Epoch 8.111: Loss = 0.524048
Epoch 8.112: Loss = 0.65033
Epoch 8.113: Loss = 0.574539
Epoch 8.114: Loss = 0.719666
Epoch 8.115: Loss = 0.658813
Epoch 8.116: Loss = 0.605667
Epoch 8.117: Loss = 0.665634
Epoch 8.118: Loss = 0.64418
Epoch 8.119: Loss = 0.662003
Epoch 8.120: Loss = 0.620575
TRAIN LOSS = 0.637848
TRAIN ACC = 80.8319 % (48501/60000)
Loss = 0.561584
Loss = 0.679398
Loss = 0.645691
Loss = 0.561661
Loss = 0.622971
Loss = 0.736862
Loss = 0.807068
Loss = 0.698318
Loss = 0.669724
Loss = 0.612244
Loss = 0.810318
Loss = 0.772964
Loss = 0.718658
Loss = 0.671448
Loss = 0.653992
Loss = 0.6595
Loss = 0.639099
Loss = 0.712555
Loss = 0.713913
Loss = 0.614151
TEST LOSS = 0.678106
TEST ACC = 485.01 % (7985/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.612305
Epoch 9.2: Loss = 0.649323
Epoch 9.3: Loss = 0.677353
Epoch 9.4: Loss = 0.684341
Epoch 9.5: Loss = 0.674103
Epoch 9.6: Loss = 0.670746
Epoch 9.7: Loss = 0.67865
Epoch 9.8: Loss = 0.67305
Epoch 9.9: Loss = 0.76767
Epoch 9.10: Loss = 0.657791
Epoch 9.11: Loss = 0.7202
Epoch 9.12: Loss = 0.588699
Epoch 9.13: Loss = 0.583313
Epoch 9.14: Loss = 0.641464
Epoch 9.15: Loss = 0.527512
Epoch 9.16: Loss = 0.670715
Epoch 9.17: Loss = 0.593582
Epoch 9.18: Loss = 0.611801
Epoch 9.19: Loss = 0.595261
Epoch 9.20: Loss = 0.611404
Epoch 9.21: Loss = 0.653549
Epoch 9.22: Loss = 0.648315
Epoch 9.23: Loss = 0.570816
Epoch 9.24: Loss = 0.721085
Epoch 9.25: Loss = 0.601303
Epoch 9.26: Loss = 0.787506
Epoch 9.27: Loss = 0.598877
Epoch 9.28: Loss = 0.630966
Epoch 9.29: Loss = 0.700043
Epoch 9.30: Loss = 0.572403
Epoch 9.31: Loss = 0.528687
Epoch 9.32: Loss = 0.636703
Epoch 9.33: Loss = 0.662262
Epoch 9.34: Loss = 0.540253
Epoch 9.35: Loss = 0.511032
Epoch 9.36: Loss = 0.544647
Epoch 9.37: Loss = 0.652069
Epoch 9.38: Loss = 0.616272
Epoch 9.39: Loss = 0.707306
Epoch 9.40: Loss = 0.646286
Epoch 9.41: Loss = 0.634033
Epoch 9.42: Loss = 0.552719
Epoch 9.43: Loss = 0.60408
Epoch 9.44: Loss = 0.659866
Epoch 9.45: Loss = 0.704926
Epoch 9.46: Loss = 0.566101
Epoch 9.47: Loss = 0.574051
Epoch 9.48: Loss = 0.652466
Epoch 9.49: Loss = 0.593506
Epoch 9.50: Loss = 0.558899
Epoch 9.51: Loss = 0.751694
Epoch 9.52: Loss = 0.7435
Epoch 9.53: Loss = 0.628922
Epoch 9.54: Loss = 0.540298
Epoch 9.55: Loss = 0.729797
Epoch 9.56: Loss = 0.589737
Epoch 9.57: Loss = 0.684006
Epoch 9.58: Loss = 0.612625
Epoch 9.59: Loss = 0.664932
Epoch 9.60: Loss = 0.679291
Epoch 9.61: Loss = 0.585526
Epoch 9.62: Loss = 0.721588
Epoch 9.63: Loss = 0.629318
Epoch 9.64: Loss = 0.572159
Epoch 9.65: Loss = 0.644547
Epoch 9.66: Loss = 0.602127
Epoch 9.67: Loss = 0.590988
Epoch 9.68: Loss = 0.651199
Epoch 9.69: Loss = 0.587021
Epoch 9.70: Loss = 0.73468
Epoch 9.71: Loss = 0.496918
Epoch 9.72: Loss = 0.658905
Epoch 9.73: Loss = 0.777344
Epoch 9.74: Loss = 0.618927
Epoch 9.75: Loss = 0.680542
Epoch 9.76: Loss = 0.575134
Epoch 9.77: Loss = 0.677261
Epoch 9.78: Loss = 0.721802
Epoch 9.79: Loss = 0.680954
Epoch 9.80: Loss = 0.564102
Epoch 9.81: Loss = 0.585678
Epoch 9.82: Loss = 0.641953
Epoch 9.83: Loss = 0.56778
Epoch 9.84: Loss = 0.680023
Epoch 9.85: Loss = 0.564072
Epoch 9.86: Loss = 0.639984
Epoch 9.87: Loss = 0.726624
Epoch 9.88: Loss = 0.67598
Epoch 9.89: Loss = 0.589233
Epoch 9.90: Loss = 0.523346
Epoch 9.91: Loss = 0.561111
Epoch 9.92: Loss = 0.638199
Epoch 9.93: Loss = 0.523987
Epoch 9.94: Loss = 0.554047
Epoch 9.95: Loss = 0.722107
Epoch 9.96: Loss = 0.601471
Epoch 9.97: Loss = 0.623566
Epoch 9.98: Loss = 0.666946
Epoch 9.99: Loss = 0.568085
Epoch 9.100: Loss = 0.563568
Epoch 9.101: Loss = 0.678436
Epoch 9.102: Loss = 0.684708
Epoch 9.103: Loss = 0.577332
Epoch 9.104: Loss = 0.614273
Epoch 9.105: Loss = 0.638702
Epoch 9.106: Loss = 0.697632
Epoch 9.107: Loss = 0.727005
Epoch 9.108: Loss = 0.567307
Epoch 9.109: Loss = 0.649765
Epoch 9.110: Loss = 0.706955
Epoch 9.111: Loss = 0.68869
Epoch 9.112: Loss = 0.653748
Epoch 9.113: Loss = 0.606125
Epoch 9.114: Loss = 0.678513
Epoch 9.115: Loss = 0.644363
Epoch 9.116: Loss = 0.68248
Epoch 9.117: Loss = 0.681061
Epoch 9.118: Loss = 0.675232
Epoch 9.119: Loss = 0.584457
Epoch 9.120: Loss = 0.608658
TRAIN LOSS = 0.634842
TRAIN ACC = 80.9158 % (48552/60000)
Loss = 0.555328
Loss = 0.661453
Loss = 0.621826
Loss = 0.549927
Loss = 0.595825
Loss = 0.735657
Loss = 0.845016
Loss = 0.689133
Loss = 0.658554
Loss = 0.6082
Loss = 0.800766
Loss = 0.734451
Loss = 0.6996
Loss = 0.648071
Loss = 0.609161
Loss = 0.64949
Loss = 0.643768
Loss = 0.688599
Loss = 0.673019
Loss = 0.608749
TEST LOSS = 0.66383
TEST ACC = 485.519 % (7977/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.699463
Epoch 10.2: Loss = 0.718399
Epoch 10.3: Loss = 0.654099
Epoch 10.4: Loss = 0.751785
Epoch 10.5: Loss = 0.636993
Epoch 10.6: Loss = 0.71991
Epoch 10.7: Loss = 0.670441
Epoch 10.8: Loss = 0.678696
Epoch 10.9: Loss = 0.620529
Epoch 10.10: Loss = 0.670868
Epoch 10.11: Loss = 0.586426
Epoch 10.12: Loss = 0.647873
Epoch 10.13: Loss = 0.683838
Epoch 10.14: Loss = 0.652496
Epoch 10.15: Loss = 0.587875
Epoch 10.16: Loss = 0.691681
Epoch 10.17: Loss = 0.627396
Epoch 10.18: Loss = 0.666428
Epoch 10.19: Loss = 0.614471
Epoch 10.20: Loss = 0.62233
Epoch 10.21: Loss = 0.703506
Epoch 10.22: Loss = 0.537247
Epoch 10.23: Loss = 0.622513
Epoch 10.24: Loss = 0.628479
Epoch 10.25: Loss = 0.588211
Epoch 10.26: Loss = 0.696045
Epoch 10.27: Loss = 0.743469
Epoch 10.28: Loss = 0.623062
Epoch 10.29: Loss = 0.576645
Epoch 10.30: Loss = 0.605972
Epoch 10.31: Loss = 0.661362
Epoch 10.32: Loss = 0.759811
Epoch 10.33: Loss = 0.642334
Epoch 10.34: Loss = 0.689392
Epoch 10.35: Loss = 0.726395
Epoch 10.36: Loss = 0.669525
Epoch 10.37: Loss = 0.670853
Epoch 10.38: Loss = 0.531311
Epoch 10.39: Loss = 0.50415
Epoch 10.40: Loss = 0.587769
Epoch 10.41: Loss = 0.589111
Epoch 10.42: Loss = 0.549622
Epoch 10.43: Loss = 0.543793
Epoch 10.44: Loss = 0.542877
Epoch 10.45: Loss = 0.660965
Epoch 10.46: Loss = 0.742844
Epoch 10.47: Loss = 0.566147
Epoch 10.48: Loss = 0.791519
Epoch 10.49: Loss = 0.648621
Epoch 10.50: Loss = 0.615417
Epoch 10.51: Loss = 0.59108
Epoch 10.52: Loss = 0.637589
Epoch 10.53: Loss = 0.555176
Epoch 10.54: Loss = 0.631943
Epoch 10.55: Loss = 0.612381
Epoch 10.56: Loss = 0.614471
Epoch 10.57: Loss = 0.69957
Epoch 10.58: Loss = 0.695068
Epoch 10.59: Loss = 0.566345
Epoch 10.60: Loss = 0.529053
Epoch 10.61: Loss = 0.741287
Epoch 10.62: Loss = 0.631409
Epoch 10.63: Loss = 0.692734
Epoch 10.64: Loss = 0.680969
Epoch 10.65: Loss = 0.647446
Epoch 10.66: Loss = 0.694946
Epoch 10.67: Loss = 0.586975
Epoch 10.68: Loss = 0.631439
Epoch 10.69: Loss = 0.693527
Epoch 10.70: Loss = 0.59494
Epoch 10.71: Loss = 0.627747
Epoch 10.72: Loss = 0.619568
Epoch 10.73: Loss = 0.631088
Epoch 10.74: Loss = 0.573349
Epoch 10.75: Loss = 0.565735
Epoch 10.76: Loss = 0.633026
Epoch 10.77: Loss = 0.512756
Epoch 10.78: Loss = 0.587601
Epoch 10.79: Loss = 0.672302
Epoch 10.80: Loss = 0.638351
Epoch 10.81: Loss = 0.664139
Epoch 10.82: Loss = 0.632568
Epoch 10.83: Loss = 0.595886
Epoch 10.84: Loss = 0.555801
Epoch 10.85: Loss = 0.833267
Epoch 10.86: Loss = 0.565872
Epoch 10.87: Loss = 0.652908
Epoch 10.88: Loss = 0.602936
Epoch 10.89: Loss = 0.596695
Epoch 10.90: Loss = 0.644897
Epoch 10.91: Loss = 0.577744
Epoch 10.92: Loss = 0.633987
Epoch 10.93: Loss = 0.668243
Epoch 10.94: Loss = 0.550674
Epoch 10.95: Loss = 0.680298
Epoch 10.96: Loss = 0.611694
Epoch 10.97: Loss = 0.495346
Epoch 10.98: Loss = 0.631195
Epoch 10.99: Loss = 0.68718
Epoch 10.100: Loss = 0.62384
Epoch 10.101: Loss = 0.594864
Epoch 10.102: Loss = 0.722946
Epoch 10.103: Loss = 0.66124
Epoch 10.104: Loss = 0.675385
Epoch 10.105: Loss = 0.548874
Epoch 10.106: Loss = 0.631622
Epoch 10.107: Loss = 0.565918
Epoch 10.108: Loss = 0.666077
Epoch 10.109: Loss = 0.659515
Epoch 10.110: Loss = 0.581619
Epoch 10.111: Loss = 0.649368
Epoch 10.112: Loss = 0.531769
Epoch 10.113: Loss = 0.580978
Epoch 10.114: Loss = 0.740585
Epoch 10.115: Loss = 0.575684
Epoch 10.116: Loss = 0.641541
Epoch 10.117: Loss = 0.6707
Epoch 10.118: Loss = 0.621918
Epoch 10.119: Loss = 0.63208
Epoch 10.120: Loss = 0.717239
TRAIN LOSS = 0.634903
TRAIN ACC = 81.1081 % (48667/60000)
Loss = 0.576721
Loss = 0.655472
Loss = 0.625458
Loss = 0.5513
Loss = 0.588974
Loss = 0.734741
Loss = 0.849731
Loss = 0.707153
Loss = 0.653336
Loss = 0.624817
Loss = 0.82399
Loss = 0.74408
Loss = 0.698288
Loss = 0.637955
Loss = 0.612137
Loss = 0.645386
Loss = 0.657303
Loss = 0.690918
Loss = 0.680695
Loss = 0.614075
TEST LOSS = 0.668626
TEST ACC = 486.67 % (7992/10000)

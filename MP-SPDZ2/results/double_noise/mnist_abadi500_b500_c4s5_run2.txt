Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.42642
Epoch 1.2: Loss = 2.3577
Epoch 1.3: Loss = 2.32579
Epoch 1.4: Loss = 2.27119
Epoch 1.5: Loss = 2.19994
Epoch 1.6: Loss = 2.17493
Epoch 1.7: Loss = 2.10376
Epoch 1.8: Loss = 2.08394
Epoch 1.9: Loss = 2.03145
Epoch 1.10: Loss = 1.9684
Epoch 1.11: Loss = 1.94449
Epoch 1.12: Loss = 1.92236
Epoch 1.13: Loss = 1.88506
Epoch 1.14: Loss = 1.83081
Epoch 1.15: Loss = 1.84492
Epoch 1.16: Loss = 1.7816
Epoch 1.17: Loss = 1.74843
Epoch 1.18: Loss = 1.69484
Epoch 1.19: Loss = 1.7007
Epoch 1.20: Loss = 1.65463
Epoch 1.21: Loss = 1.61485
Epoch 1.22: Loss = 1.62932
Epoch 1.23: Loss = 1.57372
Epoch 1.24: Loss = 1.53247
Epoch 1.25: Loss = 1.48225
Epoch 1.26: Loss = 1.48552
Epoch 1.27: Loss = 1.41606
Epoch 1.28: Loss = 1.37863
Epoch 1.29: Loss = 1.41617
Epoch 1.30: Loss = 1.37416
Epoch 1.31: Loss = 1.37471
Epoch 1.32: Loss = 1.35185
Epoch 1.33: Loss = 1.31512
Epoch 1.34: Loss = 1.29831
Epoch 1.35: Loss = 1.26736
Epoch 1.36: Loss = 1.24129
Epoch 1.37: Loss = 1.20265
Epoch 1.38: Loss = 1.23364
Epoch 1.39: Loss = 1.19063
Epoch 1.40: Loss = 1.16531
Epoch 1.41: Loss = 1.16325
Epoch 1.42: Loss = 1.16748
Epoch 1.43: Loss = 1.10796
Epoch 1.44: Loss = 1.08372
Epoch 1.45: Loss = 1.05867
Epoch 1.46: Loss = 1.06035
Epoch 1.47: Loss = 1.10214
Epoch 1.48: Loss = 1.0838
Epoch 1.49: Loss = 1.0649
Epoch 1.50: Loss = 1.02902
Epoch 1.51: Loss = 0.996872
Epoch 1.52: Loss = 0.977036
Epoch 1.53: Loss = 0.948303
Epoch 1.54: Loss = 1.02245
Epoch 1.55: Loss = 1.02266
Epoch 1.56: Loss = 0.990814
Epoch 1.57: Loss = 0.989731
Epoch 1.58: Loss = 0.92865
Epoch 1.59: Loss = 1.00719
Epoch 1.60: Loss = 0.902222
Epoch 1.61: Loss = 0.960541
Epoch 1.62: Loss = 0.890579
Epoch 1.63: Loss = 0.901413
Epoch 1.64: Loss = 0.859543
Epoch 1.65: Loss = 0.907272
Epoch 1.66: Loss = 0.862488
Epoch 1.67: Loss = 0.849564
Epoch 1.68: Loss = 0.882935
Epoch 1.69: Loss = 0.795471
Epoch 1.70: Loss = 0.793869
Epoch 1.71: Loss = 0.827759
Epoch 1.72: Loss = 0.832123
Epoch 1.73: Loss = 0.800354
Epoch 1.74: Loss = 0.754105
Epoch 1.75: Loss = 0.747269
Epoch 1.76: Loss = 0.792526
Epoch 1.77: Loss = 0.735901
Epoch 1.78: Loss = 0.805496
Epoch 1.79: Loss = 0.703018
Epoch 1.80: Loss = 0.694092
Epoch 1.81: Loss = 0.739136
Epoch 1.82: Loss = 0.713974
Epoch 1.83: Loss = 0.695297
Epoch 1.84: Loss = 0.818497
Epoch 1.85: Loss = 0.710526
Epoch 1.86: Loss = 0.688004
Epoch 1.87: Loss = 0.724152
Epoch 1.88: Loss = 0.705658
Epoch 1.89: Loss = 0.678513
Epoch 1.90: Loss = 0.696411
Epoch 1.91: Loss = 0.728043
Epoch 1.92: Loss = 0.70607
Epoch 1.93: Loss = 0.631882
Epoch 1.94: Loss = 0.67012
Epoch 1.95: Loss = 0.742798
Epoch 1.96: Loss = 0.675995
Epoch 1.97: Loss = 0.627884
Epoch 1.98: Loss = 0.609314
Epoch 1.99: Loss = 0.671249
Epoch 1.100: Loss = 0.665756
Epoch 1.101: Loss = 0.651382
Epoch 1.102: Loss = 0.620865
Epoch 1.103: Loss = 0.680878
Epoch 1.104: Loss = 0.652649
Epoch 1.105: Loss = 0.630798
Epoch 1.106: Loss = 0.604172
Epoch 1.107: Loss = 0.623138
Epoch 1.108: Loss = 0.664749
Epoch 1.109: Loss = 0.558273
Epoch 1.110: Loss = 0.673538
Epoch 1.111: Loss = 0.679077
Epoch 1.112: Loss = 0.661469
Epoch 1.113: Loss = 0.578232
Epoch 1.114: Loss = 0.617722
Epoch 1.115: Loss = 0.593567
Epoch 1.116: Loss = 0.586868
Epoch 1.117: Loss = 0.608582
Epoch 1.118: Loss = 0.569626
Epoch 1.119: Loss = 0.665527
Epoch 1.120: Loss = 0.599243
TRAIN LOSS = 1.09183
TRAIN ACC = 69.0598 % (41438/60000)
Loss = 0.630676
Loss = 0.644821
Loss = 0.75386
Loss = 0.693237
Loss = 0.735535
Loss = 0.631744
Loss = 0.605164
Loss = 0.762863
Loss = 0.711456
Loss = 0.655975
Loss = 0.357056
Loss = 0.494125
Loss = 0.378021
Loss = 0.597504
Loss = 0.476852
Loss = 0.429977
Loss = 0.44986
Loss = 0.244629
Loss = 0.412384
Loss = 0.738297
TEST LOSS = 0.570202
TEST ACC = 414.38 % (8277/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.656769
Epoch 2.2: Loss = 0.605713
Epoch 2.3: Loss = 0.584625
Epoch 2.4: Loss = 0.647171
Epoch 2.5: Loss = 0.560715
Epoch 2.6: Loss = 0.511719
Epoch 2.7: Loss = 0.638016
Epoch 2.8: Loss = 0.623962
Epoch 2.9: Loss = 0.566925
Epoch 2.10: Loss = 0.543762
Epoch 2.11: Loss = 0.64151
Epoch 2.12: Loss = 0.596756
Epoch 2.13: Loss = 0.51944
Epoch 2.14: Loss = 0.552826
Epoch 2.15: Loss = 0.585709
Epoch 2.16: Loss = 0.622391
Epoch 2.17: Loss = 0.578125
Epoch 2.18: Loss = 0.643555
Epoch 2.19: Loss = 0.597855
Epoch 2.20: Loss = 0.536453
Epoch 2.21: Loss = 0.534561
Epoch 2.22: Loss = 0.555679
Epoch 2.23: Loss = 0.505524
Epoch 2.24: Loss = 0.55629
Epoch 2.25: Loss = 0.582642
Epoch 2.26: Loss = 0.570755
Epoch 2.27: Loss = 0.52948
Epoch 2.28: Loss = 0.627243
Epoch 2.29: Loss = 0.576096
Epoch 2.30: Loss = 0.481812
Epoch 2.31: Loss = 0.569565
Epoch 2.32: Loss = 0.508423
Epoch 2.33: Loss = 0.462891
Epoch 2.34: Loss = 0.485168
Epoch 2.35: Loss = 0.638855
Epoch 2.36: Loss = 0.585129
Epoch 2.37: Loss = 0.561447
Epoch 2.38: Loss = 0.475601
Epoch 2.39: Loss = 0.586807
Epoch 2.40: Loss = 0.592026
Epoch 2.41: Loss = 0.490509
Epoch 2.42: Loss = 0.619553
Epoch 2.43: Loss = 0.557312
Epoch 2.44: Loss = 0.529846
Epoch 2.45: Loss = 0.540466
Epoch 2.46: Loss = 0.564865
Epoch 2.47: Loss = 0.577011
Epoch 2.48: Loss = 0.489365
Epoch 2.49: Loss = 0.540909
Epoch 2.50: Loss = 0.508606
Epoch 2.51: Loss = 0.612869
Epoch 2.52: Loss = 0.512558
Epoch 2.53: Loss = 0.531082
Epoch 2.54: Loss = 0.601334
Epoch 2.55: Loss = 0.535599
Epoch 2.56: Loss = 0.509445
Epoch 2.57: Loss = 0.448914
Epoch 2.58: Loss = 0.615601
Epoch 2.59: Loss = 0.524323
Epoch 2.60: Loss = 0.533081
Epoch 2.61: Loss = 0.565659
Epoch 2.62: Loss = 0.463974
Epoch 2.63: Loss = 0.506561
Epoch 2.64: Loss = 0.52359
Epoch 2.65: Loss = 0.567902
Epoch 2.66: Loss = 0.596039
Epoch 2.67: Loss = 0.696579
Epoch 2.68: Loss = 0.432983
Epoch 2.69: Loss = 0.515015
Epoch 2.70: Loss = 0.444138
Epoch 2.71: Loss = 0.552582
Epoch 2.72: Loss = 0.48587
Epoch 2.73: Loss = 0.571732
Epoch 2.74: Loss = 0.529831
Epoch 2.75: Loss = 0.471313
Epoch 2.76: Loss = 0.453094
Epoch 2.77: Loss = 0.562103
Epoch 2.78: Loss = 0.492493
Epoch 2.79: Loss = 0.499924
Epoch 2.80: Loss = 0.476212
Epoch 2.81: Loss = 0.481598
Epoch 2.82: Loss = 0.579727
Epoch 2.83: Loss = 0.544342
Epoch 2.84: Loss = 0.512466
Epoch 2.85: Loss = 0.522903
Epoch 2.86: Loss = 0.447357
Epoch 2.87: Loss = 0.540176
Epoch 2.88: Loss = 0.514175
Epoch 2.89: Loss = 0.493591
Epoch 2.90: Loss = 0.533585
Epoch 2.91: Loss = 0.563919
Epoch 2.92: Loss = 0.590561
Epoch 2.93: Loss = 0.49968
Epoch 2.94: Loss = 0.576263
Epoch 2.95: Loss = 0.584854
Epoch 2.96: Loss = 0.440613
Epoch 2.97: Loss = 0.530258
Epoch 2.98: Loss = 0.442123
Epoch 2.99: Loss = 0.455307
Epoch 2.100: Loss = 0.543884
Epoch 2.101: Loss = 0.4953
Epoch 2.102: Loss = 0.592209
Epoch 2.103: Loss = 0.607437
Epoch 2.104: Loss = 0.519974
Epoch 2.105: Loss = 0.509338
Epoch 2.106: Loss = 0.51561
Epoch 2.107: Loss = 0.545364
Epoch 2.108: Loss = 0.502716
Epoch 2.109: Loss = 0.486404
Epoch 2.110: Loss = 0.538605
Epoch 2.111: Loss = 0.408325
Epoch 2.112: Loss = 0.442413
Epoch 2.113: Loss = 0.456299
Epoch 2.114: Loss = 0.582169
Epoch 2.115: Loss = 0.521683
Epoch 2.116: Loss = 0.490738
Epoch 2.117: Loss = 0.493713
Epoch 2.118: Loss = 0.605347
Epoch 2.119: Loss = 0.527603
Epoch 2.120: Loss = 0.428162
TRAIN LOSS = 0.539337
TRAIN ACC = 83.5556 % (50136/60000)
Loss = 0.497147
Loss = 0.592026
Loss = 0.676636
Loss = 0.638565
Loss = 0.660431
Loss = 0.518311
Loss = 0.496353
Loss = 0.67952
Loss = 0.622208
Loss = 0.570587
Loss = 0.260239
Loss = 0.407425
Loss = 0.306351
Loss = 0.501007
Loss = 0.337845
Loss = 0.374527
Loss = 0.367859
Loss = 0.156036
Loss = 0.325867
Loss = 0.669144
TEST LOSS = 0.482904
TEST ACC = 501.36 % (8523/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.558548
Epoch 3.2: Loss = 0.4729
Epoch 3.3: Loss = 0.499268
Epoch 3.4: Loss = 0.565643
Epoch 3.5: Loss = 0.577042
Epoch 3.6: Loss = 0.444855
Epoch 3.7: Loss = 0.450134
Epoch 3.8: Loss = 0.549423
Epoch 3.9: Loss = 0.517181
Epoch 3.10: Loss = 0.476593
Epoch 3.11: Loss = 0.575974
Epoch 3.12: Loss = 0.458511
Epoch 3.13: Loss = 0.456665
Epoch 3.14: Loss = 0.626205
Epoch 3.15: Loss = 0.544571
Epoch 3.16: Loss = 0.529526
Epoch 3.17: Loss = 0.446167
Epoch 3.18: Loss = 0.506149
Epoch 3.19: Loss = 0.459091
Epoch 3.20: Loss = 0.537857
Epoch 3.21: Loss = 0.54332
Epoch 3.22: Loss = 0.561981
Epoch 3.23: Loss = 0.533478
Epoch 3.24: Loss = 0.58812
Epoch 3.25: Loss = 0.524811
Epoch 3.26: Loss = 0.53392
Epoch 3.27: Loss = 0.458755
Epoch 3.28: Loss = 0.534256
Epoch 3.29: Loss = 0.577576
Epoch 3.30: Loss = 0.547623
Epoch 3.31: Loss = 0.483521
Epoch 3.32: Loss = 0.481415
Epoch 3.33: Loss = 0.533585
Epoch 3.34: Loss = 0.424133
Epoch 3.35: Loss = 0.503265
Epoch 3.36: Loss = 0.500397
Epoch 3.37: Loss = 0.462112
Epoch 3.38: Loss = 0.43364
Epoch 3.39: Loss = 0.493179
Epoch 3.40: Loss = 0.509247
Epoch 3.41: Loss = 0.578171
Epoch 3.42: Loss = 0.563828
Epoch 3.43: Loss = 0.475174
Epoch 3.44: Loss = 0.574051
Epoch 3.45: Loss = 0.547043
Epoch 3.46: Loss = 0.476578
Epoch 3.47: Loss = 0.463303
Epoch 3.48: Loss = 0.568512
Epoch 3.49: Loss = 0.562378
Epoch 3.50: Loss = 0.550186
Epoch 3.51: Loss = 0.442688
Epoch 3.52: Loss = 0.425049
Epoch 3.53: Loss = 0.409027
Epoch 3.54: Loss = 0.523956
Epoch 3.55: Loss = 0.525742
Epoch 3.56: Loss = 0.491684
Epoch 3.57: Loss = 0.566986
Epoch 3.58: Loss = 0.560089
Epoch 3.59: Loss = 0.41423
Epoch 3.60: Loss = 0.516312
Epoch 3.61: Loss = 0.599213
Epoch 3.62: Loss = 0.454666
Epoch 3.63: Loss = 0.542343
Epoch 3.64: Loss = 0.528702
Epoch 3.65: Loss = 0.546768
Epoch 3.66: Loss = 0.453201
Epoch 3.67: Loss = 0.489105
Epoch 3.68: Loss = 0.460266
Epoch 3.69: Loss = 0.507217
Epoch 3.70: Loss = 0.385986
Epoch 3.71: Loss = 0.497192
Epoch 3.72: Loss = 0.545059
Epoch 3.73: Loss = 0.456131
Epoch 3.74: Loss = 0.521133
Epoch 3.75: Loss = 0.457962
Epoch 3.76: Loss = 0.481873
Epoch 3.77: Loss = 0.532013
Epoch 3.78: Loss = 0.491257
Epoch 3.79: Loss = 0.519974
Epoch 3.80: Loss = 0.542328
Epoch 3.81: Loss = 0.434616
Epoch 3.82: Loss = 0.504227
Epoch 3.83: Loss = 0.523483
Epoch 3.84: Loss = 0.539856
Epoch 3.85: Loss = 0.488358
Epoch 3.86: Loss = 0.502304
Epoch 3.87: Loss = 0.527023
Epoch 3.88: Loss = 0.506302
Epoch 3.89: Loss = 0.407394
Epoch 3.90: Loss = 0.586502
Epoch 3.91: Loss = 0.463348
Epoch 3.92: Loss = 0.586746
Epoch 3.93: Loss = 0.491165
Epoch 3.94: Loss = 0.457748
Epoch 3.95: Loss = 0.543549
Epoch 3.96: Loss = 0.523285
Epoch 3.97: Loss = 0.380417
Epoch 3.98: Loss = 0.509232
Epoch 3.99: Loss = 0.481888
Epoch 3.100: Loss = 0.516769
Epoch 3.101: Loss = 0.482666
Epoch 3.102: Loss = 0.496429
Epoch 3.103: Loss = 0.509964
Epoch 3.104: Loss = 0.553024
Epoch 3.105: Loss = 0.425858
Epoch 3.106: Loss = 0.436539
Epoch 3.107: Loss = 0.511536
Epoch 3.108: Loss = 0.534988
Epoch 3.109: Loss = 0.573349
Epoch 3.110: Loss = 0.523224
Epoch 3.111: Loss = 0.466583
Epoch 3.112: Loss = 0.480301
Epoch 3.113: Loss = 0.50235
Epoch 3.114: Loss = 0.400269
Epoch 3.115: Loss = 0.515778
Epoch 3.116: Loss = 0.481171
Epoch 3.117: Loss = 0.532272
Epoch 3.118: Loss = 0.545731
Epoch 3.119: Loss = 0.599503
Epoch 3.120: Loss = 0.514587
TRAIN LOSS = 0.506577
TRAIN ACC = 84.7504 % (50852/60000)
Loss = 0.496964
Loss = 0.56749
Loss = 0.679504
Loss = 0.642548
Loss = 0.654114
Loss = 0.494278
Loss = 0.473068
Loss = 0.711334
Loss = 0.60791
Loss = 0.54628
Loss = 0.276306
Loss = 0.36113
Loss = 0.295181
Loss = 0.417831
Loss = 0.284103
Loss = 0.410599
Loss = 0.28476
Loss = 0.105225
Loss = 0.306747
Loss = 0.656982
TEST LOSS = 0.463618
TEST ACC = 508.519 % (8631/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.527542
Epoch 4.2: Loss = 0.560547
Epoch 4.3: Loss = 0.491791
Epoch 4.4: Loss = 0.440445
Epoch 4.5: Loss = 0.451096
Epoch 4.6: Loss = 0.459534
Epoch 4.7: Loss = 0.498749
Epoch 4.8: Loss = 0.543045
Epoch 4.9: Loss = 0.521057
Epoch 4.10: Loss = 0.393524
Epoch 4.11: Loss = 0.480515
Epoch 4.12: Loss = 0.416763
Epoch 4.13: Loss = 0.55069
Epoch 4.14: Loss = 0.491745
Epoch 4.15: Loss = 0.498398
Epoch 4.16: Loss = 0.500122
Epoch 4.17: Loss = 0.52597
Epoch 4.18: Loss = 0.455048
Epoch 4.19: Loss = 0.517548
Epoch 4.20: Loss = 0.530029
Epoch 4.21: Loss = 0.499344
Epoch 4.22: Loss = 0.515396
Epoch 4.23: Loss = 0.618546
Epoch 4.24: Loss = 0.537888
Epoch 4.25: Loss = 0.531754
Epoch 4.26: Loss = 0.484665
Epoch 4.27: Loss = 0.588272
Epoch 4.28: Loss = 0.545151
Epoch 4.29: Loss = 0.512848
Epoch 4.30: Loss = 0.452515
Epoch 4.31: Loss = 0.401352
Epoch 4.32: Loss = 0.43364
Epoch 4.33: Loss = 0.484619
Epoch 4.34: Loss = 0.581909
Epoch 4.35: Loss = 0.396622
Epoch 4.36: Loss = 0.518982
Epoch 4.37: Loss = 0.513748
Epoch 4.38: Loss = 0.531723
Epoch 4.39: Loss = 0.555679
Epoch 4.40: Loss = 0.45462
Epoch 4.41: Loss = 0.535538
Epoch 4.42: Loss = 0.500351
Epoch 4.43: Loss = 0.451263
Epoch 4.44: Loss = 0.447586
Epoch 4.45: Loss = 0.475876
Epoch 4.46: Loss = 0.575012
Epoch 4.47: Loss = 0.524033
Epoch 4.48: Loss = 0.446335
Epoch 4.49: Loss = 0.65892
Epoch 4.50: Loss = 0.529999
Epoch 4.51: Loss = 0.546585
Epoch 4.52: Loss = 0.465576
Epoch 4.53: Loss = 0.49968
Epoch 4.54: Loss = 0.482758
Epoch 4.55: Loss = 0.53067
Epoch 4.56: Loss = 0.480545
Epoch 4.57: Loss = 0.444702
Epoch 4.58: Loss = 0.540924
Epoch 4.59: Loss = 0.532608
Epoch 4.60: Loss = 0.414551
Epoch 4.61: Loss = 0.488068
Epoch 4.62: Loss = 0.509857
Epoch 4.63: Loss = 0.544968
Epoch 4.64: Loss = 0.467575
Epoch 4.65: Loss = 0.491867
Epoch 4.66: Loss = 0.518631
Epoch 4.67: Loss = 0.481766
Epoch 4.68: Loss = 0.408722
Epoch 4.69: Loss = 0.470596
Epoch 4.70: Loss = 0.593628
Epoch 4.71: Loss = 0.523041
Epoch 4.72: Loss = 0.690231
Epoch 4.73: Loss = 0.575439
Epoch 4.74: Loss = 0.43869
Epoch 4.75: Loss = 0.588135
Epoch 4.76: Loss = 0.636444
Epoch 4.77: Loss = 0.538498
Epoch 4.78: Loss = 0.56102
Epoch 4.79: Loss = 0.38945
Epoch 4.80: Loss = 0.532761
Epoch 4.81: Loss = 0.580368
Epoch 4.82: Loss = 0.477722
Epoch 4.83: Loss = 0.583542
Epoch 4.84: Loss = 0.516022
Epoch 4.85: Loss = 0.495499
Epoch 4.86: Loss = 0.498367
Epoch 4.87: Loss = 0.46524
Epoch 4.88: Loss = 0.414597
Epoch 4.89: Loss = 0.502411
Epoch 4.90: Loss = 0.550491
Epoch 4.91: Loss = 0.572464
Epoch 4.92: Loss = 0.476608
Epoch 4.93: Loss = 0.492996
Epoch 4.94: Loss = 0.556656
Epoch 4.95: Loss = 0.522995
Epoch 4.96: Loss = 0.507721
Epoch 4.97: Loss = 0.614899
Epoch 4.98: Loss = 0.492844
Epoch 4.99: Loss = 0.464355
Epoch 4.100: Loss = 0.590073
Epoch 4.101: Loss = 0.520065
Epoch 4.102: Loss = 0.490723
Epoch 4.103: Loss = 0.441101
Epoch 4.104: Loss = 0.513519
Epoch 4.105: Loss = 0.421432
Epoch 4.106: Loss = 0.475037
Epoch 4.107: Loss = 0.4767
Epoch 4.108: Loss = 0.597351
Epoch 4.109: Loss = 0.467499
Epoch 4.110: Loss = 0.484833
Epoch 4.111: Loss = 0.553192
Epoch 4.112: Loss = 0.641937
Epoch 4.113: Loss = 0.46286
Epoch 4.114: Loss = 0.482986
Epoch 4.115: Loss = 0.573181
Epoch 4.116: Loss = 0.598724
Epoch 4.117: Loss = 0.535095
Epoch 4.118: Loss = 0.637955
Epoch 4.119: Loss = 0.508606
Epoch 4.120: Loss = 0.513916
TRAIN LOSS = 0.511002
TRAIN ACC = 85.3836 % (51232/60000)
Loss = 0.471985
Loss = 0.60553
Loss = 0.638046
Loss = 0.699036
Loss = 0.666168
Loss = 0.483734
Loss = 0.477325
Loss = 0.713837
Loss = 0.655075
Loss = 0.559845
Loss = 0.316986
Loss = 0.383881
Loss = 0.321121
Loss = 0.450653
Loss = 0.306839
Loss = 0.436142
Loss = 0.289795
Loss = 0.0975494
Loss = 0.33873
Loss = 0.661789
TEST LOSS = 0.478703
TEST ACC = 512.318 % (8628/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.537796
Epoch 5.2: Loss = 0.510071
Epoch 5.3: Loss = 0.507629
Epoch 5.4: Loss = 0.575943
Epoch 5.5: Loss = 0.520752
Epoch 5.6: Loss = 0.598267
Epoch 5.7: Loss = 0.505814
Epoch 5.8: Loss = 0.557831
Epoch 5.9: Loss = 0.3918
Epoch 5.10: Loss = 0.468079
Epoch 5.11: Loss = 0.530457
Epoch 5.12: Loss = 0.537949
Epoch 5.13: Loss = 0.482407
Epoch 5.14: Loss = 0.424149
Epoch 5.15: Loss = 0.606537
Epoch 5.16: Loss = 0.592148
Epoch 5.17: Loss = 0.413467
Epoch 5.18: Loss = 0.594177
Epoch 5.19: Loss = 0.546463
Epoch 5.20: Loss = 0.471176
Epoch 5.21: Loss = 0.56572
Epoch 5.22: Loss = 0.588043
Epoch 5.23: Loss = 0.53421
Epoch 5.24: Loss = 0.465088
Epoch 5.25: Loss = 0.594788
Epoch 5.26: Loss = 0.572754
Epoch 5.27: Loss = 0.481934
Epoch 5.28: Loss = 0.47316
Epoch 5.29: Loss = 0.503876
Epoch 5.30: Loss = 0.532516
Epoch 5.31: Loss = 0.580872
Epoch 5.32: Loss = 0.454193
Epoch 5.33: Loss = 0.439163
Epoch 5.34: Loss = 0.443863
Epoch 5.35: Loss = 0.492355
Epoch 5.36: Loss = 0.485138
Epoch 5.37: Loss = 0.491531
Epoch 5.38: Loss = 0.46051
Epoch 5.39: Loss = 0.549225
Epoch 5.40: Loss = 0.530884
Epoch 5.41: Loss = 0.566513
Epoch 5.42: Loss = 0.468643
Epoch 5.43: Loss = 0.523056
Epoch 5.44: Loss = 0.568222
Epoch 5.45: Loss = 0.537994
Epoch 5.46: Loss = 0.514328
Epoch 5.47: Loss = 0.605423
Epoch 5.48: Loss = 0.463455
Epoch 5.49: Loss = 0.560257
Epoch 5.50: Loss = 0.517761
Epoch 5.51: Loss = 0.488525
Epoch 5.52: Loss = 0.611115
Epoch 5.53: Loss = 0.568649
Epoch 5.54: Loss = 0.491592
Epoch 5.55: Loss = 0.5784
Epoch 5.56: Loss = 0.411026
Epoch 5.57: Loss = 0.479538
Epoch 5.58: Loss = 0.46402
Epoch 5.59: Loss = 0.485245
Epoch 5.60: Loss = 0.441467
Epoch 5.61: Loss = 0.402084
Epoch 5.62: Loss = 0.550842
Epoch 5.63: Loss = 0.493896
Epoch 5.64: Loss = 0.487015
Epoch 5.65: Loss = 0.580627
Epoch 5.66: Loss = 0.437088
Epoch 5.67: Loss = 0.514297
Epoch 5.68: Loss = 0.584869
Epoch 5.69: Loss = 0.530289
Epoch 5.70: Loss = 0.534378
Epoch 5.71: Loss = 0.532257
Epoch 5.72: Loss = 0.454086
Epoch 5.73: Loss = 0.639954
Epoch 5.74: Loss = 0.575363
Epoch 5.75: Loss = 0.590393
Epoch 5.76: Loss = 0.419006
Epoch 5.77: Loss = 0.414536
Epoch 5.78: Loss = 0.427567
Epoch 5.79: Loss = 0.645844
Epoch 5.80: Loss = 0.487259
Epoch 5.81: Loss = 0.562103
Epoch 5.82: Loss = 0.602936
Epoch 5.83: Loss = 0.617966
Epoch 5.84: Loss = 0.508408
Epoch 5.85: Loss = 0.570053
Epoch 5.86: Loss = 0.428116
Epoch 5.87: Loss = 0.493622
Epoch 5.88: Loss = 0.4655
Epoch 5.89: Loss = 0.517731
Epoch 5.90: Loss = 0.496689
Epoch 5.91: Loss = 0.49707
Epoch 5.92: Loss = 0.583664
Epoch 5.93: Loss = 0.488068
Epoch 5.94: Loss = 0.522675
Epoch 5.95: Loss = 0.532776
Epoch 5.96: Loss = 0.468338
Epoch 5.97: Loss = 0.670029
Epoch 5.98: Loss = 0.586548
Epoch 5.99: Loss = 0.562866
Epoch 5.100: Loss = 0.36972
Epoch 5.101: Loss = 0.56839
Epoch 5.102: Loss = 0.5065
Epoch 5.103: Loss = 0.622406
Epoch 5.104: Loss = 0.485458
Epoch 5.105: Loss = 0.593262
Epoch 5.106: Loss = 0.694824
Epoch 5.107: Loss = 0.535446
Epoch 5.108: Loss = 0.62381
Epoch 5.109: Loss = 0.446182
Epoch 5.110: Loss = 0.416504
Epoch 5.111: Loss = 0.622421
Epoch 5.112: Loss = 0.533813
Epoch 5.113: Loss = 0.378021
Epoch 5.114: Loss = 0.55162
Epoch 5.115: Loss = 0.480515
Epoch 5.116: Loss = 0.422485
Epoch 5.117: Loss = 0.544571
Epoch 5.118: Loss = 0.672424
Epoch 5.119: Loss = 0.562057
Epoch 5.120: Loss = 0.507614
TRAIN LOSS = 0.521454
TRAIN ACC = 85.8643 % (51521/60000)
Loss = 0.536514
Loss = 0.580002
Loss = 0.718933
Loss = 0.680908
Loss = 0.717575
Loss = 0.523392
Loss = 0.4832
Loss = 0.744064
Loss = 0.681274
Loss = 0.609604
Loss = 0.311188
Loss = 0.477234
Loss = 0.315842
Loss = 0.497253
Loss = 0.341278
Loss = 0.43718
Loss = 0.313934
Loss = 0.105133
Loss = 0.314529
Loss = 0.705688
TEST LOSS = 0.504736
TEST ACC = 515.208 % (8609/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.590042
Epoch 6.2: Loss = 0.532303
Epoch 6.3: Loss = 0.647461
Epoch 6.4: Loss = 0.488037
Epoch 6.5: Loss = 0.51358
Epoch 6.6: Loss = 0.601486
Epoch 6.7: Loss = 0.645782
Epoch 6.8: Loss = 0.484085
Epoch 6.9: Loss = 0.516327
Epoch 6.10: Loss = 0.467926
Epoch 6.11: Loss = 0.498016
Epoch 6.12: Loss = 0.509247
Epoch 6.13: Loss = 0.547546
Epoch 6.14: Loss = 0.612579
Epoch 6.15: Loss = 0.569061
Epoch 6.16: Loss = 0.560806
Epoch 6.17: Loss = 0.541077
Epoch 6.18: Loss = 0.589157
Epoch 6.19: Loss = 0.33783
Epoch 6.20: Loss = 0.569534
Epoch 6.21: Loss = 0.503937
Epoch 6.22: Loss = 0.457535
Epoch 6.23: Loss = 0.618317
Epoch 6.24: Loss = 0.487152
Epoch 6.25: Loss = 0.71196
Epoch 6.26: Loss = 0.622284
Epoch 6.27: Loss = 0.523849
Epoch 6.28: Loss = 0.739243
Epoch 6.29: Loss = 0.658493
Epoch 6.30: Loss = 0.495621
Epoch 6.31: Loss = 0.623688
Epoch 6.32: Loss = 0.513977
Epoch 6.33: Loss = 0.489426
Epoch 6.34: Loss = 0.592377
Epoch 6.35: Loss = 0.542465
Epoch 6.36: Loss = 0.483719
Epoch 6.37: Loss = 0.508179
Epoch 6.38: Loss = 0.555222
Epoch 6.39: Loss = 0.508224
Epoch 6.40: Loss = 0.494781
Epoch 6.41: Loss = 0.452377
Epoch 6.42: Loss = 0.465958
Epoch 6.43: Loss = 0.697922
Epoch 6.44: Loss = 0.463898
Epoch 6.45: Loss = 0.704041
Epoch 6.46: Loss = 0.641907
Epoch 6.47: Loss = 0.503021
Epoch 6.48: Loss = 0.51886
Epoch 6.49: Loss = 0.561722
Epoch 6.50: Loss = 0.575256
Epoch 6.51: Loss = 0.563065
Epoch 6.52: Loss = 0.493805
Epoch 6.53: Loss = 0.50264
Epoch 6.54: Loss = 0.55867
Epoch 6.55: Loss = 0.459717
Epoch 6.56: Loss = 0.515396
Epoch 6.57: Loss = 0.466614
Epoch 6.58: Loss = 0.610046
Epoch 6.59: Loss = 0.562393
Epoch 6.60: Loss = 0.593918
Epoch 6.61: Loss = 0.52652
Epoch 6.62: Loss = 0.516296
Epoch 6.63: Loss = 0.479691
Epoch 6.64: Loss = 0.576248
Epoch 6.65: Loss = 0.717667
Epoch 6.66: Loss = 0.512314
Epoch 6.67: Loss = 0.517548
Epoch 6.68: Loss = 0.528854
Epoch 6.69: Loss = 0.590164
Epoch 6.70: Loss = 0.601227
Epoch 6.71: Loss = 0.619583
Epoch 6.72: Loss = 0.475739
Epoch 6.73: Loss = 0.572296
Epoch 6.74: Loss = 0.503403
Epoch 6.75: Loss = 0.537338
Epoch 6.76: Loss = 0.616028
Epoch 6.77: Loss = 0.462433
Epoch 6.78: Loss = 0.615143
Epoch 6.79: Loss = 0.566116
Epoch 6.80: Loss = 0.56076
Epoch 6.81: Loss = 0.614075
Epoch 6.82: Loss = 0.562653
Epoch 6.83: Loss = 0.475006
Epoch 6.84: Loss = 0.512726
Epoch 6.85: Loss = 0.586884
Epoch 6.86: Loss = 0.533432
Epoch 6.87: Loss = 0.6409
Epoch 6.88: Loss = 0.581009
Epoch 6.89: Loss = 0.523087
Epoch 6.90: Loss = 0.590714
Epoch 6.91: Loss = 0.6745
Epoch 6.92: Loss = 0.547882
Epoch 6.93: Loss = 0.483826
Epoch 6.94: Loss = 0.590332
Epoch 6.95: Loss = 0.460403
Epoch 6.96: Loss = 0.512207
Epoch 6.97: Loss = 0.552567
Epoch 6.98: Loss = 0.682358
Epoch 6.99: Loss = 0.568207
Epoch 6.100: Loss = 0.6008
Epoch 6.101: Loss = 0.52626
Epoch 6.102: Loss = 0.549606
Epoch 6.103: Loss = 0.483719
Epoch 6.104: Loss = 0.622391
Epoch 6.105: Loss = 0.586807
Epoch 6.106: Loss = 0.69873
Epoch 6.107: Loss = 0.65715
Epoch 6.108: Loss = 0.539307
Epoch 6.109: Loss = 0.597321
Epoch 6.110: Loss = 0.476318
Epoch 6.111: Loss = 0.530716
Epoch 6.112: Loss = 0.487656
Epoch 6.113: Loss = 0.644226
Epoch 6.114: Loss = 0.454697
Epoch 6.115: Loss = 0.654739
Epoch 6.116: Loss = 0.600998
Epoch 6.117: Loss = 0.617554
Epoch 6.118: Loss = 0.723801
Epoch 6.119: Loss = 0.623322
Epoch 6.120: Loss = 0.545456
TRAIN LOSS = 0.557297
TRAIN ACC = 85.5652 % (51341/60000)
Loss = 0.575592
Loss = 0.641785
Loss = 0.847275
Loss = 0.701004
Loss = 0.796646
Loss = 0.566742
Loss = 0.442657
Loss = 0.814178
Loss = 0.736816
Loss = 0.658966
Loss = 0.286133
Loss = 0.511246
Loss = 0.354996
Loss = 0.532852
Loss = 0.323166
Loss = 0.520691
Loss = 0.340469
Loss = 0.0946198
Loss = 0.405975
Loss = 0.767181
TEST LOSS = 0.545949
TEST ACC = 513.409 % (8614/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.702896
Epoch 7.2: Loss = 0.446426
Epoch 7.3: Loss = 0.465454
Epoch 7.4: Loss = 0.503159
Epoch 7.5: Loss = 0.599167
Epoch 7.6: Loss = 0.6642
Epoch 7.7: Loss = 0.649704
Epoch 7.8: Loss = 0.562073
Epoch 7.9: Loss = 0.61232
Epoch 7.10: Loss = 0.543182
Epoch 7.11: Loss = 0.614609
Epoch 7.12: Loss = 0.620193
Epoch 7.13: Loss = 0.599838
Epoch 7.14: Loss = 0.4897
Epoch 7.15: Loss = 0.677841
Epoch 7.16: Loss = 0.605698
Epoch 7.17: Loss = 0.544388
Epoch 7.18: Loss = 0.557068
Epoch 7.19: Loss = 0.521271
Epoch 7.20: Loss = 0.521255
Epoch 7.21: Loss = 0.664749
Epoch 7.22: Loss = 0.546082
Epoch 7.23: Loss = 0.574356
Epoch 7.24: Loss = 0.581635
Epoch 7.25: Loss = 0.537766
Epoch 7.26: Loss = 0.401108
Epoch 7.27: Loss = 0.595047
Epoch 7.28: Loss = 0.488358
Epoch 7.29: Loss = 0.599075
Epoch 7.30: Loss = 0.613678
Epoch 7.31: Loss = 0.556213
Epoch 7.32: Loss = 0.570236
Epoch 7.33: Loss = 0.554184
Epoch 7.34: Loss = 0.499893
Epoch 7.35: Loss = 0.575836
Epoch 7.36: Loss = 0.605713
Epoch 7.37: Loss = 0.784851
Epoch 7.38: Loss = 0.653732
Epoch 7.39: Loss = 0.604828
Epoch 7.40: Loss = 0.619308
Epoch 7.41: Loss = 0.496933
Epoch 7.42: Loss = 0.531006
Epoch 7.43: Loss = 0.582382
Epoch 7.44: Loss = 0.483856
Epoch 7.45: Loss = 0.516342
Epoch 7.46: Loss = 0.526825
Epoch 7.47: Loss = 0.576492
Epoch 7.48: Loss = 0.636429
Epoch 7.49: Loss = 0.619751
Epoch 7.50: Loss = 0.629303
Epoch 7.51: Loss = 0.855606
Epoch 7.52: Loss = 0.573853
Epoch 7.53: Loss = 0.590927
Epoch 7.54: Loss = 0.477661
Epoch 7.55: Loss = 0.561768
Epoch 7.56: Loss = 0.544449
Epoch 7.57: Loss = 0.760727
Epoch 7.58: Loss = 0.503006
Epoch 7.59: Loss = 0.629868
Epoch 7.60: Loss = 0.661148
Epoch 7.61: Loss = 0.63031
Epoch 7.62: Loss = 0.74411
Epoch 7.63: Loss = 0.665863
Epoch 7.64: Loss = 0.510239
Epoch 7.65: Loss = 0.520096
Epoch 7.66: Loss = 0.720718
Epoch 7.67: Loss = 0.526398
Epoch 7.68: Loss = 0.663834
Epoch 7.69: Loss = 0.611267
Epoch 7.70: Loss = 0.572647
Epoch 7.71: Loss = 0.603882
Epoch 7.72: Loss = 0.490707
Epoch 7.73: Loss = 0.724304
Epoch 7.74: Loss = 0.530365
Epoch 7.75: Loss = 0.645737
Epoch 7.76: Loss = 0.534775
Epoch 7.77: Loss = 0.473633
Epoch 7.78: Loss = 0.541458
Epoch 7.79: Loss = 0.564941
Epoch 7.80: Loss = 0.637787
Epoch 7.81: Loss = 0.543823
Epoch 7.82: Loss = 0.768082
Epoch 7.83: Loss = 0.629654
Epoch 7.84: Loss = 0.633316
Epoch 7.85: Loss = 0.667709
Epoch 7.86: Loss = 0.589066
Epoch 7.87: Loss = 0.453781
Epoch 7.88: Loss = 0.647095
Epoch 7.89: Loss = 0.776108
Epoch 7.90: Loss = 0.667374
Epoch 7.91: Loss = 0.555313
Epoch 7.92: Loss = 0.621796
Epoch 7.93: Loss = 0.645187
Epoch 7.94: Loss = 0.51387
Epoch 7.95: Loss = 0.647736
Epoch 7.96: Loss = 0.549042
Epoch 7.97: Loss = 0.766327
Epoch 7.98: Loss = 0.500626
Epoch 7.99: Loss = 0.672485
Epoch 7.100: Loss = 0.700485
Epoch 7.101: Loss = 0.696381
Epoch 7.102: Loss = 0.589478
Epoch 7.103: Loss = 0.601303
Epoch 7.104: Loss = 0.575623
Epoch 7.105: Loss = 0.792847
Epoch 7.106: Loss = 0.48053
Epoch 7.107: Loss = 0.71312
Epoch 7.108: Loss = 0.514572
Epoch 7.109: Loss = 0.548691
Epoch 7.110: Loss = 0.649979
Epoch 7.111: Loss = 0.719727
Epoch 7.112: Loss = 0.647003
Epoch 7.113: Loss = 0.71666
Epoch 7.114: Loss = 0.825729
Epoch 7.115: Loss = 0.565765
Epoch 7.116: Loss = 0.697128
Epoch 7.117: Loss = 0.60437
Epoch 7.118: Loss = 0.557785
Epoch 7.119: Loss = 0.613388
Epoch 7.120: Loss = 0.671326
TRAIN LOSS = 0.601089
TRAIN ACC = 85.3088 % (51188/60000)
Loss = 0.544952
Loss = 0.652252
Loss = 0.840393
Loss = 0.786667
Loss = 0.847534
Loss = 0.641586
Loss = 0.48027
Loss = 0.876465
Loss = 0.748749
Loss = 0.63913
Loss = 0.329956
Loss = 0.536621
Loss = 0.441299
Loss = 0.573196
Loss = 0.423126
Loss = 0.610306
Loss = 0.367218
Loss = 0.0933075
Loss = 0.388657
Loss = 0.830948
TEST LOSS = 0.582632
TEST ACC = 511.879 % (8583/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.461731
Epoch 8.2: Loss = 0.516235
Epoch 8.3: Loss = 0.569504
Epoch 8.4: Loss = 0.458771
Epoch 8.5: Loss = 0.681305
Epoch 8.6: Loss = 0.556808
Epoch 8.7: Loss = 0.603653
Epoch 8.8: Loss = 0.634048
Epoch 8.9: Loss = 0.544754
Epoch 8.10: Loss = 0.681198
Epoch 8.11: Loss = 0.800629
Epoch 8.12: Loss = 0.760666
Epoch 8.13: Loss = 0.58139
Epoch 8.14: Loss = 0.627823
Epoch 8.15: Loss = 0.581879
Epoch 8.16: Loss = 0.606247
Epoch 8.17: Loss = 0.555038
Epoch 8.18: Loss = 0.810623
Epoch 8.19: Loss = 0.595749
Epoch 8.20: Loss = 0.717087
Epoch 8.21: Loss = 0.64711
Epoch 8.22: Loss = 0.632538
Epoch 8.23: Loss = 0.729248
Epoch 8.24: Loss = 0.838287
Epoch 8.25: Loss = 0.64769
Epoch 8.26: Loss = 0.628677
Epoch 8.27: Loss = 0.602753
Epoch 8.28: Loss = 0.568604
Epoch 8.29: Loss = 0.690613
Epoch 8.30: Loss = 0.642303
Epoch 8.31: Loss = 0.563644
Epoch 8.32: Loss = 0.562759
Epoch 8.33: Loss = 0.637115
Epoch 8.34: Loss = 0.644394
Epoch 8.35: Loss = 0.678604
Epoch 8.36: Loss = 0.438858
Epoch 8.37: Loss = 0.595444
Epoch 8.38: Loss = 0.708893
Epoch 8.39: Loss = 0.525864
Epoch 8.40: Loss = 0.667282
Epoch 8.41: Loss = 0.472427
Epoch 8.42: Loss = 0.761993
Epoch 8.43: Loss = 0.55127
Epoch 8.44: Loss = 0.759171
Epoch 8.45: Loss = 0.559494
Epoch 8.46: Loss = 0.610428
Epoch 8.47: Loss = 0.687759
Epoch 8.48: Loss = 0.576752
Epoch 8.49: Loss = 0.616257
Epoch 8.50: Loss = 0.737442
Epoch 8.51: Loss = 0.602051
Epoch 8.52: Loss = 0.660873
Epoch 8.53: Loss = 0.582886
Epoch 8.54: Loss = 0.784439
Epoch 8.55: Loss = 0.571869
Epoch 8.56: Loss = 0.565704
Epoch 8.57: Loss = 0.705261
Epoch 8.58: Loss = 0.674301
Epoch 8.59: Loss = 0.659851
Epoch 8.60: Loss = 0.575974
Epoch 8.61: Loss = 0.880463
Epoch 8.62: Loss = 0.660706
Epoch 8.63: Loss = 0.535843
Epoch 8.64: Loss = 0.643799
Epoch 8.65: Loss = 0.718826
Epoch 8.66: Loss = 0.644577
Epoch 8.67: Loss = 0.501358
Epoch 8.68: Loss = 0.615616
Epoch 8.69: Loss = 0.553024
Epoch 8.70: Loss = 0.708603
Epoch 8.71: Loss = 0.734329
Epoch 8.72: Loss = 0.573257
Epoch 8.73: Loss = 0.719528
Epoch 8.74: Loss = 0.654266
Epoch 8.75: Loss = 0.746582
Epoch 8.76: Loss = 0.566742
Epoch 8.77: Loss = 0.595001
Epoch 8.78: Loss = 0.76889
Epoch 8.79: Loss = 0.585617
Epoch 8.80: Loss = 0.625305
Epoch 8.81: Loss = 0.538223
Epoch 8.82: Loss = 0.60321
Epoch 8.83: Loss = 0.571228
Epoch 8.84: Loss = 0.703323
Epoch 8.85: Loss = 0.824203
Epoch 8.86: Loss = 0.574066
Epoch 8.87: Loss = 0.656784
Epoch 8.88: Loss = 0.54631
Epoch 8.89: Loss = 0.706223
Epoch 8.90: Loss = 0.574585
Epoch 8.91: Loss = 0.717896
Epoch 8.92: Loss = 0.805115
Epoch 8.93: Loss = 0.684784
Epoch 8.94: Loss = 0.862793
Epoch 8.95: Loss = 0.678467
Epoch 8.96: Loss = 0.789032
Epoch 8.97: Loss = 0.660645
Epoch 8.98: Loss = 0.707214
Epoch 8.99: Loss = 0.651978
Epoch 8.100: Loss = 0.673798
Epoch 8.101: Loss = 0.574966
Epoch 8.102: Loss = 0.836548
Epoch 8.103: Loss = 0.664597
Epoch 8.104: Loss = 0.652985
Epoch 8.105: Loss = 0.713226
Epoch 8.106: Loss = 0.663513
Epoch 8.107: Loss = 0.609009
Epoch 8.108: Loss = 0.708786
Epoch 8.109: Loss = 0.625076
Epoch 8.110: Loss = 0.753967
Epoch 8.111: Loss = 0.598251
Epoch 8.112: Loss = 0.766495
Epoch 8.113: Loss = 0.690674
Epoch 8.114: Loss = 0.567825
Epoch 8.115: Loss = 0.577087
Epoch 8.116: Loss = 0.598663
Epoch 8.117: Loss = 0.603485
Epoch 8.118: Loss = 0.704697
Epoch 8.119: Loss = 0.625671
Epoch 8.120: Loss = 0.577408
TRAIN LOSS = 0.644897
TRAIN ACC = 85.1883 % (51115/60000)
Loss = 0.597885
Loss = 0.690643
Loss = 0.968414
Loss = 0.83287
Loss = 0.942169
Loss = 0.686157
Loss = 0.545212
Loss = 1.01099
Loss = 0.830963
Loss = 0.713577
Loss = 0.355026
Loss = 0.544693
Loss = 0.617493
Loss = 0.645172
Loss = 0.397934
Loss = 0.679871
Loss = 0.348328
Loss = 0.11087
Loss = 0.495193
Loss = 0.898056
TEST LOSS = 0.645576
TEST ACC = 511.15 % (8576/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.625732
Epoch 9.2: Loss = 0.576218
Epoch 9.3: Loss = 0.662766
Epoch 9.4: Loss = 0.670456
Epoch 9.5: Loss = 0.860718
Epoch 9.6: Loss = 0.502701
Epoch 9.7: Loss = 0.716843
Epoch 9.8: Loss = 0.673584
Epoch 9.9: Loss = 0.710968
Epoch 9.10: Loss = 0.771637
Epoch 9.11: Loss = 0.645157
Epoch 9.12: Loss = 0.667343
Epoch 9.13: Loss = 0.511078
Epoch 9.14: Loss = 0.639893
Epoch 9.15: Loss = 0.688293
Epoch 9.16: Loss = 0.641556
Epoch 9.17: Loss = 0.565567
Epoch 9.18: Loss = 0.647644
Epoch 9.19: Loss = 0.63501
Epoch 9.20: Loss = 0.595108
Epoch 9.21: Loss = 0.60582
Epoch 9.22: Loss = 0.615067
Epoch 9.23: Loss = 0.736847
Epoch 9.24: Loss = 0.548889
Epoch 9.25: Loss = 0.499939
Epoch 9.26: Loss = 0.643677
Epoch 9.27: Loss = 0.508347
Epoch 9.28: Loss = 0.818985
Epoch 9.29: Loss = 0.607681
Epoch 9.30: Loss = 0.70752
Epoch 9.31: Loss = 0.574585
Epoch 9.32: Loss = 0.708008
Epoch 9.33: Loss = 0.717865
Epoch 9.34: Loss = 0.760422
Epoch 9.35: Loss = 0.771942
Epoch 9.36: Loss = 0.772308
Epoch 9.37: Loss = 0.515915
Epoch 9.38: Loss = 0.762329
Epoch 9.39: Loss = 0.711273
Epoch 9.40: Loss = 0.703537
Epoch 9.41: Loss = 0.727325
Epoch 9.42: Loss = 0.764511
Epoch 9.43: Loss = 0.52594
Epoch 9.44: Loss = 0.68425
Epoch 9.45: Loss = 0.609711
Epoch 9.46: Loss = 0.537048
Epoch 9.47: Loss = 0.695053
Epoch 9.48: Loss = 0.652039
Epoch 9.49: Loss = 0.57811
Epoch 9.50: Loss = 0.745438
Epoch 9.51: Loss = 0.671539
Epoch 9.52: Loss = 0.807861
Epoch 9.53: Loss = 0.758224
Epoch 9.54: Loss = 0.672241
Epoch 9.55: Loss = 0.605713
Epoch 9.56: Loss = 0.617538
Epoch 9.57: Loss = 0.671005
Epoch 9.58: Loss = 0.609039
Epoch 9.59: Loss = 0.822876
Epoch 9.60: Loss = 0.818497
Epoch 9.61: Loss = 0.580246
Epoch 9.62: Loss = 0.60524
Epoch 9.63: Loss = 0.68306
Epoch 9.64: Loss = 0.910095
Epoch 9.65: Loss = 0.708191
Epoch 9.66: Loss = 0.692856
Epoch 9.67: Loss = 0.496109
Epoch 9.68: Loss = 0.649124
Epoch 9.69: Loss = 0.59996
Epoch 9.70: Loss = 0.82251
Epoch 9.71: Loss = 0.573059
Epoch 9.72: Loss = 0.591446
Epoch 9.73: Loss = 0.612244
Epoch 9.74: Loss = 0.652847
Epoch 9.75: Loss = 0.698639
Epoch 9.76: Loss = 0.611725
Epoch 9.77: Loss = 0.584122
Epoch 9.78: Loss = 0.817841
Epoch 9.79: Loss = 0.551086
Epoch 9.80: Loss = 0.85379
Epoch 9.81: Loss = 0.716019
Epoch 9.82: Loss = 0.905029
Epoch 9.83: Loss = 0.606644
Epoch 9.84: Loss = 0.747742
Epoch 9.85: Loss = 0.622833
Epoch 9.86: Loss = 0.681778
Epoch 9.87: Loss = 0.631348
Epoch 9.88: Loss = 0.543198
Epoch 9.89: Loss = 0.690857
Epoch 9.90: Loss = 0.884506
Epoch 9.91: Loss = 0.571991
Epoch 9.92: Loss = 0.750549
Epoch 9.93: Loss = 0.817261
Epoch 9.94: Loss = 0.994156
Epoch 9.95: Loss = 0.675629
Epoch 9.96: Loss = 0.632401
Epoch 9.97: Loss = 0.731689
Epoch 9.98: Loss = 0.763657
Epoch 9.99: Loss = 0.761047
Epoch 9.100: Loss = 0.639542
Epoch 9.101: Loss = 0.703201
Epoch 9.102: Loss = 0.616898
Epoch 9.103: Loss = 0.534485
Epoch 9.104: Loss = 0.733917
Epoch 9.105: Loss = 0.626297
Epoch 9.106: Loss = 0.733749
Epoch 9.107: Loss = 0.668671
Epoch 9.108: Loss = 0.568634
Epoch 9.109: Loss = 0.69104
Epoch 9.110: Loss = 0.886139
Epoch 9.111: Loss = 0.908569
Epoch 9.112: Loss = 0.704544
Epoch 9.113: Loss = 0.867462
Epoch 9.114: Loss = 0.598984
Epoch 9.115: Loss = 0.616959
Epoch 9.116: Loss = 0.685074
Epoch 9.117: Loss = 0.555191
Epoch 9.118: Loss = 0.696259
Epoch 9.119: Loss = 0.616501
Epoch 9.120: Loss = 0.60463
TRAIN LOSS = 0.676285
TRAIN ACC = 84.8892 % (50936/60000)
Loss = 0.604523
Loss = 0.679352
Loss = 0.915482
Loss = 0.884583
Loss = 0.919205
Loss = 0.6754
Loss = 0.549988
Loss = 1.07071
Loss = 0.884567
Loss = 0.731049
Loss = 0.349319
Loss = 0.639587
Loss = 0.641724
Loss = 0.693283
Loss = 0.401901
Loss = 0.592957
Loss = 0.353577
Loss = 0.153
Loss = 0.470154
Loss = 0.875092
TEST LOSS = 0.654272
TEST ACC = 509.36 % (8513/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.743561
Epoch 10.2: Loss = 0.77446
Epoch 10.3: Loss = 0.834976
Epoch 10.4: Loss = 0.72818
Epoch 10.5: Loss = 0.820953
Epoch 10.6: Loss = 0.720398
Epoch 10.7: Loss = 0.744995
Epoch 10.8: Loss = 0.705338
Epoch 10.9: Loss = 0.431992
Epoch 10.10: Loss = 0.880035
Epoch 10.11: Loss = 0.752411
Epoch 10.12: Loss = 0.732697
Epoch 10.13: Loss = 0.560211
Epoch 10.14: Loss = 0.762497
Epoch 10.15: Loss = 0.856293
Epoch 10.16: Loss = 0.489304
Epoch 10.17: Loss = 0.761307
Epoch 10.18: Loss = 0.619293
Epoch 10.19: Loss = 0.472198
Epoch 10.20: Loss = 0.640244
Epoch 10.21: Loss = 0.61673
Epoch 10.22: Loss = 0.494385
Epoch 10.23: Loss = 0.699493
Epoch 10.24: Loss = 0.710266
Epoch 10.25: Loss = 0.718811
Epoch 10.26: Loss = 0.56369
Epoch 10.27: Loss = 0.695526
Epoch 10.28: Loss = 0.731522
Epoch 10.29: Loss = 0.629517
Epoch 10.30: Loss = 0.708267
Epoch 10.31: Loss = 0.749496
Epoch 10.32: Loss = 0.696808
Epoch 10.33: Loss = 0.613327
Epoch 10.34: Loss = 0.59375
Epoch 10.35: Loss = 0.595963
Epoch 10.36: Loss = 0.609787
Epoch 10.37: Loss = 0.763855
Epoch 10.38: Loss = 0.641235
Epoch 10.39: Loss = 0.798721
Epoch 10.40: Loss = 0.706268
Epoch 10.41: Loss = 0.697021
Epoch 10.42: Loss = 0.644653
Epoch 10.43: Loss = 0.720901
Epoch 10.44: Loss = 0.760162
Epoch 10.45: Loss = 0.560272
Epoch 10.46: Loss = 0.86647
Epoch 10.47: Loss = 0.796722
Epoch 10.48: Loss = 0.753632
Epoch 10.49: Loss = 0.67131
Epoch 10.50: Loss = 0.739349
Epoch 10.51: Loss = 0.394257
Epoch 10.52: Loss = 0.77034
Epoch 10.53: Loss = 0.572174
Epoch 10.54: Loss = 0.638733
Epoch 10.55: Loss = 0.759308
Epoch 10.56: Loss = 0.790146
Epoch 10.57: Loss = 0.562943
Epoch 10.58: Loss = 0.742981
Epoch 10.59: Loss = 0.816345
Epoch 10.60: Loss = 0.613022
Epoch 10.61: Loss = 0.625687
Epoch 10.62: Loss = 0.647751
Epoch 10.63: Loss = 0.662735
Epoch 10.64: Loss = 0.772964
Epoch 10.65: Loss = 0.703537
Epoch 10.66: Loss = 0.59494
Epoch 10.67: Loss = 0.735733
Epoch 10.68: Loss = 0.479309
Epoch 10.69: Loss = 0.716827
Epoch 10.70: Loss = 0.791321
Epoch 10.71: Loss = 0.628189
Epoch 10.72: Loss = 0.748428
Epoch 10.73: Loss = 0.614304
Epoch 10.74: Loss = 0.543503
Epoch 10.75: Loss = 0.67186
Epoch 10.76: Loss = 0.659332
Epoch 10.77: Loss = 0.811127
Epoch 10.78: Loss = 0.7827
Epoch 10.79: Loss = 0.627258
Epoch 10.80: Loss = 0.746643
Epoch 10.81: Loss = 0.732422
Epoch 10.82: Loss = 0.753098
Epoch 10.83: Loss = 0.921844
Epoch 10.84: Loss = 0.568115
Epoch 10.85: Loss = 0.671799
Epoch 10.86: Loss = 0.769623
Epoch 10.87: Loss = 0.757843
Epoch 10.88: Loss = 0.530441
Epoch 10.89: Loss = 0.722076
Epoch 10.90: Loss = 0.6577
Epoch 10.91: Loss = 0.64061
Epoch 10.92: Loss = 0.825089
Epoch 10.93: Loss = 0.595139
Epoch 10.94: Loss = 0.720779
Epoch 10.95: Loss = 0.694778
Epoch 10.96: Loss = 0.746765
Epoch 10.97: Loss = 0.689835
Epoch 10.98: Loss = 0.632294
Epoch 10.99: Loss = 0.601089
Epoch 10.100: Loss = 0.686584
Epoch 10.101: Loss = 0.687881
Epoch 10.102: Loss = 0.624146
Epoch 10.103: Loss = 0.749039
Epoch 10.104: Loss = 0.674606
Epoch 10.105: Loss = 0.649841
Epoch 10.106: Loss = 0.833084
Epoch 10.107: Loss = 0.680832
Epoch 10.108: Loss = 0.677216
Epoch 10.109: Loss = 0.689575
Epoch 10.110: Loss = 0.614914
Epoch 10.111: Loss = 0.586975
Epoch 10.112: Loss = 0.730179
Epoch 10.113: Loss = 0.694458
Epoch 10.114: Loss = 0.479599
Epoch 10.115: Loss = 0.735229
Epoch 10.116: Loss = 0.671631
Epoch 10.117: Loss = 0.725677
Epoch 10.118: Loss = 0.78862
Epoch 10.119: Loss = 0.618515
Epoch 10.120: Loss = 0.665405
TRAIN LOSS = 0.685837
TRAIN ACC = 84.9274 % (50958/60000)
Loss = 0.615555
Loss = 0.739639
Loss = 0.985031
Loss = 0.915726
Loss = 0.960983
Loss = 0.765427
Loss = 0.544571
Loss = 1.04747
Loss = 0.892456
Loss = 0.760284
Loss = 0.378159
Loss = 0.698563
Loss = 0.661545
Loss = 0.633667
Loss = 0.434814
Loss = 0.650574
Loss = 0.352997
Loss = 0.162247
Loss = 0.453506
Loss = 0.901184
TEST LOSS = 0.67772
TEST ACC = 509.579 % (8571/10000)

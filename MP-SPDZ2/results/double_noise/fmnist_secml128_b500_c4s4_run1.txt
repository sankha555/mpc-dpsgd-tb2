Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.33562
Epoch 1.2: Loss = 2.269
Epoch 1.3: Loss = 2.21515
Epoch 1.4: Loss = 2.1862
Epoch 1.5: Loss = 2.11461
Epoch 1.6: Loss = 2.06854
Epoch 1.7: Loss = 2.06232
Epoch 1.8: Loss = 1.97791
Epoch 1.9: Loss = 1.95592
Epoch 1.10: Loss = 1.93814
Epoch 1.11: Loss = 1.88156
Epoch 1.12: Loss = 1.8401
Epoch 1.13: Loss = 1.79338
Epoch 1.14: Loss = 1.73946
Epoch 1.15: Loss = 1.7081
Epoch 1.16: Loss = 1.70955
Epoch 1.17: Loss = 1.66673
Epoch 1.18: Loss = 1.61922
Epoch 1.19: Loss = 1.61092
Epoch 1.20: Loss = 1.59206
Epoch 1.21: Loss = 1.55125
Epoch 1.22: Loss = 1.48422
Epoch 1.23: Loss = 1.4478
Epoch 1.24: Loss = 1.46397
Epoch 1.25: Loss = 1.41557
Epoch 1.26: Loss = 1.37117
Epoch 1.27: Loss = 1.36804
Epoch 1.28: Loss = 1.36838
Epoch 1.29: Loss = 1.30754
Epoch 1.30: Loss = 1.3317
Epoch 1.31: Loss = 1.28348
Epoch 1.32: Loss = 1.20349
Epoch 1.33: Loss = 1.21054
Epoch 1.34: Loss = 1.23615
Epoch 1.35: Loss = 1.23083
Epoch 1.36: Loss = 1.19736
Epoch 1.37: Loss = 1.22777
Epoch 1.38: Loss = 1.13826
Epoch 1.39: Loss = 1.17796
Epoch 1.40: Loss = 1.17924
Epoch 1.41: Loss = 1.10269
Epoch 1.42: Loss = 1.07678
Epoch 1.43: Loss = 1.11751
Epoch 1.44: Loss = 1.10097
Epoch 1.45: Loss = 1.09216
Epoch 1.46: Loss = 1.03596
Epoch 1.47: Loss = 1.06827
Epoch 1.48: Loss = 1.09715
Epoch 1.49: Loss = 1.05063
Epoch 1.50: Loss = 0.96051
Epoch 1.51: Loss = 1.09904
Epoch 1.52: Loss = 1.02496
Epoch 1.53: Loss = 1.00754
Epoch 1.54: Loss = 1.05162
Epoch 1.55: Loss = 1.03497
Epoch 1.56: Loss = 0.935165
Epoch 1.57: Loss = 0.988205
Epoch 1.58: Loss = 0.953369
Epoch 1.59: Loss = 1.0255
Epoch 1.60: Loss = 0.923996
Epoch 1.61: Loss = 1.00053
Epoch 1.62: Loss = 0.925156
Epoch 1.63: Loss = 0.86322
Epoch 1.64: Loss = 0.858398
Epoch 1.65: Loss = 0.974167
Epoch 1.66: Loss = 0.928741
Epoch 1.67: Loss = 0.907623
Epoch 1.68: Loss = 0.867432
Epoch 1.69: Loss = 0.903015
Epoch 1.70: Loss = 0.889374
Epoch 1.71: Loss = 0.938675
Epoch 1.72: Loss = 0.945358
Epoch 1.73: Loss = 0.920792
Epoch 1.74: Loss = 0.918732
Epoch 1.75: Loss = 0.823822
Epoch 1.76: Loss = 0.868301
Epoch 1.77: Loss = 0.81665
Epoch 1.78: Loss = 0.871719
Epoch 1.79: Loss = 0.84523
Epoch 1.80: Loss = 0.863159
Epoch 1.81: Loss = 0.893478
Epoch 1.82: Loss = 0.876968
Epoch 1.83: Loss = 0.867493
Epoch 1.84: Loss = 0.8647
Epoch 1.85: Loss = 0.839844
Epoch 1.86: Loss = 0.865616
Epoch 1.87: Loss = 0.8461
Epoch 1.88: Loss = 0.839661
Epoch 1.89: Loss = 0.805939
Epoch 1.90: Loss = 0.788651
Epoch 1.91: Loss = 0.913834
Epoch 1.92: Loss = 0.829056
Epoch 1.93: Loss = 0.822281
Epoch 1.94: Loss = 0.829666
Epoch 1.95: Loss = 0.852814
Epoch 1.96: Loss = 0.884827
Epoch 1.97: Loss = 0.82428
Epoch 1.98: Loss = 0.867813
Epoch 1.99: Loss = 0.857605
Epoch 1.100: Loss = 0.788422
Epoch 1.101: Loss = 0.828857
Epoch 1.102: Loss = 0.807526
Epoch 1.103: Loss = 0.84053
Epoch 1.104: Loss = 0.861282
Epoch 1.105: Loss = 0.725296
Epoch 1.106: Loss = 0.820496
Epoch 1.107: Loss = 0.773071
Epoch 1.108: Loss = 0.723648
Epoch 1.109: Loss = 0.77832
Epoch 1.110: Loss = 0.82811
Epoch 1.111: Loss = 0.843689
Epoch 1.112: Loss = 0.77327
Epoch 1.113: Loss = 0.834976
Epoch 1.114: Loss = 0.738159
Epoch 1.115: Loss = 0.810989
Epoch 1.116: Loss = 0.762497
Epoch 1.117: Loss = 0.839691
Epoch 1.118: Loss = 0.816742
Epoch 1.119: Loss = 0.762711
Epoch 1.120: Loss = 0.777405
TRAIN LOSS = 1.13387
TRAIN ACC = 62.1628 % (37299/60000)
Loss = 0.733963
Loss = 0.836685
Loss = 0.791245
Loss = 0.7211
Loss = 0.70784
Loss = 0.892731
Loss = 0.891541
Loss = 0.857483
Loss = 0.777374
Loss = 0.71254
Loss = 0.844055
Loss = 0.779343
Loss = 0.825775
Loss = 0.795364
Loss = 0.752197
Loss = 0.855453
Loss = 0.72467
Loss = 0.811279
Loss = 0.822815
Loss = 0.787781
TEST LOSS = 0.796061
TEST ACC = 372.989 % (7266/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.770279
Epoch 2.2: Loss = 0.730667
Epoch 2.3: Loss = 0.808319
Epoch 2.4: Loss = 0.832596
Epoch 2.5: Loss = 0.854446
Epoch 2.6: Loss = 0.752502
Epoch 2.7: Loss = 0.757614
Epoch 2.8: Loss = 0.839233
Epoch 2.9: Loss = 0.727386
Epoch 2.10: Loss = 0.761017
Epoch 2.11: Loss = 0.853943
Epoch 2.12: Loss = 0.722443
Epoch 2.13: Loss = 0.68631
Epoch 2.14: Loss = 0.681519
Epoch 2.15: Loss = 0.736954
Epoch 2.16: Loss = 0.741669
Epoch 2.17: Loss = 0.695648
Epoch 2.18: Loss = 0.798477
Epoch 2.19: Loss = 0.764008
Epoch 2.20: Loss = 0.66069
Epoch 2.21: Loss = 0.765518
Epoch 2.22: Loss = 0.752197
Epoch 2.23: Loss = 0.752121
Epoch 2.24: Loss = 0.7715
Epoch 2.25: Loss = 0.785889
Epoch 2.26: Loss = 0.637695
Epoch 2.27: Loss = 0.630112
Epoch 2.28: Loss = 0.675827
Epoch 2.29: Loss = 0.651398
Epoch 2.30: Loss = 0.830215
Epoch 2.31: Loss = 0.80574
Epoch 2.32: Loss = 0.675583
Epoch 2.33: Loss = 0.703659
Epoch 2.34: Loss = 0.856628
Epoch 2.35: Loss = 0.788162
Epoch 2.36: Loss = 0.827545
Epoch 2.37: Loss = 0.779495
Epoch 2.38: Loss = 0.796356
Epoch 2.39: Loss = 0.73436
Epoch 2.40: Loss = 0.683548
Epoch 2.41: Loss = 0.786774
Epoch 2.42: Loss = 0.718765
Epoch 2.43: Loss = 0.718658
Epoch 2.44: Loss = 0.729385
Epoch 2.45: Loss = 0.811981
Epoch 2.46: Loss = 0.851242
Epoch 2.47: Loss = 0.785324
Epoch 2.48: Loss = 0.8172
Epoch 2.49: Loss = 0.737885
Epoch 2.50: Loss = 0.764481
Epoch 2.51: Loss = 0.786667
Epoch 2.52: Loss = 0.719666
Epoch 2.53: Loss = 0.801971
Epoch 2.54: Loss = 0.735962
Epoch 2.55: Loss = 0.66864
Epoch 2.56: Loss = 0.790527
Epoch 2.57: Loss = 0.672592
Epoch 2.58: Loss = 0.787292
Epoch 2.59: Loss = 0.782623
Epoch 2.60: Loss = 0.836716
Epoch 2.61: Loss = 0.798203
Epoch 2.62: Loss = 0.717743
Epoch 2.63: Loss = 0.708481
Epoch 2.64: Loss = 0.851135
Epoch 2.65: Loss = 0.760498
Epoch 2.66: Loss = 0.761963
Epoch 2.67: Loss = 0.81868
Epoch 2.68: Loss = 0.802368
Epoch 2.69: Loss = 0.771225
Epoch 2.70: Loss = 0.725021
Epoch 2.71: Loss = 0.701584
Epoch 2.72: Loss = 0.78125
Epoch 2.73: Loss = 0.812134
Epoch 2.74: Loss = 0.829498
Epoch 2.75: Loss = 0.730988
Epoch 2.76: Loss = 0.77002
Epoch 2.77: Loss = 0.713348
Epoch 2.78: Loss = 0.684235
Epoch 2.79: Loss = 0.759598
Epoch 2.80: Loss = 0.785309
Epoch 2.81: Loss = 0.787994
Epoch 2.82: Loss = 0.776581
Epoch 2.83: Loss = 0.771942
Epoch 2.84: Loss = 0.692108
Epoch 2.85: Loss = 0.740677
Epoch 2.86: Loss = 0.737152
Epoch 2.87: Loss = 0.736404
Epoch 2.88: Loss = 0.709167
Epoch 2.89: Loss = 0.71785
Epoch 2.90: Loss = 0.768768
Epoch 2.91: Loss = 0.717361
Epoch 2.92: Loss = 0.752151
Epoch 2.93: Loss = 0.687637
Epoch 2.94: Loss = 0.7397
Epoch 2.95: Loss = 0.748337
Epoch 2.96: Loss = 0.701477
Epoch 2.97: Loss = 0.750504
Epoch 2.98: Loss = 0.764618
Epoch 2.99: Loss = 0.675903
Epoch 2.100: Loss = 0.696732
Epoch 2.101: Loss = 0.623428
Epoch 2.102: Loss = 0.697769
Epoch 2.103: Loss = 0.731369
Epoch 2.104: Loss = 0.812317
Epoch 2.105: Loss = 0.674438
Epoch 2.106: Loss = 0.673508
Epoch 2.107: Loss = 0.764359
Epoch 2.108: Loss = 0.755035
Epoch 2.109: Loss = 0.648315
Epoch 2.110: Loss = 0.734116
Epoch 2.111: Loss = 0.795502
Epoch 2.112: Loss = 0.669189
Epoch 2.113: Loss = 0.675644
Epoch 2.114: Loss = 0.734177
Epoch 2.115: Loss = 0.766739
Epoch 2.116: Loss = 0.678146
Epoch 2.117: Loss = 0.670242
Epoch 2.118: Loss = 0.743332
Epoch 2.119: Loss = 0.748245
Epoch 2.120: Loss = 0.675293
TRAIN LOSS = 0.74678
TRAIN ACC = 74.6155 % (44771/60000)
Loss = 0.657272
Loss = 0.79863
Loss = 0.716141
Loss = 0.654205
Loss = 0.665634
Loss = 0.853088
Loss = 0.880219
Loss = 0.817062
Loss = 0.729187
Loss = 0.665344
Loss = 0.845047
Loss = 0.794693
Loss = 0.765503
Loss = 0.739609
Loss = 0.731934
Loss = 0.808365
Loss = 0.654297
Loss = 0.767853
Loss = 0.847748
Loss = 0.733322
TEST LOSS = 0.756257
TEST ACC = 447.71 % (7582/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.695297
Epoch 3.2: Loss = 0.625122
Epoch 3.3: Loss = 0.720825
Epoch 3.4: Loss = 0.682022
Epoch 3.5: Loss = 0.681152
Epoch 3.6: Loss = 0.77565
Epoch 3.7: Loss = 0.729523
Epoch 3.8: Loss = 0.727997
Epoch 3.9: Loss = 0.788437
Epoch 3.10: Loss = 0.726624
Epoch 3.11: Loss = 0.796402
Epoch 3.12: Loss = 0.689194
Epoch 3.13: Loss = 0.79393
Epoch 3.14: Loss = 0.770676
Epoch 3.15: Loss = 0.757187
Epoch 3.16: Loss = 0.7314
Epoch 3.17: Loss = 0.728409
Epoch 3.18: Loss = 0.720352
Epoch 3.19: Loss = 0.813507
Epoch 3.20: Loss = 0.759369
Epoch 3.21: Loss = 0.87674
Epoch 3.22: Loss = 0.668457
Epoch 3.23: Loss = 0.882217
Epoch 3.24: Loss = 0.764709
Epoch 3.25: Loss = 0.753494
Epoch 3.26: Loss = 0.69339
Epoch 3.27: Loss = 0.750839
Epoch 3.28: Loss = 0.768494
Epoch 3.29: Loss = 0.723022
Epoch 3.30: Loss = 0.752365
Epoch 3.31: Loss = 0.72377
Epoch 3.32: Loss = 0.631775
Epoch 3.33: Loss = 0.806732
Epoch 3.34: Loss = 0.807068
Epoch 3.35: Loss = 0.711487
Epoch 3.36: Loss = 0.582687
Epoch 3.37: Loss = 0.892792
Epoch 3.38: Loss = 0.718048
Epoch 3.39: Loss = 0.781967
Epoch 3.40: Loss = 0.834213
Epoch 3.41: Loss = 0.680542
Epoch 3.42: Loss = 0.725327
Epoch 3.43: Loss = 0.831238
Epoch 3.44: Loss = 0.712479
Epoch 3.45: Loss = 0.761337
Epoch 3.46: Loss = 0.732452
Epoch 3.47: Loss = 0.735901
Epoch 3.48: Loss = 0.757416
Epoch 3.49: Loss = 0.64212
Epoch 3.50: Loss = 0.646744
Epoch 3.51: Loss = 0.660294
Epoch 3.52: Loss = 0.736389
Epoch 3.53: Loss = 0.72316
Epoch 3.54: Loss = 0.6642
Epoch 3.55: Loss = 0.716278
Epoch 3.56: Loss = 0.612686
Epoch 3.57: Loss = 0.758804
Epoch 3.58: Loss = 0.785156
Epoch 3.59: Loss = 0.720383
Epoch 3.60: Loss = 0.677124
Epoch 3.61: Loss = 0.817581
Epoch 3.62: Loss = 0.85878
Epoch 3.63: Loss = 0.784515
Epoch 3.64: Loss = 0.759262
Epoch 3.65: Loss = 0.698807
Epoch 3.66: Loss = 0.833862
Epoch 3.67: Loss = 0.681992
Epoch 3.68: Loss = 0.729996
Epoch 3.69: Loss = 0.797516
Epoch 3.70: Loss = 0.696808
Epoch 3.71: Loss = 0.721863
Epoch 3.72: Loss = 0.696259
Epoch 3.73: Loss = 0.827408
Epoch 3.74: Loss = 0.774994
Epoch 3.75: Loss = 0.734283
Epoch 3.76: Loss = 0.743988
Epoch 3.77: Loss = 0.732346
Epoch 3.78: Loss = 0.644653
Epoch 3.79: Loss = 0.640961
Epoch 3.80: Loss = 0.657883
Epoch 3.81: Loss = 0.732986
Epoch 3.82: Loss = 0.661926
Epoch 3.83: Loss = 0.645203
Epoch 3.84: Loss = 0.752365
Epoch 3.85: Loss = 0.734818
Epoch 3.86: Loss = 0.733231
Epoch 3.87: Loss = 0.604691
Epoch 3.88: Loss = 0.6866
Epoch 3.89: Loss = 0.751053
Epoch 3.90: Loss = 0.813202
Epoch 3.91: Loss = 0.814133
Epoch 3.92: Loss = 0.796402
Epoch 3.93: Loss = 0.71051
Epoch 3.94: Loss = 0.848587
Epoch 3.95: Loss = 0.808487
Epoch 3.96: Loss = 0.824203
Epoch 3.97: Loss = 0.850052
Epoch 3.98: Loss = 0.72876
Epoch 3.99: Loss = 0.671417
Epoch 3.100: Loss = 0.660965
Epoch 3.101: Loss = 0.809509
Epoch 3.102: Loss = 0.755066
Epoch 3.103: Loss = 0.702591
Epoch 3.104: Loss = 0.774597
Epoch 3.105: Loss = 0.773163
Epoch 3.106: Loss = 0.680984
Epoch 3.107: Loss = 0.743988
Epoch 3.108: Loss = 0.594803
Epoch 3.109: Loss = 0.879852
Epoch 3.110: Loss = 0.68779
Epoch 3.111: Loss = 0.704681
Epoch 3.112: Loss = 0.832687
Epoch 3.113: Loss = 0.635162
Epoch 3.114: Loss = 0.70224
Epoch 3.115: Loss = 0.659561
Epoch 3.116: Loss = 0.715454
Epoch 3.117: Loss = 0.715668
Epoch 3.118: Loss = 0.829422
Epoch 3.119: Loss = 0.764893
Epoch 3.120: Loss = 0.756729
TRAIN LOSS = 0.737518
TRAIN ACC = 76.7654 % (46061/60000)
Loss = 0.680252
Loss = 0.807922
Loss = 0.721054
Loss = 0.64856
Loss = 0.666031
Loss = 0.861389
Loss = 0.878998
Loss = 0.78801
Loss = 0.72731
Loss = 0.711929
Loss = 0.834427
Loss = 0.810745
Loss = 0.744171
Loss = 0.754318
Loss = 0.747955
Loss = 0.814621
Loss = 0.672501
Loss = 0.821838
Loss = 0.840897
Loss = 0.713486
TEST LOSS = 0.76232
TEST ACC = 460.609 % (7660/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.704086
Epoch 4.2: Loss = 0.690491
Epoch 4.3: Loss = 0.757736
Epoch 4.4: Loss = 0.670593
Epoch 4.5: Loss = 0.698105
Epoch 4.6: Loss = 0.780838
Epoch 4.7: Loss = 0.701202
Epoch 4.8: Loss = 0.630264
Epoch 4.9: Loss = 0.761856
Epoch 4.10: Loss = 0.677628
Epoch 4.11: Loss = 0.683563
Epoch 4.12: Loss = 0.750153
Epoch 4.13: Loss = 0.743469
Epoch 4.14: Loss = 0.770905
Epoch 4.15: Loss = 0.884247
Epoch 4.16: Loss = 0.765244
Epoch 4.17: Loss = 0.703461
Epoch 4.18: Loss = 0.71669
Epoch 4.19: Loss = 0.769531
Epoch 4.20: Loss = 0.65979
Epoch 4.21: Loss = 0.694473
Epoch 4.22: Loss = 0.653046
Epoch 4.23: Loss = 0.734161
Epoch 4.24: Loss = 0.737625
Epoch 4.25: Loss = 0.835922
Epoch 4.26: Loss = 0.683243
Epoch 4.27: Loss = 0.737106
Epoch 4.28: Loss = 0.710907
Epoch 4.29: Loss = 0.790085
Epoch 4.30: Loss = 0.648041
Epoch 4.31: Loss = 0.79599
Epoch 4.32: Loss = 0.771667
Epoch 4.33: Loss = 0.740982
Epoch 4.34: Loss = 0.801849
Epoch 4.35: Loss = 0.746323
Epoch 4.36: Loss = 0.621368
Epoch 4.37: Loss = 0.763748
Epoch 4.38: Loss = 0.912079
Epoch 4.39: Loss = 0.696716
Epoch 4.40: Loss = 0.713043
Epoch 4.41: Loss = 0.909454
Epoch 4.42: Loss = 0.726883
Epoch 4.43: Loss = 0.831085
Epoch 4.44: Loss = 0.737167
Epoch 4.45: Loss = 0.735474
Epoch 4.46: Loss = 0.806671
Epoch 4.47: Loss = 0.752579
Epoch 4.48: Loss = 0.920914
Epoch 4.49: Loss = 0.854218
Epoch 4.50: Loss = 0.75148
Epoch 4.51: Loss = 0.740829
Epoch 4.52: Loss = 0.743576
Epoch 4.53: Loss = 0.758453
Epoch 4.54: Loss = 0.815964
Epoch 4.55: Loss = 0.657379
Epoch 4.56: Loss = 0.652756
Epoch 4.57: Loss = 0.889023
Epoch 4.58: Loss = 0.623093
Epoch 4.59: Loss = 0.797958
Epoch 4.60: Loss = 0.740952
Epoch 4.61: Loss = 0.756836
Epoch 4.62: Loss = 0.72084
Epoch 4.63: Loss = 0.809021
Epoch 4.64: Loss = 0.82608
Epoch 4.65: Loss = 0.760712
Epoch 4.66: Loss = 0.809402
Epoch 4.67: Loss = 0.776901
Epoch 4.68: Loss = 0.863113
Epoch 4.69: Loss = 0.870758
Epoch 4.70: Loss = 0.866211
Epoch 4.71: Loss = 0.795288
Epoch 4.72: Loss = 0.851151
Epoch 4.73: Loss = 0.742905
Epoch 4.74: Loss = 0.773376
Epoch 4.75: Loss = 0.639069
Epoch 4.76: Loss = 0.78215
Epoch 4.77: Loss = 0.563019
Epoch 4.78: Loss = 0.761719
Epoch 4.79: Loss = 0.80481
Epoch 4.80: Loss = 0.746185
Epoch 4.81: Loss = 0.795898
Epoch 4.82: Loss = 0.696976
Epoch 4.83: Loss = 0.830063
Epoch 4.84: Loss = 0.767227
Epoch 4.85: Loss = 0.860809
Epoch 4.86: Loss = 0.791656
Epoch 4.87: Loss = 0.751984
Epoch 4.88: Loss = 0.688217
Epoch 4.89: Loss = 0.710312
Epoch 4.90: Loss = 0.757599
Epoch 4.91: Loss = 0.638733
Epoch 4.92: Loss = 0.687592
Epoch 4.93: Loss = 0.782394
Epoch 4.94: Loss = 0.711487
Epoch 4.95: Loss = 0.765457
Epoch 4.96: Loss = 0.792267
Epoch 4.97: Loss = 0.796783
Epoch 4.98: Loss = 0.870422
Epoch 4.99: Loss = 0.779236
Epoch 4.100: Loss = 0.732849
Epoch 4.101: Loss = 0.7258
Epoch 4.102: Loss = 0.795746
Epoch 4.103: Loss = 0.821152
Epoch 4.104: Loss = 0.771347
Epoch 4.105: Loss = 0.680832
Epoch 4.106: Loss = 0.630203
Epoch 4.107: Loss = 0.763153
Epoch 4.108: Loss = 0.796158
Epoch 4.109: Loss = 0.70787
Epoch 4.110: Loss = 0.750626
Epoch 4.111: Loss = 0.774307
Epoch 4.112: Loss = 0.755524
Epoch 4.113: Loss = 0.823151
Epoch 4.114: Loss = 0.807205
Epoch 4.115: Loss = 0.868835
Epoch 4.116: Loss = 0.778534
Epoch 4.117: Loss = 0.714462
Epoch 4.118: Loss = 0.739105
Epoch 4.119: Loss = 0.753891
Epoch 4.120: Loss = 0.715836
TRAIN LOSS = 0.755478
TRAIN ACC = 77.4658 % (46482/60000)
Loss = 0.729645
Loss = 0.901474
Loss = 0.749008
Loss = 0.720291
Loss = 0.72554
Loss = 0.872742
Loss = 0.961105
Loss = 0.861816
Loss = 0.813171
Loss = 0.743805
Loss = 0.870728
Loss = 0.864822
Loss = 0.827026
Loss = 0.768356
Loss = 0.777466
Loss = 0.845749
Loss = 0.654297
Loss = 0.825714
Loss = 0.878845
Loss = 0.75032
TEST LOSS = 0.807096
TEST ACC = 464.819 % (7677/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.804291
Epoch 5.2: Loss = 0.830887
Epoch 5.3: Loss = 0.699554
Epoch 5.4: Loss = 0.744431
Epoch 5.5: Loss = 0.811325
Epoch 5.6: Loss = 0.773911
Epoch 5.7: Loss = 0.798996
Epoch 5.8: Loss = 0.68306
Epoch 5.9: Loss = 0.761002
Epoch 5.10: Loss = 0.874756
Epoch 5.11: Loss = 0.851776
Epoch 5.12: Loss = 0.856216
Epoch 5.13: Loss = 0.841324
Epoch 5.14: Loss = 0.701691
Epoch 5.15: Loss = 0.84964
Epoch 5.16: Loss = 0.799667
Epoch 5.17: Loss = 0.772308
Epoch 5.18: Loss = 0.631485
Epoch 5.19: Loss = 0.859909
Epoch 5.20: Loss = 0.728149
Epoch 5.21: Loss = 0.833771
Epoch 5.22: Loss = 0.693939
Epoch 5.23: Loss = 0.745255
Epoch 5.24: Loss = 0.713455
Epoch 5.25: Loss = 0.895737
Epoch 5.26: Loss = 0.691437
Epoch 5.27: Loss = 0.617722
Epoch 5.28: Loss = 0.803635
Epoch 5.29: Loss = 0.731033
Epoch 5.30: Loss = 0.839325
Epoch 5.31: Loss = 0.741989
Epoch 5.32: Loss = 0.885193
Epoch 5.33: Loss = 0.872253
Epoch 5.34: Loss = 0.724747
Epoch 5.35: Loss = 0.774979
Epoch 5.36: Loss = 0.846725
Epoch 5.37: Loss = 0.764923
Epoch 5.38: Loss = 0.806717
Epoch 5.39: Loss = 0.807404
Epoch 5.40: Loss = 0.694107
Epoch 5.41: Loss = 0.701538
Epoch 5.42: Loss = 0.714661
Epoch 5.43: Loss = 0.619247
Epoch 5.44: Loss = 0.695251
Epoch 5.45: Loss = 0.735535
Epoch 5.46: Loss = 0.844833
Epoch 5.47: Loss = 0.763519
Epoch 5.48: Loss = 0.738174
Epoch 5.49: Loss = 0.854614
Epoch 5.50: Loss = 0.692932
Epoch 5.51: Loss = 0.686005
Epoch 5.52: Loss = 0.702988
Epoch 5.53: Loss = 0.742493
Epoch 5.54: Loss = 0.688965
Epoch 5.55: Loss = 0.755859
Epoch 5.56: Loss = 0.723694
Epoch 5.57: Loss = 0.71759
Epoch 5.58: Loss = 0.709259
Epoch 5.59: Loss = 0.831802
Epoch 5.60: Loss = 0.595047
Epoch 5.61: Loss = 0.855087
Epoch 5.62: Loss = 0.701859
Epoch 5.63: Loss = 0.885696
Epoch 5.64: Loss = 0.791367
Epoch 5.65: Loss = 0.787155
Epoch 5.66: Loss = 0.652008
Epoch 5.67: Loss = 0.857483
Epoch 5.68: Loss = 0.805725
Epoch 5.69: Loss = 0.799088
Epoch 5.70: Loss = 0.861649
Epoch 5.71: Loss = 0.932358
Epoch 5.72: Loss = 0.867172
Epoch 5.73: Loss = 0.777115
Epoch 5.74: Loss = 0.636246
Epoch 5.75: Loss = 0.723816
Epoch 5.76: Loss = 0.6931
Epoch 5.77: Loss = 0.856201
Epoch 5.78: Loss = 0.837646
Epoch 5.79: Loss = 0.77739
Epoch 5.80: Loss = 0.680115
Epoch 5.81: Loss = 0.674225
Epoch 5.82: Loss = 0.868912
Epoch 5.83: Loss = 0.875504
Epoch 5.84: Loss = 0.942505
Epoch 5.85: Loss = 0.801926
Epoch 5.86: Loss = 0.742447
Epoch 5.87: Loss = 0.723694
Epoch 5.88: Loss = 0.711868
Epoch 5.89: Loss = 0.670319
Epoch 5.90: Loss = 0.579712
Epoch 5.91: Loss = 0.732178
Epoch 5.92: Loss = 0.789276
Epoch 5.93: Loss = 0.756134
Epoch 5.94: Loss = 0.86496
Epoch 5.95: Loss = 0.906845
Epoch 5.96: Loss = 0.796997
Epoch 5.97: Loss = 0.739243
Epoch 5.98: Loss = 0.712265
Epoch 5.99: Loss = 0.836441
Epoch 5.100: Loss = 0.785294
Epoch 5.101: Loss = 0.721329
Epoch 5.102: Loss = 0.690262
Epoch 5.103: Loss = 0.674225
Epoch 5.104: Loss = 0.765762
Epoch 5.105: Loss = 0.737335
Epoch 5.106: Loss = 0.764893
Epoch 5.107: Loss = 0.729462
Epoch 5.108: Loss = 0.714417
Epoch 5.109: Loss = 0.789642
Epoch 5.110: Loss = 0.822647
Epoch 5.111: Loss = 0.826141
Epoch 5.112: Loss = 0.824539
Epoch 5.113: Loss = 0.774185
Epoch 5.114: Loss = 0.715988
Epoch 5.115: Loss = 0.851028
Epoch 5.116: Loss = 0.723694
Epoch 5.117: Loss = 0.801498
Epoch 5.118: Loss = 0.603317
Epoch 5.119: Loss = 1.0546
Epoch 5.120: Loss = 0.752411
TRAIN LOSS = 0.769165
TRAIN ACC = 78.1998 % (46922/60000)
Loss = 0.704803
Loss = 0.878555
Loss = 0.783478
Loss = 0.711044
Loss = 0.743393
Loss = 0.968399
Loss = 0.965637
Loss = 0.881683
Loss = 0.815765
Loss = 0.768494
Loss = 0.953491
Loss = 0.878036
Loss = 0.846024
Loss = 0.7995
Loss = 0.840179
Loss = 0.885834
Loss = 0.705109
Loss = 0.891174
Loss = 0.91832
Loss = 0.735992
TEST LOSS = 0.833745
TEST ACC = 469.218 % (7798/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.73436
Epoch 6.2: Loss = 0.783936
Epoch 6.3: Loss = 0.76796
Epoch 6.4: Loss = 0.817581
Epoch 6.5: Loss = 0.777908
Epoch 6.6: Loss = 0.988922
Epoch 6.7: Loss = 0.787155
Epoch 6.8: Loss = 0.667969
Epoch 6.9: Loss = 0.708481
Epoch 6.10: Loss = 0.744919
Epoch 6.11: Loss = 0.818909
Epoch 6.12: Loss = 1.045
Epoch 6.13: Loss = 0.828583
Epoch 6.14: Loss = 0.768066
Epoch 6.15: Loss = 0.713974
Epoch 6.16: Loss = 0.671768
Epoch 6.17: Loss = 0.798172
Epoch 6.18: Loss = 0.886856
Epoch 6.19: Loss = 0.873062
Epoch 6.20: Loss = 0.732391
Epoch 6.21: Loss = 0.856445
Epoch 6.22: Loss = 0.755936
Epoch 6.23: Loss = 0.813477
Epoch 6.24: Loss = 0.720963
Epoch 6.25: Loss = 0.675583
Epoch 6.26: Loss = 0.739151
Epoch 6.27: Loss = 0.790039
Epoch 6.28: Loss = 0.658524
Epoch 6.29: Loss = 0.724365
Epoch 6.30: Loss = 0.849106
Epoch 6.31: Loss = 0.734344
Epoch 6.32: Loss = 0.694336
Epoch 6.33: Loss = 0.800018
Epoch 6.34: Loss = 0.783752
Epoch 6.35: Loss = 0.809723
Epoch 6.36: Loss = 0.796173
Epoch 6.37: Loss = 0.769287
Epoch 6.38: Loss = 0.679672
Epoch 6.39: Loss = 0.87738
Epoch 6.40: Loss = 0.822372
Epoch 6.41: Loss = 0.789261
Epoch 6.42: Loss = 0.671524
Epoch 6.43: Loss = 0.980438
Epoch 6.44: Loss = 0.750565
Epoch 6.45: Loss = 0.857559
Epoch 6.46: Loss = 0.694427
Epoch 6.47: Loss = 0.680542
Epoch 6.48: Loss = 0.811218
Epoch 6.49: Loss = 0.887405
Epoch 6.50: Loss = 0.805145
Epoch 6.51: Loss = 0.699738
Epoch 6.52: Loss = 0.786255
Epoch 6.53: Loss = 0.793091
Epoch 6.54: Loss = 0.644257
Epoch 6.55: Loss = 0.852707
Epoch 6.56: Loss = 0.716599
Epoch 6.57: Loss = 0.798233
Epoch 6.58: Loss = 0.764023
Epoch 6.59: Loss = 0.766617
Epoch 6.60: Loss = 0.908051
Epoch 6.61: Loss = 0.798523
Epoch 6.62: Loss = 0.879883
Epoch 6.63: Loss = 0.796509
Epoch 6.64: Loss = 0.740601
Epoch 6.65: Loss = 0.800018
Epoch 6.66: Loss = 0.918457
Epoch 6.67: Loss = 0.696732
Epoch 6.68: Loss = 0.693375
Epoch 6.69: Loss = 0.769363
Epoch 6.70: Loss = 0.846008
Epoch 6.71: Loss = 0.749313
Epoch 6.72: Loss = 0.853775
Epoch 6.73: Loss = 0.79512
Epoch 6.74: Loss = 0.866486
Epoch 6.75: Loss = 0.795303
Epoch 6.76: Loss = 0.845963
Epoch 6.77: Loss = 0.885452
Epoch 6.78: Loss = 0.901718
Epoch 6.79: Loss = 0.60762
Epoch 6.80: Loss = 0.787338
Epoch 6.81: Loss = 0.819809
Epoch 6.82: Loss = 0.718552
Epoch 6.83: Loss = 0.776901
Epoch 6.84: Loss = 0.883621
Epoch 6.85: Loss = 0.777222
Epoch 6.86: Loss = 0.88121
Epoch 6.87: Loss = 0.690506
Epoch 6.88: Loss = 0.712677
Epoch 6.89: Loss = 0.707565
Epoch 6.90: Loss = 0.908417
Epoch 6.91: Loss = 0.884384
Epoch 6.92: Loss = 0.755478
Epoch 6.93: Loss = 0.777069
Epoch 6.94: Loss = 0.857407
Epoch 6.95: Loss = 0.883606
Epoch 6.96: Loss = 0.780869
Epoch 6.97: Loss = 0.744308
Epoch 6.98: Loss = 0.704361
Epoch 6.99: Loss = 0.865768
Epoch 6.100: Loss = 0.762039
Epoch 6.101: Loss = 0.871902
Epoch 6.102: Loss = 0.799393
Epoch 6.103: Loss = 0.779236
Epoch 6.104: Loss = 0.87558
Epoch 6.105: Loss = 0.739349
Epoch 6.106: Loss = 0.912109
Epoch 6.107: Loss = 0.861282
Epoch 6.108: Loss = 0.622879
Epoch 6.109: Loss = 0.86853
Epoch 6.110: Loss = 0.770157
Epoch 6.111: Loss = 0.84906
Epoch 6.112: Loss = 1.03308
Epoch 6.113: Loss = 0.698914
Epoch 6.114: Loss = 0.735123
Epoch 6.115: Loss = 0.752579
Epoch 6.116: Loss = 0.874466
Epoch 6.117: Loss = 0.876694
Epoch 6.118: Loss = 0.860748
Epoch 6.119: Loss = 0.783432
Epoch 6.120: Loss = 0.751877
TRAIN LOSS = 0.793015
TRAIN ACC = 78.5645 % (47141/60000)
Loss = 0.733139
Loss = 0.886566
Loss = 0.847733
Loss = 0.712585
Loss = 0.749237
Loss = 0.99913
Loss = 1.00031
Loss = 0.941574
Loss = 0.829803
Loss = 0.776886
Loss = 0.964142
Loss = 0.88887
Loss = 0.864746
Loss = 0.828705
Loss = 0.854645
Loss = 0.919769
Loss = 0.766327
Loss = 0.930969
Loss = 0.906708
Loss = 0.779373
TEST LOSS = 0.85906
TEST ACC = 471.41 % (7788/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.880005
Epoch 7.2: Loss = 0.655502
Epoch 7.3: Loss = 0.79953
Epoch 7.4: Loss = 0.781509
Epoch 7.5: Loss = 0.875946
Epoch 7.6: Loss = 0.823593
Epoch 7.7: Loss = 0.824112
Epoch 7.8: Loss = 0.813446
Epoch 7.9: Loss = 0.860428
Epoch 7.10: Loss = 0.789688
Epoch 7.11: Loss = 0.887543
Epoch 7.12: Loss = 0.700363
Epoch 7.13: Loss = 0.727997
Epoch 7.14: Loss = 0.768784
Epoch 7.15: Loss = 0.914566
Epoch 7.16: Loss = 0.845734
Epoch 7.17: Loss = 0.779221
Epoch 7.18: Loss = 0.99707
Epoch 7.19: Loss = 0.858261
Epoch 7.20: Loss = 0.792801
Epoch 7.21: Loss = 0.90181
Epoch 7.22: Loss = 1.01094
Epoch 7.23: Loss = 0.726532
Epoch 7.24: Loss = 0.706451
Epoch 7.25: Loss = 0.804489
Epoch 7.26: Loss = 0.943924
Epoch 7.27: Loss = 0.787354
Epoch 7.28: Loss = 0.824432
Epoch 7.29: Loss = 1.0015
Epoch 7.30: Loss = 0.720795
Epoch 7.31: Loss = 0.840805
Epoch 7.32: Loss = 0.728653
Epoch 7.33: Loss = 0.779724
Epoch 7.34: Loss = 0.933365
Epoch 7.35: Loss = 1.12848
Epoch 7.36: Loss = 0.656891
Epoch 7.37: Loss = 0.772003
Epoch 7.38: Loss = 0.853333
Epoch 7.39: Loss = 0.949142
Epoch 7.40: Loss = 0.688263
Epoch 7.41: Loss = 0.914795
Epoch 7.42: Loss = 0.752365
Epoch 7.43: Loss = 0.940125
Epoch 7.44: Loss = 0.648163
Epoch 7.45: Loss = 0.850784
Epoch 7.46: Loss = 0.75946
Epoch 7.47: Loss = 0.924637
Epoch 7.48: Loss = 0.859406
Epoch 7.49: Loss = 0.89476
Epoch 7.50: Loss = 0.807877
Epoch 7.51: Loss = 0.695221
Epoch 7.52: Loss = 0.871307
Epoch 7.53: Loss = 0.696472
Epoch 7.54: Loss = 0.830811
Epoch 7.55: Loss = 0.962616
Epoch 7.56: Loss = 0.823151
Epoch 7.57: Loss = 0.944534
Epoch 7.58: Loss = 0.814285
Epoch 7.59: Loss = 0.715439
Epoch 7.60: Loss = 0.700851
Epoch 7.61: Loss = 0.907211
Epoch 7.62: Loss = 0.785583
Epoch 7.63: Loss = 0.878967
Epoch 7.64: Loss = 0.756866
Epoch 7.65: Loss = 0.657364
Epoch 7.66: Loss = 0.770798
Epoch 7.67: Loss = 0.884277
Epoch 7.68: Loss = 0.913696
Epoch 7.69: Loss = 0.795868
Epoch 7.70: Loss = 0.740616
Epoch 7.71: Loss = 0.930695
Epoch 7.72: Loss = 0.868607
Epoch 7.73: Loss = 0.901886
Epoch 7.74: Loss = 0.939056
Epoch 7.75: Loss = 0.836563
Epoch 7.76: Loss = 0.829773
Epoch 7.77: Loss = 0.797028
Epoch 7.78: Loss = 0.759354
Epoch 7.79: Loss = 0.894424
Epoch 7.80: Loss = 0.961212
Epoch 7.81: Loss = 0.824951
Epoch 7.82: Loss = 0.760269
Epoch 7.83: Loss = 1.05954
Epoch 7.84: Loss = 0.891937
Epoch 7.85: Loss = 0.673721
Epoch 7.86: Loss = 0.866058
Epoch 7.87: Loss = 0.764526
Epoch 7.88: Loss = 0.721497
Epoch 7.89: Loss = 1.06259
Epoch 7.90: Loss = 0.994568
Epoch 7.91: Loss = 0.798599
Epoch 7.92: Loss = 0.719269
Epoch 7.93: Loss = 0.786209
Epoch 7.94: Loss = 0.918549
Epoch 7.95: Loss = 0.895111
Epoch 7.96: Loss = 0.826553
Epoch 7.97: Loss = 0.808945
Epoch 7.98: Loss = 0.787125
Epoch 7.99: Loss = 0.607544
Epoch 7.100: Loss = 0.630432
Epoch 7.101: Loss = 0.912216
Epoch 7.102: Loss = 0.876846
Epoch 7.103: Loss = 0.909332
Epoch 7.104: Loss = 0.773148
Epoch 7.105: Loss = 0.927673
Epoch 7.106: Loss = 0.786148
Epoch 7.107: Loss = 0.815521
Epoch 7.108: Loss = 0.678238
Epoch 7.109: Loss = 0.808685
Epoch 7.110: Loss = 0.910843
Epoch 7.111: Loss = 0.912903
Epoch 7.112: Loss = 0.800735
Epoch 7.113: Loss = 0.86528
Epoch 7.114: Loss = 0.768875
Epoch 7.115: Loss = 0.881241
Epoch 7.116: Loss = 0.777893
Epoch 7.117: Loss = 0.790863
Epoch 7.118: Loss = 0.822891
Epoch 7.119: Loss = 0.785797
Epoch 7.120: Loss = 0.774521
TRAIN LOSS = 0.827759
TRAIN ACC = 78.7064 % (47226/60000)
Loss = 0.804993
Loss = 0.94458
Loss = 0.847412
Loss = 0.742462
Loss = 0.767914
Loss = 1.01018
Loss = 1.04755
Loss = 0.911972
Loss = 0.868515
Loss = 0.815033
Loss = 1.02429
Loss = 0.934677
Loss = 0.879639
Loss = 0.852081
Loss = 0.830521
Loss = 0.919647
Loss = 0.781204
Loss = 0.956894
Loss = 0.951721
Loss = 0.822586
TEST LOSS = 0.885693
TEST ACC = 472.26 % (7812/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.88826
Epoch 8.2: Loss = 0.8759
Epoch 8.3: Loss = 0.90834
Epoch 8.4: Loss = 0.892502
Epoch 8.5: Loss = 0.760345
Epoch 8.6: Loss = 0.922073
Epoch 8.7: Loss = 0.840759
Epoch 8.8: Loss = 0.74321
Epoch 8.9: Loss = 0.829391
Epoch 8.10: Loss = 0.792267
Epoch 8.11: Loss = 0.905869
Epoch 8.12: Loss = 0.858551
Epoch 8.13: Loss = 0.856628
Epoch 8.14: Loss = 0.708603
Epoch 8.15: Loss = 0.883545
Epoch 8.16: Loss = 0.872238
Epoch 8.17: Loss = 0.925537
Epoch 8.18: Loss = 0.870483
Epoch 8.19: Loss = 0.769333
Epoch 8.20: Loss = 0.814224
Epoch 8.21: Loss = 0.871063
Epoch 8.22: Loss = 1.019
Epoch 8.23: Loss = 0.815262
Epoch 8.24: Loss = 0.84256
Epoch 8.25: Loss = 1.04265
Epoch 8.26: Loss = 0.875412
Epoch 8.27: Loss = 0.784729
Epoch 8.28: Loss = 0.828278
Epoch 8.29: Loss = 1.01994
Epoch 8.30: Loss = 0.702591
Epoch 8.31: Loss = 0.971329
Epoch 8.32: Loss = 0.891113
Epoch 8.33: Loss = 0.73494
Epoch 8.34: Loss = 0.776154
Epoch 8.35: Loss = 0.889481
Epoch 8.36: Loss = 0.922607
Epoch 8.37: Loss = 0.863983
Epoch 8.38: Loss = 0.813797
Epoch 8.39: Loss = 1.0592
Epoch 8.40: Loss = 0.797348
Epoch 8.41: Loss = 0.718231
Epoch 8.42: Loss = 0.741852
Epoch 8.43: Loss = 0.907928
Epoch 8.44: Loss = 0.73584
Epoch 8.45: Loss = 0.862381
Epoch 8.46: Loss = 0.849579
Epoch 8.47: Loss = 0.646347
Epoch 8.48: Loss = 0.883804
Epoch 8.49: Loss = 0.623184
Epoch 8.50: Loss = 0.910751
Epoch 8.51: Loss = 0.786789
Epoch 8.52: Loss = 0.931259
Epoch 8.53: Loss = 0.911209
Epoch 8.54: Loss = 0.888077
Epoch 8.55: Loss = 0.951965
Epoch 8.56: Loss = 1.04689
Epoch 8.57: Loss = 0.752548
Epoch 8.58: Loss = 0.746246
Epoch 8.59: Loss = 0.8358
Epoch 8.60: Loss = 0.937851
Epoch 8.61: Loss = 0.743149
Epoch 8.62: Loss = 0.946793
Epoch 8.63: Loss = 0.730255
Epoch 8.64: Loss = 0.821014
Epoch 8.65: Loss = 0.975449
Epoch 8.66: Loss = 0.929596
Epoch 8.67: Loss = 1.12317
Epoch 8.68: Loss = 0.718475
Epoch 8.69: Loss = 0.67572
Epoch 8.70: Loss = 0.925171
Epoch 8.71: Loss = 0.873764
Epoch 8.72: Loss = 0.858963
Epoch 8.73: Loss = 0.759064
Epoch 8.74: Loss = 0.69812
Epoch 8.75: Loss = 0.763809
Epoch 8.76: Loss = 0.722565
Epoch 8.77: Loss = 0.745941
Epoch 8.78: Loss = 0.821335
Epoch 8.79: Loss = 0.875626
Epoch 8.80: Loss = 0.872009
Epoch 8.81: Loss = 0.996048
Epoch 8.82: Loss = 0.695953
Epoch 8.83: Loss = 0.931152
Epoch 8.84: Loss = 0.828903
Epoch 8.85: Loss = 0.788162
Epoch 8.86: Loss = 0.830124
Epoch 8.87: Loss = 0.812103
Epoch 8.88: Loss = 0.882019
Epoch 8.89: Loss = 0.86911
Epoch 8.90: Loss = 0.854507
Epoch 8.91: Loss = 0.838165
Epoch 8.92: Loss = 0.827271
Epoch 8.93: Loss = 0.92659
Epoch 8.94: Loss = 0.860428
Epoch 8.95: Loss = 0.831116
Epoch 8.96: Loss = 1.01199
Epoch 8.97: Loss = 1.12735
Epoch 8.98: Loss = 1.02708
Epoch 8.99: Loss = 0.929535
Epoch 8.100: Loss = 0.917603
Epoch 8.101: Loss = 0.940948
Epoch 8.102: Loss = 0.823349
Epoch 8.103: Loss = 0.894821
Epoch 8.104: Loss = 0.809036
Epoch 8.105: Loss = 0.788803
Epoch 8.106: Loss = 0.850983
Epoch 8.107: Loss = 0.802536
Epoch 8.108: Loss = 0.75592
Epoch 8.109: Loss = 0.890427
Epoch 8.110: Loss = 0.666718
Epoch 8.111: Loss = 0.884857
Epoch 8.112: Loss = 0.783997
Epoch 8.113: Loss = 0.764252
Epoch 8.114: Loss = 0.78717
Epoch 8.115: Loss = 1.04958
Epoch 8.116: Loss = 0.72142
Epoch 8.117: Loss = 0.807892
Epoch 8.118: Loss = 0.768661
Epoch 8.119: Loss = 0.851273
Epoch 8.120: Loss = 0.644745
TRAIN LOSS = 0.848022
TRAIN ACC = 78.4714 % (47085/60000)
Loss = 0.822632
Loss = 0.911575
Loss = 0.901001
Loss = 0.797333
Loss = 0.770523
Loss = 1.0477
Loss = 1.16245
Loss = 0.997574
Loss = 0.942642
Loss = 0.852264
Loss = 1.02885
Loss = 1.02519
Loss = 0.933868
Loss = 0.895432
Loss = 0.838547
Loss = 0.914871
Loss = 0.819275
Loss = 0.969254
Loss = 1.01416
Loss = 0.809082
TEST LOSS = 0.922711
TEST ACC = 470.85 % (7753/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.945465
Epoch 9.2: Loss = 0.884644
Epoch 9.3: Loss = 0.993195
Epoch 9.4: Loss = 0.972031
Epoch 9.5: Loss = 0.849564
Epoch 9.6: Loss = 0.814285
Epoch 9.7: Loss = 0.956711
Epoch 9.8: Loss = 0.911865
Epoch 9.9: Loss = 0.728195
Epoch 9.10: Loss = 0.879196
Epoch 9.11: Loss = 0.824432
Epoch 9.12: Loss = 0.81601
Epoch 9.13: Loss = 0.904556
Epoch 9.14: Loss = 0.882568
Epoch 9.15: Loss = 0.924484
Epoch 9.16: Loss = 0.741333
Epoch 9.17: Loss = 0.846298
Epoch 9.18: Loss = 0.905334
Epoch 9.19: Loss = 0.80394
Epoch 9.20: Loss = 0.821808
Epoch 9.21: Loss = 0.810837
Epoch 9.22: Loss = 0.768753
Epoch 9.23: Loss = 0.801178
Epoch 9.24: Loss = 0.766983
Epoch 9.25: Loss = 1.00354
Epoch 9.26: Loss = 0.830292
Epoch 9.27: Loss = 0.835159
Epoch 9.28: Loss = 1.01785
Epoch 9.29: Loss = 0.903564
Epoch 9.30: Loss = 0.875107
Epoch 9.31: Loss = 0.860794
Epoch 9.32: Loss = 0.853195
Epoch 9.33: Loss = 0.873291
Epoch 9.34: Loss = 0.833084
Epoch 9.35: Loss = 0.697205
Epoch 9.36: Loss = 0.642227
Epoch 9.37: Loss = 0.841141
Epoch 9.38: Loss = 1.00328
Epoch 9.39: Loss = 0.78096
Epoch 9.40: Loss = 0.820419
Epoch 9.41: Loss = 0.869827
Epoch 9.42: Loss = 0.919464
Epoch 9.43: Loss = 0.799927
Epoch 9.44: Loss = 0.870132
Epoch 9.45: Loss = 0.807953
Epoch 9.46: Loss = 0.632019
Epoch 9.47: Loss = 0.904831
Epoch 9.48: Loss = 0.857895
Epoch 9.49: Loss = 0.836777
Epoch 9.50: Loss = 0.882126
Epoch 9.51: Loss = 0.743622
Epoch 9.52: Loss = 0.950821
Epoch 9.53: Loss = 0.757187
Epoch 9.54: Loss = 0.969055
Epoch 9.55: Loss = 0.929855
Epoch 9.56: Loss = 0.774948
Epoch 9.57: Loss = 0.785355
Epoch 9.58: Loss = 0.978729
Epoch 9.59: Loss = 0.854706
Epoch 9.60: Loss = 0.719589
Epoch 9.61: Loss = 0.864853
Epoch 9.62: Loss = 0.860016
Epoch 9.63: Loss = 0.726105
Epoch 9.64: Loss = 0.793427
Epoch 9.65: Loss = 1.00621
Epoch 9.66: Loss = 0.865326
Epoch 9.67: Loss = 0.889145
Epoch 9.68: Loss = 0.955475
Epoch 9.69: Loss = 0.864182
Epoch 9.70: Loss = 0.802185
Epoch 9.71: Loss = 0.888748
Epoch 9.72: Loss = 0.837082
Epoch 9.73: Loss = 0.789154
Epoch 9.74: Loss = 1.01544
Epoch 9.75: Loss = 0.899734
Epoch 9.76: Loss = 0.924042
Epoch 9.77: Loss = 0.860779
Epoch 9.78: Loss = 1.01411
Epoch 9.79: Loss = 0.857574
Epoch 9.80: Loss = 0.798889
Epoch 9.81: Loss = 0.733597
Epoch 9.82: Loss = 1.0248
Epoch 9.83: Loss = 0.953476
Epoch 9.84: Loss = 0.832336
Epoch 9.85: Loss = 0.734177
Epoch 9.86: Loss = 0.877792
Epoch 9.87: Loss = 0.933578
Epoch 9.88: Loss = 0.867157
Epoch 9.89: Loss = 0.893997
Epoch 9.90: Loss = 0.786713
Epoch 9.91: Loss = 0.819061
Epoch 9.92: Loss = 0.925278
Epoch 9.93: Loss = 0.896362
Epoch 9.94: Loss = 0.859604
Epoch 9.95: Loss = 0.728943
Epoch 9.96: Loss = 0.778183
Epoch 9.97: Loss = 1.02269
Epoch 9.98: Loss = 0.814819
Epoch 9.99: Loss = 0.770355
Epoch 9.100: Loss = 0.879395
Epoch 9.101: Loss = 0.820908
Epoch 9.102: Loss = 1.08101
Epoch 9.103: Loss = 0.802872
Epoch 9.104: Loss = 0.867386
Epoch 9.105: Loss = 0.927582
Epoch 9.106: Loss = 0.773422
Epoch 9.107: Loss = 0.935165
Epoch 9.108: Loss = 0.860519
Epoch 9.109: Loss = 0.820068
Epoch 9.110: Loss = 0.900848
Epoch 9.111: Loss = 0.929779
Epoch 9.112: Loss = 0.935104
Epoch 9.113: Loss = 0.829758
Epoch 9.114: Loss = 1.02782
Epoch 9.115: Loss = 0.630402
Epoch 9.116: Loss = 0.700211
Epoch 9.117: Loss = 1.02739
Epoch 9.118: Loss = 0.796844
Epoch 9.119: Loss = 0.712982
Epoch 9.120: Loss = 0.760269
TRAIN LOSS = 0.857147
TRAIN ACC = 78.5309 % (47121/60000)
Loss = 0.798019
Loss = 0.890884
Loss = 0.84491
Loss = 0.781586
Loss = 0.74115
Loss = 1.05699
Loss = 1.0977
Loss = 0.971985
Loss = 0.919556
Loss = 0.819351
Loss = 1.01396
Loss = 0.95108
Loss = 0.861526
Loss = 0.825119
Loss = 0.826523
Loss = 0.917191
Loss = 0.780731
Loss = 0.927765
Loss = 0.945999
Loss = 0.783279
TEST LOSS = 0.887765
TEST ACC = 471.21 % (7725/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.915497
Epoch 10.2: Loss = 0.853394
Epoch 10.3: Loss = 0.844131
Epoch 10.4: Loss = 0.709808
Epoch 10.5: Loss = 0.946381
Epoch 10.6: Loss = 0.951584
Epoch 10.7: Loss = 0.855804
Epoch 10.8: Loss = 0.943527
Epoch 10.9: Loss = 0.927078
Epoch 10.10: Loss = 0.837097
Epoch 10.11: Loss = 0.847275
Epoch 10.12: Loss = 0.955841
Epoch 10.13: Loss = 0.80809
Epoch 10.14: Loss = 0.868454
Epoch 10.15: Loss = 0.859573
Epoch 10.16: Loss = 0.837967
Epoch 10.17: Loss = 0.886169
Epoch 10.18: Loss = 0.82103
Epoch 10.19: Loss = 0.812027
Epoch 10.20: Loss = 0.855515
Epoch 10.21: Loss = 0.991669
Epoch 10.22: Loss = 0.840347
Epoch 10.23: Loss = 0.963257
Epoch 10.24: Loss = 0.91333
Epoch 10.25: Loss = 0.761383
Epoch 10.26: Loss = 0.930435
Epoch 10.27: Loss = 0.660156
Epoch 10.28: Loss = 0.626144
Epoch 10.29: Loss = 0.874985
Epoch 10.30: Loss = 0.781982
Epoch 10.31: Loss = 0.863068
Epoch 10.32: Loss = 0.872787
Epoch 10.33: Loss = 0.784027
Epoch 10.34: Loss = 0.82811
Epoch 10.35: Loss = 0.929901
Epoch 10.36: Loss = 0.864639
Epoch 10.37: Loss = 0.771942
Epoch 10.38: Loss = 0.907211
Epoch 10.39: Loss = 0.810547
Epoch 10.40: Loss = 0.879913
Epoch 10.41: Loss = 0.883224
Epoch 10.42: Loss = 0.921463
Epoch 10.43: Loss = 0.910965
Epoch 10.44: Loss = 0.897919
Epoch 10.45: Loss = 1.01788
Epoch 10.46: Loss = 0.912979
Epoch 10.47: Loss = 0.928696
Epoch 10.48: Loss = 0.899277
Epoch 10.49: Loss = 0.938095
Epoch 10.50: Loss = 0.856537
Epoch 10.51: Loss = 0.808899
Epoch 10.52: Loss = 0.873001
Epoch 10.53: Loss = 0.923477
Epoch 10.54: Loss = 0.973541
Epoch 10.55: Loss = 0.791702
Epoch 10.56: Loss = 0.840271
Epoch 10.57: Loss = 0.912079
Epoch 10.58: Loss = 0.855362
Epoch 10.59: Loss = 0.930237
Epoch 10.60: Loss = 0.860428
Epoch 10.61: Loss = 0.781708
Epoch 10.62: Loss = 0.683945
Epoch 10.63: Loss = 0.853073
Epoch 10.64: Loss = 0.720016
Epoch 10.65: Loss = 0.781052
Epoch 10.66: Loss = 0.789139
Epoch 10.67: Loss = 0.74585
Epoch 10.68: Loss = 0.862717
Epoch 10.69: Loss = 0.688675
Epoch 10.70: Loss = 0.906326
Epoch 10.71: Loss = 0.805038
Epoch 10.72: Loss = 0.869034
Epoch 10.73: Loss = 0.828827
Epoch 10.74: Loss = 0.86348
Epoch 10.75: Loss = 0.682724
Epoch 10.76: Loss = 0.745346
Epoch 10.77: Loss = 0.964233
Epoch 10.78: Loss = 0.807175
Epoch 10.79: Loss = 0.799728
Epoch 10.80: Loss = 0.763794
Epoch 10.81: Loss = 1.02151
Epoch 10.82: Loss = 1.01395
Epoch 10.83: Loss = 0.817093
Epoch 10.84: Loss = 0.865067
Epoch 10.85: Loss = 0.942093
Epoch 10.86: Loss = 0.883392
Epoch 10.87: Loss = 1.07257
Epoch 10.88: Loss = 0.954727
Epoch 10.89: Loss = 1.02681
Epoch 10.90: Loss = 0.931671
Epoch 10.91: Loss = 0.79422
Epoch 10.92: Loss = 0.99115
Epoch 10.93: Loss = 0.834518
Epoch 10.94: Loss = 0.957947
Epoch 10.95: Loss = 0.864075
Epoch 10.96: Loss = 0.947418
Epoch 10.97: Loss = 0.812134
Epoch 10.98: Loss = 0.923294
Epoch 10.99: Loss = 0.852249
Epoch 10.100: Loss = 0.886642
Epoch 10.101: Loss = 0.865295
Epoch 10.102: Loss = 0.948959
Epoch 10.103: Loss = 0.912369
Epoch 10.104: Loss = 0.766312
Epoch 10.105: Loss = 0.796692
Epoch 10.106: Loss = 0.950867
Epoch 10.107: Loss = 0.826416
Epoch 10.108: Loss = 0.770813
Epoch 10.109: Loss = 0.682663
Epoch 10.110: Loss = 0.857666
Epoch 10.111: Loss = 0.865265
Epoch 10.112: Loss = 0.974777
Epoch 10.113: Loss = 0.626404
Epoch 10.114: Loss = 0.941132
Epoch 10.115: Loss = 0.741806
Epoch 10.116: Loss = 1.01741
Epoch 10.117: Loss = 0.998093
Epoch 10.118: Loss = 1.00302
Epoch 10.119: Loss = 0.999237
Epoch 10.120: Loss = 0.828354
TRAIN LOSS = 0.865311
TRAIN ACC = 78.6758 % (47207/60000)
Loss = 0.8134
Loss = 0.933502
Loss = 0.883194
Loss = 0.841415
Loss = 0.782333
Loss = 1.02986
Loss = 1.18498
Loss = 1.03627
Loss = 0.926971
Loss = 0.871582
Loss = 1.06769
Loss = 0.969116
Loss = 0.904251
Loss = 0.880905
Loss = 0.822205
Loss = 0.981781
Loss = 0.824203
Loss = 0.967819
Loss = 0.930954
Loss = 0.796463
TEST LOSS = 0.922445
TEST ACC = 472.069 % (7698/10000)

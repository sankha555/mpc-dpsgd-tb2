Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 600]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 20
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.4017
Epoch 1.2: Loss = 2.36543
Epoch 1.3: Loss = 2.27049
Epoch 1.4: Loss = 2.27336
Epoch 1.5: Loss = 2.1916
Epoch 1.6: Loss = 2.13873
Epoch 1.7: Loss = 2.11174
Epoch 1.8: Loss = 2.05083
Epoch 1.9: Loss = 1.99078
Epoch 1.10: Loss = 1.96268
Epoch 1.11: Loss = 1.92029
Epoch 1.12: Loss = 1.88779
Epoch 1.13: Loss = 1.84247
Epoch 1.14: Loss = 1.80469
Epoch 1.15: Loss = 1.76509
Epoch 1.16: Loss = 1.71852
Epoch 1.17: Loss = 1.71507
Epoch 1.18: Loss = 1.64775
Epoch 1.19: Loss = 1.61552
Epoch 1.20: Loss = 1.62134
Epoch 1.21: Loss = 1.58313
Epoch 1.22: Loss = 1.59085
Epoch 1.23: Loss = 1.50435
Epoch 1.24: Loss = 1.51076
Epoch 1.25: Loss = 1.48772
Epoch 1.26: Loss = 1.48132
Epoch 1.27: Loss = 1.41283
Epoch 1.28: Loss = 1.332
Epoch 1.29: Loss = 1.33163
Epoch 1.30: Loss = 1.41229
Epoch 1.31: Loss = 1.3084
Epoch 1.32: Loss = 1.30432
Epoch 1.33: Loss = 1.28827
Epoch 1.34: Loss = 1.23161
Epoch 1.35: Loss = 1.21149
Epoch 1.36: Loss = 1.28604
Epoch 1.37: Loss = 1.17595
Epoch 1.38: Loss = 1.18944
Epoch 1.39: Loss = 1.13899
Epoch 1.40: Loss = 1.11435
Epoch 1.41: Loss = 1.16066
Epoch 1.42: Loss = 1.16295
Epoch 1.43: Loss = 1.04207
Epoch 1.44: Loss = 1.08676
Epoch 1.45: Loss = 1.04892
Epoch 1.46: Loss = 0.986023
Epoch 1.47: Loss = 0.995163
Epoch 1.48: Loss = 0.994019
Epoch 1.49: Loss = 1.02916
Epoch 1.50: Loss = 1.04626
Epoch 1.51: Loss = 0.970276
Epoch 1.52: Loss = 0.994217
Epoch 1.53: Loss = 0.943588
Epoch 1.54: Loss = 0.937744
Epoch 1.55: Loss = 0.928253
Epoch 1.56: Loss = 0.877258
Epoch 1.57: Loss = 0.818954
Epoch 1.58: Loss = 0.900436
Epoch 1.59: Loss = 0.904526
Epoch 1.60: Loss = 0.817429
Epoch 1.61: Loss = 0.836914
Epoch 1.62: Loss = 0.882813
Epoch 1.63: Loss = 0.8284
Epoch 1.64: Loss = 0.809906
Epoch 1.65: Loss = 0.846024
Epoch 1.66: Loss = 0.729996
Epoch 1.67: Loss = 0.81311
Epoch 1.68: Loss = 0.805908
Epoch 1.69: Loss = 0.723511
Epoch 1.70: Loss = 0.757858
Epoch 1.71: Loss = 0.786606
Epoch 1.72: Loss = 0.747208
Epoch 1.73: Loss = 0.757584
Epoch 1.74: Loss = 0.816925
Epoch 1.75: Loss = 0.73642
Epoch 1.76: Loss = 0.767624
Epoch 1.77: Loss = 0.706207
Epoch 1.78: Loss = 0.7043
Epoch 1.79: Loss = 0.707809
Epoch 1.80: Loss = 0.696487
Epoch 1.81: Loss = 0.692551
Epoch 1.82: Loss = 0.730377
Epoch 1.83: Loss = 0.74707
Epoch 1.84: Loss = 0.683075
Epoch 1.85: Loss = 0.703094
Epoch 1.86: Loss = 0.634552
Epoch 1.87: Loss = 0.687088
Epoch 1.88: Loss = 0.644836
Epoch 1.89: Loss = 0.691071
Epoch 1.90: Loss = 0.585281
Epoch 1.91: Loss = 0.596329
Epoch 1.92: Loss = 0.684326
Epoch 1.93: Loss = 0.639374
Epoch 1.94: Loss = 0.709122
Epoch 1.95: Loss = 0.664169
Epoch 1.96: Loss = 0.677811
Epoch 1.97: Loss = 0.683731
Epoch 1.98: Loss = 0.570724
Epoch 1.99: Loss = 0.598053
Epoch 1.100: Loss = 0.610123
TRAIN LOSS = 1.14531
TRAIN ACC = 69.783 % (41872/60000)
Loss = 0.659943
Loss = 0.6548
Loss = 0.801178
Loss = 0.735519
Loss = 0.6642
Loss = 0.651703
Loss = 0.735214
Loss = 0.701508
Loss = 0.547287
Loss = 0.518372
Loss = 0.434326
Loss = 0.525604
Loss = 0.492233
Loss = 0.485458
Loss = 0.292358
Loss = 0.444916
Loss = 0.764999
TEST LOSS = 0.591277
TEST ACC = 418.719 % (8348/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.687241
Epoch 2.2: Loss = 0.613754
Epoch 2.3: Loss = 0.607468
Epoch 2.4: Loss = 0.566666
Epoch 2.5: Loss = 0.578247
Epoch 2.6: Loss = 0.554443
Epoch 2.7: Loss = 0.625092
Epoch 2.8: Loss = 0.601151
Epoch 2.9: Loss = 0.584457
Epoch 2.10: Loss = 0.562851
Epoch 2.11: Loss = 0.61026
Epoch 2.12: Loss = 0.614792
Epoch 2.13: Loss = 0.580597
Epoch 2.14: Loss = 0.5849
Epoch 2.15: Loss = 0.560699
Epoch 2.16: Loss = 0.573288
Epoch 2.17: Loss = 0.556458
Epoch 2.18: Loss = 0.57399
Epoch 2.19: Loss = 0.560913
Epoch 2.20: Loss = 0.568222
Epoch 2.21: Loss = 0.51442
Epoch 2.22: Loss = 0.541397
Epoch 2.23: Loss = 0.611389
Epoch 2.24: Loss = 0.559906
Epoch 2.25: Loss = 0.622696
Epoch 2.26: Loss = 0.587341
Epoch 2.27: Loss = 0.577606
Epoch 2.28: Loss = 0.597214
Epoch 2.29: Loss = 0.566498
Epoch 2.30: Loss = 0.503799
Epoch 2.31: Loss = 0.559494
Epoch 2.32: Loss = 0.613388
Epoch 2.33: Loss = 0.554443
Epoch 2.34: Loss = 0.554977
Epoch 2.35: Loss = 0.569138
Epoch 2.36: Loss = 0.501923
Epoch 2.37: Loss = 0.542633
Epoch 2.38: Loss = 0.592545
Epoch 2.39: Loss = 0.575912
Epoch 2.40: Loss = 0.570816
Epoch 2.41: Loss = 0.535141
Epoch 2.42: Loss = 0.538696
Epoch 2.43: Loss = 0.538208
Epoch 2.44: Loss = 0.506943
Epoch 2.45: Loss = 0.574966
Epoch 2.46: Loss = 0.48175
Epoch 2.47: Loss = 0.511749
Epoch 2.48: Loss = 0.452255
Epoch 2.49: Loss = 0.51976
Epoch 2.50: Loss = 0.495605
Epoch 2.51: Loss = 0.501129
Epoch 2.52: Loss = 0.546417
Epoch 2.53: Loss = 0.601532
Epoch 2.54: Loss = 0.509323
Epoch 2.55: Loss = 0.515503
Epoch 2.56: Loss = 0.521454
Epoch 2.57: Loss = 0.516205
Epoch 2.58: Loss = 0.501556
Epoch 2.59: Loss = 0.573776
Epoch 2.60: Loss = 0.505295
Epoch 2.61: Loss = 0.449585
Epoch 2.62: Loss = 0.534973
Epoch 2.63: Loss = 0.473694
Epoch 2.64: Loss = 0.522247
Epoch 2.65: Loss = 0.508194
Epoch 2.66: Loss = 0.474869
Epoch 2.67: Loss = 0.471878
Epoch 2.68: Loss = 0.482574
Epoch 2.69: Loss = 0.495087
Epoch 2.70: Loss = 0.527496
Epoch 2.71: Loss = 0.540833
Epoch 2.72: Loss = 0.521896
Epoch 2.73: Loss = 0.5177
Epoch 2.74: Loss = 0.463287
Epoch 2.75: Loss = 0.428741
Epoch 2.76: Loss = 0.502319
Epoch 2.77: Loss = 0.481644
Epoch 2.78: Loss = 0.494492
Epoch 2.79: Loss = 0.515991
Epoch 2.80: Loss = 0.512741
Epoch 2.81: Loss = 0.505905
Epoch 2.82: Loss = 0.616745
Epoch 2.83: Loss = 0.498764
Epoch 2.84: Loss = 0.463348
Epoch 2.85: Loss = 0.523926
Epoch 2.86: Loss = 0.452698
Epoch 2.87: Loss = 0.449295
Epoch 2.88: Loss = 0.484161
Epoch 2.89: Loss = 0.526443
Epoch 2.90: Loss = 0.430847
Epoch 2.91: Loss = 0.524292
Epoch 2.92: Loss = 0.458588
Epoch 2.93: Loss = 0.425827
Epoch 2.94: Loss = 0.507278
Epoch 2.95: Loss = 0.546768
Epoch 2.96: Loss = 0.497543
Epoch 2.97: Loss = 0.474869
Epoch 2.98: Loss = 0.562729
Epoch 2.99: Loss = 0.489441
Epoch 2.100: Loss = 0.453827
TRAIN LOSS = 0.533432
TRAIN ACC = 84.4757 % (50688/60000)
Loss = 0.519714
Loss = 0.555954
Loss = 0.670532
Loss = 0.63559
Loss = 0.520691
Loss = 0.499939
Loss = 0.630325
Loss = 0.574341
Loss = 0.410477
Loss = 0.354858
Loss = 0.365707
Loss = 0.380478
Loss = 0.325729
Loss = 0.387634
Loss = 0.163559
Loss = 0.305389
Loss = 0.690659
TEST LOSS = 0.465681
TEST ACC = 506.879 % (8603/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.459518
Epoch 3.2: Loss = 0.485794
Epoch 3.3: Loss = 0.548035
Epoch 3.4: Loss = 0.468735
Epoch 3.5: Loss = 0.477921
Epoch 3.6: Loss = 0.545609
Epoch 3.7: Loss = 0.463745
Epoch 3.8: Loss = 0.463989
Epoch 3.9: Loss = 0.457275
Epoch 3.10: Loss = 0.532364
Epoch 3.11: Loss = 0.529236
Epoch 3.12: Loss = 0.478333
Epoch 3.13: Loss = 0.496872
Epoch 3.14: Loss = 0.525482
Epoch 3.15: Loss = 0.534149
Epoch 3.16: Loss = 0.515717
Epoch 3.17: Loss = 0.5065
Epoch 3.18: Loss = 0.512878
Epoch 3.19: Loss = 0.536697
Epoch 3.20: Loss = 0.398972
Epoch 3.21: Loss = 0.458557
Epoch 3.22: Loss = 0.462173
Epoch 3.23: Loss = 0.359177
Epoch 3.24: Loss = 0.41095
Epoch 3.25: Loss = 0.448151
Epoch 3.26: Loss = 0.53862
Epoch 3.27: Loss = 0.442459
Epoch 3.28: Loss = 0.471954
Epoch 3.29: Loss = 0.486099
Epoch 3.30: Loss = 0.451004
Epoch 3.31: Loss = 0.437881
Epoch 3.32: Loss = 0.468674
Epoch 3.33: Loss = 0.438889
Epoch 3.34: Loss = 0.477676
Epoch 3.35: Loss = 0.454865
Epoch 3.36: Loss = 0.436005
Epoch 3.37: Loss = 0.443253
Epoch 3.38: Loss = 0.492218
Epoch 3.39: Loss = 0.492065
Epoch 3.40: Loss = 0.495941
Epoch 3.41: Loss = 0.386871
Epoch 3.42: Loss = 0.460815
Epoch 3.43: Loss = 0.460907
Epoch 3.44: Loss = 0.479401
Epoch 3.45: Loss = 0.458618
Epoch 3.46: Loss = 0.501038
Epoch 3.47: Loss = 0.469818
Epoch 3.48: Loss = 0.444565
Epoch 3.49: Loss = 0.472885
Epoch 3.50: Loss = 0.489792
Epoch 3.51: Loss = 0.448013
Epoch 3.52: Loss = 0.472458
Epoch 3.53: Loss = 0.437653
Epoch 3.54: Loss = 0.44101
Epoch 3.55: Loss = 0.436813
Epoch 3.56: Loss = 0.44902
Epoch 3.57: Loss = 0.519669
Epoch 3.58: Loss = 0.492477
Epoch 3.59: Loss = 0.407455
Epoch 3.60: Loss = 0.436966
Epoch 3.61: Loss = 0.434113
Epoch 3.62: Loss = 0.410889
Epoch 3.63: Loss = 0.45343
Epoch 3.64: Loss = 0.439178
Epoch 3.65: Loss = 0.410706
Epoch 3.66: Loss = 0.450943
Epoch 3.67: Loss = 0.473969
Epoch 3.68: Loss = 0.438675
Epoch 3.69: Loss = 0.433945
Epoch 3.70: Loss = 0.394211
Epoch 3.71: Loss = 0.382416
Epoch 3.72: Loss = 0.431122
Epoch 3.73: Loss = 0.387589
Epoch 3.74: Loss = 0.423752
Epoch 3.75: Loss = 0.416855
Epoch 3.76: Loss = 0.4104
Epoch 3.77: Loss = 0.432465
Epoch 3.78: Loss = 0.488647
Epoch 3.79: Loss = 0.415039
Epoch 3.80: Loss = 0.408951
Epoch 3.81: Loss = 0.457199
Epoch 3.82: Loss = 0.3797
Epoch 3.83: Loss = 0.422638
Epoch 3.84: Loss = 0.454559
Epoch 3.85: Loss = 0.409149
Epoch 3.86: Loss = 0.424789
Epoch 3.87: Loss = 0.471024
Epoch 3.88: Loss = 0.497574
Epoch 3.89: Loss = 0.501999
Epoch 3.90: Loss = 0.519211
Epoch 3.91: Loss = 0.489243
Epoch 3.92: Loss = 0.448685
Epoch 3.93: Loss = 0.393143
Epoch 3.94: Loss = 0.479614
Epoch 3.95: Loss = 0.454453
Epoch 3.96: Loss = 0.418594
Epoch 3.97: Loss = 0.497894
Epoch 3.98: Loss = 0.486557
Epoch 3.99: Loss = 0.465134
Epoch 3.100: Loss = 0.450607
TRAIN LOSS = 0.45929
TRAIN ACC = 86.0153 % (51611/60000)
Loss = 0.454651
Loss = 0.502853
Loss = 0.637238
Loss = 0.619995
Loss = 0.451492
Loss = 0.458496
Loss = 0.600372
Loss = 0.53302
Loss = 0.369049
Loss = 0.335129
Loss = 0.317444
Loss = 0.327301
Loss = 0.255356
Loss = 0.321335
Loss = 0.121765
Loss = 0.26886
Loss = 0.641052
TEST LOSS = 0.420103
TEST ACC = 516.109 % (8716/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.414261
Epoch 4.2: Loss = 0.379166
Epoch 4.3: Loss = 0.430038
Epoch 4.4: Loss = 0.505142
Epoch 4.5: Loss = 0.42308
Epoch 4.6: Loss = 0.439651
Epoch 4.7: Loss = 0.422424
Epoch 4.8: Loss = 0.427383
Epoch 4.9: Loss = 0.429138
Epoch 4.10: Loss = 0.484955
Epoch 4.11: Loss = 0.488586
Epoch 4.12: Loss = 0.428406
Epoch 4.13: Loss = 0.425217
Epoch 4.14: Loss = 0.422363
Epoch 4.15: Loss = 0.469467
Epoch 4.16: Loss = 0.480057
Epoch 4.17: Loss = 0.473282
Epoch 4.18: Loss = 0.509109
Epoch 4.19: Loss = 0.537125
Epoch 4.20: Loss = 0.443481
Epoch 4.21: Loss = 0.434998
Epoch 4.22: Loss = 0.433746
Epoch 4.23: Loss = 0.400024
Epoch 4.24: Loss = 0.489517
Epoch 4.25: Loss = 0.481567
Epoch 4.26: Loss = 0.390213
Epoch 4.27: Loss = 0.414352
Epoch 4.28: Loss = 0.461639
Epoch 4.29: Loss = 0.449661
Epoch 4.30: Loss = 0.441299
Epoch 4.31: Loss = 0.365906
Epoch 4.32: Loss = 0.410126
Epoch 4.33: Loss = 0.437485
Epoch 4.34: Loss = 0.384888
Epoch 4.35: Loss = 0.415237
Epoch 4.36: Loss = 0.45578
Epoch 4.37: Loss = 0.412506
Epoch 4.38: Loss = 0.403824
Epoch 4.39: Loss = 0.388733
Epoch 4.40: Loss = 0.403381
Epoch 4.41: Loss = 0.366898
Epoch 4.42: Loss = 0.426132
Epoch 4.43: Loss = 0.460953
Epoch 4.44: Loss = 0.463394
Epoch 4.45: Loss = 0.412125
Epoch 4.46: Loss = 0.413254
Epoch 4.47: Loss = 0.468552
Epoch 4.48: Loss = 0.350021
Epoch 4.49: Loss = 0.452026
Epoch 4.50: Loss = 0.44899
Epoch 4.51: Loss = 0.391815
Epoch 4.52: Loss = 0.377045
Epoch 4.53: Loss = 0.370163
Epoch 4.54: Loss = 0.390152
Epoch 4.55: Loss = 0.468552
Epoch 4.56: Loss = 0.441742
Epoch 4.57: Loss = 0.442932
Epoch 4.58: Loss = 0.382553
Epoch 4.59: Loss = 0.493073
Epoch 4.60: Loss = 0.446381
Epoch 4.61: Loss = 0.422318
Epoch 4.62: Loss = 0.483856
Epoch 4.63: Loss = 0.476486
Epoch 4.64: Loss = 0.530304
Epoch 4.65: Loss = 0.459808
Epoch 4.66: Loss = 0.43277
Epoch 4.67: Loss = 0.358215
Epoch 4.68: Loss = 0.360458
Epoch 4.69: Loss = 0.45105
Epoch 4.70: Loss = 0.45816
Epoch 4.71: Loss = 0.457565
Epoch 4.72: Loss = 0.473434
Epoch 4.73: Loss = 0.453232
Epoch 4.74: Loss = 0.394592
Epoch 4.75: Loss = 0.42276
Epoch 4.76: Loss = 0.461365
Epoch 4.77: Loss = 0.427292
Epoch 4.78: Loss = 0.440903
Epoch 4.79: Loss = 0.391983
Epoch 4.80: Loss = 0.420181
Epoch 4.81: Loss = 0.371841
Epoch 4.82: Loss = 0.468369
Epoch 4.83: Loss = 0.427612
Epoch 4.84: Loss = 0.460068
Epoch 4.85: Loss = 0.462418
Epoch 4.86: Loss = 0.378326
Epoch 4.87: Loss = 0.476425
Epoch 4.88: Loss = 0.387085
Epoch 4.89: Loss = 0.411469
Epoch 4.90: Loss = 0.433716
Epoch 4.91: Loss = 0.425568
Epoch 4.92: Loss = 0.453751
Epoch 4.93: Loss = 0.441223
Epoch 4.94: Loss = 0.396118
Epoch 4.95: Loss = 0.415298
Epoch 4.96: Loss = 0.434402
Epoch 4.97: Loss = 0.452927
Epoch 4.98: Loss = 0.439072
Epoch 4.99: Loss = 0.376465
Epoch 4.100: Loss = 0.446808
TRAIN LOSS = 0.433105
TRAIN ACC = 86.9675 % (52183/60000)
Loss = 0.428436
Loss = 0.481918
Loss = 0.610626
Loss = 0.59845
Loss = 0.435181
Loss = 0.410583
Loss = 0.604446
Loss = 0.509537
Loss = 0.350464
Loss = 0.293686
Loss = 0.317703
Loss = 0.312515
Loss = 0.227829
Loss = 0.31955
Loss = 0.101135
Loss = 0.246902
Loss = 0.686554
TEST LOSS = 0.4024
TEST ACC = 521.829 % (8800/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.438065
Epoch 5.2: Loss = 0.341766
Epoch 5.3: Loss = 0.397324
Epoch 5.4: Loss = 0.365677
Epoch 5.5: Loss = 0.411484
Epoch 5.6: Loss = 0.400314
Epoch 5.7: Loss = 0.350128
Epoch 5.8: Loss = 0.368988
Epoch 5.9: Loss = 0.35347
Epoch 5.10: Loss = 0.484421
Epoch 5.11: Loss = 0.323639
Epoch 5.12: Loss = 0.367981
Epoch 5.13: Loss = 0.439377
Epoch 5.14: Loss = 0.422104
Epoch 5.15: Loss = 0.437698
Epoch 5.16: Loss = 0.390137
Epoch 5.17: Loss = 0.41304
Epoch 5.18: Loss = 0.403259
Epoch 5.19: Loss = 0.425446
Epoch 5.20: Loss = 0.406998
Epoch 5.21: Loss = 0.400223
Epoch 5.22: Loss = 0.41629
Epoch 5.23: Loss = 0.444229
Epoch 5.24: Loss = 0.458069
Epoch 5.25: Loss = 0.440186
Epoch 5.26: Loss = 0.384308
Epoch 5.27: Loss = 0.428162
Epoch 5.28: Loss = 0.447998
Epoch 5.29: Loss = 0.413681
Epoch 5.30: Loss = 0.41922
Epoch 5.31: Loss = 0.376358
Epoch 5.32: Loss = 0.328354
Epoch 5.33: Loss = 0.433212
Epoch 5.34: Loss = 0.449936
Epoch 5.35: Loss = 0.432632
Epoch 5.36: Loss = 0.525269
Epoch 5.37: Loss = 0.421936
Epoch 5.38: Loss = 0.41185
Epoch 5.39: Loss = 0.441696
Epoch 5.40: Loss = 0.499634
Epoch 5.41: Loss = 0.389893
Epoch 5.42: Loss = 0.358536
Epoch 5.43: Loss = 0.446716
Epoch 5.44: Loss = 0.408157
Epoch 5.45: Loss = 0.362198
Epoch 5.46: Loss = 0.4487
Epoch 5.47: Loss = 0.445282
Epoch 5.48: Loss = 0.323593
Epoch 5.49: Loss = 0.397736
Epoch 5.50: Loss = 0.499481
Epoch 5.51: Loss = 0.434311
Epoch 5.52: Loss = 0.419708
Epoch 5.53: Loss = 0.399796
Epoch 5.54: Loss = 0.353577
Epoch 5.55: Loss = 0.446121
Epoch 5.56: Loss = 0.39772
Epoch 5.57: Loss = 0.348907
Epoch 5.58: Loss = 0.413116
Epoch 5.59: Loss = 0.549423
Epoch 5.60: Loss = 0.361572
Epoch 5.61: Loss = 0.399765
Epoch 5.62: Loss = 0.509079
Epoch 5.63: Loss = 0.417191
Epoch 5.64: Loss = 0.552765
Epoch 5.65: Loss = 0.44313
Epoch 5.66: Loss = 0.416336
Epoch 5.67: Loss = 0.407303
Epoch 5.68: Loss = 0.422287
Epoch 5.69: Loss = 0.470444
Epoch 5.70: Loss = 0.473572
Epoch 5.71: Loss = 0.41507
Epoch 5.72: Loss = 0.46048
Epoch 5.73: Loss = 0.445648
Epoch 5.74: Loss = 0.374252
Epoch 5.75: Loss = 0.403839
Epoch 5.76: Loss = 0.418457
Epoch 5.77: Loss = 0.442764
Epoch 5.78: Loss = 0.521835
Epoch 5.79: Loss = 0.428101
Epoch 5.80: Loss = 0.386871
Epoch 5.81: Loss = 0.437988
Epoch 5.82: Loss = 0.398865
Epoch 5.83: Loss = 0.412643
Epoch 5.84: Loss = 0.40892
Epoch 5.85: Loss = 0.430725
Epoch 5.86: Loss = 0.376587
Epoch 5.87: Loss = 0.438126
Epoch 5.88: Loss = 0.367523
Epoch 5.89: Loss = 0.389053
Epoch 5.90: Loss = 0.472702
Epoch 5.91: Loss = 0.435547
Epoch 5.92: Loss = 0.338364
Epoch 5.93: Loss = 0.404053
Epoch 5.94: Loss = 0.388672
Epoch 5.95: Loss = 0.379562
Epoch 5.96: Loss = 0.474182
Epoch 5.97: Loss = 0.366638
Epoch 5.98: Loss = 0.420471
Epoch 5.99: Loss = 0.425522
Epoch 5.100: Loss = 0.419601
TRAIN LOSS = 0.417145
TRAIN ACC = 87.6648 % (52601/60000)
Loss = 0.427933
Loss = 0.481094
Loss = 0.621887
Loss = 0.599594
Loss = 0.418625
Loss = 0.397751
Loss = 0.587555
Loss = 0.492889
Loss = 0.34407
Loss = 0.30101
Loss = 0.329224
Loss = 0.304398
Loss = 0.222092
Loss = 0.332306
Loss = 0.0837097
Loss = 0.251633
Loss = 0.663193
TEST LOSS = 0.398274
TEST ACC = 526.009 % (8815/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.39653
Epoch 6.2: Loss = 0.379639
Epoch 6.3: Loss = 0.378235
Epoch 6.4: Loss = 0.4142
Epoch 6.5: Loss = 0.410431
Epoch 6.6: Loss = 0.388702
Epoch 6.7: Loss = 0.380035
Epoch 6.8: Loss = 0.411331
Epoch 6.9: Loss = 0.421494
Epoch 6.10: Loss = 0.38063
Epoch 6.11: Loss = 0.414597
Epoch 6.12: Loss = 0.443939
Epoch 6.13: Loss = 0.387985
Epoch 6.14: Loss = 0.404236
Epoch 6.15: Loss = 0.385132
Epoch 6.16: Loss = 0.409744
Epoch 6.17: Loss = 0.42186
Epoch 6.18: Loss = 0.521423
Epoch 6.19: Loss = 0.439301
Epoch 6.20: Loss = 0.386353
Epoch 6.21: Loss = 0.371475
Epoch 6.22: Loss = 0.471283
Epoch 6.23: Loss = 0.403229
Epoch 6.24: Loss = 0.381821
Epoch 6.25: Loss = 0.351715
Epoch 6.26: Loss = 0.396805
Epoch 6.27: Loss = 0.542786
Epoch 6.28: Loss = 0.466156
Epoch 6.29: Loss = 0.396713
Epoch 6.30: Loss = 0.373108
Epoch 6.31: Loss = 0.422424
Epoch 6.32: Loss = 0.39859
Epoch 6.33: Loss = 0.350937
Epoch 6.34: Loss = 0.359024
Epoch 6.35: Loss = 0.464401
Epoch 6.36: Loss = 0.402115
Epoch 6.37: Loss = 0.403412
Epoch 6.38: Loss = 0.375595
Epoch 6.39: Loss = 0.470596
Epoch 6.40: Loss = 0.388824
Epoch 6.41: Loss = 0.368439
Epoch 6.42: Loss = 0.416824
Epoch 6.43: Loss = 0.426529
Epoch 6.44: Loss = 0.465302
Epoch 6.45: Loss = 0.439102
Epoch 6.46: Loss = 0.511826
Epoch 6.47: Loss = 0.370087
Epoch 6.48: Loss = 0.405685
Epoch 6.49: Loss = 0.401947
Epoch 6.50: Loss = 0.434189
Epoch 6.51: Loss = 0.33432
Epoch 6.52: Loss = 0.485306
Epoch 6.53: Loss = 0.387726
Epoch 6.54: Loss = 0.457535
Epoch 6.55: Loss = 0.446304
Epoch 6.56: Loss = 0.437408
Epoch 6.57: Loss = 0.42778
Epoch 6.58: Loss = 0.422485
Epoch 6.59: Loss = 0.488281
Epoch 6.60: Loss = 0.355301
Epoch 6.61: Loss = 0.428101
Epoch 6.62: Loss = 0.3638
Epoch 6.63: Loss = 0.49588
Epoch 6.64: Loss = 0.413208
Epoch 6.65: Loss = 0.39975
Epoch 6.66: Loss = 0.467911
Epoch 6.67: Loss = 0.345261
Epoch 6.68: Loss = 0.413025
Epoch 6.69: Loss = 0.413315
Epoch 6.70: Loss = 0.3936
Epoch 6.71: Loss = 0.366272
Epoch 6.72: Loss = 0.347214
Epoch 6.73: Loss = 0.409683
Epoch 6.74: Loss = 0.450958
Epoch 6.75: Loss = 0.432693
Epoch 6.76: Loss = 0.438095
Epoch 6.77: Loss = 0.446396
Epoch 6.78: Loss = 0.467499
Epoch 6.79: Loss = 0.407227
Epoch 6.80: Loss = 0.417877
Epoch 6.81: Loss = 0.466171
Epoch 6.82: Loss = 0.465393
Epoch 6.83: Loss = 0.416199
Epoch 6.84: Loss = 0.394455
Epoch 6.85: Loss = 0.453552
Epoch 6.86: Loss = 0.518723
Epoch 6.87: Loss = 0.410843
Epoch 6.88: Loss = 0.423248
Epoch 6.89: Loss = 0.370728
Epoch 6.90: Loss = 0.357422
Epoch 6.91: Loss = 0.457077
Epoch 6.92: Loss = 0.395126
Epoch 6.93: Loss = 0.391357
Epoch 6.94: Loss = 0.359283
Epoch 6.95: Loss = 0.463898
Epoch 6.96: Loss = 0.472412
Epoch 6.97: Loss = 0.393936
Epoch 6.98: Loss = 0.504562
Epoch 6.99: Loss = 0.437729
Epoch 6.100: Loss = 0.395218
TRAIN LOSS = 0.417145
TRAIN ACC = 87.8036 % (52684/60000)
Loss = 0.428833
Loss = 0.49704
Loss = 0.637024
Loss = 0.610779
Loss = 0.414368
Loss = 0.402878
Loss = 0.623383
Loss = 0.517349
Loss = 0.347488
Loss = 0.303452
Loss = 0.341751
Loss = 0.293518
Loss = 0.214844
Loss = 0.336487
Loss = 0.0832214
Loss = 0.238846
Loss = 0.673889
TEST LOSS = 0.404431
TEST ACC = 526.839 % (8856/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.517578
Epoch 7.2: Loss = 0.437958
Epoch 7.3: Loss = 0.388397
Epoch 7.4: Loss = 0.385437
Epoch 7.5: Loss = 0.37825
Epoch 7.6: Loss = 0.40593
Epoch 7.7: Loss = 0.420319
Epoch 7.8: Loss = 0.397171
Epoch 7.9: Loss = 0.400238
Epoch 7.10: Loss = 0.338058
Epoch 7.11: Loss = 0.381439
Epoch 7.12: Loss = 0.414215
Epoch 7.13: Loss = 0.457184
Epoch 7.14: Loss = 0.417801
Epoch 7.15: Loss = 0.432327
Epoch 7.16: Loss = 0.381317
Epoch 7.17: Loss = 0.353546
Epoch 7.18: Loss = 0.467636
Epoch 7.19: Loss = 0.354691
Epoch 7.20: Loss = 0.353043
Epoch 7.21: Loss = 0.392471
Epoch 7.22: Loss = 0.414368
Epoch 7.23: Loss = 0.335938
Epoch 7.24: Loss = 0.383362
Epoch 7.25: Loss = 0.397217
Epoch 7.26: Loss = 0.412994
Epoch 7.27: Loss = 0.542053
Epoch 7.28: Loss = 0.394806
Epoch 7.29: Loss = 0.440186
Epoch 7.30: Loss = 0.391708
Epoch 7.31: Loss = 0.391647
Epoch 7.32: Loss = 0.437408
Epoch 7.33: Loss = 0.398102
Epoch 7.34: Loss = 0.48053
Epoch 7.35: Loss = 0.403061
Epoch 7.36: Loss = 0.535355
Epoch 7.37: Loss = 0.45874
Epoch 7.38: Loss = 0.390427
Epoch 7.39: Loss = 0.422012
Epoch 7.40: Loss = 0.393158
Epoch 7.41: Loss = 0.396088
Epoch 7.42: Loss = 0.342422
Epoch 7.43: Loss = 0.453232
Epoch 7.44: Loss = 0.388504
Epoch 7.45: Loss = 0.408859
Epoch 7.46: Loss = 0.391235
Epoch 7.47: Loss = 0.469254
Epoch 7.48: Loss = 0.415848
Epoch 7.49: Loss = 0.387405
Epoch 7.50: Loss = 0.375259
Epoch 7.51: Loss = 0.404419
Epoch 7.52: Loss = 0.370789
Epoch 7.53: Loss = 0.395447
Epoch 7.54: Loss = 0.38324
Epoch 7.55: Loss = 0.392899
Epoch 7.56: Loss = 0.461929
Epoch 7.57: Loss = 0.499329
Epoch 7.58: Loss = 0.359848
Epoch 7.59: Loss = 0.413589
Epoch 7.60: Loss = 0.394852
Epoch 7.61: Loss = 0.394806
Epoch 7.62: Loss = 0.483902
Epoch 7.63: Loss = 0.437119
Epoch 7.64: Loss = 0.44603
Epoch 7.65: Loss = 0.442307
Epoch 7.66: Loss = 0.409607
Epoch 7.67: Loss = 0.337402
Epoch 7.68: Loss = 0.449753
Epoch 7.69: Loss = 0.364914
Epoch 7.70: Loss = 0.448654
Epoch 7.71: Loss = 0.412674
Epoch 7.72: Loss = 0.438187
Epoch 7.73: Loss = 0.467621
Epoch 7.74: Loss = 0.440582
Epoch 7.75: Loss = 0.428452
Epoch 7.76: Loss = 0.491074
Epoch 7.77: Loss = 0.29335
Epoch 7.78: Loss = 0.436996
Epoch 7.79: Loss = 0.344345
Epoch 7.80: Loss = 0.34642
Epoch 7.81: Loss = 0.387039
Epoch 7.82: Loss = 0.342346
Epoch 7.83: Loss = 0.389603
Epoch 7.84: Loss = 0.429916
Epoch 7.85: Loss = 0.470032
Epoch 7.86: Loss = 0.462646
Epoch 7.87: Loss = 0.442322
Epoch 7.88: Loss = 0.411713
Epoch 7.89: Loss = 0.426392
Epoch 7.90: Loss = 0.336517
Epoch 7.91: Loss = 0.386292
Epoch 7.92: Loss = 0.409668
Epoch 7.93: Loss = 0.426956
Epoch 7.94: Loss = 0.414276
Epoch 7.95: Loss = 0.400284
Epoch 7.96: Loss = 0.431152
Epoch 7.97: Loss = 0.443314
Epoch 7.98: Loss = 0.490097
Epoch 7.99: Loss = 0.392792
Epoch 7.100: Loss = 0.367554
TRAIN LOSS = 0.411758
TRAIN ACC = 88.205 % (52926/60000)
Loss = 0.423447
Loss = 0.488525
Loss = 0.619324
Loss = 0.598907
Loss = 0.400192
Loss = 0.400269
Loss = 0.615494
Loss = 0.509903
Loss = 0.332092
Loss = 0.299103
Loss = 0.36142
Loss = 0.289413
Loss = 0.200165
Loss = 0.351837
Loss = 0.0853577
Loss = 0.218903
Loss = 0.652603
TEST LOSS = 0.397765
TEST ACC = 529.259 % (8858/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.399094
Epoch 8.2: Loss = 0.39444
Epoch 8.3: Loss = 0.345856
Epoch 8.4: Loss = 0.372803
Epoch 8.5: Loss = 0.433044
Epoch 8.6: Loss = 0.354279
Epoch 8.7: Loss = 0.472733
Epoch 8.8: Loss = 0.417572
Epoch 8.9: Loss = 0.401337
Epoch 8.10: Loss = 0.350006
Epoch 8.11: Loss = 0.450943
Epoch 8.12: Loss = 0.405411
Epoch 8.13: Loss = 0.40126
Epoch 8.14: Loss = 0.471619
Epoch 8.15: Loss = 0.331848
Epoch 8.16: Loss = 0.411133
Epoch 8.17: Loss = 0.42598
Epoch 8.18: Loss = 0.290268
Epoch 8.19: Loss = 0.358139
Epoch 8.20: Loss = 0.452621
Epoch 8.21: Loss = 0.338638
Epoch 8.22: Loss = 0.388077
Epoch 8.23: Loss = 0.39061
Epoch 8.24: Loss = 0.359558
Epoch 8.25: Loss = 0.533401
Epoch 8.26: Loss = 0.460403
Epoch 8.27: Loss = 0.401917
Epoch 8.28: Loss = 0.63266
Epoch 8.29: Loss = 0.454697
Epoch 8.30: Loss = 0.388885
Epoch 8.31: Loss = 0.516266
Epoch 8.32: Loss = 0.434616
Epoch 8.33: Loss = 0.369797
Epoch 8.34: Loss = 0.37262
Epoch 8.35: Loss = 0.456421
Epoch 8.36: Loss = 0.472321
Epoch 8.37: Loss = 0.359879
Epoch 8.38: Loss = 0.444839
Epoch 8.39: Loss = 0.383591
Epoch 8.40: Loss = 0.363846
Epoch 8.41: Loss = 0.47908
Epoch 8.42: Loss = 0.415573
Epoch 8.43: Loss = 0.399475
Epoch 8.44: Loss = 0.487564
Epoch 8.45: Loss = 0.40831
Epoch 8.46: Loss = 0.395706
Epoch 8.47: Loss = 0.369049
Epoch 8.48: Loss = 0.359772
Epoch 8.49: Loss = 0.478806
Epoch 8.50: Loss = 0.395355
Epoch 8.51: Loss = 0.46843
Epoch 8.52: Loss = 0.450974
Epoch 8.53: Loss = 0.361343
Epoch 8.54: Loss = 0.406952
Epoch 8.55: Loss = 0.407333
Epoch 8.56: Loss = 0.41246
Epoch 8.57: Loss = 0.475525
Epoch 8.58: Loss = 0.517822
Epoch 8.59: Loss = 0.367401
Epoch 8.60: Loss = 0.420502
Epoch 8.61: Loss = 0.442947
Epoch 8.62: Loss = 0.420242
Epoch 8.63: Loss = 0.413986
Epoch 8.64: Loss = 0.346069
Epoch 8.65: Loss = 0.417694
Epoch 8.66: Loss = 0.378113
Epoch 8.67: Loss = 0.346313
Epoch 8.68: Loss = 0.379242
Epoch 8.69: Loss = 0.396469
Epoch 8.70: Loss = 0.409027
Epoch 8.71: Loss = 0.410629
Epoch 8.72: Loss = 0.363281
Epoch 8.73: Loss = 0.464951
Epoch 8.74: Loss = 0.44841
Epoch 8.75: Loss = 0.47525
Epoch 8.76: Loss = 0.348618
Epoch 8.77: Loss = 0.429214
Epoch 8.78: Loss = 0.457611
Epoch 8.79: Loss = 0.442444
Epoch 8.80: Loss = 0.380005
Epoch 8.81: Loss = 0.530884
Epoch 8.82: Loss = 0.381897
Epoch 8.83: Loss = 0.405624
Epoch 8.84: Loss = 0.405579
Epoch 8.85: Loss = 0.38353
Epoch 8.86: Loss = 0.417374
Epoch 8.87: Loss = 0.381836
Epoch 8.88: Loss = 0.432327
Epoch 8.89: Loss = 0.447296
Epoch 8.90: Loss = 0.362152
Epoch 8.91: Loss = 0.41803
Epoch 8.92: Loss = 0.344971
Epoch 8.93: Loss = 0.378464
Epoch 8.94: Loss = 0.356506
Epoch 8.95: Loss = 0.387283
Epoch 8.96: Loss = 0.383972
Epoch 8.97: Loss = 0.357269
Epoch 8.98: Loss = 0.442993
Epoch 8.99: Loss = 0.375
Epoch 8.100: Loss = 0.311432
TRAIN LOSS = 0.410156
TRAIN ACC = 88.3667 % (53022/60000)
Loss = 0.400604
Loss = 0.46936
Loss = 0.604004
Loss = 0.572342
Loss = 0.388641
Loss = 0.386566
Loss = 0.609619
Loss = 0.504639
Loss = 0.33667
Loss = 0.298035
Loss = 0.375488
Loss = 0.293793
Loss = 0.203033
Loss = 0.347641
Loss = 0.095108
Loss = 0.216934
Loss = 0.654572
TEST LOSS = 0.392331
TEST ACC = 530.219 % (8885/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.455612
Epoch 9.2: Loss = 0.406113
Epoch 9.3: Loss = 0.41893
Epoch 9.4: Loss = 0.402115
Epoch 9.5: Loss = 0.506073
Epoch 9.6: Loss = 0.346603
Epoch 9.7: Loss = 0.464264
Epoch 9.8: Loss = 0.458771
Epoch 9.9: Loss = 0.393707
Epoch 9.10: Loss = 0.463074
Epoch 9.11: Loss = 0.391022
Epoch 9.12: Loss = 0.392517
Epoch 9.13: Loss = 0.454712
Epoch 9.14: Loss = 0.417175
Epoch 9.15: Loss = 0.440048
Epoch 9.16: Loss = 0.345337
Epoch 9.17: Loss = 0.380127
Epoch 9.18: Loss = 0.368469
Epoch 9.19: Loss = 0.341873
Epoch 9.20: Loss = 0.404816
Epoch 9.21: Loss = 0.342957
Epoch 9.22: Loss = 0.348358
Epoch 9.23: Loss = 0.297821
Epoch 9.24: Loss = 0.440506
Epoch 9.25: Loss = 0.376862
Epoch 9.26: Loss = 0.357864
Epoch 9.27: Loss = 0.502167
Epoch 9.28: Loss = 0.432175
Epoch 9.29: Loss = 0.408157
Epoch 9.30: Loss = 0.390259
Epoch 9.31: Loss = 0.380127
Epoch 9.32: Loss = 0.417999
Epoch 9.33: Loss = 0.416672
Epoch 9.34: Loss = 0.457657
Epoch 9.35: Loss = 0.431152
Epoch 9.36: Loss = 0.434097
Epoch 9.37: Loss = 0.380722
Epoch 9.38: Loss = 0.393829
Epoch 9.39: Loss = 0.428619
Epoch 9.40: Loss = 0.385498
Epoch 9.41: Loss = 0.448532
Epoch 9.42: Loss = 0.285172
Epoch 9.43: Loss = 0.472351
Epoch 9.44: Loss = 0.374008
Epoch 9.45: Loss = 0.415756
Epoch 9.46: Loss = 0.391891
Epoch 9.47: Loss = 0.454163
Epoch 9.48: Loss = 0.48645
Epoch 9.49: Loss = 0.353226
Epoch 9.50: Loss = 0.38826
Epoch 9.51: Loss = 0.386276
Epoch 9.52: Loss = 0.347916
Epoch 9.53: Loss = 0.40892
Epoch 9.54: Loss = 0.537872
Epoch 9.55: Loss = 0.384338
Epoch 9.56: Loss = 0.394241
Epoch 9.57: Loss = 0.368317
Epoch 9.58: Loss = 0.483383
Epoch 9.59: Loss = 0.373215
Epoch 9.60: Loss = 0.388748
Epoch 9.61: Loss = 0.473328
Epoch 9.62: Loss = 0.40329
Epoch 9.63: Loss = 0.454605
Epoch 9.64: Loss = 0.359955
Epoch 9.65: Loss = 0.4263
Epoch 9.66: Loss = 0.448303
Epoch 9.67: Loss = 0.494476
Epoch 9.68: Loss = 0.395721
Epoch 9.69: Loss = 0.434692
Epoch 9.70: Loss = 0.43541
Epoch 9.71: Loss = 0.368332
Epoch 9.72: Loss = 0.37355
Epoch 9.73: Loss = 0.40538
Epoch 9.74: Loss = 0.345474
Epoch 9.75: Loss = 0.398422
Epoch 9.76: Loss = 0.432678
Epoch 9.77: Loss = 0.404312
Epoch 9.78: Loss = 0.454041
Epoch 9.79: Loss = 0.378784
Epoch 9.80: Loss = 0.419937
Epoch 9.81: Loss = 0.464127
Epoch 9.82: Loss = 0.466339
Epoch 9.83: Loss = 0.401657
Epoch 9.84: Loss = 0.47287
Epoch 9.85: Loss = 0.393799
Epoch 9.86: Loss = 0.334702
Epoch 9.87: Loss = 0.425446
terminate called after throwing an instance of 'std::runtime_error'
  what():  client 0 already connected

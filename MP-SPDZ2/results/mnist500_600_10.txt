Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 10
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 2.5
***********************************************************
Epoch 1.1: Loss = 2.36046
Epoch 1.2: Loss = 2.31586
Epoch 1.3: Loss = 2.26758
Epoch 1.4: Loss = 2.21132
Epoch 1.5: Loss = 2.16951
Epoch 1.6: Loss = 2.12328
Epoch 1.7: Loss = 2.08298
Epoch 1.8: Loss = 2.05876
Epoch 1.9: Loss = 1.99748
Epoch 1.10: Loss = 1.97078
Epoch 1.11: Loss = 1.90932
Epoch 1.12: Loss = 1.88879
Epoch 1.13: Loss = 1.86922
Epoch 1.14: Loss = 1.83524
Epoch 1.15: Loss = 1.81483
Epoch 1.16: Loss = 1.71439
Epoch 1.17: Loss = 1.71667
Epoch 1.18: Loss = 1.70792
Epoch 1.19: Loss = 1.6501
Epoch 1.20: Loss = 1.63861
Epoch 1.21: Loss = 1.55736
Epoch 1.22: Loss = 1.57782
Epoch 1.23: Loss = 1.53914
Epoch 1.24: Loss = 1.47966
Epoch 1.25: Loss = 1.49626
Epoch 1.26: Loss = 1.45467
Epoch 1.27: Loss = 1.39758
Epoch 1.28: Loss = 1.41148
Epoch 1.29: Loss = 1.36322
Epoch 1.30: Loss = 1.32614
Epoch 1.31: Loss = 1.31914
Epoch 1.32: Loss = 1.29358
Epoch 1.33: Loss = 1.25652
Epoch 1.34: Loss = 1.25137
Epoch 1.35: Loss = 1.25941
Epoch 1.36: Loss = 1.2681
Epoch 1.37: Loss = 1.20384
Epoch 1.38: Loss = 1.14549
Epoch 1.39: Loss = 1.14368
Epoch 1.40: Loss = 1.15887
Epoch 1.41: Loss = 1.09293
Epoch 1.42: Loss = 1.09451
Epoch 1.43: Loss = 1.09604
Epoch 1.44: Loss = 1.08185
Epoch 1.45: Loss = 1.02815
Epoch 1.46: Loss = 0.999268
Epoch 1.47: Loss = 1.02026
Epoch 1.48: Loss = 0.962952
Epoch 1.49: Loss = 0.983002
Epoch 1.50: Loss = 1.00179
Epoch 1.51: Loss = 0.950867
Epoch 1.52: Loss = 1.00542
Epoch 1.53: Loss = 0.935867
Epoch 1.54: Loss = 0.925323
Epoch 1.55: Loss = 0.916443
Epoch 1.56: Loss = 0.913986
Epoch 1.57: Loss = 0.878906
Epoch 1.58: Loss = 0.874542
Epoch 1.59: Loss = 0.928848
Epoch 1.60: Loss = 0.826218
Epoch 1.61: Loss = 0.908279
Epoch 1.62: Loss = 0.831146
Epoch 1.63: Loss = 0.855835
Epoch 1.64: Loss = 0.802536
Epoch 1.65: Loss = 0.802261
Epoch 1.66: Loss = 0.802536
Epoch 1.67: Loss = 0.782852
Epoch 1.68: Loss = 0.737579
Epoch 1.69: Loss = 0.747971
Epoch 1.70: Loss = 0.728333
Epoch 1.71: Loss = 0.748428
Epoch 1.72: Loss = 0.745773
Epoch 1.73: Loss = 0.776627
Epoch 1.74: Loss = 0.730911
Epoch 1.75: Loss = 0.718506
Epoch 1.76: Loss = 0.733704
Epoch 1.77: Loss = 0.769958
Epoch 1.78: Loss = 0.70784
Epoch 1.79: Loss = 0.708191
Epoch 1.80: Loss = 0.692673
Epoch 1.81: Loss = 0.683609
Epoch 1.82: Loss = 0.683823
Epoch 1.83: Loss = 0.722183
Epoch 1.84: Loss = 0.746078
Epoch 1.85: Loss = 0.696411
Epoch 1.86: Loss = 0.68013
Epoch 1.87: Loss = 0.691437
Epoch 1.88: Loss = 0.631561
Epoch 1.89: Loss = 0.699265
Epoch 1.90: Loss = 0.626984
Epoch 1.91: Loss = 0.653152
Epoch 1.92: Loss = 0.615921
Epoch 1.93: Loss = 0.581253
Epoch 1.94: Loss = 0.638153
Epoch 1.95: Loss = 0.631683
Epoch 1.96: Loss = 0.682739
Epoch 1.97: Loss = 0.642899
Epoch 1.98: Loss = 0.627701
Epoch 1.99: Loss = 0.581665
Epoch 1.100: Loss = 0.630478
TRAIN LOSS = 1.14204
TRAIN ACC = 69.8883 % (41935/60000)
Loss = 0.663788
Loss = 0.660736
Loss = 0.782883
Loss = 0.739517
Loss = 0.660217
Loss = 0.655792
Loss = 0.719238
Loss = 0.713821
Loss = 0.531158
Loss = 0.509201
Loss = 0.437531
Loss = 0.527527
Loss = 0.485123
Loss = 0.470383
Loss = 0.292862
Loss = 0.446396
Loss = 0.777893
TEST LOSS = 0.588886
TEST ACC = 419.35 % (8384/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.597321
Epoch 2.2: Loss = 0.586945
Epoch 2.3: Loss = 0.625397
Epoch 2.4: Loss = 0.58844
Epoch 2.5: Loss = 0.552933
Epoch 2.6: Loss = 0.642731
Epoch 2.7: Loss = 0.61673
Epoch 2.8: Loss = 0.640793
Epoch 2.9: Loss = 0.604889
Epoch 2.10: Loss = 0.585983
Epoch 2.11: Loss = 0.55809
Epoch 2.12: Loss = 0.592087
Epoch 2.13: Loss = 0.596786
Epoch 2.14: Loss = 0.632034
Epoch 2.15: Loss = 0.547226
Epoch 2.16: Loss = 0.555969
Epoch 2.17: Loss = 0.559433
Epoch 2.18: Loss = 0.587189
Epoch 2.19: Loss = 0.531799
Epoch 2.20: Loss = 0.585327
Epoch 2.21: Loss = 0.558746
Epoch 2.22: Loss = 0.575653
Epoch 2.23: Loss = 0.562836
Epoch 2.24: Loss = 0.527847
Epoch 2.25: Loss = 0.55304
Epoch 2.26: Loss = 0.553375
Epoch 2.27: Loss = 0.552536
Epoch 2.28: Loss = 0.507248
Epoch 2.29: Loss = 0.537308
Epoch 2.30: Loss = 0.575241
Epoch 2.31: Loss = 0.62529
Epoch 2.32: Loss = 0.533829
Epoch 2.33: Loss = 0.528351
Epoch 2.34: Loss = 0.600586
Epoch 2.35: Loss = 0.527954
Epoch 2.36: Loss = 0.484665
Epoch 2.37: Loss = 0.492188
Epoch 2.38: Loss = 0.500336
Epoch 2.39: Loss = 0.51387
Epoch 2.40: Loss = 0.566727
Epoch 2.41: Loss = 0.550491
Epoch 2.42: Loss = 0.529358
Epoch 2.43: Loss = 0.586823
Epoch 2.44: Loss = 0.528183
Epoch 2.45: Loss = 0.543381
Epoch 2.46: Loss = 0.558823
Epoch 2.47: Loss = 0.49826
Epoch 2.48: Loss = 0.565353
Epoch 2.49: Loss = 0.529022
Epoch 2.50: Loss = 0.486359
Epoch 2.51: Loss = 0.502792
Epoch 2.52: Loss = 0.491089
Epoch 2.53: Loss = 0.509888
Epoch 2.54: Loss = 0.542313
Epoch 2.55: Loss = 0.494522
Epoch 2.56: Loss = 0.522568
Epoch 2.57: Loss = 0.49942
Epoch 2.58: Loss = 0.514023
Epoch 2.59: Loss = 0.517212
Epoch 2.60: Loss = 0.44101
Epoch 2.61: Loss = 0.503036
Epoch 2.62: Loss = 0.540344
Epoch 2.63: Loss = 0.520142
Epoch 2.64: Loss = 0.537674
Epoch 2.65: Loss = 0.495178
Epoch 2.66: Loss = 0.495209
Epoch 2.67: Loss = 0.47522
Epoch 2.68: Loss = 0.485901
Epoch 2.69: Loss = 0.521591
Epoch 2.70: Loss = 0.459778
Epoch 2.71: Loss = 0.514084
Epoch 2.72: Loss = 0.464523
Epoch 2.73: Loss = 0.511292
Epoch 2.74: Loss = 0.4655
Epoch 2.75: Loss = 0.4496
Epoch 2.76: Loss = 0.499207
Epoch 2.77: Loss = 0.504272
Epoch 2.78: Loss = 0.474243
Epoch 2.79: Loss = 0.480209
Epoch 2.80: Loss = 0.48764
Epoch 2.81: Loss = 0.417175
Epoch 2.82: Loss = 0.498352
Epoch 2.83: Loss = 0.472763
Epoch 2.84: Loss = 0.500671
Epoch 2.85: Loss = 0.516739
Epoch 2.86: Loss = 0.457779
Epoch 2.87: Loss = 0.43222
Epoch 2.88: Loss = 0.51976
Epoch 2.89: Loss = 0.479736
Epoch 2.90: Loss = 0.44664
Epoch 2.91: Loss = 0.471481
Epoch 2.92: Loss = 0.529388
Epoch 2.93: Loss = 0.446014
Epoch 2.94: Loss = 0.470596
Epoch 2.95: Loss = 0.440277
Epoch 2.96: Loss = 0.427704
Epoch 2.97: Loss = 0.483597
Epoch 2.98: Loss = 0.480621
Epoch 2.99: Loss = 0.415451
Epoch 2.100: Loss = 0.548096
TRAIN LOSS = 0.524185
TRAIN ACC = 84.758 % (50857/60000)
Loss = 0.501022
Loss = 0.512589
Loss = 0.641495
Loss = 0.609634
Loss = 0.489548
Loss = 0.495209
Loss = 0.593964
Loss = 0.567169
Loss = 0.386597
Loss = 0.360184
Loss = 0.340057
Loss = 0.364761
Loss = 0.333328
Loss = 0.3582
Loss = 0.160904
Loss = 0.305008
Loss = 0.64537
TEST LOSS = 0.446995
TEST ACC = 508.569 % (8687/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.435028
Epoch 3.2: Loss = 0.409637
Epoch 3.3: Loss = 0.437103
Epoch 3.4: Loss = 0.487381
Epoch 3.5: Loss = 0.452713
Epoch 3.6: Loss = 0.438049
Epoch 3.7: Loss = 0.464905
Epoch 3.8: Loss = 0.479706
Epoch 3.9: Loss = 0.472626
Epoch 3.10: Loss = 0.472443
Epoch 3.11: Loss = 0.440201
Epoch 3.12: Loss = 0.455246
Epoch 3.13: Loss = 0.422318
Epoch 3.14: Loss = 0.406845
Epoch 3.15: Loss = 0.46257
Epoch 3.16: Loss = 0.457611
Epoch 3.17: Loss = 0.482056
Epoch 3.18: Loss = 0.474274
Epoch 3.19: Loss = 0.401031
Epoch 3.20: Loss = 0.416245
Epoch 3.21: Loss = 0.421875
Epoch 3.22: Loss = 0.450058
Epoch 3.23: Loss = 0.452606
Epoch 3.24: Loss = 0.465805
Epoch 3.25: Loss = 0.450607
Epoch 3.26: Loss = 0.522751
Epoch 3.27: Loss = 0.374527
Epoch 3.28: Loss = 0.506256
Epoch 3.29: Loss = 0.400085
Epoch 3.30: Loss = 0.515167
Epoch 3.31: Loss = 0.461121
Epoch 3.32: Loss = 0.513199
Epoch 3.33: Loss = 0.50058
Epoch 3.34: Loss = 0.451309
Epoch 3.35: Loss = 0.406113
Epoch 3.36: Loss = 0.420883
Epoch 3.37: Loss = 0.394867
Epoch 3.38: Loss = 0.412018
Epoch 3.39: Loss = 0.427261
Epoch 3.40: Loss = 0.45256
Epoch 3.41: Loss = 0.504837
Epoch 3.42: Loss = 0.409515
Epoch 3.43: Loss = 0.494293
Epoch 3.44: Loss = 0.436188
Epoch 3.45: Loss = 0.415222
Epoch 3.46: Loss = 0.394119
Epoch 3.47: Loss = 0.476318
Epoch 3.48: Loss = 0.492462
Epoch 3.49: Loss = 0.446518
Epoch 3.50: Loss = 0.467941
Epoch 3.51: Loss = 0.460709
Epoch 3.52: Loss = 0.439255
Epoch 3.53: Loss = 0.405869
Epoch 3.54: Loss = 0.471848
Epoch 3.55: Loss = 0.521591
Epoch 3.56: Loss = 0.395035
Epoch 3.57: Loss = 0.4263
Epoch 3.58: Loss = 0.415298
Epoch 3.59: Loss = 0.470337
Epoch 3.60: Loss = 0.45871
Epoch 3.61: Loss = 0.449677
Epoch 3.62: Loss = 0.39447
Epoch 3.63: Loss = 0.45192
Epoch 3.64: Loss = 0.479858
Epoch 3.65: Loss = 0.387634
Epoch 3.66: Loss = 0.465073
Epoch 3.67: Loss = 0.368103
Epoch 3.68: Loss = 0.380753
Epoch 3.69: Loss = 0.408875
Epoch 3.70: Loss = 0.385529
Epoch 3.71: Loss = 0.413574
Epoch 3.72: Loss = 0.453354
Epoch 3.73: Loss = 0.4207
Epoch 3.74: Loss = 0.441528
Epoch 3.75: Loss = 0.436279
Epoch 3.76: Loss = 0.394928
Epoch 3.77: Loss = 0.423553
Epoch 3.78: Loss = 0.421402
Epoch 3.79: Loss = 0.505707
Epoch 3.80: Loss = 0.455963
Epoch 3.81: Loss = 0.420303
Epoch 3.82: Loss = 0.383743
Epoch 3.83: Loss = 0.402451
Epoch 3.84: Loss = 0.442963
Epoch 3.85: Loss = 0.408371
Epoch 3.86: Loss = 0.435745
Epoch 3.87: Loss = 0.431595
Epoch 3.88: Loss = 0.40332
Epoch 3.89: Loss = 0.454407
Epoch 3.90: Loss = 0.411255
Epoch 3.91: Loss = 0.497574
Epoch 3.92: Loss = 0.437653
Epoch 3.93: Loss = 0.4133
Epoch 3.94: Loss = 0.389999
Epoch 3.95: Loss = 0.445511
Epoch 3.96: Loss = 0.381348
Epoch 3.97: Loss = 0.446899
Epoch 3.98: Loss = 0.434753
Epoch 3.99: Loss = 0.384552
Epoch 3.100: Loss = 0.426498
TRAIN LOSS = 0.439636
TRAIN ACC = 86.8439 % (52108/60000)
Loss = 0.447327
Loss = 0.461182
Loss = 0.598373
Loss = 0.568649
Loss = 0.429626
Loss = 0.429031
Loss = 0.563004
Loss = 0.510071
Loss = 0.343826
Loss = 0.310181
Loss = 0.319214
Loss = 0.306885
Loss = 0.264526
Loss = 0.314331
Loss = 0.118149
Loss = 0.254593
Loss = 0.604431
TEST LOSS = 0.398515
TEST ACC = 521.078 % (8800/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.427521
Epoch 4.2: Loss = 0.425903
Epoch 4.3: Loss = 0.407883
Epoch 4.4: Loss = 0.399734
Epoch 4.5: Loss = 0.349884
Epoch 4.6: Loss = 0.492188
Epoch 4.7: Loss = 0.383255
Epoch 4.8: Loss = 0.366043
Epoch 4.9: Loss = 0.368805
Epoch 4.10: Loss = 0.418701
Epoch 4.11: Loss = 0.419159
Epoch 4.12: Loss = 0.433456
Epoch 4.13: Loss = 0.445419
Epoch 4.14: Loss = 0.388123
Epoch 4.15: Loss = 0.451431
Epoch 4.16: Loss = 0.373901
Epoch 4.17: Loss = 0.394806
Epoch 4.18: Loss = 0.447372
Epoch 4.19: Loss = 0.411728
Epoch 4.20: Loss = 0.44783
Epoch 4.21: Loss = 0.376526
Epoch 4.22: Loss = 0.45787
Epoch 4.23: Loss = 0.429047
Epoch 4.24: Loss = 0.436752
Epoch 4.25: Loss = 0.407715
Epoch 4.26: Loss = 0.425873
Epoch 4.27: Loss = 0.411331
Epoch 4.28: Loss = 0.418121
Epoch 4.29: Loss = 0.444122
Epoch 4.30: Loss = 0.408615
Epoch 4.31: Loss = 0.401123
Epoch 4.32: Loss = 0.429306
Epoch 4.33: Loss = 0.411743
Epoch 4.34: Loss = 0.393295
Epoch 4.35: Loss = 0.437485
Epoch 4.36: Loss = 0.427826
Epoch 4.37: Loss = 0.357437
Epoch 4.38: Loss = 0.411392
Epoch 4.39: Loss = 0.487366
Epoch 4.40: Loss = 0.424438
Epoch 4.41: Loss = 0.406219
Epoch 4.42: Loss = 0.455231
Epoch 4.43: Loss = 0.411087
Epoch 4.44: Loss = 0.484238
Epoch 4.45: Loss = 0.358353
Epoch 4.46: Loss = 0.391525
Epoch 4.47: Loss = 0.349564
Epoch 4.48: Loss = 0.416977
Epoch 4.49: Loss = 0.463989
Epoch 4.50: Loss = 0.352783
Epoch 4.51: Loss = 0.363724
Epoch 4.52: Loss = 0.34082
Epoch 4.53: Loss = 0.331055
Epoch 4.54: Loss = 0.408463
Epoch 4.55: Loss = 0.480698
Epoch 4.56: Loss = 0.434692
Epoch 4.57: Loss = 0.419601
Epoch 4.58: Loss = 0.378922
Epoch 4.59: Loss = 0.400803
Epoch 4.60: Loss = 0.347763
Epoch 4.61: Loss = 0.392578
Epoch 4.62: Loss = 0.393677
Epoch 4.63: Loss = 0.45282
Epoch 4.64: Loss = 0.437546
Epoch 4.65: Loss = 0.443512
Epoch 4.66: Loss = 0.422714
Epoch 4.67: Loss = 0.383057
Epoch 4.68: Loss = 0.393448
Epoch 4.69: Loss = 0.344376
Epoch 4.70: Loss = 0.439636
Epoch 4.71: Loss = 0.312485
Epoch 4.72: Loss = 0.43074
Epoch 4.73: Loss = 0.409592
Epoch 4.74: Loss = 0.412796
Epoch 4.75: Loss = 0.397629
Epoch 4.76: Loss = 0.427246
Epoch 4.77: Loss = 0.426254
Epoch 4.78: Loss = 0.377686
Epoch 4.79: Loss = 0.508087
Epoch 4.80: Loss = 0.37883
Epoch 4.81: Loss = 0.417542
Epoch 4.82: Loss = 0.325714
Epoch 4.83: Loss = 0.430786
Epoch 4.84: Loss = 0.316849
Epoch 4.85: Loss = 0.430313
Epoch 4.86: Loss = 0.39949
Epoch 4.87: Loss = 0.398804
Epoch 4.88: Loss = 0.36618
Epoch 4.89: Loss = 0.381699
Epoch 4.90: Loss = 0.387695
Epoch 4.91: Loss = 0.384033
Epoch 4.92: Loss = 0.463776
Epoch 4.93: Loss = 0.412109
Epoch 4.94: Loss = 0.31842
Epoch 4.95: Loss = 0.338684
Epoch 4.96: Loss = 0.385635
Epoch 4.97: Loss = 0.454681
Epoch 4.98: Loss = 0.325867
Epoch 4.99: Loss = 0.407898
Epoch 4.100: Loss = 0.356628
TRAIN LOSS = 0.40535
TRAIN ACC = 87.854 % (52715/60000)
Loss = 0.421692
Loss = 0.436401
Loss = 0.569412
Loss = 0.55159
Loss = 0.398529
Loss = 0.409073
Loss = 0.547913
Loss = 0.492172
Loss = 0.317551
Loss = 0.281464
Loss = 0.299667
Loss = 0.282852
Loss = 0.229355
Loss = 0.283661
Loss = 0.0986481
Loss = 0.226059
Loss = 0.594238
TEST LOSS = 0.374532
TEST ACC = 527.148 % (8890/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.444443
Epoch 5.2: Loss = 0.398697
Epoch 5.3: Loss = 0.383545
Epoch 5.4: Loss = 0.417542
Epoch 5.5: Loss = 0.35524
Epoch 5.6: Loss = 0.325485
Epoch 5.7: Loss = 0.351288
Epoch 5.8: Loss = 0.414932
Epoch 5.9: Loss = 0.393616
Epoch 5.10: Loss = 0.414749
Epoch 5.11: Loss = 0.392792
Epoch 5.12: Loss = 0.379959
Epoch 5.13: Loss = 0.368134
Epoch 5.14: Loss = 0.474319
Epoch 5.15: Loss = 0.385162
Epoch 5.16: Loss = 0.402359
Epoch 5.17: Loss = 0.373993
Epoch 5.18: Loss = 0.397141
Epoch 5.19: Loss = 0.35466
Epoch 5.20: Loss = 0.45192
Epoch 5.21: Loss = 0.353424
Epoch 5.22: Loss = 0.306412
Epoch 5.23: Loss = 0.385803
Epoch 5.24: Loss = 0.416397
Epoch 5.25: Loss = 0.364975
Epoch 5.26: Loss = 0.428009
Epoch 5.27: Loss = 0.422394
Epoch 5.28: Loss = 0.373581
Epoch 5.29: Loss = 0.370148
Epoch 5.30: Loss = 0.448181
Epoch 5.31: Loss = 0.303314
Epoch 5.32: Loss = 0.3405
Epoch 5.33: Loss = 0.391434
Epoch 5.34: Loss = 0.327072
Epoch 5.35: Loss = 0.422943
Epoch 5.36: Loss = 0.403778
Epoch 5.37: Loss = 0.385559
Epoch 5.38: Loss = 0.388992
Epoch 5.39: Loss = 0.414581
Epoch 5.40: Loss = 0.405319
Epoch 5.41: Loss = 0.35405
Epoch 5.42: Loss = 0.474411
Epoch 5.43: Loss = 0.405792
Epoch 5.44: Loss = 0.400436
Epoch 5.45: Loss = 0.396378
Epoch 5.46: Loss = 0.366135
Epoch 5.47: Loss = 0.387939
Epoch 5.48: Loss = 0.422592
Epoch 5.49: Loss = 0.407333
Epoch 5.50: Loss = 0.365646
Epoch 5.51: Loss = 0.359009
Epoch 5.52: Loss = 0.404007
Epoch 5.53: Loss = 0.428131
Epoch 5.54: Loss = 0.433807
Epoch 5.55: Loss = 0.35878
Epoch 5.56: Loss = 0.371841
Epoch 5.57: Loss = 0.331039
Epoch 5.58: Loss = 0.397385
Epoch 5.59: Loss = 0.377625
Epoch 5.60: Loss = 0.39035
Epoch 5.61: Loss = 0.445099
Epoch 5.62: Loss = 0.392654
Epoch 5.63: Loss = 0.395111
Epoch 5.64: Loss = 0.406693
Epoch 5.65: Loss = 0.391266
Epoch 5.66: Loss = 0.335358
Epoch 5.67: Loss = 0.374832
Epoch 5.68: Loss = 0.361115
Epoch 5.69: Loss = 0.383408
Epoch 5.70: Loss = 0.391785
Epoch 5.71: Loss = 0.408737
Epoch 5.72: Loss = 0.419388
Epoch 5.73: Loss = 0.434509
Epoch 5.74: Loss = 0.299286
Epoch 5.75: Loss = 0.382156
Epoch 5.76: Loss = 0.393936
Epoch 5.77: Loss = 0.344818
Epoch 5.78: Loss = 0.353485
Epoch 5.79: Loss = 0.425369
Epoch 5.80: Loss = 0.455246
Epoch 5.81: Loss = 0.449799
Epoch 5.82: Loss = 0.390549
Epoch 5.83: Loss = 0.378983
Epoch 5.84: Loss = 0.414688
Epoch 5.85: Loss = 0.37886
Epoch 5.86: Loss = 0.344772
Epoch 5.87: Loss = 0.358673
Epoch 5.88: Loss = 0.336761
Epoch 5.89: Loss = 0.440979
Epoch 5.90: Loss = 0.319717
Epoch 5.91: Loss = 0.380432
Epoch 5.92: Loss = 0.342514
Epoch 5.93: Loss = 0.393494
Epoch 5.94: Loss = 0.401978
Epoch 5.95: Loss = 0.388123
Epoch 5.96: Loss = 0.40332
Epoch 5.97: Loss = 0.380905
Epoch 5.98: Loss = 0.34938
Epoch 5.99: Loss = 0.367737
Epoch 5.100: Loss = 0.426758
TRAIN LOSS = 0.388092
TRAIN ACC = 88.5529 % (53134/60000)
Loss = 0.404846
Loss = 0.422363
Loss = 0.550354
Loss = 0.532303
Loss = 0.38205
Loss = 0.384171
Loss = 0.527756
Loss = 0.471863
Loss = 0.30423
Loss = 0.267517
Loss = 0.305038
Loss = 0.26062
Loss = 0.21402
Loss = 0.284836
Loss = 0.0817413
Loss = 0.210709
Loss = 0.580215
TEST LOSS = 0.359473
TEST ACC = 531.339 % (8922/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.342056
Epoch 6.2: Loss = 0.416901
Epoch 6.3: Loss = 0.375839
Epoch 6.4: Loss = 0.411179
Epoch 6.5: Loss = 0.269699
Epoch 6.6: Loss = 0.359177
Epoch 6.7: Loss = 0.406204
Epoch 6.8: Loss = 0.30455
Epoch 6.9: Loss = 0.394974
Epoch 6.10: Loss = 0.376648
Epoch 6.11: Loss = 0.312134
Epoch 6.12: Loss = 0.330566
Epoch 6.13: Loss = 0.37851
Epoch 6.14: Loss = 0.430374
Epoch 6.15: Loss = 0.365616
Epoch 6.16: Loss = 0.375565
Epoch 6.17: Loss = 0.344879
Epoch 6.18: Loss = 0.434708
Epoch 6.19: Loss = 0.367508
Epoch 6.20: Loss = 0.412125
Epoch 6.21: Loss = 0.429123
Epoch 6.22: Loss = 0.397018
Epoch 6.23: Loss = 0.41478
Epoch 6.24: Loss = 0.331253
Epoch 6.25: Loss = 0.393723
Epoch 6.26: Loss = 0.383194
Epoch 6.27: Loss = 0.375916
Epoch 6.28: Loss = 0.369217
Epoch 6.29: Loss = 0.386002
Epoch 6.30: Loss = 0.408264
Epoch 6.31: Loss = 0.390854
Epoch 6.32: Loss = 0.395477
Epoch 6.33: Loss = 0.297089
Epoch 6.34: Loss = 0.331329
Epoch 6.35: Loss = 0.396423
Epoch 6.36: Loss = 0.454544
Epoch 6.37: Loss = 0.330353
Epoch 6.38: Loss = 0.335312
Epoch 6.39: Loss = 0.386765
Epoch 6.40: Loss = 0.351746
Epoch 6.41: Loss = 0.39621
Epoch 6.42: Loss = 0.364563
Epoch 6.43: Loss = 0.316895
Epoch 6.44: Loss = 0.365097
Epoch 6.45: Loss = 0.386887
Epoch 6.46: Loss = 0.396255
Epoch 6.47: Loss = 0.382751
Epoch 6.48: Loss = 0.279221
Epoch 6.49: Loss = 0.382889
Epoch 6.50: Loss = 0.413498
Epoch 6.51: Loss = 0.378281
Epoch 6.52: Loss = 0.429413
Epoch 6.53: Loss = 0.441681
Epoch 6.54: Loss = 0.381866
Epoch 6.55: Loss = 0.33931
Epoch 6.56: Loss = 0.421768
Epoch 6.57: Loss = 0.465851
Epoch 6.58: Loss = 0.356247
Epoch 6.59: Loss = 0.509399
Epoch 6.60: Loss = 0.366486
Epoch 6.61: Loss = 0.309113
Epoch 6.62: Loss = 0.364563
Epoch 6.63: Loss = 0.350021
Epoch 6.64: Loss = 0.371857
Epoch 6.65: Loss = 0.380371
Epoch 6.66: Loss = 0.35817
Epoch 6.67: Loss = 0.439377
Epoch 6.68: Loss = 0.321991
Epoch 6.69: Loss = 0.306931
Epoch 6.70: Loss = 0.311356
Epoch 6.71: Loss = 0.40947
Epoch 6.72: Loss = 0.353882
Epoch 6.73: Loss = 0.333023
Epoch 6.74: Loss = 0.346741
Epoch 6.75: Loss = 0.428375
Epoch 6.76: Loss = 0.380005
Epoch 6.77: Loss = 0.379074
Epoch 6.78: Loss = 0.448303
Epoch 6.79: Loss = 0.354019
Epoch 6.80: Loss = 0.264969
Epoch 6.81: Loss = 0.322723
Epoch 6.82: Loss = 0.439194
Epoch 6.83: Loss = 0.375061
Epoch 6.84: Loss = 0.331848
Epoch 6.85: Loss = 0.414825
Epoch 6.86: Loss = 0.378159
Epoch 6.87: Loss = 0.35907
Epoch 6.88: Loss = 0.439011
Epoch 6.89: Loss = 0.392532
Epoch 6.90: Loss = 0.40416
Epoch 6.91: Loss = 0.376572
Epoch 6.92: Loss = 0.443451
Epoch 6.93: Loss = 0.366333
Epoch 6.94: Loss = 0.34108
Epoch 6.95: Loss = 0.419556
Epoch 6.96: Loss = 0.396133
Epoch 6.97: Loss = 0.347977
Epoch 6.98: Loss = 0.414124
Epoch 6.99: Loss = 0.327515
Epoch 6.100: Loss = 0.455597
TRAIN LOSS = 0.376968
TRAIN ACC = 89.064 % (53441/60000)
Loss = 0.39389
Loss = 0.416168
Loss = 0.549988
Loss = 0.531006
Loss = 0.376724
Loss = 0.377838
Loss = 0.520493
Loss = 0.466721
Loss = 0.30899
Loss = 0.251068
Loss = 0.304428
Loss = 0.263184
Loss = 0.204498
Loss = 0.291946
Loss = 0.0753326
Loss = 0.215363
Loss = 0.582336
TEST LOSS = 0.356152
TEST ACC = 534.409 % (8960/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.299759
Epoch 7.2: Loss = 0.405777
Epoch 7.3: Loss = 0.415527
Epoch 7.4: Loss = 0.323013
Epoch 7.5: Loss = 0.360275
Epoch 7.6: Loss = 0.442108
Epoch 7.7: Loss = 0.354324
Epoch 7.8: Loss = 0.311859
Epoch 7.9: Loss = 0.388153
Epoch 7.10: Loss = 0.396729
Epoch 7.11: Loss = 0.300964
Epoch 7.12: Loss = 0.342224
Epoch 7.13: Loss = 0.368408
Epoch 7.14: Loss = 0.318542
Epoch 7.15: Loss = 0.400803
Epoch 7.16: Loss = 0.338226
Epoch 7.17: Loss = 0.38472
Epoch 7.18: Loss = 0.377975
Epoch 7.19: Loss = 0.329758
Epoch 7.20: Loss = 0.385025
Epoch 7.21: Loss = 0.372528
Epoch 7.22: Loss = 0.312042
Epoch 7.23: Loss = 0.392685
Epoch 7.24: Loss = 0.394608
Epoch 7.25: Loss = 0.290558
Epoch 7.26: Loss = 0.329117
Epoch 7.27: Loss = 0.404236
Epoch 7.28: Loss = 0.347076
Epoch 7.29: Loss = 0.433105
Epoch 7.30: Loss = 0.379517
Epoch 7.31: Loss = 0.359528
Epoch 7.32: Loss = 0.384827
Epoch 7.33: Loss = 0.442505
Epoch 7.34: Loss = 0.278534
Epoch 7.35: Loss = 0.399429
Epoch 7.36: Loss = 0.407303
Epoch 7.37: Loss = 0.316895
Epoch 7.38: Loss = 0.302139
Epoch 7.39: Loss = 0.418961
Epoch 7.40: Loss = 0.382111
Epoch 7.41: Loss = 0.40451
Epoch 7.42: Loss = 0.361969
Epoch 7.43: Loss = 0.391266
Epoch 7.44: Loss = 0.410339
Epoch 7.45: Loss = 0.395248
Epoch 7.46: Loss = 0.351822
Epoch 7.47: Loss = 0.383301
Epoch 7.48: Loss = 0.360062
Epoch 7.49: Loss = 0.324081
Epoch 7.50: Loss = 0.338318
Epoch 7.51: Loss = 0.362076
Epoch 7.52: Loss = 0.356842
Epoch 7.53: Loss = 0.333496
Epoch 7.54: Loss = 0.407272
Epoch 7.55: Loss = 0.366272
Epoch 7.56: Loss = 0.40918
Epoch 7.57: Loss = 0.325546
Epoch 7.58: Loss = 0.398071
Epoch 7.59: Loss = 0.365768
Epoch 7.60: Loss = 0.420898
Epoch 7.61: Loss = 0.402161
Epoch 7.62: Loss = 0.297821
Epoch 7.63: Loss = 0.437424
Epoch 7.64: Loss = 0.333267
Epoch 7.65: Loss = 0.39389
Epoch 7.66: Loss = 0.319839
Epoch 7.67: Loss = 0.490814
Epoch 7.68: Loss = 0.364548
Epoch 7.69: Loss = 0.429092
Epoch 7.70: Loss = 0.425995
Epoch 7.71: Loss = 0.444534
Epoch 7.72: Loss = 0.402405
Epoch 7.73: Loss = 0.318222
Epoch 7.74: Loss = 0.394928
Epoch 7.75: Loss = 0.473221
Epoch 7.76: Loss = 0.391418
Epoch 7.77: Loss = 0.403931
Epoch 7.78: Loss = 0.369247
Epoch 7.79: Loss = 0.401688
Epoch 7.80: Loss = 0.309952
Epoch 7.81: Loss = 0.378189
Epoch 7.82: Loss = 0.342651
Epoch 7.83: Loss = 0.396973
Epoch 7.84: Loss = 0.384262
Epoch 7.85: Loss = 0.324844
Epoch 7.86: Loss = 0.36853
Epoch 7.87: Loss = 0.292343
Epoch 7.88: Loss = 0.377136
Epoch 7.89: Loss = 0.386536
Epoch 7.90: Loss = 0.38855
Epoch 7.91: Loss = 0.407715
Epoch 7.92: Loss = 0.32045
Epoch 7.93: Loss = 0.340897
Epoch 7.94: Loss = 0.361694
Epoch 7.95: Loss = 0.320511
Epoch 7.96: Loss = 0.346878
Epoch 7.97: Loss = 0.320145
Epoch 7.98: Loss = 0.399765
Epoch 7.99: Loss = 0.338715
Epoch 7.100: Loss = 0.326706
TRAIN LOSS = 0.369888
TRAIN ACC = 89.3707 % (53624/60000)
Loss = 0.381516
Loss = 0.40715
Loss = 0.542297
Loss = 0.513763
Loss = 0.362411
Loss = 0.368683
Loss = 0.514709
Loss = 0.454727
Loss = 0.294357
Loss = 0.244003
Loss = 0.300995
Loss = 0.243652
Loss = 0.197266
Loss = 0.292709
Loss = 0.0695343
Loss = 0.207962
Loss = 0.55661
TEST LOSS = 0.346009
TEST ACC = 536.24 % (8995/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.352234
Epoch 8.2: Loss = 0.358826
Epoch 8.3: Loss = 0.372391
Epoch 8.4: Loss = 0.384048
Epoch 8.5: Loss = 0.324097
Epoch 8.6: Loss = 0.376358
Epoch 8.7: Loss = 0.405151
Epoch 8.8: Loss = 0.366821
Epoch 8.9: Loss = 0.466537
Epoch 8.10: Loss = 0.34581
Epoch 8.11: Loss = 0.348358
Epoch 8.12: Loss = 0.364655
Epoch 8.13: Loss = 0.420166
Epoch 8.14: Loss = 0.298737
Epoch 8.15: Loss = 0.277817
Epoch 8.16: Loss = 0.377609
Epoch 8.17: Loss = 0.333389
Epoch 8.18: Loss = 0.373505
Epoch 8.19: Loss = 0.359558
Epoch 8.20: Loss = 0.260788
Epoch 8.21: Loss = 0.345139
Epoch 8.22: Loss = 0.389984
Epoch 8.23: Loss = 0.302231
Epoch 8.24: Loss = 0.384079
Epoch 8.25: Loss = 0.330032
Epoch 8.26: Loss = 0.355637
Epoch 8.27: Loss = 0.372971
Epoch 8.28: Loss = 0.396118
Epoch 8.29: Loss = 0.369156
Epoch 8.30: Loss = 0.40419
Epoch 8.31: Loss = 0.374802
Epoch 8.32: Loss = 0.323486
Epoch 8.33: Loss = 0.336319
Epoch 8.34: Loss = 0.340149
Epoch 8.35: Loss = 0.426422
Epoch 8.36: Loss = 0.45253
Epoch 8.37: Loss = 0.355942
Epoch 8.38: Loss = 0.323318
Epoch 8.39: Loss = 0.375854
Epoch 8.40: Loss = 0.380585
Epoch 8.41: Loss = 0.370285
Epoch 8.42: Loss = 0.425186
Epoch 8.43: Loss = 0.301376
Epoch 8.44: Loss = 0.363663
Epoch 8.45: Loss = 0.464462
Epoch 8.46: Loss = 0.341599
Epoch 8.47: Loss = 0.386017
Epoch 8.48: Loss = 0.354355
Epoch 8.49: Loss = 0.342285
Epoch 8.50: Loss = 0.416107
Epoch 8.51: Loss = 0.305664
Epoch 8.52: Loss = 0.45282
Epoch 8.53: Loss = 0.414337
Epoch 8.54: Loss = 0.358429
Epoch 8.55: Loss = 0.367783
Epoch 8.56: Loss = 0.401306
Epoch 8.57: Loss = 0.37471
Epoch 8.58: Loss = 0.322937
Epoch 8.59: Loss = 0.355179
Epoch 8.60: Loss = 0.305771
Epoch 8.61: Loss = 0.416763
Epoch 8.62: Loss = 0.345795
Epoch 8.63: Loss = 0.388687
Epoch 8.64: Loss = 0.373703
Epoch 8.65: Loss = 0.323883
Epoch 8.66: Loss = 0.315964
Epoch 8.67: Loss = 0.377167
Epoch 8.68: Loss = 0.448029
Epoch 8.69: Loss = 0.374664
Epoch 8.70: Loss = 0.408234
Epoch 8.71: Loss = 0.327835
Epoch 8.72: Loss = 0.304962
Epoch 8.73: Loss = 0.350769
Epoch 8.74: Loss = 0.347992
Epoch 8.75: Loss = 0.30452
Epoch 8.76: Loss = 0.363129
Epoch 8.77: Loss = 0.391281
Epoch 8.78: Loss = 0.357224
Epoch 8.79: Loss = 0.440979
Epoch 8.80: Loss = 0.291229
Epoch 8.81: Loss = 0.330048
Epoch 8.82: Loss = 0.305298
Epoch 8.83: Loss = 0.339233
Epoch 8.84: Loss = 0.424057
Epoch 8.85: Loss = 0.386002
Epoch 8.86: Loss = 0.348328
Epoch 8.87: Loss = 0.366272
Epoch 8.88: Loss = 0.372452
Epoch 8.89: Loss = 0.377533
Epoch 8.90: Loss = 0.369812
Epoch 8.91: Loss = 0.331055
Epoch 8.92: Loss = 0.286621
Epoch 8.93: Loss = 0.336075
Epoch 8.94: Loss = 0.331375
Epoch 8.95: Loss = 0.410706
Epoch 8.96: Loss = 0.33313
Epoch 8.97: Loss = 0.433319
Epoch 8.98: Loss = 0.389633
Epoch 8.99: Loss = 0.352844
Epoch 8.100: Loss = 0.43454
TRAIN LOSS = 0.364426
TRAIN ACC = 89.5554 % (53735/60000)
Loss = 0.378632
Loss = 0.408569
Loss = 0.537781
Loss = 0.503845
Loss = 0.356812
Loss = 0.370361
Loss = 0.512451
Loss = 0.443207
Loss = 0.296738
Loss = 0.250931
Loss = 0.292465
Loss = 0.233704
Loss = 0.183868
Loss = 0.285904
Loss = 0.06604
Loss = 0.214066
Loss = 0.550705
TEST LOSS = 0.34215
TEST ACC = 537.349 % (9011/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.329544
Epoch 9.2: Loss = 0.392227
Epoch 9.3: Loss = 0.284531
Epoch 9.4: Loss = 0.393509
Epoch 9.5: Loss = 0.342316
Epoch 9.6: Loss = 0.439346
Epoch 9.7: Loss = 0.323334
Epoch 9.8: Loss = 0.354553
Epoch 9.9: Loss = 0.302139
Epoch 9.10: Loss = 0.432556
Epoch 9.11: Loss = 0.35025
Epoch 9.12: Loss = 0.339966
Epoch 9.13: Loss = 0.331055
Epoch 9.14: Loss = 0.34256
Epoch 9.15: Loss = 0.375534
Epoch 9.16: Loss = 0.424667
Epoch 9.17: Loss = 0.301804
Epoch 9.18: Loss = 0.318405
Epoch 9.19: Loss = 0.370422
Epoch 9.20: Loss = 0.331665
Epoch 9.21: Loss = 0.319305
Epoch 9.22: Loss = 0.355179
Epoch 9.23: Loss = 0.370163
Epoch 9.24: Loss = 0.366699
Epoch 9.25: Loss = 0.339783
Epoch 9.26: Loss = 0.317307
Epoch 9.27: Loss = 0.443649
Epoch 9.28: Loss = 0.44043
Epoch 9.29: Loss = 0.302261
Epoch 9.30: Loss = 0.401505
Epoch 9.31: Loss = 0.345963
Epoch 9.32: Loss = 0.395767
Epoch 9.33: Loss = 0.399551
Epoch 9.34: Loss = 0.283325
Epoch 9.35: Loss = 0.449631
Epoch 9.36: Loss = 0.316422
Epoch 9.37: Loss = 0.294907
Epoch 9.38: Loss = 0.309677
Epoch 9.39: Loss = 0.294678
Epoch 9.40: Loss = 0.336243
Epoch 9.41: Loss = 0.377274
Epoch 9.42: Loss = 0.369965
Epoch 9.43: Loss = 0.395264
Epoch 9.44: Loss = 0.390015
Epoch 9.45: Loss = 0.446747
Epoch 9.46: Loss = 0.444229
Epoch 9.47: Loss = 0.404724
Epoch 9.48: Loss = 0.378571
Epoch 9.49: Loss = 0.292099
Epoch 9.50: Loss = 0.329742
Epoch 9.51: Loss = 0.406128
Epoch 9.52: Loss = 0.381775
Epoch 9.53: Loss = 0.319077
Epoch 9.54: Loss = 0.341202
Epoch 9.55: Loss = 0.446228
Epoch 9.56: Loss = 0.431458
Epoch 9.57: Loss = 0.319336
Epoch 9.58: Loss = 0.410873
Epoch 9.59: Loss = 0.399536
Epoch 9.60: Loss = 0.353958
Epoch 9.61: Loss = 0.408844
Epoch 9.62: Loss = 0.273117
Epoch 9.63: Loss = 0.389877
Epoch 9.64: Loss = 0.346634
Epoch 9.65: Loss = 0.314453
Epoch 9.66: Loss = 0.40863
Epoch 9.67: Loss = 0.359528
Epoch 9.68: Loss = 0.340225
Epoch 9.69: Loss = 0.361694
Epoch 9.70: Loss = 0.318024
Epoch 9.71: Loss = 0.365356
Epoch 9.72: Loss = 0.39299
Epoch 9.73: Loss = 0.300995
Epoch 9.74: Loss = 0.364883
Epoch 9.75: Loss = 0.409882
Epoch 9.76: Loss = 0.294678
Epoch 9.77: Loss = 0.36499
Epoch 9.78: Loss = 0.322342
Epoch 9.79: Loss = 0.368881
Epoch 9.80: Loss = 0.335861
Epoch 9.81: Loss = 0.406357
Epoch 9.82: Loss = 0.384552
Epoch 9.83: Loss = 0.336349
Epoch 9.84: Loss = 0.357101
Epoch 9.85: Loss = 0.355408
Epoch 9.86: Loss = 0.342102
Epoch 9.87: Loss = 0.356339
Epoch 9.88: Loss = 0.396484
Epoch 9.89: Loss = 0.418335
Epoch 9.90: Loss = 0.282349
Epoch 9.91: Loss = 0.432266
Epoch 9.92: Loss = 0.42189
Epoch 9.93: Loss = 0.321533
Epoch 9.94: Loss = 0.367706
Epoch 9.95: Loss = 0.397934
Epoch 9.96: Loss = 0.31488
Epoch 9.97: Loss = 0.338943
Epoch 9.98: Loss = 0.362808
Epoch 9.99: Loss = 0.296036
Epoch 9.100: Loss = 0.315155
TRAIN LOSS = 0.360474
TRAIN ACC = 89.6881 % (53816/60000)
Loss = 0.373322
Loss = 0.403244
Loss = 0.533554
Loss = 0.507675
Loss = 0.352234
Loss = 0.358994
Loss = 0.509094
Loss = 0.435989
Loss = 0.294281
Loss = 0.242798
Loss = 0.303864
Loss = 0.236923
Loss = 0.178467
Loss = 0.276321
Loss = 0.0635986
Loss = 0.212402
Loss = 0.559036
TEST LOSS = 0.339327
TEST ACC = 538.159 % (9037/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.389053
Epoch 10.2: Loss = 0.351349
Epoch 10.3: Loss = 0.335022
Epoch 10.4: Loss = 0.378036
Epoch 10.5: Loss = 0.329483
Epoch 10.6: Loss = 0.383194
Epoch 10.7: Loss = 0.29834
Epoch 10.8: Loss = 0.417801
Epoch 10.9: Loss = 0.394318
Epoch 10.10: Loss = 0.322128
Epoch 10.11: Loss = 0.295547
Epoch 10.12: Loss = 0.407349
Epoch 10.13: Loss = 0.319534
Epoch 10.14: Loss = 0.377563
Epoch 10.15: Loss = 0.369064
Epoch 10.16: Loss = 0.344208
Epoch 10.17: Loss = 0.309387
Epoch 10.18: Loss = 0.274399
Epoch 10.19: Loss = 0.322601
Epoch 10.20: Loss = 0.38356
Epoch 10.21: Loss = 0.345749
Epoch 10.22: Loss = 0.304855
Epoch 10.23: Loss = 0.302673
Epoch 10.24: Loss = 0.337631
Epoch 10.25: Loss = 0.392334
Epoch 10.26: Loss = 0.384857
Epoch 10.27: Loss = 0.303223
Epoch 10.28: Loss = 0.333405
Epoch 10.29: Loss = 0.301239
Epoch 10.30: Loss = 0.330475
Epoch 10.31: Loss = 0.267975
Epoch 10.32: Loss = 0.378876
Epoch 10.33: Loss = 0.365158
Epoch 10.34: Loss = 0.33287
Epoch 10.35: Loss = 0.393723
Epoch 10.36: Loss = 0.316757
Epoch 10.37: Loss = 0.31955
Epoch 10.38: Loss = 0.427277
Epoch 10.39: Loss = 0.449005
Epoch 10.40: Loss = 0.399841
Epoch 10.41: Loss = 0.388504
Epoch 10.42: Loss = 0.42865
Epoch 10.43: Loss = 0.542084
Epoch 10.44: Loss = 0.422379
Epoch 10.45: Loss = 0.364227
Epoch 10.46: Loss = 0.441605
Epoch 10.47: Loss = 0.340088
Epoch 10.48: Loss = 0.364105
Epoch 10.49: Loss = 0.462067
Epoch 10.50: Loss = 0.36264
Epoch 10.51: Loss = 0.332199
Epoch 10.52: Loss = 0.343994
Epoch 10.53: Loss = 0.37706
Epoch 10.54: Loss = 0.358078
Epoch 10.55: Loss = 0.37149
Epoch 10.56: Loss = 0.287033
Epoch 10.57: Loss = 0.364838
Epoch 10.58: Loss = 0.344391
Epoch 10.59: Loss = 0.331711
Epoch 10.60: Loss = 0.327194
Epoch 10.61: Loss = 0.333511
Epoch 10.62: Loss = 0.325699
Epoch 10.63: Loss = 0.412079
Epoch 10.64: Loss = 0.313919
Epoch 10.65: Loss = 0.368866
Epoch 10.66: Loss = 0.290665
Epoch 10.67: Loss = 0.37294
Epoch 10.68: Loss = 0.298996
Epoch 10.69: Loss = 0.290833
Epoch 10.70: Loss = 0.427246
Epoch 10.71: Loss = 0.364151
Epoch 10.72: Loss = 0.304565
Epoch 10.73: Loss = 0.318726
Epoch 10.74: Loss = 0.324524
Epoch 10.75: Loss = 0.358246
Epoch 10.76: Loss = 0.348755
Epoch 10.77: Loss = 0.323959
Epoch 10.78: Loss = 0.445862
Epoch 10.79: Loss = 0.352646
Epoch 10.80: Loss = 0.36879
Epoch 10.81: Loss = 0.413681
Epoch 10.82: Loss = 0.372925
Epoch 10.83: Loss = 0.294724
Epoch 10.84: Loss = 0.374924
Epoch 10.85: Loss = 0.386063
Epoch 10.86: Loss = 0.349411
Epoch 10.87: Loss = 0.412323
Epoch 10.88: Loss = 0.299118
Epoch 10.89: Loss = 0.391708
Epoch 10.90: Loss = 0.317459
Epoch 10.91: Loss = 0.270493
Epoch 10.92: Loss = 0.369034
Epoch 10.93: Loss = 0.328217
Epoch 10.94: Loss = 0.376236
Epoch 10.95: Loss = 0.369675
Epoch 10.96: Loss = 0.355423
Epoch 10.97: Loss = 0.353455
Epoch 10.98: Loss = 0.392273
Epoch 10.99: Loss = 0.381378
Epoch 10.100: Loss = 0.375595
TRAIN LOSS = 0.35672
TRAIN ACC = 89.8407 % (53908/60000)
Loss = 0.37149
Loss = 0.400742
Loss = 0.527573
Loss = 0.506729
Loss = 0.351913
Loss = 0.358917
Loss = 0.507462
Loss = 0.430908
Loss = 0.288574
Loss = 0.236969
Loss = 0.301849
Loss = 0.223862
Loss = 0.170471
Loss = 0.275101
Loss = 0.0585632
Loss = 0.207886
Loss = 0.56221
TEST LOSS = 0.335629
TEST ACC = 539.079 % (9060/10000)

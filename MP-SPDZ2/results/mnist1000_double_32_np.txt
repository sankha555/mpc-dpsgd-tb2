Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 1000]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 20
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 3.5
***********************************************************
Epoch 1.1: Loss = 2.33942
Epoch 1.2: Loss = 2.28589
Epoch 1.3: Loss = 2.27644
Epoch 1.4: Loss = 2.20425
Epoch 1.5: Loss = 2.16933
Epoch 1.6: Loss = 2.13017
Epoch 1.7: Loss = 2.08479
Epoch 1.8: Loss = 2.04744
Epoch 1.9: Loss = 2.0005
Epoch 1.10: Loss = 1.93965
Epoch 1.11: Loss = 1.94571
Epoch 1.12: Loss = 1.8831
Epoch 1.13: Loss = 1.87411
Epoch 1.14: Loss = 1.7916
Epoch 1.15: Loss = 1.7605
Epoch 1.16: Loss = 1.72177
Epoch 1.17: Loss = 1.73441
Epoch 1.18: Loss = 1.69734
Epoch 1.19: Loss = 1.64137
Epoch 1.20: Loss = 1.59348
Epoch 1.21: Loss = 1.59132
Epoch 1.22: Loss = 1.51561
Epoch 1.23: Loss = 1.50752
Epoch 1.24: Loss = 1.44519
Epoch 1.25: Loss = 1.42223
Epoch 1.26: Loss = 1.44371
Epoch 1.27: Loss = 1.36647
Epoch 1.28: Loss = 1.36678
Epoch 1.29: Loss = 1.28325
Epoch 1.30: Loss = 1.29236
Epoch 1.31: Loss = 1.32072
Epoch 1.32: Loss = 1.27092
Epoch 1.33: Loss = 1.22821
Epoch 1.34: Loss = 1.17427
Epoch 1.35: Loss = 1.19693
Epoch 1.36: Loss = 1.15237
Epoch 1.37: Loss = 1.13107
Epoch 1.38: Loss = 1.14078
Epoch 1.39: Loss = 1.10336
Epoch 1.40: Loss = 1.10239
Epoch 1.41: Loss = 1.09915
Epoch 1.42: Loss = 1.04997
Epoch 1.43: Loss = 1.0952
Epoch 1.44: Loss = 1.08748
Epoch 1.45: Loss = 1.01933
Epoch 1.46: Loss = 0.949097
Epoch 1.47: Loss = 1.01213
Epoch 1.48: Loss = 0.927078
Epoch 1.49: Loss = 1.01402
Epoch 1.50: Loss = 1.03615
Epoch 1.51: Loss = 0.923401
Epoch 1.52: Loss = 0.957306
Epoch 1.53: Loss = 0.925018
Epoch 1.54: Loss = 0.877289
Epoch 1.55: Loss = 0.915054
Epoch 1.56: Loss = 0.914749
Epoch 1.57: Loss = 0.943512
Epoch 1.58: Loss = 0.802612
Epoch 1.59: Loss = 0.914948
Epoch 1.60: Loss = 0.90184
Epoch 1.61: Loss = 0.894363
Epoch 1.62: Loss = 0.911667
Epoch 1.63: Loss = 0.839203
Epoch 1.64: Loss = 0.827515
Epoch 1.65: Loss = 0.8638
Epoch 1.66: Loss = 0.841461
Epoch 1.67: Loss = 0.784225
Epoch 1.68: Loss = 0.775574
Epoch 1.69: Loss = 0.744659
Epoch 1.70: Loss = 0.877808
Epoch 1.71: Loss = 0.826447
Epoch 1.72: Loss = 0.868362
Epoch 1.73: Loss = 0.825424
Epoch 1.74: Loss = 0.779343
Epoch 1.75: Loss = 0.760437
Epoch 1.76: Loss = 0.765228
Epoch 1.77: Loss = 0.800064
Epoch 1.78: Loss = 0.722397
Epoch 1.79: Loss = 0.754852
Epoch 1.80: Loss = 0.688217
Epoch 1.81: Loss = 0.767242
Epoch 1.82: Loss = 0.690353
Epoch 1.83: Loss = 0.746216
Epoch 1.84: Loss = 0.734879
Epoch 1.85: Loss = 0.745697
Epoch 1.86: Loss = 0.748154
Epoch 1.87: Loss = 0.743759
Epoch 1.88: Loss = 0.702042
Epoch 1.89: Loss = 0.735809
Epoch 1.90: Loss = 0.721451
Epoch 1.91: Loss = 0.773163
Epoch 1.92: Loss = 0.681274
Epoch 1.93: Loss = 0.719818
Epoch 1.94: Loss = 0.677124
Epoch 1.95: Loss = 0.648575
Epoch 1.96: Loss = 0.659378
Epoch 1.97: Loss = 0.709564
Epoch 1.98: Loss = 0.703552
Epoch 1.99: Loss = 0.656143
Epoch 1.100: Loss = 0.728683
TRAIN LOSS = 1.14987
TRAIN ACC = 66.33 % (39800/60000)
Loss = 0.749878
Loss = 0.761124
Loss = 0.871933
Loss = 0.846649
Loss = 0.712296
Loss = 0.729034
Loss = 0.827072
Loss = 0.783676
Loss = 0.615005
Loss = 0.529343
Loss = 0.45047
Loss = 0.560852
Loss = 0.565384
Loss = 0.49408
Loss = 0.298584
Loss = 0.51297
Loss = 0.897141
TEST LOSS = 0.654386
TEST ACC = 398 % (7913/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.711197
Epoch 2.2: Loss = 0.668655
Epoch 2.3: Loss = 0.724869
Epoch 2.4: Loss = 0.608719
Epoch 2.5: Loss = 0.726273
Epoch 2.6: Loss = 0.643951
Epoch 2.7: Loss = 0.754715
Epoch 2.8: Loss = 0.640533
Epoch 2.9: Loss = 0.586975
Epoch 2.10: Loss = 0.626038
Epoch 2.11: Loss = 0.72467
Epoch 2.12: Loss = 0.661346
Epoch 2.13: Loss = 0.699051
Epoch 2.14: Loss = 0.617599
Epoch 2.15: Loss = 0.692017
Epoch 2.16: Loss = 0.677994
Epoch 2.17: Loss = 0.64743
Epoch 2.18: Loss = 0.742416
Epoch 2.19: Loss = 0.619308
Epoch 2.20: Loss = 0.724716
Epoch 2.21: Loss = 0.634109
Epoch 2.22: Loss = 0.650177
Epoch 2.23: Loss = 0.630127
Epoch 2.24: Loss = 0.727737
Epoch 2.25: Loss = 0.616562
Epoch 2.26: Loss = 0.710815
Epoch 2.27: Loss = 0.59668
Epoch 2.28: Loss = 0.581833
Epoch 2.29: Loss = 0.636917
Epoch 2.30: Loss = 0.671997
Epoch 2.31: Loss = 0.706039
Epoch 2.32: Loss = 0.673218
Epoch 2.33: Loss = 0.702057
Epoch 2.34: Loss = 0.631897
Epoch 2.35: Loss = 0.636017
Epoch 2.36: Loss = 0.673218
Epoch 2.37: Loss = 0.71048
Epoch 2.38: Loss = 0.753876
Epoch 2.39: Loss = 0.667755
Epoch 2.40: Loss = 0.709351
Epoch 2.41: Loss = 0.678818
Epoch 2.42: Loss = 0.767212
Epoch 2.43: Loss = 0.660156
Epoch 2.44: Loss = 0.634262
Epoch 2.45: Loss = 0.684174
Epoch 2.46: Loss = 0.687134
Epoch 2.47: Loss = 0.639648
Epoch 2.48: Loss = 0.680649
Epoch 2.49: Loss = 0.600723
Epoch 2.50: Loss = 0.687119
Epoch 2.51: Loss = 0.663101
Epoch 2.52: Loss = 0.702667
Epoch 2.53: Loss = 0.647507
Epoch 2.54: Loss = 0.612076
Epoch 2.55: Loss = 0.62233
Epoch 2.56: Loss = 0.548019
Epoch 2.57: Loss = 0.650711
Epoch 2.58: Loss = 0.563324
Epoch 2.59: Loss = 0.638672
Epoch 2.60: Loss = 0.64859
Epoch 2.61: Loss = 0.663254
Epoch 2.62: Loss = 0.600677
Epoch 2.63: Loss = 0.641998
Epoch 2.64: Loss = 0.631256
Epoch 2.65: Loss = 0.60437
Epoch 2.66: Loss = 0.690948
Epoch 2.67: Loss = 0.636566
Epoch 2.68: Loss = 0.630798
Epoch 2.69: Loss = 0.679947
Epoch 2.70: Loss = 0.633804
Epoch 2.71: Loss = 0.641144
Epoch 2.72: Loss = 0.629379
Epoch 2.73: Loss = 0.685303
Epoch 2.74: Loss = 0.526749
Epoch 2.75: Loss = 0.630829
Epoch 2.76: Loss = 0.623169
Epoch 2.77: Loss = 0.637939
Epoch 2.78: Loss = 0.541183
Epoch 2.79: Loss = 0.604141
Epoch 2.80: Loss = 0.618942
Epoch 2.81: Loss = 0.650833
Epoch 2.82: Loss = 0.712173
Epoch 2.83: Loss = 0.621277
Epoch 2.84: Loss = 0.579422
Epoch 2.85: Loss = 0.471695
Epoch 2.86: Loss = 0.590805
Epoch 2.87: Loss = 0.677917
Epoch 2.88: Loss = 0.636032
Epoch 2.89: Loss = 0.720078
Epoch 2.90: Loss = 0.750702
Epoch 2.91: Loss = 0.556519
Epoch 2.92: Loss = 0.650192
Epoch 2.93: Loss = 0.703262
Epoch 2.94: Loss = 0.669434
Epoch 2.95: Loss = 0.637848
Epoch 2.96: Loss = 0.641449
Epoch 2.97: Loss = 0.609238
Epoch 2.98: Loss = 0.664154
Epoch 2.99: Loss = 0.648987
Epoch 2.100: Loss = 0.629364
TRAIN LOSS = 0.6521
TRAIN ACC = 79.7165 % (47833/60000)
Loss = 0.66243
Loss = 0.688477
Loss = 0.8013
Loss = 0.787186
Loss = 0.588654
Loss = 0.617996
Loss = 0.744781
Loss = 0.696106
Loss = 0.573746
Loss = 0.467392
Loss = 0.405975
Loss = 0.529251
Loss = 0.429306
Loss = 0.528015
Loss = 0.186569
Loss = 0.457825
Loss = 0.867035
TEST LOSS = 0.584582
TEST ACC = 478.329 % (8187/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.580627
Epoch 3.2: Loss = 0.593658
Epoch 3.3: Loss = 0.704636
Epoch 3.4: Loss = 0.69841
Epoch 3.5: Loss = 0.674698
Epoch 3.6: Loss = 0.560669
Epoch 3.7: Loss = 0.661987
Epoch 3.8: Loss = 0.628983
Epoch 3.9: Loss = 0.639893
Epoch 3.10: Loss = 0.742615
Epoch 3.11: Loss = 0.632278
Epoch 3.12: Loss = 0.596436
Epoch 3.13: Loss = 0.680939
Epoch 3.14: Loss = 0.636963
Epoch 3.15: Loss = 0.599701
Epoch 3.16: Loss = 0.715439
Epoch 3.17: Loss = 0.603714
Epoch 3.18: Loss = 0.59996
Epoch 3.19: Loss = 0.563766
Epoch 3.20: Loss = 0.649628
Epoch 3.21: Loss = 0.716888
Epoch 3.22: Loss = 0.653137
Epoch 3.23: Loss = 0.612015
Epoch 3.24: Loss = 0.800873
Epoch 3.25: Loss = 0.620697
Epoch 3.26: Loss = 0.699631
Epoch 3.27: Loss = 0.588211
Epoch 3.28: Loss = 0.632248
Epoch 3.29: Loss = 0.796432
Epoch 3.30: Loss = 0.552994
Epoch 3.31: Loss = 0.61377
Epoch 3.32: Loss = 0.633545
Epoch 3.33: Loss = 0.656525
Epoch 3.34: Loss = 0.649231
Epoch 3.35: Loss = 0.654999
Epoch 3.36: Loss = 0.635834
Epoch 3.37: Loss = 0.577164
Epoch 3.38: Loss = 0.727661
Epoch 3.39: Loss = 0.598877
Epoch 3.40: Loss = 0.548401
Epoch 3.41: Loss = 0.5793
Epoch 3.42: Loss = 0.632477
Epoch 3.43: Loss = 0.65686
Epoch 3.44: Loss = 0.569595
Epoch 3.45: Loss = 0.614151
Epoch 3.46: Loss = 0.661285
Epoch 3.47: Loss = 0.694092
Epoch 3.48: Loss = 0.655243
Epoch 3.49: Loss = 0.648605
Epoch 3.50: Loss = 0.816696
Epoch 3.51: Loss = 0.669769
Epoch 3.52: Loss = 0.588242
Epoch 3.53: Loss = 0.642181
Epoch 3.54: Loss = 0.666519
Epoch 3.55: Loss = 0.656326
Epoch 3.56: Loss = 0.591904
Epoch 3.57: Loss = 0.763916
Epoch 3.58: Loss = 0.637222
Epoch 3.59: Loss = 0.591843
Epoch 3.60: Loss = 0.648224
Epoch 3.61: Loss = 0.618179
Epoch 3.62: Loss = 0.697571
Epoch 3.63: Loss = 0.612366
Epoch 3.64: Loss = 0.737305
Epoch 3.65: Loss = 0.696533
Epoch 3.66: Loss = 0.739914
Epoch 3.67: Loss = 0.621033
Epoch 3.68: Loss = 0.796768
Epoch 3.69: Loss = 0.564621
Epoch 3.70: Loss = 0.678574
Epoch 3.71: Loss = 0.719727
Epoch 3.72: Loss = 0.740997
Epoch 3.73: Loss = 0.661026
Epoch 3.74: Loss = 0.659622
Epoch 3.75: Loss = 0.705658
Epoch 3.76: Loss = 0.701508
Epoch 3.77: Loss = 0.751633
Epoch 3.78: Loss = 0.68396
Epoch 3.79: Loss = 0.547272
Epoch 3.80: Loss = 0.716583
Epoch 3.81: Loss = 0.613525
Epoch 3.82: Loss = 0.694656
Epoch 3.83: Loss = 0.637543
Epoch 3.84: Loss = 0.721436
Epoch 3.85: Loss = 0.6745
Epoch 3.86: Loss = 0.695007
Epoch 3.87: Loss = 0.70488
Epoch 3.88: Loss = 0.720444
Epoch 3.89: Loss = 0.661331
Epoch 3.90: Loss = 0.61293
Epoch 3.91: Loss = 0.668533
Epoch 3.92: Loss = 0.662247
Epoch 3.93: Loss = 0.851273
Epoch 3.94: Loss = 0.699356
Epoch 3.95: Loss = 0.620529
Epoch 3.96: Loss = 0.670547
Epoch 3.97: Loss = 0.634003
Epoch 3.98: Loss = 0.655853
Epoch 3.99: Loss = 0.673096
Epoch 3.100: Loss = 0.675369
TRAIN LOSS = 0.65918
TRAIN ACC = 80.835 % (48503/60000)
Loss = 0.707642
Loss = 0.775208
Loss = 0.889938
Loss = 0.909271
Loss = 0.630264
Loss = 0.708313
Loss = 0.911377
Loss = 0.77388
Loss = 0.592453
Loss = 0.568558
Loss = 0.468323
Loss = 0.508179
Loss = 0.426224
Loss = 0.644745
Loss = 0.231201
Loss = 0.430634
Loss = 1.00563
TEST LOSS = 0.650798
TEST ACC = 485.03 % (8182/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.666183
Epoch 4.2: Loss = 0.642303
Epoch 4.3: Loss = 0.643906
Epoch 4.4: Loss = 0.740021
Epoch 4.5: Loss = 0.734177
Epoch 4.6: Loss = 0.800797
Epoch 4.7: Loss = 0.623138
Epoch 4.8: Loss = 0.579376
Epoch 4.9: Loss = 0.73735
Epoch 4.10: Loss = 0.732758
Epoch 4.11: Loss = 0.721741
Epoch 4.12: Loss = 0.652161
Epoch 4.13: Loss = 0.572693
Epoch 4.14: Loss = 0.625702
Epoch 4.15: Loss = 0.677185
Epoch 4.16: Loss = 0.707291
Epoch 4.17: Loss = 0.672897
Epoch 4.18: Loss = 0.645798
Epoch 4.19: Loss = 0.715622
Epoch 4.20: Loss = 0.612335
Epoch 4.21: Loss = 0.658585
Epoch 4.22: Loss = 0.722565
Epoch 4.23: Loss = 0.647064
Epoch 4.24: Loss = 0.687759
Epoch 4.25: Loss = 0.528732
Epoch 4.26: Loss = 0.733658
Epoch 4.27: Loss = 0.684219
Epoch 4.28: Loss = 0.708679
Epoch 4.29: Loss = 0.717133
Epoch 4.30: Loss = 0.662186
Epoch 4.31: Loss = 0.647446
Epoch 4.32: Loss = 0.645325
Epoch 4.33: Loss = 0.659256
Epoch 4.34: Loss = 0.630569
Epoch 4.35: Loss = 0.670273
Epoch 4.36: Loss = 0.747543
Epoch 4.37: Loss = 0.654266
Epoch 4.38: Loss = 0.78833
Epoch 4.39: Loss = 0.644363
Epoch 4.40: Loss = 0.746002
Epoch 4.41: Loss = 0.73082
Epoch 4.42: Loss = 0.745178
Epoch 4.43: Loss = 0.727707
Epoch 4.44: Loss = 0.761826
Epoch 4.45: Loss = 0.63588
Epoch 4.46: Loss = 0.688629
Epoch 4.47: Loss = 0.612671
Epoch 4.48: Loss = 0.696747
Epoch 4.49: Loss = 0.661652
Epoch 4.50: Loss = 0.703827
Epoch 4.51: Loss = 0.694351
Epoch 4.52: Loss = 0.684723
Epoch 4.53: Loss = 0.751755
Epoch 4.54: Loss = 0.698257
Epoch 4.55: Loss = 0.781235
Epoch 4.56: Loss = 0.719864
Epoch 4.57: Loss = 0.642212
Epoch 4.58: Loss = 0.714951
Epoch 4.59: Loss = 0.778168
Epoch 4.60: Loss = 0.692642
Epoch 4.61: Loss = 0.652985
Epoch 4.62: Loss = 0.715027
Epoch 4.63: Loss = 0.750076
Epoch 4.64: Loss = 0.656662
Epoch 4.65: Loss = 0.651337
Epoch 4.66: Loss = 0.599319
Epoch 4.67: Loss = 0.655746
Epoch 4.68: Loss = 0.675842
Epoch 4.69: Loss = 0.760254
Epoch 4.70: Loss = 0.712265
Epoch 4.71: Loss = 0.654358
Epoch 4.72: Loss = 0.653412
Epoch 4.73: Loss = 0.721252
Epoch 4.74: Loss = 0.606766
Epoch 4.75: Loss = 0.78009
Epoch 4.76: Loss = 0.773804
Epoch 4.77: Loss = 0.664795
Epoch 4.78: Loss = 0.749832
Epoch 4.79: Loss = 0.763138
Epoch 4.80: Loss = 0.63974
Epoch 4.81: Loss = 0.762741
Epoch 4.82: Loss = 0.69191
Epoch 4.83: Loss = 0.782532
Epoch 4.84: Loss = 0.89209
Epoch 4.85: Loss = 0.649643
Epoch 4.86: Loss = 0.770767
Epoch 4.87: Loss = 0.877243
Epoch 4.88: Loss = 0.701965
Epoch 4.89: Loss = 0.680191
Epoch 4.90: Loss = 0.871094
Epoch 4.91: Loss = 0.743729
Epoch 4.92: Loss = 0.717087
Epoch 4.93: Loss = 0.738815
Epoch 4.94: Loss = 0.720718
Epoch 4.95: Loss = 0.693207
Epoch 4.96: Loss = 0.823456
Epoch 4.97: Loss = 0.665451
Epoch 4.98: Loss = 0.692047
Epoch 4.99: Loss = 0.755219
Epoch 4.100: Loss = 0.727463
TRAIN LOSS = 0.700058
TRAIN ACC = 81.1707 % (48704/60000)
Loss = 0.753448
Loss = 0.867279
Loss = 0.98468
Loss = 1.07014
Loss = 0.70755
Loss = 0.795883
Loss = 1.05367
Loss = 0.838058
Loss = 0.635483
Loss = 0.503235
Loss = 0.527985
Loss = 0.56366
Loss = 0.444275
Loss = 0.546036
Loss = 0.226563
Loss = 0.409714
Loss = 1.16231
TEST LOSS = 0.702152
TEST ACC = 487.039 % (8239/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.815948
Epoch 5.2: Loss = 0.802002
Epoch 5.3: Loss = 0.735138
Epoch 5.4: Loss = 0.753891
Epoch 5.5: Loss = 0.783936
Epoch 5.6: Loss = 0.742615
Epoch 5.7: Loss = 0.661148
Epoch 5.8: Loss = 0.703232
Epoch 5.9: Loss = 0.640793
Epoch 5.10: Loss = 0.654907
Epoch 5.11: Loss = 0.780731
Epoch 5.12: Loss = 0.71199
Epoch 5.13: Loss = 0.737762
Epoch 5.14: Loss = 0.6828
Epoch 5.15: Loss = 0.801529
Epoch 5.16: Loss = 0.767807
Epoch 5.17: Loss = 0.62178
Epoch 5.18: Loss = 0.635025
Epoch 5.19: Loss = 0.575104
Epoch 5.20: Loss = 0.773804
Epoch 5.21: Loss = 0.765244
Epoch 5.22: Loss = 0.640274
Epoch 5.23: Loss = 0.667297
Epoch 5.24: Loss = 0.785019
Epoch 5.25: Loss = 0.76889
Epoch 5.26: Loss = 0.617722
Epoch 5.27: Loss = 0.702927
Epoch 5.28: Loss = 0.677277
Epoch 5.29: Loss = 0.704987
Epoch 5.30: Loss = 0.765823
Epoch 5.31: Loss = 0.721802
Epoch 5.32: Loss = 0.761795
Epoch 5.33: Loss = 0.79657
Epoch 5.34: Loss = 0.949066
Epoch 5.35: Loss = 0.739258
Epoch 5.36: Loss = 0.732758
Epoch 5.37: Loss = 0.755493
Epoch 5.38: Loss = 0.807449
Epoch 5.39: Loss = 0.755157
Epoch 5.40: Loss = 0.564621
Epoch 5.41: Loss = 0.718246
Epoch 5.42: Loss = 0.810242
Epoch 5.43: Loss = 0.700684
Epoch 5.44: Loss = 0.691925
Epoch 5.45: Loss = 0.732773
Epoch 5.46: Loss = 0.797424
Epoch 5.47: Loss = 0.80394
Epoch 5.48: Loss = 0.774109
Epoch 5.49: Loss = 0.666031
Epoch 5.50: Loss = 0.816956
Epoch 5.51: Loss = 0.748062
Epoch 5.52: Loss = 0.815948
Epoch 5.53: Loss = 0.828781
Epoch 5.54: Loss = 0.70047
Epoch 5.55: Loss = 0.74675
Epoch 5.56: Loss = 0.721191
Epoch 5.57: Loss = 0.713745
Epoch 5.58: Loss = 0.809494
Epoch 5.59: Loss = 0.698837
Epoch 5.60: Loss = 0.674835
Epoch 5.61: Loss = 0.673172
Epoch 5.62: Loss = 0.909164
Epoch 5.63: Loss = 0.750671
Epoch 5.64: Loss = 0.739838
Epoch 5.65: Loss = 0.810318
Epoch 5.66: Loss = 0.746918
Epoch 5.67: Loss = 0.759903
Epoch 5.68: Loss = 0.775635
Epoch 5.69: Loss = 0.738449
Epoch 5.70: Loss = 0.679413
Epoch 5.71: Loss = 0.741272
Epoch 5.72: Loss = 0.777451
Epoch 5.73: Loss = 0.775436
Epoch 5.74: Loss = 0.858002
Epoch 5.75: Loss = 0.715912
Epoch 5.76: Loss = 0.798355
Epoch 5.77: Loss = 0.783783
Epoch 5.78: Loss = 0.496445
Epoch 5.79: Loss = 0.741394
Epoch 5.80: Loss = 0.795334
Epoch 5.81: Loss = 0.839447
Epoch 5.82: Loss = 0.713348
Epoch 5.83: Loss = 0.668304
Epoch 5.84: Loss = 0.805206
Epoch 5.85: Loss = 0.702896
Epoch 5.86: Loss = 0.705292
Epoch 5.87: Loss = 0.580521
Epoch 5.88: Loss = 0.795135
Epoch 5.89: Loss = 0.836151
Epoch 5.90: Loss = 0.82666
Epoch 5.91: Loss = 0.785706
Epoch 5.92: Loss = 0.67131
Epoch 5.93: Loss = 0.549698
Epoch 5.94: Loss = 0.766174
Epoch 5.95: Loss = 0.659164
Epoch 5.96: Loss = 0.789063
Epoch 5.97: Loss = 0.92923
Epoch 5.98: Loss = 0.779053
Epoch 5.99: Loss = 0.827423
Epoch 5.100: Loss = 0.85321
TRAIN LOSS = 0.741837
TRAIN ACC = 81.7123 % (49030/60000)
Loss = 0.777283
Loss = 0.839676
Loss = 0.98703
Loss = 1.07825
Loss = 0.708511
Loss = 0.740128
Loss = 0.955444
Loss = 0.793732
Loss = 0.667831
Loss = 0.534851
Loss = 0.583008
Loss = 0.557571
Loss = 0.497681
Loss = 0.61348
Loss = 0.211914
Loss = 0.496643
Loss = 1.16281
TEST LOSS = 0.709094
TEST ACC = 490.298 % (8278/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.696884
Epoch 6.2: Loss = 0.691437
Epoch 6.3: Loss = 0.647308
Epoch 6.4: Loss = 0.661758
Epoch 6.5: Loss = 0.686539
Epoch 6.6: Loss = 0.889801
Epoch 6.7: Loss = 0.82756
Epoch 6.8: Loss = 0.702271
Epoch 6.9: Loss = 0.751099
Epoch 6.10: Loss = 0.808502
Epoch 6.11: Loss = 0.864944
Epoch 6.12: Loss = 0.607513
Epoch 6.13: Loss = 0.866074
Epoch 6.14: Loss = 0.786118
Epoch 6.15: Loss = 0.795898
Epoch 6.16: Loss = 0.907425
Epoch 6.17: Loss = 0.75116
Epoch 6.18: Loss = 0.692001
Epoch 6.19: Loss = 0.753143
Epoch 6.20: Loss = 0.848846
Epoch 6.21: Loss = 0.702591
Epoch 6.22: Loss = 0.78038
Epoch 6.23: Loss = 0.764099
Epoch 6.24: Loss = 0.722015
Epoch 6.25: Loss = 0.822525
Epoch 6.26: Loss = 0.663025
Epoch 6.27: Loss = 0.705338
Epoch 6.28: Loss = 0.657806
Epoch 6.29: Loss = 0.681458
Epoch 6.30: Loss = 0.793823
Epoch 6.31: Loss = 0.751511
Epoch 6.32: Loss = 0.80661
Epoch 6.33: Loss = 0.908417
Epoch 6.34: Loss = 0.637207
Epoch 6.35: Loss = 0.818665
Epoch 6.36: Loss = 0.778046
Epoch 6.37: Loss = 0.75238
Epoch 6.38: Loss = 0.88652
Epoch 6.39: Loss = 0.844208
Epoch 6.40: Loss = 0.636978
Epoch 6.41: Loss = 0.633667
Epoch 6.42: Loss = 0.775314
Epoch 6.43: Loss = 0.793045
Epoch 6.44: Loss = 0.695969
Epoch 6.45: Loss = 0.750732
Epoch 6.46: Loss = 0.882935
Epoch 6.47: Loss = 0.793411
Epoch 6.48: Loss = 0.860046
Epoch 6.49: Loss = 0.773224
Epoch 6.50: Loss = 0.810913
Epoch 6.51: Loss = 0.677002
Epoch 6.52: Loss = 0.808075
Epoch 6.53: Loss = 0.745361
Epoch 6.54: Loss = 0.738892
Epoch 6.55: Loss = 0.894745
Epoch 6.56: Loss = 0.719666
Epoch 6.57: Loss = 0.678131
Epoch 6.58: Loss = 0.664795
Epoch 6.59: Loss = 0.795761
Epoch 6.60: Loss = 0.765289
Epoch 6.61: Loss = 0.694824
Epoch 6.62: Loss = 0.69516
Epoch 6.63: Loss = 0.720108
Epoch 6.64: Loss = 0.93988
Epoch 6.65: Loss = 0.713531
Epoch 6.66: Loss = 0.758972
Epoch 6.67: Loss = 0.719147
Epoch 6.68: Loss = 0.700729
Epoch 6.69: Loss = 0.671555
Epoch 6.70: Loss = 0.808182
Epoch 6.71: Loss = 0.835861
Epoch 6.72: Loss = 0.758636
Epoch 6.73: Loss = 0.760162
Epoch 6.74: Loss = 0.761566
Epoch 6.75: Loss = 0.84996
Epoch 6.76: Loss = 0.688995
Epoch 6.77: Loss = 0.820221
Epoch 6.78: Loss = 0.778397
Epoch 6.79: Loss = 0.856354
Epoch 6.80: Loss = 0.691132
Epoch 6.81: Loss = 0.80751
Epoch 6.82: Loss = 0.905487
Epoch 6.83: Loss = 0.815842
Epoch 6.84: Loss = 0.92868
Epoch 6.85: Loss = 0.912888
Epoch 6.86: Loss = 0.77739
Epoch 6.87: Loss = 0.84726
Epoch 6.88: Loss = 0.839859
Epoch 6.89: Loss = 0.67923
Epoch 6.90: Loss = 0.771988
Epoch 6.91: Loss = 0.645096
Epoch 6.92: Loss = 0.799088
Epoch 6.93: Loss = 0.871902
Epoch 6.94: Loss = 0.79483
Epoch 6.95: Loss = 0.849548
Epoch 6.96: Loss = 0.752533
Epoch 6.97: Loss = 0.80954
Epoch 6.98: Loss = 0.780884
Epoch 6.99: Loss = 0.763916
Epoch 6.100: Loss = 0.899429
TRAIN LOSS = 0.770844
TRAIN ACC = 81.7108 % (49029/60000)
Loss = 0.812149
Loss = 0.892731
Loss = 1.12279
Loss = 1.22728
Loss = 0.760117
Loss = 0.875458
Loss = 1.0473
Loss = 0.91893
Loss = 0.6595
Loss = 0.636871
Loss = 0.563919
Loss = 0.684082
Loss = 0.454178
Loss = 0.60228
Loss = 0.200623
Loss = 0.505066
Loss = 1.06865
TEST LOSS = 0.760542
TEST ACC = 490.289 % (8263/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.694077
Epoch 7.2: Loss = 0.913132
Epoch 7.3: Loss = 0.78981
Epoch 7.4: Loss = 0.75827
Epoch 7.5: Loss = 0.782104
Epoch 7.6: Loss = 0.886597
Epoch 7.7: Loss = 1.08057
Epoch 7.8: Loss = 0.860626
Epoch 7.9: Loss = 1.01842
Epoch 7.10: Loss = 0.782028
Epoch 7.11: Loss = 0.764603
Epoch 7.12: Loss = 0.734085
Epoch 7.13: Loss = 0.879684
Epoch 7.14: Loss = 0.695053
Epoch 7.15: Loss = 0.790421
Epoch 7.16: Loss = 0.939148
Epoch 7.17: Loss = 0.838745
Epoch 7.18: Loss = 0.804306
Epoch 7.19: Loss = 0.843735
Epoch 7.20: Loss = 0.863464
Epoch 7.21: Loss = 0.727798
Epoch 7.22: Loss = 0.757874
Epoch 7.23: Loss = 0.804962
Epoch 7.24: Loss = 0.827576
Epoch 7.25: Loss = 0.6586
Epoch 7.26: Loss = 0.748795
Epoch 7.27: Loss = 0.843933
Epoch 7.28: Loss = 0.859741
Epoch 7.29: Loss = 0.924133
Epoch 7.30: Loss = 0.762192
Epoch 7.31: Loss = 0.86731
Epoch 7.32: Loss = 0.79808
Epoch 7.33: Loss = 0.867584
Epoch 7.34: Loss = 0.797058
Epoch 7.35: Loss = 0.741165
Epoch 7.36: Loss = 0.901001
Epoch 7.37: Loss = 0.947281
Epoch 7.38: Loss = 0.839142
Epoch 7.39: Loss = 0.914825
Epoch 7.40: Loss = 0.751938
Epoch 7.41: Loss = 0.891373
Epoch 7.42: Loss = 1.007
Epoch 7.43: Loss = 0.833023
Epoch 7.44: Loss = 0.871063
Epoch 7.45: Loss = 0.880066
Epoch 7.46: Loss = 0.720764
Epoch 7.47: Loss = 0.797073
Epoch 7.48: Loss = 1.00902
Epoch 7.49: Loss = 0.823166
Epoch 7.50: Loss = 0.912567
Epoch 7.51: Loss = 0.884644
Epoch 7.52: Loss = 0.872192
Epoch 7.53: Loss = 0.98111
Epoch 7.54: Loss = 0.550095
Epoch 7.55: Loss = 0.954315
Epoch 7.56: Loss = 0.772537
Epoch 7.57: Loss = 0.833115
Epoch 7.58: Loss = 0.886826
Epoch 7.59: Loss = 0.856567
Epoch 7.60: Loss = 0.809326
Epoch 7.61: Loss = 0.890717
Epoch 7.62: Loss = 0.931305
Epoch 7.63: Loss = 0.981232
Epoch 7.64: Loss = 0.889023
Epoch 7.65: Loss = 0.964478
Epoch 7.66: Loss = 0.674759
Epoch 7.67: Loss = 0.672943
Epoch 7.68: Loss = 0.876968
Epoch 7.69: Loss = 0.842392
Epoch 7.70: Loss = 0.676285
Epoch 7.71: Loss = 0.869339
Epoch 7.72: Loss = 0.848358
Epoch 7.73: Loss = 0.903366
Epoch 7.74: Loss = 0.884384
Epoch 7.75: Loss = 1.01141
Epoch 7.76: Loss = 0.88269
Epoch 7.77: Loss = 0.739365
Epoch 7.78: Loss = 0.748825
Epoch 7.79: Loss = 0.834335
Epoch 7.80: Loss = 0.799438
Epoch 7.81: Loss = 0.804016
Epoch 7.82: Loss = 0.671066
Epoch 7.83: Loss = 0.710922
Epoch 7.84: Loss = 0.837555
Epoch 7.85: Loss = 0.93161
Epoch 7.86: Loss = 0.78688
Epoch 7.87: Loss = 0.758575
Epoch 7.88: Loss = 0.72728
Epoch 7.89: Loss = 0.845795
Epoch 7.90: Loss = 0.732208
Epoch 7.91: Loss = 0.829697
Epoch 7.92: Loss = 0.704071
Epoch 7.93: Loss = 0.640488
Epoch 7.94: Loss = 0.755585
Epoch 7.95: Loss = 0.827362
Epoch 7.96: Loss = 0.7444
Epoch 7.97: Loss = 0.875381
Epoch 7.98: Loss = 0.824951
Epoch 7.99: Loss = 0.598358
Epoch 7.100: Loss = 0.707397
TRAIN LOSS = 0.824417
TRAIN ACC = 81.8283 % (49100/60000)
Loss = 0.809692
Loss = 0.888504
Loss = 1.1909
Loss = 1.3259
Loss = 0.80423
Loss = 0.943817
Loss = 1.05637
Loss = 0.89621
Loss = 0.648392
Loss = 0.610519
Loss = 0.606873
Loss = 0.637192
Loss = 0.454742
Loss = 0.605133
Loss = 0.194687
Loss = 0.460205
Loss = 1.06133
TEST LOSS = 0.770454
TEST ACC = 490.999 % (8280/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.832275
Epoch 8.2: Loss = 0.891525
Epoch 8.3: Loss = 0.726669
Epoch 8.4: Loss = 0.859253
Epoch 8.5: Loss = 0.883362
Epoch 8.6: Loss = 0.910919
Epoch 8.7: Loss = 0.700974
Epoch 8.8: Loss = 0.759811
Epoch 8.9: Loss = 0.915848
Epoch 8.10: Loss = 0.887207
Epoch 8.11: Loss = 0.825607
Epoch 8.12: Loss = 0.718597
Epoch 8.13: Loss = 0.763855
Epoch 8.14: Loss = 1.04539
Epoch 8.15: Loss = 0.889984
Epoch 8.16: Loss = 0.875534
Epoch 8.17: Loss = 0.850128
Epoch 8.18: Loss = 0.807297
Epoch 8.19: Loss = 0.90509
Epoch 8.20: Loss = 0.7789
Epoch 8.21: Loss = 0.827957
Epoch 8.22: Loss = 0.970947
Epoch 8.23: Loss = 0.754562
Epoch 8.24: Loss = 0.986649
Epoch 8.25: Loss = 0.80632
Epoch 8.26: Loss = 0.87381
Epoch 8.27: Loss = 0.903412
Epoch 8.28: Loss = 0.97226
Epoch 8.29: Loss = 0.975815
Epoch 8.30: Loss = 0.926453
Epoch 8.31: Loss = 0.816391
Epoch 8.32: Loss = 0.883255
Epoch 8.33: Loss = 0.816147
Epoch 8.34: Loss = 0.885559
Epoch 8.35: Loss = 0.912292
Epoch 8.36: Loss = 0.677322
Epoch 8.37: Loss = 0.755554
Epoch 8.38: Loss = 0.820801
Epoch 8.39: Loss = 0.894913
Epoch 8.40: Loss = 0.773666
Epoch 8.41: Loss = 0.800125
Epoch 8.42: Loss = 0.807465
Epoch 8.43: Loss = 0.929321
Epoch 8.44: Loss = 0.909134
Epoch 8.45: Loss = 0.96701
Epoch 8.46: Loss = 0.981522
Epoch 8.47: Loss = 0.832626
Epoch 8.48: Loss = 0.778946
Epoch 8.49: Loss = 0.720261
Epoch 8.50: Loss = 0.959564
Epoch 8.51: Loss = 0.887314
Epoch 8.52: Loss = 0.975983
Epoch 8.53: Loss = 0.635513
Epoch 8.54: Loss = 0.871872
Epoch 8.55: Loss = 0.830917
Epoch 8.56: Loss = 0.86087
Epoch 8.57: Loss = 0.848572
Epoch 8.58: Loss = 0.907211
Epoch 8.59: Loss = 0.953903
Epoch 8.60: Loss = 0.842865
Epoch 8.61: Loss = 0.851303
Epoch 8.62: Loss = 0.865829
Epoch 8.63: Loss = 0.890549
Epoch 8.64: Loss = 1.02786
Epoch 8.65: Loss = 0.892273
Epoch 8.66: Loss = 1.04483
Epoch 8.67: Loss = 0.898468
Epoch 8.68: Loss = 0.933014
Epoch 8.69: Loss = 0.702286
Epoch 8.70: Loss = 0.988022
Epoch 8.71: Loss = 0.87738
Epoch 8.72: Loss = 0.927414
Epoch 8.73: Loss = 0.848373
Epoch 8.74: Loss = 0.972961
Epoch 8.75: Loss = 1.00157
Epoch 8.76: Loss = 0.970306
Epoch 8.77: Loss = 1.01624
Epoch 8.78: Loss = 0.659683
Epoch 8.79: Loss = 1.03078
Epoch 8.80: Loss = 0.874664
Epoch 8.81: Loss = 0.857437
Epoch 8.82: Loss = 0.772491
Epoch 8.83: Loss = 0.768005
Epoch 8.84: Loss = 0.87587
Epoch 8.85: Loss = 0.788589
Epoch 8.86: Loss = 0.990234
Epoch 8.87: Loss = 0.88147
Epoch 8.88: Loss = 0.868286
Epoch 8.89: Loss = 0.765274
Epoch 8.90: Loss = 1.09793
Epoch 8.91: Loss = 0.873154
Epoch 8.92: Loss = 0.831345
Epoch 8.93: Loss = 0.893173
Epoch 8.94: Loss = 0.788544
Epoch 8.95: Loss = 0.923935
Epoch 8.96: Loss = 0.783005
Epoch 8.97: Loss = 0.973358
Epoch 8.98: Loss = 0.834702
Epoch 8.99: Loss = 0.91127
Epoch 8.100: Loss = 0.813171
TRAIN LOSS = 0.869308
TRAIN ACC = 81.7902 % (49077/60000)
Loss = 0.856247
Loss = 0.92717
Loss = 1.27081
Loss = 1.39626
Loss = 0.863541
Loss = 0.96936
Loss = 1.14729
Loss = 0.974823
Loss = 0.76767
Loss = 0.652649
Loss = 0.625549
Loss = 0.717834
Loss = 0.475464
Loss = 0.686371
Loss = 0.172409
Loss = 0.417435
Loss = 1.18034
TEST LOSS = 0.822467
TEST ACC = 490.768 % (8294/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.80246
Epoch 9.2: Loss = 0.843475
Epoch 9.3: Loss = 0.978271
Epoch 9.4: Loss = 0.935654
Epoch 9.5: Loss = 0.726532
Epoch 9.6: Loss = 0.933868
Epoch 9.7: Loss = 0.865768
Epoch 9.8: Loss = 0.812103
Epoch 9.9: Loss = 0.982925
Epoch 9.10: Loss = 0.957001
Epoch 9.11: Loss = 0.9673
Epoch 9.12: Loss = 0.796234
Epoch 9.13: Loss = 0.823029
Epoch 9.14: Loss = 0.938995
Epoch 9.15: Loss = 0.968338
Epoch 9.16: Loss = 0.883957
Epoch 9.17: Loss = 0.955902
Epoch 9.18: Loss = 0.970428
Epoch 9.19: Loss = 0.901703
Epoch 9.20: Loss = 0.937622
Epoch 9.21: Loss = 0.903015
Epoch 9.22: Loss = 0.923294
Epoch 9.23: Loss = 0.878082
Epoch 9.24: Loss = 1.22569
Epoch 9.25: Loss = 1.03294
Epoch 9.26: Loss = 0.852249
Epoch 9.27: Loss = 0.956116
Epoch 9.28: Loss = 0.760147
Epoch 9.29: Loss = 0.794632
Epoch 9.30: Loss = 0.993851
Epoch 9.31: Loss = 0.792923
Epoch 9.32: Loss = 1.05559
Epoch 9.33: Loss = 1.10007
Epoch 9.34: Loss = 0.85611
Epoch 9.35: Loss = 0.814346
Epoch 9.36: Loss = 0.830063
Epoch 9.37: Loss = 0.946533
Epoch 9.38: Loss = 1.00876
Epoch 9.39: Loss = 0.88269
Epoch 9.40: Loss = 1.09607
Epoch 9.41: Loss = 0.752228
Epoch 9.42: Loss = 0.896484
Epoch 9.43: Loss = 1.01657
Epoch 9.44: Loss = 0.902252
Epoch 9.45: Loss = 0.899399
Epoch 9.46: Loss = 1.03433
Epoch 9.47: Loss = 0.933304
Epoch 9.48: Loss = 0.828857
Epoch 9.49: Loss = 0.998322
Epoch 9.50: Loss = 1.08788
Epoch 9.51: Loss = 0.867035
Epoch 9.52: Loss = 0.898941
Epoch 9.53: Loss = 0.698349
Epoch 9.54: Loss = 0.8573
Epoch 9.55: Loss = 1.00592
Epoch 9.56: Loss = 0.913483
Epoch 9.57: Loss = 0.8414
Epoch 9.58: Loss = 1.04364
Epoch 9.59: Loss = 0.934982
Epoch 9.60: Loss = 0.833588
Epoch 9.61: Loss = 1.01111
Epoch 9.62: Loss = 1.06917
Epoch 9.63: Loss = 0.737991
Epoch 9.64: Loss = 0.935989
Epoch 9.65: Loss = 1.03181
Epoch 9.66: Loss = 0.676041
Epoch 9.67: Loss = 0.855331
Epoch 9.68: Loss = 0.912292
Epoch 9.69: Loss = 1.08057
Epoch 9.70: Loss = 0.73204
Epoch 9.71: Loss = 0.899338
Epoch 9.72: Loss = 0.941696
Epoch 9.73: Loss = 0.940567
Epoch 9.74: Loss = 0.842072
Epoch 9.75: Loss = 0.987381
Epoch 9.76: Loss = 0.861877
Epoch 9.77: Loss = 0.944687
Epoch 9.78: Loss = 0.91806
Epoch 9.79: Loss = 0.877472
Epoch 9.80: Loss = 0.785004
Epoch 9.81: Loss = 1.05534
Epoch 9.82: Loss = 0.871979
Epoch 9.83: Loss = 1.05223
Epoch 9.84: Loss = 0.868713
Epoch 9.85: Loss = 0.998352
Epoch 9.86: Loss = 0.87236
Epoch 9.87: Loss = 0.903778
Epoch 9.88: Loss = 0.792068
Epoch 9.89: Loss = 0.799698
Epoch 9.90: Loss = 0.979568
Epoch 9.91: Loss = 0.718109
Epoch 9.92: Loss = 0.973129
Epoch 9.93: Loss = 0.906815
Epoch 9.94: Loss = 0.916962
Epoch 9.95: Loss = 0.9189
Epoch 9.96: Loss = 0.973434
Epoch 9.97: Loss = 0.996964
Epoch 9.98: Loss = 0.791763
Epoch 9.99: Loss = 0.937241
Epoch 9.100: Loss = 0.875076
TRAIN LOSS = 0.911743
TRAIN ACC = 81.7795 % (49070/60000)
Loss = 0.892059
Loss = 1.00017
Loss = 1.30334
Loss = 1.42024
Loss = 0.917404
Loss = 1.06429
Loss = 1.20502
Loss = 1.05045
Loss = 0.74382
Loss = 0.564529
Loss = 0.703705
Loss = 0.768677
Loss = 0.513657
Loss = 0.696548
Loss = 0.18248
Loss = 0.41748
Loss = 1.32549
TEST LOSS = 0.859651
TEST ACC = 490.7 % (8292/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 1.0896
Epoch 10.2: Loss = 0.843796
Epoch 10.3: Loss = 1.02541
Epoch 10.4: Loss = 1.04811
Epoch 10.5: Loss = 0.99823
Epoch 10.6: Loss = 0.916702
Epoch 10.7: Loss = 1.00636
Epoch 10.8: Loss = 1.00145
Epoch 10.9: Loss = 0.899384
Epoch 10.10: Loss = 1.06813
Epoch 10.11: Loss = 1.02087
Epoch 10.12: Loss = 0.877335
Epoch 10.13: Loss = 1.05621
Epoch 10.14: Loss = 1.19031
Epoch 10.15: Loss = 0.934006
Epoch 10.16: Loss = 0.940002
Epoch 10.17: Loss = 0.622604
Epoch 10.18: Loss = 1.04019
Epoch 10.19: Loss = 0.769653
Epoch 10.20: Loss = 0.893326
Epoch 10.21: Loss = 1.1163
Epoch 10.22: Loss = 0.885635
Epoch 10.23: Loss = 0.733826
Epoch 10.24: Loss = 0.873215
Epoch 10.25: Loss = 0.835083
Epoch 10.26: Loss = 0.841217
Epoch 10.27: Loss = 1.05598
Epoch 10.28: Loss = 0.870193
Epoch 10.29: Loss = 0.729858
Epoch 10.30: Loss = 0.823318
Epoch 10.31: Loss = 0.934967
Epoch 10.32: Loss = 0.699356
Epoch 10.33: Loss = 0.878571
Epoch 10.34: Loss = 0.735855
Epoch 10.35: Loss = 0.86499
Epoch 10.36: Loss = 0.834183
Epoch 10.37: Loss = 0.960495
Epoch 10.38: Loss = 0.821259
Epoch 10.39: Loss = 0.965317
Epoch 10.40: Loss = 0.821335
Epoch 10.41: Loss = 0.802872
Epoch 10.42: Loss = 1.0202
Epoch 10.43: Loss = 0.92334
Epoch 10.44: Loss = 1.11804
Epoch 10.45: Loss = 0.719559
Epoch 10.46: Loss = 1.07292
Epoch 10.47: Loss = 0.843246
Epoch 10.48: Loss = 0.779144
Epoch 10.49: Loss = 0.850235
Epoch 10.50: Loss = 0.762238
Epoch 10.51: Loss = 0.842834
Epoch 10.52: Loss = 0.92067
Epoch 10.53: Loss = 0.848984
Epoch 10.54: Loss = 0.773956
Epoch 10.55: Loss = 1.02075
Epoch 10.56: Loss = 0.816833
Epoch 10.57: Loss = 0.907562
Epoch 10.58: Loss = 0.775681
Epoch 10.59: Loss = 1.05881
Epoch 10.60: Loss = 0.867294
Epoch 10.61: Loss = 0.844208
Epoch 10.62: Loss = 0.865646
Epoch 10.63: Loss = 0.908279
Epoch 10.64: Loss = 0.804779
Epoch 10.65: Loss = 0.818085
Epoch 10.66: Loss = 0.963379
Epoch 10.67: Loss = 0.93779
Epoch 10.68: Loss = 0.948212
Epoch 10.69: Loss = 0.918808
Epoch 10.70: Loss = 1.01212
Epoch 10.71: Loss = 1.04117
Epoch 10.72: Loss = 0.820435
Epoch 10.73: Loss = 0.989304
Epoch 10.74: Loss = 1.00887
Epoch 10.75: Loss = 0.82486
Epoch 10.76: Loss = 1.04814
Epoch 10.77: Loss = 0.925125
Epoch 10.78: Loss = 0.898651
Epoch 10.79: Loss = 0.741745
Epoch 10.80: Loss = 1.00969
Epoch 10.81: Loss = 0.91423
Epoch 10.82: Loss = 0.923981
Epoch 10.83: Loss = 0.957397
Epoch 10.84: Loss = 0.832306
Epoch 10.85: Loss = 0.887939
Epoch 10.86: Loss = 0.77066
Epoch 10.87: Loss = 0.727707
Epoch 10.88: Loss = 0.826691
Epoch 10.89: Loss = 1.17393
Epoch 10.90: Loss = 0.753998
Epoch 10.91: Loss = 0.972153
Epoch 10.92: Loss = 0.922104
Epoch 10.93: Loss = 0.991791
Epoch 10.94: Loss = 0.774475
Epoch 10.95: Loss = 1.00478
Epoch 10.96: Loss = 1.10384
Epoch 10.97: Loss = 1.03934
Epoch 10.98: Loss = 1.02913
Epoch 10.99: Loss = 0.862747
Epoch 10.100: Loss = 0.937881
TRAIN LOSS = 0.909851
TRAIN ACC = 82.3441 % (49409/60000)
Loss = 0.93071
Loss = 1.0013
Loss = 1.30264
Loss = 1.39159
Loss = 0.863922
Loss = 1.06384
Loss = 1.21466
Loss = 1.00714
Loss = 0.717133
Loss = 0.579758
Loss = 0.715088
Loss = 0.7948
Loss = 0.545914
Loss = 0.681473
Loss = 0.172379
Loss = 0.491119
Loss = 1.34006
TEST LOSS = 0.86201
TEST ACC = 494.089 % (8294/10000)

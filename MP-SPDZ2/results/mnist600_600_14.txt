Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 600]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 10
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 3.5
***********************************************************
Epoch 1.1: Loss = 2.34785
Epoch 1.2: Loss = 2.31102
Epoch 1.3: Loss = 2.25899
Epoch 1.4: Loss = 2.22716
Epoch 1.5: Loss = 2.17496
Epoch 1.6: Loss = 2.11833
Epoch 1.7: Loss = 2.07632
Epoch 1.8: Loss = 2.06354
Epoch 1.9: Loss = 2.02016
Epoch 1.10: Loss = 1.98064
Epoch 1.11: Loss = 1.9189
Epoch 1.12: Loss = 1.86801
Epoch 1.13: Loss = 1.86378
Epoch 1.14: Loss = 1.83629
Epoch 1.15: Loss = 1.81453
Epoch 1.16: Loss = 1.75916
Epoch 1.17: Loss = 1.72128
Epoch 1.18: Loss = 1.69202
Epoch 1.19: Loss = 1.65987
Epoch 1.20: Loss = 1.64925
Epoch 1.21: Loss = 1.59047
Epoch 1.22: Loss = 1.57434
Epoch 1.23: Loss = 1.51018
Epoch 1.24: Loss = 1.52637
Epoch 1.25: Loss = 1.51564
Epoch 1.26: Loss = 1.41306
Epoch 1.27: Loss = 1.40813
Epoch 1.28: Loss = 1.40112
Epoch 1.29: Loss = 1.32832
Epoch 1.30: Loss = 1.40846
Epoch 1.31: Loss = 1.32613
Epoch 1.32: Loss = 1.23253
Epoch 1.33: Loss = 1.28615
Epoch 1.34: Loss = 1.21829
Epoch 1.35: Loss = 1.28563
Epoch 1.36: Loss = 1.17656
Epoch 1.37: Loss = 1.19351
Epoch 1.38: Loss = 1.18582
Epoch 1.39: Loss = 1.09962
Epoch 1.40: Loss = 1.16418
Epoch 1.41: Loss = 1.11847
Epoch 1.42: Loss = 1.09541
Epoch 1.43: Loss = 1.07497
Epoch 1.44: Loss = 1.092
Epoch 1.45: Loss = 1.03166
Epoch 1.46: Loss = 1.01129
Epoch 1.47: Loss = 1.0439
Epoch 1.48: Loss = 1.02859
Epoch 1.49: Loss = 0.958267
Epoch 1.50: Loss = 0.978394
Epoch 1.51: Loss = 0.986343
Epoch 1.52: Loss = 0.945374
Epoch 1.53: Loss = 0.938507
Epoch 1.54: Loss = 0.911896
Epoch 1.55: Loss = 0.879318
Epoch 1.56: Loss = 0.923401
Epoch 1.57: Loss = 0.914551
Epoch 1.58: Loss = 0.878113
Epoch 1.59: Loss = 0.897507
Epoch 1.60: Loss = 0.842865
Epoch 1.61: Loss = 0.842102
Epoch 1.62: Loss = 0.923737
Epoch 1.63: Loss = 0.803223
Epoch 1.64: Loss = 0.845779
Epoch 1.65: Loss = 0.805649
Epoch 1.66: Loss = 0.834259
Epoch 1.67: Loss = 0.787582
Epoch 1.68: Loss = 0.802765
Epoch 1.69: Loss = 0.768692
Epoch 1.70: Loss = 0.742905
Epoch 1.71: Loss = 0.692993
Epoch 1.72: Loss = 0.733841
Epoch 1.73: Loss = 0.788177
Epoch 1.74: Loss = 0.746841
Epoch 1.75: Loss = 0.722458
Epoch 1.76: Loss = 0.736374
Epoch 1.77: Loss = 0.715881
Epoch 1.78: Loss = 0.702194
Epoch 1.79: Loss = 0.740234
Epoch 1.80: Loss = 0.744308
Epoch 1.81: Loss = 0.706451
Epoch 1.82: Loss = 0.779007
Epoch 1.83: Loss = 0.72789
Epoch 1.84: Loss = 0.696548
Epoch 1.85: Loss = 0.679092
Epoch 1.86: Loss = 0.623962
Epoch 1.87: Loss = 0.665131
Epoch 1.88: Loss = 0.654968
Epoch 1.89: Loss = 0.655655
Epoch 1.90: Loss = 0.630936
Epoch 1.91: Loss = 0.676025
Epoch 1.92: Loss = 0.672623
Epoch 1.93: Loss = 0.637344
Epoch 1.94: Loss = 0.616791
Epoch 1.95: Loss = 0.643448
Epoch 1.96: Loss = 0.662781
Epoch 1.97: Loss = 0.712128
Epoch 1.98: Loss = 0.638321
Epoch 1.99: Loss = 0.623184
Epoch 1.100: Loss = 0.598801
TRAIN LOSS = 1.14543
TRAIN ACC = 69.6014 % (41762/60000)
Loss = 0.6828
Loss = 0.664963
Loss = 0.811157
Loss = 0.724548
Loss = 0.65123
Loss = 0.668686
Loss = 0.741867
Loss = 0.735382
Loss = 0.555725
Loss = 0.501862
Loss = 0.446457
Loss = 0.548508
Loss = 0.51268
Loss = 0.511978
Loss = 0.308151
Loss = 0.469315
Loss = 0.81955
TEST LOSS = 0.6049
TEST ACC = 417.619 % (8252/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.635727
Epoch 2.2: Loss = 0.595367
Epoch 2.3: Loss = 0.603668
Epoch 2.4: Loss = 0.680115
Epoch 2.5: Loss = 0.597031
Epoch 2.6: Loss = 0.554932
Epoch 2.7: Loss = 0.569031
Epoch 2.8: Loss = 0.646576
Epoch 2.9: Loss = 0.591003
Epoch 2.10: Loss = 0.575439
Epoch 2.11: Loss = 0.548264
Epoch 2.12: Loss = 0.584793
Epoch 2.13: Loss = 0.602737
Epoch 2.14: Loss = 0.598358
Epoch 2.15: Loss = 0.605164
Epoch 2.16: Loss = 0.581268
Epoch 2.17: Loss = 0.599213
Epoch 2.18: Loss = 0.67691
Epoch 2.19: Loss = 0.547455
Epoch 2.20: Loss = 0.569382
Epoch 2.21: Loss = 0.597061
Epoch 2.22: Loss = 0.553543
Epoch 2.23: Loss = 0.631973
Epoch 2.24: Loss = 0.575302
Epoch 2.25: Loss = 0.595764
Epoch 2.26: Loss = 0.486588
Epoch 2.27: Loss = 0.591629
Epoch 2.28: Loss = 0.578796
Epoch 2.29: Loss = 0.520721
Epoch 2.30: Loss = 0.540329
Epoch 2.31: Loss = 0.522049
Epoch 2.32: Loss = 0.520737
Epoch 2.33: Loss = 0.579742
Epoch 2.34: Loss = 0.523132
Epoch 2.35: Loss = 0.586456
Epoch 2.36: Loss = 0.513702
Epoch 2.37: Loss = 0.506668
Epoch 2.38: Loss = 0.508682
Epoch 2.39: Loss = 0.537003
Epoch 2.40: Loss = 0.520645
Epoch 2.41: Loss = 0.53685
Epoch 2.42: Loss = 0.545944
Epoch 2.43: Loss = 0.518112
Epoch 2.44: Loss = 0.634216
Epoch 2.45: Loss = 0.547379
Epoch 2.46: Loss = 0.529037
Epoch 2.47: Loss = 0.436493
Epoch 2.48: Loss = 0.551941
Epoch 2.49: Loss = 0.568954
Epoch 2.50: Loss = 0.468857
Epoch 2.51: Loss = 0.509552
Epoch 2.52: Loss = 0.48349
Epoch 2.53: Loss = 0.527222
Epoch 2.54: Loss = 0.541382
Epoch 2.55: Loss = 0.496994
Epoch 2.56: Loss = 0.513107
Epoch 2.57: Loss = 0.539398
Epoch 2.58: Loss = 0.527924
Epoch 2.59: Loss = 0.472687
Epoch 2.60: Loss = 0.435867
Epoch 2.61: Loss = 0.560562
Epoch 2.62: Loss = 0.504089
Epoch 2.63: Loss = 0.481476
Epoch 2.64: Loss = 0.556381
Epoch 2.65: Loss = 0.516907
Epoch 2.66: Loss = 0.505325
Epoch 2.67: Loss = 0.509079
Epoch 2.68: Loss = 0.562866
Epoch 2.69: Loss = 0.570129
Epoch 2.70: Loss = 0.453476
Epoch 2.71: Loss = 0.449402
Epoch 2.72: Loss = 0.526352
Epoch 2.73: Loss = 0.524887
Epoch 2.74: Loss = 0.464966
Epoch 2.75: Loss = 0.490784
Epoch 2.76: Loss = 0.506866
Epoch 2.77: Loss = 0.506821
Epoch 2.78: Loss = 0.465714
Epoch 2.79: Loss = 0.474869
Epoch 2.80: Loss = 0.482147
Epoch 2.81: Loss = 0.507965
Epoch 2.82: Loss = 0.488937
Epoch 2.83: Loss = 0.477173
Epoch 2.84: Loss = 0.502899
Epoch 2.85: Loss = 0.437271
Epoch 2.86: Loss = 0.471985
Epoch 2.87: Loss = 0.475021
Epoch 2.88: Loss = 0.557053
Epoch 2.89: Loss = 0.488815
Epoch 2.90: Loss = 0.456955
Epoch 2.91: Loss = 0.509293
Epoch 2.92: Loss = 0.492386
Epoch 2.93: Loss = 0.499786
Epoch 2.94: Loss = 0.467087
Epoch 2.95: Loss = 0.46489
Epoch 2.96: Loss = 0.493454
Epoch 2.97: Loss = 0.482925
Epoch 2.98: Loss = 0.464218
Epoch 2.99: Loss = 0.530502
Epoch 2.100: Loss = 0.396729
TRAIN LOSS = 0.531158
TRAIN ACC = 84.2972 % (50581/60000)
Loss = 0.501633
Loss = 0.508194
Loss = 0.659592
Loss = 0.58934
Loss = 0.472
Loss = 0.49884
Loss = 0.613907
Loss = 0.562378
Loss = 0.398743
Loss = 0.348724
Loss = 0.326782
Loss = 0.372833
Loss = 0.330353
Loss = 0.332687
Loss = 0.156799
Loss = 0.316864
Loss = 0.661209
TEST LOSS = 0.445828
TEST ACC = 505.809 % (8678/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.489075
Epoch 3.2: Loss = 0.50354
Epoch 3.3: Loss = 0.485199
Epoch 3.4: Loss = 0.494843
Epoch 3.5: Loss = 0.473236
Epoch 3.6: Loss = 0.539536
Epoch 3.7: Loss = 0.446884
Epoch 3.8: Loss = 0.450668
Epoch 3.9: Loss = 0.546066
Epoch 3.10: Loss = 0.499893
Epoch 3.11: Loss = 0.440063
Epoch 3.12: Loss = 0.5103
Epoch 3.13: Loss = 0.429321
Epoch 3.14: Loss = 0.434753
Epoch 3.15: Loss = 0.459015
Epoch 3.16: Loss = 0.542252
Epoch 3.17: Loss = 0.439743
Epoch 3.18: Loss = 0.533707
Epoch 3.19: Loss = 0.471939
Epoch 3.20: Loss = 0.458557
Epoch 3.21: Loss = 0.410812
Epoch 3.22: Loss = 0.431946
Epoch 3.23: Loss = 0.515701
Epoch 3.24: Loss = 0.43338
Epoch 3.25: Loss = 0.482666
Epoch 3.26: Loss = 0.477509
Epoch 3.27: Loss = 0.439178
Epoch 3.28: Loss = 0.473328
Epoch 3.29: Loss = 0.484192
Epoch 3.30: Loss = 0.458969
Epoch 3.31: Loss = 0.401993
Epoch 3.32: Loss = 0.473068
Epoch 3.33: Loss = 0.463196
Epoch 3.34: Loss = 0.374588
Epoch 3.35: Loss = 0.425339
Epoch 3.36: Loss = 0.529678
Epoch 3.37: Loss = 0.436005
Epoch 3.38: Loss = 0.40686
Epoch 3.39: Loss = 0.469025
Epoch 3.40: Loss = 0.399994
Epoch 3.41: Loss = 0.466339
Epoch 3.42: Loss = 0.451508
Epoch 3.43: Loss = 0.445572
Epoch 3.44: Loss = 0.466965
Epoch 3.45: Loss = 0.432999
Epoch 3.46: Loss = 0.399063
Epoch 3.47: Loss = 0.448532
Epoch 3.48: Loss = 0.39859
Epoch 3.49: Loss = 0.424103
Epoch 3.50: Loss = 0.432465
Epoch 3.51: Loss = 0.417633
Epoch 3.52: Loss = 0.468567
Epoch 3.53: Loss = 0.421066
Epoch 3.54: Loss = 0.499023
Epoch 3.55: Loss = 0.471725
Epoch 3.56: Loss = 0.440887
Epoch 3.57: Loss = 0.44455
Epoch 3.58: Loss = 0.416763
Epoch 3.59: Loss = 0.474503
Epoch 3.60: Loss = 0.442505
Epoch 3.61: Loss = 0.437653
Epoch 3.62: Loss = 0.43129
Epoch 3.63: Loss = 0.413666
Epoch 3.64: Loss = 0.498505
Epoch 3.65: Loss = 0.38768
Epoch 3.66: Loss = 0.444153
Epoch 3.67: Loss = 0.427612
Epoch 3.68: Loss = 0.445267
Epoch 3.69: Loss = 0.457336
Epoch 3.70: Loss = 0.392975
Epoch 3.71: Loss = 0.431381
Epoch 3.72: Loss = 0.439331
Epoch 3.73: Loss = 0.47493
Epoch 3.74: Loss = 0.478867
Epoch 3.75: Loss = 0.450134
Epoch 3.76: Loss = 0.429779
Epoch 3.77: Loss = 0.402771
Epoch 3.78: Loss = 0.502945
Epoch 3.79: Loss = 0.47995
Epoch 3.80: Loss = 0.499466
Epoch 3.81: Loss = 0.455338
Epoch 3.82: Loss = 0.464157
Epoch 3.83: Loss = 0.438248
Epoch 3.84: Loss = 0.406265
Epoch 3.85: Loss = 0.482651
Epoch 3.86: Loss = 0.404022
Epoch 3.87: Loss = 0.409439
Epoch 3.88: Loss = 0.470322
Epoch 3.89: Loss = 0.423492
Epoch 3.90: Loss = 0.402588
Epoch 3.91: Loss = 0.417023
Epoch 3.92: Loss = 0.427841
Epoch 3.93: Loss = 0.481155
Epoch 3.94: Loss = 0.440048
Epoch 3.95: Loss = 0.369019
Epoch 3.96: Loss = 0.332748
Epoch 3.97: Loss = 0.396973
Epoch 3.98: Loss = 0.48497
Epoch 3.99: Loss = 0.418823
Epoch 3.100: Loss = 0.402832
TRAIN LOSS = 0.449539
TRAIN ACC = 86.4594 % (51878/60000)
Loss = 0.45108
Loss = 0.469986
Loss = 0.63533
Loss = 0.56308
Loss = 0.428192
Loss = 0.45282
Loss = 0.584091
Loss = 0.514145
Loss = 0.346237
Loss = 0.303101
Loss = 0.295486
Loss = 0.312897
Loss = 0.270645
Loss = 0.292542
Loss = 0.11377
Loss = 0.292404
Loss = 0.628571
TEST LOSS = 0.404691
TEST ACC = 518.779 % (8786/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.463852
Epoch 4.2: Loss = 0.447723
Epoch 4.3: Loss = 0.500992
Epoch 4.4: Loss = 0.468399
Epoch 4.5: Loss = 0.404739
Epoch 4.6: Loss = 0.416367
Epoch 4.7: Loss = 0.492844
Epoch 4.8: Loss = 0.433578
Epoch 4.9: Loss = 0.322968
Epoch 4.10: Loss = 0.443481
Epoch 4.11: Loss = 0.478104
Epoch 4.12: Loss = 0.451248
Epoch 4.13: Loss = 0.432068
Epoch 4.14: Loss = 0.41713
Epoch 4.15: Loss = 0.474716
Epoch 4.16: Loss = 0.371643
Epoch 4.17: Loss = 0.415298
Epoch 4.18: Loss = 0.473709
Epoch 4.19: Loss = 0.477051
Epoch 4.20: Loss = 0.440552
Epoch 4.21: Loss = 0.410736
Epoch 4.22: Loss = 0.387436
Epoch 4.23: Loss = 0.488678
Epoch 4.24: Loss = 0.462646
Epoch 4.25: Loss = 0.473785
Epoch 4.26: Loss = 0.394012
Epoch 4.27: Loss = 0.351532
Epoch 4.28: Loss = 0.371841
Epoch 4.29: Loss = 0.417755
Epoch 4.30: Loss = 0.361694
Epoch 4.31: Loss = 0.382294
Epoch 4.32: Loss = 0.365738
Epoch 4.33: Loss = 0.436951
Epoch 4.34: Loss = 0.488098
Epoch 4.35: Loss = 0.448303
Epoch 4.36: Loss = 0.432922
Epoch 4.37: Loss = 0.472931
Epoch 4.38: Loss = 0.414276
Epoch 4.39: Loss = 0.423431
Epoch 4.40: Loss = 0.397842
Epoch 4.41: Loss = 0.367554
Epoch 4.42: Loss = 0.438446
Epoch 4.43: Loss = 0.45166
Epoch 4.44: Loss = 0.384857
Epoch 4.45: Loss = 0.398788
Epoch 4.46: Loss = 0.449661
Epoch 4.47: Loss = 0.434097
Epoch 4.48: Loss = 0.364685
Epoch 4.49: Loss = 0.45929
Epoch 4.50: Loss = 0.488449
Epoch 4.51: Loss = 0.435272
Epoch 4.52: Loss = 0.391998
Epoch 4.53: Loss = 0.426407
Epoch 4.54: Loss = 0.494858
Epoch 4.55: Loss = 0.433212
Epoch 4.56: Loss = 0.46489
Epoch 4.57: Loss = 0.314011
Epoch 4.58: Loss = 0.426361
Epoch 4.59: Loss = 0.459229
Epoch 4.60: Loss = 0.392914
Epoch 4.61: Loss = 0.442841
Epoch 4.62: Loss = 0.417084
Epoch 4.63: Loss = 0.417999
Epoch 4.64: Loss = 0.379929
Epoch 4.65: Loss = 0.42038
Epoch 4.66: Loss = 0.339142
Epoch 4.67: Loss = 0.431061
Epoch 4.68: Loss = 0.42923
Epoch 4.69: Loss = 0.41806
Epoch 4.70: Loss = 0.440445
Epoch 4.71: Loss = 0.409256
Epoch 4.72: Loss = 0.355606
Epoch 4.73: Loss = 0.425705
Epoch 4.74: Loss = 0.400467
Epoch 4.75: Loss = 0.397293
Epoch 4.76: Loss = 0.539581
Epoch 4.77: Loss = 0.441879
Epoch 4.78: Loss = 0.43811
Epoch 4.79: Loss = 0.477127
Epoch 4.80: Loss = 0.417999
Epoch 4.81: Loss = 0.353119
Epoch 4.82: Loss = 0.421555
Epoch 4.83: Loss = 0.485199
Epoch 4.84: Loss = 0.463654
Epoch 4.85: Loss = 0.353912
Epoch 4.86: Loss = 0.406647
Epoch 4.87: Loss = 0.387405
Epoch 4.88: Loss = 0.476715
Epoch 4.89: Loss = 0.495255
Epoch 4.90: Loss = 0.369217
Epoch 4.91: Loss = 0.480881
Epoch 4.92: Loss = 0.399475
Epoch 4.93: Loss = 0.340698
Epoch 4.94: Loss = 0.361633
Epoch 4.95: Loss = 0.338181
Epoch 4.96: Loss = 0.327072
Epoch 4.97: Loss = 0.397812
Epoch 4.98: Loss = 0.463776
Epoch 4.99: Loss = 0.466934
Epoch 4.100: Loss = 0.459976
TRAIN LOSS = 0.423752
TRAIN ACC = 87.294 % (52380/60000)
Loss = 0.443085
Loss = 0.455887
Loss = 0.624344
Loss = 0.550735
Loss = 0.429703
Loss = 0.434402
Loss = 0.587891
Loss = 0.512375
Loss = 0.334244
Loss = 0.273773
Loss = 0.29158
Loss = 0.291824
Loss = 0.241943
Loss = 0.277039
Loss = 0.0955353
Loss = 0.274551
Loss = 0.628601
TEST LOSS = 0.392279
TEST ACC = 523.799 % (8862/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.432083
Epoch 5.2: Loss = 0.283249
Epoch 5.3: Loss = 0.372925
Epoch 5.4: Loss = 0.337997
Epoch 5.5: Loss = 0.419098
Epoch 5.6: Loss = 0.377777
Epoch 5.7: Loss = 0.43045
Epoch 5.8: Loss = 0.450089
Epoch 5.9: Loss = 0.472504
Epoch 5.10: Loss = 0.380615
Epoch 5.11: Loss = 0.403412
Epoch 5.12: Loss = 0.378281
Epoch 5.13: Loss = 0.441986
Epoch 5.14: Loss = 0.439224
Epoch 5.15: Loss = 0.416626
Epoch 5.16: Loss = 0.557938
Epoch 5.17: Loss = 0.438904
Epoch 5.18: Loss = 0.442215
Epoch 5.19: Loss = 0.467972
Epoch 5.20: Loss = 0.453903
Epoch 5.21: Loss = 0.441132
Epoch 5.22: Loss = 0.361084
Epoch 5.23: Loss = 0.531982
Epoch 5.24: Loss = 0.427979
Epoch 5.25: Loss = 0.407791
Epoch 5.26: Loss = 0.408875
Epoch 5.27: Loss = 0.442657
Epoch 5.28: Loss = 0.39476
Epoch 5.29: Loss = 0.366577
Epoch 5.30: Loss = 0.331421
Epoch 5.31: Loss = 0.395157
Epoch 5.32: Loss = 0.443741
Epoch 5.33: Loss = 0.396454
Epoch 5.34: Loss = 0.360199
Epoch 5.35: Loss = 0.474258
Epoch 5.36: Loss = 0.424347
Epoch 5.37: Loss = 0.431488
Epoch 5.38: Loss = 0.363937
Epoch 5.39: Loss = 0.367035
Epoch 5.40: Loss = 0.356369
Epoch 5.41: Loss = 0.501511
Epoch 5.42: Loss = 0.402664
Epoch 5.43: Loss = 0.390015
Epoch 5.44: Loss = 0.405701
Epoch 5.45: Loss = 0.407394
Epoch 5.46: Loss = 0.493423
Epoch 5.47: Loss = 0.390945
Epoch 5.48: Loss = 0.378021
Epoch 5.49: Loss = 0.455902
Epoch 5.50: Loss = 0.507721
Epoch 5.51: Loss = 0.42836
Epoch 5.52: Loss = 0.37413
Epoch 5.53: Loss = 0.403992
Epoch 5.54: Loss = 0.45694
Epoch 5.55: Loss = 0.487274
Epoch 5.56: Loss = 0.443054
Epoch 5.57: Loss = 0.426666
Epoch 5.58: Loss = 0.381912
Epoch 5.59: Loss = 0.44899
Epoch 5.60: Loss = 0.393997
Epoch 5.61: Loss = 0.428513
Epoch 5.62: Loss = 0.360474
Epoch 5.63: Loss = 0.392075
Epoch 5.64: Loss = 0.424667
Epoch 5.65: Loss = 0.398682
Epoch 5.66: Loss = 0.466553
Epoch 5.67: Loss = 0.392395
Epoch 5.68: Loss = 0.381454
Epoch 5.69: Loss = 0.490723
Epoch 5.70: Loss = 0.446152
Epoch 5.71: Loss = 0.411163
Epoch 5.72: Loss = 0.418488
Epoch 5.73: Loss = 0.370407
Epoch 5.74: Loss = 0.423264
Epoch 5.75: Loss = 0.348282
Epoch 5.76: Loss = 0.364609
Epoch 5.77: Loss = 0.382294
Epoch 5.78: Loss = 0.445663
Epoch 5.79: Loss = 0.502045
Epoch 5.80: Loss = 0.413239
Epoch 5.81: Loss = 0.426178
Epoch 5.82: Loss = 0.364929
Epoch 5.83: Loss = 0.389053
Epoch 5.84: Loss = 0.396469
Epoch 5.85: Loss = 0.375031
Epoch 5.86: Loss = 0.446503
Epoch 5.87: Loss = 0.396042
Epoch 5.88: Loss = 0.349426
Epoch 5.89: Loss = 0.365051
Epoch 5.90: Loss = 0.420654
Epoch 5.91: Loss = 0.369766
Epoch 5.92: Loss = 0.482285
Epoch 5.93: Loss = 0.36171
Epoch 5.94: Loss = 0.397934
Epoch 5.95: Loss = 0.414124
Epoch 5.96: Loss = 0.4263
Epoch 5.97: Loss = 0.343933
Epoch 5.98: Loss = 0.370087
Epoch 5.99: Loss = 0.304886
Epoch 5.100: Loss = 0.362045
TRAIN LOSS = 0.411285
TRAIN ACC = 87.8159 % (52692/60000)
Loss = 0.418991
Loss = 0.435318
Loss = 0.608185
Loss = 0.54155
Loss = 0.411301
Loss = 0.416443
Loss = 0.567963
Loss = 0.487778
Loss = 0.327271
Loss = 0.271545
Loss = 0.295242
Loss = 0.282486
Loss = 0.23674
Loss = 0.276337
Loss = 0.0832062
Loss = 0.244797
Loss = 0.615082
TEST LOSS = 0.378912
TEST ACC = 526.92 % (8927/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.451996
Epoch 6.2: Loss = 0.447968
Epoch 6.3: Loss = 0.487823
Epoch 6.4: Loss = 0.341293
Epoch 6.5: Loss = 0.426437
Epoch 6.6: Loss = 0.294922
Epoch 6.7: Loss = 0.43782
Epoch 6.8: Loss = 0.399109
Epoch 6.9: Loss = 0.398239
Epoch 6.10: Loss = 0.412476
Epoch 6.11: Loss = 0.396729
Epoch 6.12: Loss = 0.457626
Epoch 6.13: Loss = 0.377975
Epoch 6.14: Loss = 0.488403
Epoch 6.15: Loss = 0.417862
Epoch 6.16: Loss = 0.414978
Epoch 6.17: Loss = 0.403641
Epoch 6.18: Loss = 0.367279
Epoch 6.19: Loss = 0.393585
Epoch 6.20: Loss = 0.391052
Epoch 6.21: Loss = 0.367828
Epoch 6.22: Loss = 0.32637
Epoch 6.23: Loss = 0.488052
Epoch 6.24: Loss = 0.413849
Epoch 6.25: Loss = 0.395615
Epoch 6.26: Loss = 0.374054
Epoch 6.27: Loss = 0.384094
Epoch 6.28: Loss = 0.341446
Epoch 6.29: Loss = 0.359375
Epoch 6.30: Loss = 0.399719
Epoch 6.31: Loss = 0.341049
Epoch 6.32: Loss = 0.441391
Epoch 6.33: Loss = 0.382187
Epoch 6.34: Loss = 0.420761
Epoch 6.35: Loss = 0.515381
Epoch 6.36: Loss = 0.320755
Epoch 6.37: Loss = 0.429749
Epoch 6.38: Loss = 0.449753
Epoch 6.39: Loss = 0.434341
Epoch 6.40: Loss = 0.465149
Epoch 6.41: Loss = 0.338608
Epoch 6.42: Loss = 0.400909
Epoch 6.43: Loss = 0.369263
Epoch 6.44: Loss = 0.414551
Epoch 6.45: Loss = 0.436966
Epoch 6.46: Loss = 0.37262
Epoch 6.47: Loss = 0.340134
Epoch 6.48: Loss = 0.411179
Epoch 6.49: Loss = 0.367981
Epoch 6.50: Loss = 0.456451
Epoch 6.51: Loss = 0.316696
Epoch 6.52: Loss = 0.479996
Epoch 6.53: Loss = 0.39241
Epoch 6.54: Loss = 0.359161
Epoch 6.55: Loss = 0.507614
Epoch 6.56: Loss = 0.366516
Epoch 6.57: Loss = 0.47522
Epoch 6.58: Loss = 0.334167
Epoch 6.59: Loss = 0.384644
Epoch 6.60: Loss = 0.471497
Epoch 6.61: Loss = 0.367294
Epoch 6.62: Loss = 0.359436
Epoch 6.63: Loss = 0.418991
Epoch 6.64: Loss = 0.313263
Epoch 6.65: Loss = 0.370972
Epoch 6.66: Loss = 0.433105
Epoch 6.67: Loss = 0.322922
Epoch 6.68: Loss = 0.383713
Epoch 6.69: Loss = 0.410095
Epoch 6.70: Loss = 0.38385
Epoch 6.71: Loss = 0.380966
Epoch 6.72: Loss = 0.318634
Epoch 6.73: Loss = 0.392914
Epoch 6.74: Loss = 0.407822
Epoch 6.75: Loss = 0.400818
Epoch 6.76: Loss = 0.356461
Epoch 6.77: Loss = 0.387543
Epoch 6.78: Loss = 0.449036
Epoch 6.79: Loss = 0.40477
Epoch 6.80: Loss = 0.42218
Epoch 6.81: Loss = 0.444687
Epoch 6.82: Loss = 0.356339
Epoch 6.83: Loss = 0.446686
Epoch 6.84: Loss = 0.355453
Epoch 6.85: Loss = 0.437149
Epoch 6.86: Loss = 0.398895
Epoch 6.87: Loss = 0.528992
Epoch 6.88: Loss = 0.310349
Epoch 6.89: Loss = 0.345352
Epoch 6.90: Loss = 0.421097
Epoch 6.91: Loss = 0.41954
Epoch 6.92: Loss = 0.359421
Epoch 6.93: Loss = 0.474319
Epoch 6.94: Loss = 0.430923
Epoch 6.95: Loss = 0.403473
Epoch 6.96: Loss = 0.408737
Epoch 6.97: Loss = 0.430054
Epoch 6.98: Loss = 0.353455
Epoch 6.99: Loss = 0.416672
Epoch 6.100: Loss = 0.412247
TRAIN LOSS = 0.40094
TRAIN ACC = 88.2004 % (52923/60000)
Loss = 0.407364
Loss = 0.428253
Loss = 0.59465
Loss = 0.543793
Loss = 0.416122
Loss = 0.415131
Loss = 0.549988
Loss = 0.482956
Loss = 0.32489
Loss = 0.270554
Loss = 0.314926
Loss = 0.26767
Loss = 0.217224
Loss = 0.283646
Loss = 0.0774841
Loss = 0.223724
Loss = 0.625992
TEST LOSS = 0.374142
TEST ACC = 529.23 % (8924/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.499176
Epoch 7.2: Loss = 0.469864
Epoch 7.3: Loss = 0.454422
Epoch 7.4: Loss = 0.36348
Epoch 7.5: Loss = 0.352951
Epoch 7.6: Loss = 0.426056
Epoch 7.7: Loss = 0.355408
Epoch 7.8: Loss = 0.376358
Epoch 7.9: Loss = 0.37999
Epoch 7.10: Loss = 0.375519
Epoch 7.11: Loss = 0.406189
Epoch 7.12: Loss = 0.407562
Epoch 7.13: Loss = 0.380325
Epoch 7.14: Loss = 0.347992
Epoch 7.15: Loss = 0.334244
Epoch 7.16: Loss = 0.36554
Epoch 7.17: Loss = 0.392456
Epoch 7.18: Loss = 0.479904
Epoch 7.19: Loss = 0.40477
Epoch 7.20: Loss = 0.377701
Epoch 7.21: Loss = 0.385315
Epoch 7.22: Loss = 0.437317
Epoch 7.23: Loss = 0.39299
Epoch 7.24: Loss = 0.31813
Epoch 7.25: Loss = 0.348404
Epoch 7.26: Loss = 0.428177
Epoch 7.27: Loss = 0.325531
Epoch 7.28: Loss = 0.403915
Epoch 7.29: Loss = 0.384003
Epoch 7.30: Loss = 0.366974
Epoch 7.31: Loss = 0.46669
Epoch 7.32: Loss = 0.430939
Epoch 7.33: Loss = 0.39299
Epoch 7.34: Loss = 0.375763
Epoch 7.35: Loss = 0.360962
Epoch 7.36: Loss = 0.309891
Epoch 7.37: Loss = 0.399811
Epoch 7.38: Loss = 0.413193
Epoch 7.39: Loss = 0.424545
Epoch 7.40: Loss = 0.36676
Epoch 7.41: Loss = 0.372299
Epoch 7.42: Loss = 0.321899
Epoch 7.43: Loss = 0.449661
Epoch 7.44: Loss = 0.512909
Epoch 7.45: Loss = 0.35321
Epoch 7.46: Loss = 0.336288
Epoch 7.47: Loss = 0.434677
Epoch 7.48: Loss = 0.327957
Epoch 7.49: Loss = 0.398575
Epoch 7.50: Loss = 0.491226
Epoch 7.51: Loss = 0.454926
Epoch 7.52: Loss = 0.411469
Epoch 7.53: Loss = 0.445236
Epoch 7.54: Loss = 0.381592
Epoch 7.55: Loss = 0.401459
Epoch 7.56: Loss = 0.437881
Epoch 7.57: Loss = 0.388443
Epoch 7.58: Loss = 0.435211
Epoch 7.59: Loss = 0.400818
Epoch 7.60: Loss = 0.431534
Epoch 7.61: Loss = 0.380676
Epoch 7.62: Loss = 0.369781
Epoch 7.63: Loss = 0.418808
Epoch 7.64: Loss = 0.329971
Epoch 7.65: Loss = 0.368408
Epoch 7.66: Loss = 0.380203
Epoch 7.67: Loss = 0.39328
Epoch 7.68: Loss = 0.416107
Epoch 7.69: Loss = 0.307816
Epoch 7.70: Loss = 0.402466
Epoch 7.71: Loss = 0.397476
Epoch 7.72: Loss = 0.390015
Epoch 7.73: Loss = 0.290726
Epoch 7.74: Loss = 0.41507
Epoch 7.75: Loss = 0.457687
Epoch 7.76: Loss = 0.350998
Epoch 7.77: Loss = 0.381332
Epoch 7.78: Loss = 0.39064
Epoch 7.79: Loss = 0.502457
Epoch 7.80: Loss = 0.331757
Epoch 7.81: Loss = 0.392441
Epoch 7.82: Loss = 0.453033
Epoch 7.83: Loss = 0.412918
Epoch 7.84: Loss = 0.440689
Epoch 7.85: Loss = 0.351563
Epoch 7.86: Loss = 0.371902
Epoch 7.87: Loss = 0.396423
Epoch 7.88: Loss = 0.332825
Epoch 7.89: Loss = 0.401581
Epoch 7.90: Loss = 0.391861
Epoch 7.91: Loss = 0.413544
Epoch 7.92: Loss = 0.369766
Epoch 7.93: Loss = 0.416656
Epoch 7.94: Loss = 0.362442
Epoch 7.95: Loss = 0.31575
Epoch 7.96: Loss = 0.489105
Epoch 7.97: Loss = 0.44455
Epoch 7.98: Loss = 0.468048
Epoch 7.99: Loss = 0.388596
Epoch 7.100: Loss = 0.414688
TRAIN LOSS = 0.395737
TRAIN ACC = 88.591 % (53157/60000)
Loss = 0.406982
Loss = 0.429367
Loss = 0.60228
Loss = 0.528992
Loss = 0.391739
Loss = 0.399185
Loss = 0.527496
Loss = 0.464874
Loss = 0.324371
Loss = 0.271194
Loss = 0.322998
Loss = 0.259003
Loss = 0.205383
Loss = 0.270523
Loss = 0.0643158
Loss = 0.218826
Loss = 0.629654
TEST LOSS = 0.366438
TEST ACC = 531.569 % (8966/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.409729
Epoch 8.2: Loss = 0.409378
Epoch 8.3: Loss = 0.320282
Epoch 8.4: Loss = 0.406143
Epoch 8.5: Loss = 0.307999
Epoch 8.6: Loss = 0.337997
Epoch 8.7: Loss = 0.361404
Epoch 8.8: Loss = 0.447754
Epoch 8.9: Loss = 0.40155
Epoch 8.10: Loss = 0.357803
Epoch 8.11: Loss = 0.358597
Epoch 8.12: Loss = 0.454376
Epoch 8.13: Loss = 0.313095
Epoch 8.14: Loss = 0.385544
Epoch 8.15: Loss = 0.376144
Epoch 8.16: Loss = 0.38562
Epoch 8.17: Loss = 0.477417
Epoch 8.18: Loss = 0.401321
Epoch 8.19: Loss = 0.396667
Epoch 8.20: Loss = 0.320114
Epoch 8.21: Loss = 0.435837
Epoch 8.22: Loss = 0.356018
Epoch 8.23: Loss = 0.398468
Epoch 8.24: Loss = 0.347458
Epoch 8.25: Loss = 0.381134
Epoch 8.26: Loss = 0.418015
Epoch 8.27: Loss = 0.361801
Epoch 8.28: Loss = 0.398346
Epoch 8.29: Loss = 0.329865
Epoch 8.30: Loss = 0.383255
Epoch 8.31: Loss = 0.378906
Epoch 8.32: Loss = 0.335526
Epoch 8.33: Loss = 0.311737
Epoch 8.34: Loss = 0.359879
Epoch 8.35: Loss = 0.358337
Epoch 8.36: Loss = 0.339264
Epoch 8.37: Loss = 0.383972
Epoch 8.38: Loss = 0.422073
Epoch 8.39: Loss = 0.369858
Epoch 8.40: Loss = 0.399719
Epoch 8.41: Loss = 0.426514
Epoch 8.42: Loss = 0.35936
Epoch 8.43: Loss = 0.373016
Epoch 8.44: Loss = 0.388718
Epoch 8.45: Loss = 0.50325
Epoch 8.46: Loss = 0.470779
Epoch 8.47: Loss = 0.409912
Epoch 8.48: Loss = 0.375473
Epoch 8.49: Loss = 0.411682
Epoch 8.50: Loss = 0.389893
Epoch 8.51: Loss = 0.402054
Epoch 8.52: Loss = 0.327667
Epoch 8.53: Loss = 0.407486
Epoch 8.54: Loss = 0.381424
Epoch 8.55: Loss = 0.495239
Epoch 8.56: Loss = 0.351944
Epoch 8.57: Loss = 0.324661
Epoch 8.58: Loss = 0.441437
Epoch 8.59: Loss = 0.282654
Epoch 8.60: Loss = 0.415268
Epoch 8.61: Loss = 0.324234
Epoch 8.62: Loss = 0.492065
Epoch 8.63: Loss = 0.407867
Epoch 8.64: Loss = 0.378769
Epoch 8.65: Loss = 0.325104
Epoch 8.66: Loss = 0.39563
Epoch 8.67: Loss = 0.376801
Epoch 8.68: Loss = 0.398392
Epoch 8.69: Loss = 0.362549
Epoch 8.70: Loss = 0.408264
Epoch 8.71: Loss = 0.399353
Epoch 8.72: Loss = 0.42659
Epoch 8.73: Loss = 0.29277
Epoch 8.74: Loss = 0.430115
Epoch 8.75: Loss = 0.41037
Epoch 8.76: Loss = 0.376892
Epoch 8.77: Loss = 0.385239
Epoch 8.78: Loss = 0.406738
Epoch 8.79: Loss = 0.350189
Epoch 8.80: Loss = 0.393372
Epoch 8.81: Loss = 0.425781
Epoch 8.82: Loss = 0.408157
Epoch 8.83: Loss = 0.395218
Epoch 8.84: Loss = 0.355515
Epoch 8.85: Loss = 0.427017
Epoch 8.86: Loss = 0.42662
Epoch 8.87: Loss = 0.438828
Epoch 8.88: Loss = 0.414108
Epoch 8.89: Loss = 0.404388
Epoch 8.90: Loss = 0.368515
Epoch 8.91: Loss = 0.466537
Epoch 8.92: Loss = 0.378296
Epoch 8.93: Loss = 0.475266
Epoch 8.94: Loss = 0.334259
Epoch 8.95: Loss = 0.465988
Epoch 8.96: Loss = 0.428177
Epoch 8.97: Loss = 0.383209
Epoch 8.98: Loss = 0.43631
Epoch 8.99: Loss = 0.443222
Epoch 8.100: Loss = 0.379044
TRAIN LOSS = 0.39035
TRAIN ACC = 88.7512 % (53254/60000)
Loss = 0.3983
Loss = 0.429337
Loss = 0.595123
Loss = 0.535324
Loss = 0.384033
Loss = 0.390839
Loss = 0.536423
Loss = 0.454056
Loss = 0.31485
Loss = 0.258408
Loss = 0.326172
Loss = 0.249756
Loss = 0.19455
Loss = 0.291779
Loss = 0.0670624
Loss = 0.218063
Loss = 0.617569
TEST LOSS = 0.363347
TEST ACC = 532.539 % (8972/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.388809
Epoch 9.2: Loss = 0.348953
Epoch 9.3: Loss = 0.480652
Epoch 9.4: Loss = 0.4133
Epoch 9.5: Loss = 0.44194
Epoch 9.6: Loss = 0.38591
Epoch 9.7: Loss = 0.370407
Epoch 9.8: Loss = 0.410812
Epoch 9.9: Loss = 0.405228
Epoch 9.10: Loss = 0.43898
Epoch 9.11: Loss = 0.347351
Epoch 9.12: Loss = 0.406555
Epoch 9.13: Loss = 0.405533
Epoch 9.14: Loss = 0.314301
Epoch 9.15: Loss = 0.446396
Epoch 9.16: Loss = 0.452744
Epoch 9.17: Loss = 0.356995
Epoch 9.18: Loss = 0.37854
Epoch 9.19: Loss = 0.351837
Epoch 9.20: Loss = 0.426926
Epoch 9.21: Loss = 0.310623
Epoch 9.22: Loss = 0.391174
Epoch 9.23: Loss = 0.38298
Epoch 9.24: Loss = 0.401627
Epoch 9.25: Loss = 0.423157
Epoch 9.26: Loss = 0.370575
Epoch 9.27: Loss = 0.393234
Epoch 9.28: Loss = 0.375885
Epoch 9.29: Loss = 0.311295
Epoch 9.30: Loss = 0.367355
Epoch 9.31: Loss = 0.405777
Epoch 9.32: Loss = 0.482025
Epoch 9.33: Loss = 0.313812
Epoch 9.34: Loss = 0.373474
Epoch 9.35: Loss = 0.423782
Epoch 9.36: Loss = 0.3591
Epoch 9.37: Loss = 0.337311
Epoch 9.38: Loss = 0.378265
Epoch 9.39: Loss = 0.369675
Epoch 9.40: Loss = 0.465286
Epoch 9.41: Loss = 0.469666
Epoch 9.42: Loss = 0.332413
Epoch 9.43: Loss = 0.476166
Epoch 9.44: Loss = 0.376984
Epoch 9.45: Loss = 0.348618
Epoch 9.46: Loss = 0.400284
Epoch 9.47: Loss = 0.414215
Epoch 9.48: Loss = 0.369034
Epoch 9.49: Loss = 0.461212
Epoch 9.50: Loss = 0.354111
Epoch 9.51: Loss = 0.440994
Epoch 9.52: Loss = 0.393494
Epoch 9.53: Loss = 0.457886
Epoch 9.54: Loss = 0.396667
Epoch 9.55: Loss = 0.377396
Epoch 9.56: Loss = 0.443985
Epoch 9.57: Loss = 0.37001
Epoch 9.58: Loss = 0.354843
Epoch 9.59: Loss = 0.276382
Epoch 9.60: Loss = 0.338913
Epoch 9.61: Loss = 0.401123
Epoch 9.62: Loss = 0.34111
Epoch 9.63: Loss = 0.42366
Epoch 9.64: Loss = 0.416397
Epoch 9.65: Loss = 0.405396
Epoch 9.66: Loss = 0.328888
Epoch 9.67: Loss = 0.34436
Epoch 9.68: Loss = 0.387177
Epoch 9.69: Loss = 0.391815
Epoch 9.70: Loss = 0.394623
Epoch 9.71: Loss = 0.421143
Epoch 9.72: Loss = 0.42337
Epoch 9.73: Loss = 0.297256
Epoch 9.74: Loss = 0.326523
Epoch 9.75: Loss = 0.366928
Epoch 9.76: Loss = 0.405014
Epoch 9.77: Loss = 0.335983
Epoch 9.78: Loss = 0.393845
Epoch 9.79: Loss = 0.297104
Epoch 9.80: Loss = 0.37587
Epoch 9.81: Loss = 0.377045
Epoch 9.82: Loss = 0.425644
Epoch 9.83: Loss = 0.410736
Epoch 9.84: Loss = 0.316818
Epoch 9.85: Loss = 0.406479
Epoch 9.86: Loss = 0.386887
Epoch 9.87: Loss = 0.314697
Epoch 9.88: Loss = 0.362656
Epoch 9.89: Loss = 0.40506
Epoch 9.90: Loss = 0.392334
Epoch 9.91: Loss = 0.36731
Epoch 9.92: Loss = 0.364594
Epoch 9.93: Loss = 0.405228
Epoch 9.94: Loss = 0.465714
Epoch 9.95: Loss = 0.365677
Epoch 9.96: Loss = 0.301102
Epoch 9.97: Loss = 0.388412
Epoch 9.98: Loss = 0.386276
Epoch 9.99: Loss = 0.494263
Epoch 9.100: Loss = 0.39032
TRAIN LOSS = 0.386688
TRAIN ACC = 88.9572 % (53377/60000)
Loss = 0.381561
Loss = 0.40834
Loss = 0.589035
Loss = 0.516617
Loss = 0.370926
Loss = 0.372559
Loss = 0.528442
Loss = 0.443039
Loss = 0.306259
Loss = 0.266861
Loss = 0.311188
Loss = 0.246475
Loss = 0.190552
Loss = 0.291534
Loss = 0.0639191
Loss = 0.22348
Loss = 0.592255
TEST LOSS = 0.354337
TEST ACC = 533.769 % (8996/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.42984
Epoch 10.2: Loss = 0.429626
Epoch 10.3: Loss = 0.418152
Epoch 10.4: Loss = 0.406891
Epoch 10.5: Loss = 0.34465
Epoch 10.6: Loss = 0.313782
Epoch 10.7: Loss = 0.330429
Epoch 10.8: Loss = 0.48172
Epoch 10.9: Loss = 0.396088
Epoch 10.10: Loss = 0.401871
Epoch 10.11: Loss = 0.384521
Epoch 10.12: Loss = 0.309372
Epoch 10.13: Loss = 0.393845
Epoch 10.14: Loss = 0.358536
Epoch 10.15: Loss = 0.348343
Epoch 10.16: Loss = 0.395691
Epoch 10.17: Loss = 0.335068
Epoch 10.18: Loss = 0.370148
Epoch 10.19: Loss = 0.33728
Epoch 10.20: Loss = 0.404541
Epoch 10.21: Loss = 0.37326
Epoch 10.22: Loss = 0.397583
Epoch 10.23: Loss = 0.419693
Epoch 10.24: Loss = 0.392166
Epoch 10.25: Loss = 0.35376
Epoch 10.26: Loss = 0.320068
Epoch 10.27: Loss = 0.281097
Epoch 10.28: Loss = 0.324295
Epoch 10.29: Loss = 0.443665
Epoch 10.30: Loss = 0.327408
Epoch 10.31: Loss = 0.415314
Epoch 10.32: Loss = 0.363602
Epoch 10.33: Loss = 0.350784
Epoch 10.34: Loss = 0.409622
Epoch 10.35: Loss = 0.376587
Epoch 10.36: Loss = 0.444839
Epoch 10.37: Loss = 0.400513
Epoch 10.38: Loss = 0.277466
Epoch 10.39: Loss = 0.342392
Epoch 10.40: Loss = 0.356918
Epoch 10.41: Loss = 0.382263
Epoch 10.42: Loss = 0.392227
Epoch 10.43: Loss = 0.333969
Epoch 10.44: Loss = 0.392563
Epoch 10.45: Loss = 0.350143
Epoch 10.46: Loss = 0.365982
Epoch 10.47: Loss = 0.345459
Epoch 10.48: Loss = 0.396927
Epoch 10.49: Loss = 0.438477
Epoch 10.50: Loss = 0.431335
Epoch 10.51: Loss = 0.355301
Epoch 10.52: Loss = 0.335251
Epoch 10.53: Loss = 0.394073
Epoch 10.54: Loss = 0.46904
Epoch 10.55: Loss = 0.401535
Epoch 10.56: Loss = 0.39711
Epoch 10.57: Loss = 0.399338
Epoch 10.58: Loss = 0.42923
Epoch 10.59: Loss = 0.363968
Epoch 10.60: Loss = 0.357254
Epoch 10.61: Loss = 0.374619
Epoch 10.62: Loss = 0.254074
Epoch 10.63: Loss = 0.399689
Epoch 10.64: Loss = 0.356506
Epoch 10.65: Loss = 0.317551
Epoch 10.66: Loss = 0.362152
Epoch 10.67: Loss = 0.310516
Epoch 10.68: Loss = 0.378601
Epoch 10.69: Loss = 0.306091
Epoch 10.70: Loss = 0.386322
Epoch 10.71: Loss = 0.465897
Epoch 10.72: Loss = 0.441986
Epoch 10.73: Loss = 0.498871
Epoch 10.74: Loss = 0.448639
Epoch 10.75: Loss = 0.410004
Epoch 10.76: Loss = 0.42038
Epoch 10.77: Loss = 0.434387
Epoch 10.78: Loss = 0.450104
Epoch 10.79: Loss = 0.410828
Epoch 10.80: Loss = 0.331726
Epoch 10.81: Loss = 0.352005
Epoch 10.82: Loss = 0.413452
Epoch 10.83: Loss = 0.324524
Epoch 10.84: Loss = 0.35907
Epoch 10.85: Loss = 0.396027
Epoch 10.86: Loss = 0.333908
Epoch 10.87: Loss = 0.437973
Epoch 10.88: Loss = 0.37059
Epoch 10.89: Loss = 0.449722
Epoch 10.90: Loss = 0.352844
Epoch 10.91: Loss = 0.39624
Epoch 10.92: Loss = 0.378937
Epoch 10.93: Loss = 0.459518
Epoch 10.94: Loss = 0.465118
Epoch 10.95: Loss = 0.359756
Epoch 10.96: Loss = 0.340973
Epoch 10.97: Loss = 0.443207
Epoch 10.98: Loss = 0.2612
Epoch 10.99: Loss = 0.399429
Epoch 10.100: Loss = 0.351105
TRAIN LOSS = 0.380981
TRAIN ACC = 89.2456 % (53549/60000)
Loss = 0.376648
Loss = 0.399155
Loss = 0.584167
Loss = 0.512665
Loss = 0.370224
Loss = 0.370712
Loss = 0.522919
Loss = 0.441788
Loss = 0.313477
Loss = 0.260803
Loss = 0.319962
Loss = 0.255051
Loss = 0.187744
Loss = 0.28952
Loss = 0.0638885
Loss = 0.225098
Loss = 0.593552
TEST LOSS = 0.353371
TEST ACC = 535.489 % (9000/10000)
The following benchmarks are including preprocessing (offline phase).
Time = 25160.6 seconds 
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
